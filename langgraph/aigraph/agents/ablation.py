import logging
import operator
from pathlib import Path
from typing import Annotated, Any, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.graph import START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from langgraph.types import Checkpointer
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import ablation_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    """State for ablation studies stage.

    Attributes:
        cwd: Directory for saving ablation.py and outputs.
             Passed to utils.exec_code() to run ablation script.
        task: Context for ablation study design.
              Included in all LLM prompts.
        code: Tuned code to modify for ablations.
              Input to build_prompt_code_ablation() as base.
        idea: Guides which components to ablate.
              Fed to prompts for hypothesis-relevant ablations.
        research: Background for LLM decisions.
                  Included in prompts for informed ablation design.
        ablations: List of already-run ablations.
                   Passed to propose prompt to avoid duplicates.
        last_ablation: Current ablation being tested.
                       Used in code prompt to specify what to disable/modify.
        ablation_retry_count: Tracks retry attempts for ablation code.
                              Compared against max (5) to prevent infinite loops.
        ablation_code: Generated ablation code with component modifications.
                       Written to ablation.py and executed.
        ablation_plan: LLM-generated plan for ablation approach.
                       Used for debugging.
        ablation_deps: Python dependencies for ablation code.
                       Installed before execution.
        ablation_returncode: Exit code from ablation execution.
                              Non-zero indicates failure.
        ablation_stdout: Standard output from ablation execution.
                          Used to detect bugs and extract results.
        ablation_stderr: Standard error from ablation execution.
                          Used to detect bugs.
        ablation_filename: Filename where ablation code was saved.
                            Used for debugging.
        ablation_is_bug: Whether ablation output indicates a bug.
                          True triggers re-generation with error context.
        ablation_summary: LLM-generated summary of ablation output.
                          Included in memory for retry attempts.
        parser_retry_count: Tracks retry attempts for parser code.
                            Compared against max (5) to prevent infinite loops.
        parser_plan: LLM-generated plan for parser code.
                     Used for debugging.
        parser_code: Generated parser code to extract ablation metrics.
                      Written to ablation_parser.py and executed.
        parser_deps: Python dependencies for parser code.
                      Installed before execution.
        parser_stdout: Standard output from parser execution.
                        Contains extracted metrics, passed to writeup for results.
        parser_stderr: Standard error from parser execution.
                       Used to detect bugs.
        parser_returncode: Exit code from parser execution.
                           Non-zero indicates failure.
        parser_filename: Filename where parser code was saved.
                          Used for debugging.
        parse_is_bug: Whether parser output indicates a bug.
                       True triggers re-generation with error context.
        parse_summary: LLM-generated summary of parser output.
                       Included in memory for retry attempts.
        notes: Full experiment history for writeup.
               Extended with new note, final list used in judge prompt.
    """

    # inputs
    cwd: Path
    task: utils.Task
    code: str
    idea: utils.Idea
    research: str

    ablations: list[utils.Ablation] = []
    last_ablation: utils.Ablation | None = None

    # counts how many times we tried to code the ablation
    ablation_retry_count: int = 0

    # generated by `node_code_ablation`
    ablation_code: str | None = None
    ablation_plan: str | None = None
    ablation_deps: list[str] = []

    # generated by `node_exec_ablation`
    ablation_returncode: int | None = None
    ablation_stdout: str | None = None
    ablation_stderr: str | None = None
    ablation_filename: str | None = None

    # generated by `node_parse_ablation_output`
    ablation_is_bug: bool | None = None
    ablation_summary: str | None = None

    # counts how many times we tried to code the parser
    parser_retry_count: int = 0

    # generated by `node_code_metrics_parser`
    parser_plan: str | None = None
    parser_code: str | None = None
    parser_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parser_stdout: str | None = None
    parser_stderr: str | None = None
    parser_returncode: int | None = None
    parser_filename: str | None = None

    # generated by `node_parse_metrics_output`
    parse_is_bug: bool | None = None
    parse_summary: str | None = None

    # notes accumulated from experiment output parsing
    notes: Annotated[list[str], operator.add] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_ablation_propose_ablation(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_ablation_propose_ablation")

    class Schema(BaseModel):
        name: str
        description: str

    prompt = prompts.build_prompt_propose_ablation(
        code=state.code,
        ablations=[i.name for i in state.ablations],
        idea=state.idea,
        research=state.research,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    ablation = utils.Ablation(
        name=response.name,
        description=response.description,
    )

    logger.debug(f"ablation_name: {ablation.name}")
    logger.debug(f"ablation_description: {ablation.description[:32]!r}")
    logger.info("Finished node_ablation_propose_ablation")
    return {
        "last_ablation": ablation,
        "ablations": [ablation],
    }


async def node_ablation_code_ablation(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_ablation_code_ablation")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    memory = ""

    if state.ablation_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.ablation_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"<CODE>\n{state.ablation_code or 'NA'}\n</CODE>\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"<DEPENDENCIES>\n{state.ablation_deps or 'NA'}\n</DEPENDENCIES>\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"<STDOUT>\n{state.ablation_stdout or 'NA'}\n</STDOUT>\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"<STDERR>\n{state.ablation_stderr or 'NA'}\n</STDERR>\n\n"

    assert state.last_ablation, "last_ablation is required"

    prompt = prompts.build_prompt_code_ablation(
        task=state.task,
        name=state.last_ablation.name,
        description=state.last_ablation.description,
        code=state.code,
        memory=memory,
        idea=state.idea,
        research=state.research,
        notes=state.notes,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"ablation_plan: {response.plan[:32]!r}")
    logger.debug(f"ablation_code: {response.code[:32]!r}")
    logger.debug(f"ablation_deps: {response.dependencies}")
    logger.debug(f"ablation_retry_count: {state.ablation_retry_count + 1}")

    logger.info("Finished node_ablation_code_ablation")
    return {
        "ablation_code": response.code,
        "ablation_plan": response.plan,
        "ablation_deps": response.dependencies,
        "ablation_retry_count": state.ablation_retry_count + 1,
    }


async def node_ablation_exec_ablation(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_ablation_exec_ablation")
    assert state.ablation_code, "ablation_code is required"

    result = await utils.exec_code(
        state.cwd, "ablation.py", state.ablation_code, state.ablation_deps
    )

    logger.debug(f"ablation_stdout: {result.stdout[:32]!r}")
    logger.debug(f"ablation_stderr: {result.stderr[:32]!r}")
    logger.debug(f"ablation_returncode: {result.returncode}")
    logger.debug(f"ablation_filename: {result.filename}")

    logger.info("Finished node_ablation_exec_ablation")
    return {
        "ablation_stdout": result.stdout,
        "ablation_stderr": result.stderr,
        "ablation_returncode": result.returncode,
        "ablation_filename": result.filename,
    }


async def node_ablation_parse_ablation_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_ablation_parse_ablation_output")
    assert state.ablation_code, "ablation_code is required"

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_ablation_output(
        state.task,
        state.ablation_code,
        state.ablation_stdout or "",
        state.ablation_stderr or "",
        state.idea,
        notes=state.notes,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"ablation_is_bug: {response.is_bug}")
    logger.debug(f"ablation_summary: {response.summary[:32]!r}")

    # Create note summarizing ablation results
    note = await _create_notes(state, runtime)

    logger.info(
        f"Finished node_ablation_parse_ablation_output. Is bug: {response.is_bug}"
    )
    return {
        "ablation_is_bug": response.is_bug,
        "ablation_summary": response.summary,
        "notes": [note],
    }


async def node_ablation_should_retry_code_from_ablation_output(
    state: State, runtime: Runtime[Context]
) -> Literal[
    "node_ablation_code_ablation", "node_ablation_code_metrics_parser", "__end__"
]:
    logger.info("Starting node_ablation_should_retry_code_from_ablation_output")

    if state.ablation_retry_count > 5:
        logger.info("Max retry count reached, going to `__end__`")
        return "__end__"

    if state.ablation_is_bug:
        logger.info("Going to `node_ablation_code_ablation`")
        return "node_ablation_code_ablation"

    logger.info("Going to `node_ablation_code_metrics_parser`")
    return "node_ablation_code_metrics_parser"


async def node_ablation_code_metrics_parser(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_ablation_code_metrics_parser")
    assert state.ablation_code, "ablation_code is required"

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    memory = ""
    if state.parse_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.parse_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"<CODE>\n{state.parser_code or 'NA'}\n</CODE>\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"<DEPENDENCIES>\n{state.parser_deps or 'NA'}\n</DEPENDENCIES>\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"<STDOUT>\n{state.parser_stdout or 'NA'}\n</STDOUT>\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"<STDERR>\n{state.parser_stderr or 'NA'}\n</STDERR>\n\n"

    prompt = prompts.build_prompt_ablation_parser_code(
        state.ablation_code, memory=memory
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"parser_code: {response.code[:32]!r}")
    logger.debug(f"parser_plan: {response.plan[:32]!r}")
    logger.debug(f"parser_deps: {response.dependencies}")
    logger.debug(f"parser_retry_count: {state.parser_retry_count + 1}")

    logger.info("Finished node_ablation_code_metrics_parser")
    return {
        "parser_code": response.code,
        "parser_plan": response.plan,
        "parser_deps": response.dependencies,
        "parser_retry_count": state.parser_retry_count + 1,
    }


async def node_ablation_exec_metrics_parser(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_ablation_exec_metrics_parser")
    assert state.parser_code, "parser_code is required"

    result = await utils.exec_code(
        state.cwd,
        "ablation_parser.py",
        state.parser_code,
        state.parser_deps,
    )

    logger.debug(f"parser_stdout: {result.stdout[:32]!r}")
    logger.debug(f"parser_stderr: {result.stderr[:32]!r}")
    logger.debug(f"parser_returncode: {result.returncode}")
    logger.debug(f"parser_filename: {result.filename}")

    logger.info("Finished node_ablation_exec_metrics_parser")
    return {
        "parser_stdout": result.stdout,
        "parser_stderr": result.stderr,
        "parser_returncode": result.returncode,
        "parser_filename": result.filename,
    }


async def node_ablation_parse_metrics_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_ablation_parse_metrics_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_ablation_parser_output(
        state.parser_code or "",
        state.parser_stdout or "",
        state.parser_stderr or "",
        state.idea,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"parse_is_bug: {response.is_bug}")
    logger.debug(f"parse_summary: {response.summary[:32]!r}")

    logger.info("Finished node_ablation_parse_metrics_output")
    return {
        "parse_is_bug": response.is_bug,
        "parse_summary": response.summary,
    }


async def node_ablation_should_retry_parser_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_ablation_code_metrics_parser", "__end__"]:
    logger.info("Starting node_ablation_should_retry_parser_from_output")

    if state.parser_retry_count > 5:
        logger.info("Max retry count reached, going to `__end__`")
        return "__end__"

    if state.parse_is_bug is True:
        logger.info("Going to `node_ablation_code_metrics_parser`")
        return "node_ablation_code_metrics_parser"

    logger.info("Going to `__end__`")
    return "__end__"


async def _create_notes(state: State, runtime: Runtime[Context]) -> str:
    logger.info("Creating notes for ablation experiment")

    class Schema(BaseModel):
        note: str

    prompt = prompts.build_prompt_create_notes(state)
    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"note: {response.note[:64]!r}")
    return response.note


def build(
    checkpointer: Checkpointer | None = None,
) -> CompiledStateGraph[State, Context, State, State]:
    """Build the Stage 4 ablation studies graph."""
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node(
        "node_ablation_propose_ablation",
        node_ablation_propose_ablation,
    )
    builder.add_node(
        "node_ablation_code_ablation",
        node_ablation_code_ablation,
    )
    builder.add_node(
        "node_ablation_exec_ablation",
        node_ablation_exec_ablation,
    )
    builder.add_node(
        "node_ablation_parse_ablation_output",
        node_ablation_parse_ablation_output,
    )
    builder.add_node(
        "node_ablation_code_metrics_parser",
        node_ablation_code_metrics_parser,
    )
    builder.add_node(
        "node_ablation_exec_metrics_parser",
        node_ablation_exec_metrics_parser,
    )
    builder.add_node(
        "node_ablation_parse_metrics_output",
        node_ablation_parse_metrics_output,
    )

    # Add edges
    builder.add_edge(
        START,
        "node_ablation_propose_ablation",
    )
    builder.add_edge(
        "node_ablation_propose_ablation",
        "node_ablation_code_ablation",
    )
    builder.add_edge(
        "node_ablation_code_ablation",
        "node_ablation_exec_ablation",
    )
    builder.add_edge(
        "node_ablation_exec_ablation",
        "node_ablation_parse_ablation_output",
    )
    builder.add_conditional_edges(
        "node_ablation_parse_ablation_output",
        node_ablation_should_retry_code_from_ablation_output,
        ["node_ablation_code_ablation", "node_ablation_code_metrics_parser", "__end__"],
    )
    builder.add_edge(
        "node_ablation_code_metrics_parser",
        "node_ablation_exec_metrics_parser",
    )
    builder.add_edge(
        "node_ablation_exec_metrics_parser",
        "node_ablation_parse_metrics_output",
    )
    builder.add_conditional_edges(
        "node_ablation_parse_metrics_output",
        node_ablation_should_retry_parser_from_output,
        ["node_ablation_code_metrics_parser", "__end__"],
    )

    return builder.compile(name="graph_ablation", checkpointer=checkpointer)  # type: ignore
