import base64
import logging
import operator
from pathlib import Path
from typing import Annotated, Any, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.errors import GraphRecursionError
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from langgraph.types import Checkpointer, Send
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import plotting_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    """State for plotting and visualization stage.

    Attributes:
        cwd: Directory to save PNG plots.
             glob("*.png") finds generated plots for analysis.
        task: Context for plot design.
              Included in code and analysis prompts.
        code: Experiment code to understand data format.
              Input to prompt so LLM knows what data files exist.
        idea: Guides what visualizations are relevant.
              Fed to prompts for hypothesis-relevant plots.
        research: Background for plot interpretation.
                  Included in prompts for context-aware visualization.
        plots: Output list with path + LLM analysis.
               Each PNG sent to node_plotting_analyze_single_plot with base64 image.
        plotting_retry_count: Tracks retry attempts for plotting code.
                              Compared against max (5) to prevent infinite loops.
        plotting_plan: LLM-generated plan for plotting approach.
                       Used for debugging.
        plotting_code: Generated plotting code to create visualizations.
                       Written to plotting.py and executed.
        plotting_deps: Python dependencies for plotting code.
                        Installed before execution.
        plotting_stdout: Standard output from plotting execution.
                         Used to detect bugs.
        plotting_stderr: Standard error from plotting execution.
                          Used to detect bugs.
        plotting_returncode: Exit code from plotting execution.
                              Non-zero indicates failure.
        plotting_filename: Filename where plotting code was saved.
                            Used for debugging.
        plotting_is_bug: Whether plotting output indicates a bug.
                          True triggers re-generation.
        plotting_summary: LLM-generated summary of plotting output.
                          Included in memory for retry attempts.
    """

    # inputs
    cwd: Path
    task: utils.Task
    code: str  # The experiment code that generated the data
    idea: utils.Idea
    research: str

    plots: Annotated[list[utils.Plot], operator.add] = []

    # counts how many times we tried to code the plotting
    plotting_retry_count: int = 0

    # generated by `node_code_plotting`
    plotting_plan: str | None = None
    plotting_code: str | None = None
    plotting_deps: list[str] = []

    # generated by `node_exec_plotting`
    plotting_stdout: str | None = None
    plotting_stderr: str | None = None
    plotting_returncode: int | None = None
    plotting_filename: str | None = None

    # generated by `node_parse_plotting_output`
    plotting_is_bug: bool | None = None
    plotting_summary: str | None = None


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_plotting_code_plotting(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_plotting_code_plotting")

    class Schema(BaseModel):
        plan: str
        code: str
        dependencies: list[str]

    if state.plotting_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""

    if state.plotting_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.plotting_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"<CODE>\n{state.plotting_code or 'NA'}\n</CODE>\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"<DEPENDENCIES>\n{state.plotting_deps or 'NA'}\n</DEPENDENCIES>\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"<STDOUT>\n{state.plotting_stdout or 'NA'}\n</STDOUT>\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"<STDERR>\n{state.plotting_stderr or 'NA'}\n</STDERR>\n\n"

    prompt = prompts.build_prompt_plotting_code(
        state.task,
        state.code,
        memory,
        state.idea,
        state.research,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"plotting_plan: {response.plan[:32]!r}")
    logger.debug(f"plotting_code: {response.code[:32]!r}")
    logger.debug(f"plotting_deps: {response.dependencies}")
    logger.debug(f"plotting_retry_count: {state.plotting_retry_count + 1}")

    logger.info("Finished node_plotting_code_plotting")
    return {
        "plotting_plan": response.plan,
        "plotting_code": response.code,
        "plotting_deps": response.dependencies,
        "plotting_retry_count": state.plotting_retry_count + 1,
    }


async def node_plotting_exec_plotting(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_plotting_exec_plotting")

    response = await utils.exec_code(
        state.cwd,
        "plotting.py",
        state.plotting_code or "NA",
        state.plotting_deps or [],
    )

    logger.debug(f"plotting_stdout: {response.stdout[:32]!r}")
    logger.debug(f"plotting_stderr: {response.stderr[:32]!r}")
    logger.debug(f"plotting_returncode: {response.returncode}")
    logger.debug(f"plotting_filename: {response.filename}")

    logger.info("Finished node_plotting_exec_plotting")
    return {
        "plotting_stdout": response.stdout,
        "plotting_stderr": response.stderr,
        "plotting_returncode": response.returncode,
        "plotting_filename": response.filename,
    }


async def node_plotting_parse_plotting_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_plotting_parse_plotting_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_plotting_output(
        state.task,
        state.plotting_code or "",
        state.plotting_stdout or "",
        state.plotting_stderr or "",
        state.idea,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"plotting_is_bug: {response.is_bug}")
    logger.debug(f"plotting_summary: {response.summary[:32]!r}")

    logger.info("Finished node_plotting_parse_plotting_output")
    return {
        "plotting_is_bug": response.is_bug,
        "plotting_summary": response.summary,
    }


class StateSinglePlot(BaseModel):
    task: utils.Task
    idea: utils.Idea
    image: Path


async def node_plotting_should_retry_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_plotting_code_plotting"] | list[Send]:
    logger.info("Starting node_plotting_should_retry_from_output")

    if state.plotting_is_bug is True:
        logger.info("Going to `node_plotting_code_plotting`")
        return "node_plotting_code_plotting"

    logger.info("Preparing analysis sends")
    pngs = sorted(list(state.cwd.glob("*.png")))
    for png in pngs:
        logger.debug(f"Found PNG file: {png}")

    sends: list[Send] = []
    for png in pngs:
        st = StateSinglePlot(task=state.task, idea=state.idea, image=png)
        sends.append(Send("node_plotting_analyze_single_plot", st))

    return sends


async def node_plotting_analyze_single_plot(
    state: StateSinglePlot, runtime: Runtime[Context]
) -> dict:
    logger.info(f"Starting node_plotting_analyze_single_plot for {state.image}")

    class Schema(BaseModel):
        analysis: str

    prompt = prompts.build_prompt_analyze_plots(state.task, state.idea)

    with open(state.image, "rb") as f:
        data = f.read()
        data = base64.b64encode(data)

    messages = [
        {
            "role": "system",
            "content": prompt,
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": f"Analyze this plot: {state.image.name}",
                },
                {
                    "type": "image",
                    "base64": data.decode("utf-8"),
                    "mime_type": "image/png",
                },
            ],
        },
    ]

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(messages)  # type: ignore

    logger.debug(f"image: {state.image}")
    logger.debug(f"analysis: {response.analysis[:32]!r}")

    logger.info(f"Finished node_plotting_analyze_single_plot for {state.image}")

    plot = utils.Plot(path=state.image, analysis=response.analysis)
    return {"plots": [plot]}


def build(
    checkpointer: Checkpointer | None = None,
) -> CompiledStateGraph[State, Context]:
    builder = StateGraph(State, Context)

    # Add nodes
    builder.add_node(
        "node_plotting_code_plotting",
        node_plotting_code_plotting,
    )
    builder.add_node(
        "node_plotting_exec_plotting",
        node_plotting_exec_plotting,
    )
    builder.add_node(
        "node_plotting_parse_plotting_output",
        node_plotting_parse_plotting_output,
    )
    builder.add_node(
        "node_plotting_analyze_single_plot",
        node_plotting_analyze_single_plot,
    )

    # Add edges
    builder.add_edge(
        START,
        "node_plotting_code_plotting",
    )
    builder.add_edge(
        "node_plotting_code_plotting",
        "node_plotting_exec_plotting",
    )
    builder.add_edge(
        "node_plotting_exec_plotting",
        "node_plotting_parse_plotting_output",
    )
    builder.add_conditional_edges(
        "node_plotting_parse_plotting_output",
        node_plotting_should_retry_from_output,
        ["node_plotting_code_plotting", "node_plotting_analyze_single_plot"],
    )
    builder.add_edge(
        "node_plotting_analyze_single_plot",
        END,
    )

    return builder.compile(name="graph_plotting", checkpointer=checkpointer)  # type: ignore
