import logging
import operator
from pathlib import Path
from typing import Annotated, Any, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.graph import START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from langgraph.types import Checkpointer
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import tuning_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    """State for hyperparameter tuning stage.

    Attributes:
        cwd: Directory for saving tuning.py and outputs.
             Passed to utils.exec_code() to run tuning script.
        task: Context for hyperparameter selection.
              Included in propose and code prompts.
        code: Baseline code to modify with tuning.
              Input to build_prompt_tuning_code() as base to modify.
        idea: Guides which aspects to tune.
              Fed to prompts for idea-relevant hyperparam selection.
        research: Background for LLM decisions.
                  Included in prompts for informed tuning choices.
        hyperparams: List of already-tuned parameters.
                     Passed to propose prompt to avoid duplicates.
        last_hyperparam: Current hyperparam being tuned.
                         Used in code prompt to specify what to tune this iteration.
        tuning_retry_count: Tracks retry attempts for tuning code.
                            Compared against max (5) to prevent infinite loops.
        tuning_code: Generated tuning code with hyperparameter modifications.
                     Written to tuning.py and executed.
        tuning_plan: LLM-generated plan for tuning approach.
                     Used for debugging.
        tuning_deps: Python dependencies for tuning code.
                     Installed before execution.
        tuning_returncode: Exit code from tuning execution.
                           Non-zero indicates failure.
        tuning_stdout: Standard output from tuning execution.
                        Used to detect bugs and extract results.
        tuning_stderr: Standard error from tuning execution.
                       Used to detect bugs.
        tuning_filename: Filename where tuning code was saved.
                         Used for debugging.
        tuning_is_bug: Whether tuning output indicates a bug.
                        True triggers re-generation with previous stdout/stderr.
        tuning_summary: LLM-generated summary of tuning output.
                        Included in memory for retry attempts.
        parser_retry_count: Tracks retry attempts for parser code.
                            Compared against max (5) to prevent infinite loops.
        parser_plan: LLM-generated plan for parser code.
                     Used for debugging.
        parser_code: Generated parser code to extract tuning results.
                      Written to tuning_parser.py and executed.
        parser_deps: Python dependencies for parser code.
                      Installed before execution.
        parser_stdout: Standard output from parser execution.
                        Contains extracted metrics in structured format.
        parser_stderr: Standard error from parser execution.
                       Used to detect bugs.
        parser_returncode: Exit code from parser execution.
                           Non-zero indicates failure.
        parser_filename: Filename where parser code was saved.
                          Used for debugging.
        parse_is_bug: Whether parser output indicates a bug.
                       True triggers re-generation with error context.
        parse_summary: LLM-generated summary of parser output.
                       Included in memory for retry attempts.
        notes: Accumulated learnings from baseline + tuning.
               Extended with new note after successful parse.
    """

    # inputs
    cwd: Path
    task: utils.Task
    code: str
    idea: utils.Idea
    research: str

    hyperparams: list[utils.Hyperparam] = []
    last_hyperparam: utils.Hyperparam | None = None

    # counts how many times we tried to code the tuning
    tuning_retry_count: int = 0

    # generated by `node_code_tuning`
    tuning_code: str | None = None
    tuning_plan: str | None = None
    tuning_deps: list[str] = []

    # generated by `node_exec_tuning`
    tuning_returncode: int | None = None
    tuning_stdout: str | None = None
    tuning_stderr: str | None = None
    tuning_filename: str | None = None

    # generated by `node_parse_tuning_output`
    tuning_is_bug: bool | None = None
    tuning_summary: str | None = None

    # counts how many times we tried to code the parser
    parser_retry_count: int = 0

    # generated by `node_code_metrics_parser`
    parser_plan: str | None = None
    parser_code: str | None = None
    parser_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parser_stdout: str | None = None
    parser_stderr: str | None = None
    parser_returncode: int | None = None
    parser_filename: str | None = None

    # generated by `node_parse_metrics_output`
    parse_is_bug: bool | None = None
    parse_summary: str | None = None

    # notes accumulated from experiment output parsing
    notes: Annotated[list[str], operator.add] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0
    max_retries: int = 5

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_tuning_propose_hyperparam(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_propose_hyperparam")

    class Schema(BaseModel):
        name: str
        description: str

    prompt = prompts.build_prompt_tuning_propose(
        code=state.code,
        hyperparams=[i.name for i in state.hyperparams],
        idea=state.idea,
        research=state.research,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    hp = utils.Hyperparam(
        name=response.name,
        description=response.description,
    )

    logger.debug(f"hyperparam_name: {hp.name}")
    logger.debug(f"hyperparam_description: {hp.description[:32]!r}")
    logger.info("Finished node_tuning_propose_hyperparam")
    return {
        "last_hyperparam": hp,
        "hyperparams": [hp],
    }


async def node_tuning_code_tuning(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_code_tuning")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    memory = ""

    if state.tuning_returncode is not None and state.tuning_returncode > 0:
        memory += "Previous code:\n"
        memory += f"<CODE>\n{state.tuning_code or ''}\n</CODE>\n\n"
        memory += "Previous dependencies:\n"
        memory += f"<DEPENDENCIES>\n{state.tuning_deps or 'NA'}\n</DEPENDENCIES>\n\n"
        memory += "Stdout of executing the previous code:\n"
        memory += f"<STDOUT>\n{state.tuning_stdout or 'NA'}\n</STDOUT>\n\n"
        memory += "Stderr of executing the previous code:\n"
        memory += f"<STDERR>\n{state.tuning_stderr or 'NA'}\n</STDERR>\n\n"

    if state.tuning_is_bug is True:
        memory += "Bug identified:\n"
        memory += f"{state.tuning_summary or 'NA'}\n\n"

    assert state.last_hyperparam, "last_hyperparam is required"

    prompt = prompts.build_prompt_tuning_code(
        task=state.task,
        name=state.last_hyperparam.name,
        description=state.last_hyperparam.description,
        code=state.code,
        memory=memory,
        idea=state.idea,
        research=state.research,
        notes=state.notes,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"tuning_plan: {response.plan[:32]!r}")
    logger.debug(f"tuning_code: {response.code[:32]!r}")
    logger.debug(f"tuning_deps: {response.dependencies}")
    logger.debug(f"tuning_retry_count: {state.tuning_retry_count + 1}")

    logger.info("Finished node_tuning_code_tuning")
    return {
        "tuning_code": response.code,
        "tuning_plan": response.plan,
        "tuning_deps": response.dependencies,
        "tuning_retry_count": state.tuning_retry_count + 1,
    }


async def node_tuning_exec_tuning(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_exec_tuning")
    assert state.tuning_code, "tuning_code is required"

    result = await utils.exec_code(
        state.cwd, "tuning.py", state.tuning_code, state.tuning_deps
    )

    logger.debug(f"tuning_stdout: {result.stdout[:32]!r}")
    logger.debug(f"tuning_stderr: {result.stderr[:32]!r}")
    logger.debug(f"tuning_returncode: {result.returncode}")
    logger.debug(f"tuning_filename: {result.filename}")

    logger.info("Finished node_tuning_exec_tuning")
    return {
        "tuning_stdout": result.stdout,
        "tuning_stderr": result.stderr,
        "tuning_returncode": result.returncode,
        "tuning_filename": result.filename,
    }


async def node_tuning_parse_tuning_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_parse_tuning_output")
    assert state.tuning_code, "tuning_code is required"

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_tuning_code_output(
        state.task,
        state.tuning_code,
        state.tuning_stdout or "",
        state.tuning_stderr or "",
        state.idea,
        notes=state.notes,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"tuning_is_bug: {response.is_bug}")
    logger.debug(f"tuning_summary: {response.summary[:32]!r}")

    # Create note summarizing tuning results
    note = await _create_notes(state, runtime)

    logger.info(f"Finished node_tuning_parse_tuning_output. Is bug: {response.is_bug}")
    return {
        "tuning_is_bug": response.is_bug,
        "tuning_summary": response.summary,
        "notes": [note],
    }


async def node_tuning_should_retry_code_from_tuning_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_tuning_code_tuning", "node_tuning_code_metrics_parser", "__end__"]:
    logger.info("Starting node_tuning_should_retry_code_from_tuning_output")

    if state.tuning_retry_count > runtime.context.max_retries:
        logger.info("Max retry count reached, going to `__end__`")
        return "__end__"

    if state.tuning_is_bug:
        logger.info("Going to `node_tuning_code_tuning`")
        return "node_tuning_code_tuning"

    logger.info("Going to `node_tuning_code_metrics_parser`")
    return "node_tuning_code_metrics_parser"


async def node_tuning_code_metrics_parser(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_code_metrics_parser")
    assert state.tuning_code, "tuning_code is required"

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    memory = ""
    if state.parse_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.parse_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"<CODE>\n{state.parser_code or 'NA'}\n</CODE>\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"<DEPENDENCIES>\n{state.parser_deps or 'NA'}\n</DEPENDENCIES>\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"<STDOUT>\n{state.parser_stdout or 'NA'}\n</STDOUT>\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"<STDERR>\n{state.parser_stderr or 'NA'}\n</STDERR>\n\n"

    prompt = prompts.build_prompt_tuning_parser_code(state.tuning_code, memory=memory)

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"parser_code: {response.code[:32]!r}")
    logger.debug(f"parser_plan: {response.plan[:32]!r}")
    logger.debug(f"parser_deps: {response.dependencies}")
    logger.debug(f"parser_retry_count: {state.parser_retry_count + 1}")

    logger.info("Finished node_tuning_code_metrics_parser")
    return {
        "parser_code": response.code,
        "parser_plan": response.plan,
        "parser_deps": response.dependencies,
        "parser_retry_count": state.parser_retry_count + 1,
    }


async def node_tuning_exec_metrics_parser(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_exec_metrics_parser")
    assert state.parser_code, "parser_code is required"

    result = await utils.exec_code(
        state.cwd,
        "tuning_parser.py",
        state.parser_code,
        state.parser_deps,
    )

    logger.debug(f"parser_stdout: {result.stdout[:32]!r}")
    logger.debug(f"parser_stderr: {result.stderr[:32]!r}")
    logger.debug(f"parser_returncode: {result.returncode}")
    logger.debug(f"parser_filename: {result.filename}")

    logger.info("Finished node_tuning_exec_metrics_parser")
    return {
        "parser_stdout": result.stdout,
        "parser_stderr": result.stderr,
        "parser_returncode": result.returncode,
        "parser_filename": result.filename,
    }


async def node_tuning_parse_metrics_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_parse_metrics_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_tuning_parser_output(
        state.parser_code or "NA",
        state.parser_stdout or "NA",
        state.parser_stderr or "NA",
        state.idea,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"parse_is_bug: {response.is_bug}")
    logger.debug(f"parse_summary: {response.summary[:32]!r}")
    logger.info("Finished node_tuning_parse_metrics_output")
    return {
        "parse_is_bug": response.is_bug,
        "parse_summary": response.summary,
    }


async def node_tuning_should_retry_parser_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_tuning_code_metrics_parser", "__end__"]:
    logger.info("Starting node_tuning_should_retry_parser_from_output")

    if state.parser_retry_count > runtime.context.max_retries:
        logger.info("Max retry count reached, going to `__end__`")
        return "__end__"

    if state.parse_is_bug is True:
        logger.info("Going to `node_tuning_code_metrics_parser`")
        return "node_tuning_code_metrics_parser"

    logger.info("Going to `__end__`")
    return "__end__"


async def _create_notes(state: State, runtime: Runtime[Context]) -> str:
    logger.info("Creating notes for tuning experiment")

    class Schema(BaseModel):
        note: str

    prompt = prompts.build_prompt_create_notes(state)
    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"note: {response.note[:64]!r}")
    return response.note


def build(
    checkpointer: Checkpointer | None = None,
) -> CompiledStateGraph[State, Context, State, State]:
    """Build the Stage 2 hyperparameter tuning graph."""
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node(
        "node_tuning_propose_hyperparam",
        node_tuning_propose_hyperparam,
    )
    builder.add_node(
        "node_tuning_code_tuning",
        node_tuning_code_tuning,
    )
    builder.add_node(
        "node_tuning_exec_tuning",
        node_tuning_exec_tuning,
    )
    builder.add_node(
        "node_tuning_parse_tuning_output",
        node_tuning_parse_tuning_output,
    )
    builder.add_node(
        "node_tuning_code_metrics_parser",
        node_tuning_code_metrics_parser,
    )
    builder.add_node(
        "node_tuning_exec_metrics_parser",
        node_tuning_exec_metrics_parser,
    )
    builder.add_node(
        "node_tuning_parse_metrics_output",
        node_tuning_parse_metrics_output,
    )

    # Add edges
    builder.add_edge(
        START,
        "node_tuning_propose_hyperparam",
    )
    builder.add_edge(
        "node_tuning_propose_hyperparam",
        "node_tuning_code_tuning",
    )
    builder.add_edge(
        "node_tuning_code_tuning",
        "node_tuning_exec_tuning",
    )
    builder.add_edge(
        "node_tuning_exec_tuning",
        "node_tuning_parse_tuning_output",
    )
    builder.add_conditional_edges(
        "node_tuning_parse_tuning_output",
        node_tuning_should_retry_code_from_tuning_output,
        ["node_tuning_code_tuning", "node_tuning_code_metrics_parser", "__end__"],
    )
    builder.add_edge(
        "node_tuning_code_metrics_parser",
        "node_tuning_exec_metrics_parser",
    )
    builder.add_edge(
        "node_tuning_exec_metrics_parser",
        "node_tuning_parse_metrics_output",
    )
    builder.add_conditional_edges(
        "node_tuning_parse_metrics_output",
        node_tuning_should_retry_parser_from_output,
        ["node_tuning_code_metrics_parser", "__end__"],
    )

    return builder.compile(name="graph_tuning", checkpointer=checkpointer)  # type: ignore
