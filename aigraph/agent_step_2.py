import logging
from operator import add
from pathlib import Path
from typing import Annotated

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from pydantic import BaseModel

from aigraph import prompts, utils

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task
    code: str

    hyperparams: Annotated[list[utils.Hyperparam], add] = []

    # generated by `node_code_tuning`
    tuning_code: str | None = None
    tuning_plan: str | None = None
    tuning_deps: list[str] = []

    # generated by `node_exec_tuning`
    tuning_returncode: int | None = None
    tuning_stdout: str | None = None
    tuning_stderr: str | None = None
    tuning_filename: str | None = None

    # generated by `node_parse_tuning_output`
    tuning_is_bug: bool | None = None
    tuning_summary: str | None = None

    # generated by `node_code_metrics_parser`
    parser_plan: str | None = None
    parser_code: str | None = None
    parser_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parser_stdout: str | None = None
    parser_stderr: str | None = None
    parser_returncode: int | None = None

    # generated by `node_parse_metrics_output`
    parse_valid_metrics_received: bool | None = None
    parse_metric_names: list[utils.MetricValue] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_propose_hyperparam(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_propose_hyperparam")

    class Schema(BaseModel):
        name: str
        description: str

    prompt = prompts.build_prompt_propose_hyperparam(
        code=state.code,
        hyperparams=[i.name for i in state.hyperparams],
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    state.hyperparams = [
        utils.Hyperparam(
            name=response.name,
            description=response.description,
        )
    ]

    logger.info("node_propose_hyperparam completed")
    return state


async def node_code_tuning(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_tuning")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    prompt = prompts.build_prompt_code_tuning(
        name=state.hyperparams[-1].name,
        description=state.hyperparams[-1].description,
        code=state.code,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.tuning_code = response.code
    state.tuning_plan = response.plan
    state.tuning_deps = response.dependencies

    logger.info("node_code_tuning completed")
    return state


async def node_exec_tuning(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_exec_tuning")
    assert state.tuning_code, "tuning_code is required"

    result = await utils.exec_code(state.cwd, state.tuning_code, state.tuning_deps)
    state.tuning_stdout = result.stdout
    state.tuning_stderr = result.stderr
    state.tuning_returncode = result.returncode
    state.tuning_filename = result.filename

    logger.info("node_exec_tuning completed")
    return state


async def node_parse_tuning_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_parse_tuning_output")
    assert state.tuning_code, "tuning_code is required"
    assert state.tuning_stdout is not None, "tuning_stdout is required"
    assert state.tuning_stderr is not None, "tuning_stderr is required"

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_parse_experiment_output(
        state.task,
        state.tuning_code,
        state.tuning_stdout,
        state.tuning_stderr,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.tuning_is_bug = response.is_bug
    state.tuning_summary = response.summary

    logger.info(f"node_parse_tuning_output completed. Is bug: {response.is_bug}")
    return state


async def node_code_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_metrics_parser")
    assert state.tuning_code, "tuning_code is required"

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    prompt = prompts.build_prompt_code_tuning_metrics_parser(state.tuning_code)

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parser_code = response.code
    state.parser_plan = response.plan
    state.parser_deps = response.dependencies

    logger.info("node_code_metrics_parser completed")
    return state


async def node_exec_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_exec_metrics_parser")
    assert state.parser_code, "parser_code is required"

    result = await utils.exec_code(
        state.cwd,
        state.parser_code,
        state.parser_deps,
    )

    state.parser_stdout = result.stdout
    state.parser_stderr = result.stderr
    state.parser_returncode = result.returncode

    logger.info("node_exec_metrics_parser completed")
    return state


async def node_parse_metrics_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_parse_metrics_output")
    assert state.parser_stdout, "parser_stdout is required"

    class Schema(BaseModel):
        valid_metrics_received: bool
        metric_names: list[utils.MetricValue]

    prompt = prompts.build_prompt_parse_metrics(state.parser_stdout)

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parse_valid_metrics_received = response.valid_metrics_received
    state.parse_metric_names = response.metric_names

    logger.info("node_parse_metrics_output completed")
    return state


def build() -> CompiledStateGraph[State, Context, State, State]:
    """Build the Stage 2 hyperparameter tuning graph."""
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node("propose_hyperparam", node_propose_hyperparam)
    builder.add_node("code_tuning", node_code_tuning)
    builder.add_node("exec_tuning", node_exec_tuning)
    builder.add_node("parse_tuning_output", node_parse_tuning_output)
    builder.add_node("code_metrics_parser", node_code_metrics_parser)
    builder.add_node("exec_metrics_parser", node_exec_metrics_parser)
    builder.add_node("parse_metrics_output", node_parse_metrics_output)

    # Add edges
    builder.add_edge(START, "propose_hyperparam")
    builder.add_edge("propose_hyperparam", "code_tuning")
    builder.add_edge("code_tuning", "exec_tuning")
    builder.add_edge("exec_tuning", "parse_tuning_output")
    builder.add_edge("parse_tuning_output", "code_metrics_parser")
    builder.add_edge("code_metrics_parser", "exec_metrics_parser")
    builder.add_edge("exec_metrics_parser", "parse_metrics_output")
    builder.add_edge("parse_metrics_output", END)

    return builder.compile()  # type: ignore
