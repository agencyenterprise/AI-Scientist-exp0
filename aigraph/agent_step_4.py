import logging
from operator import add
from typing import Annotated

from langchain.chat_models import init_chat_model
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from pydantic import BaseModel

from aigraph import prompts, utils

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    task: utils.Task
    code: str

    ablations: Annotated[list[utils.Ablation], add] = []

    # generated by `node_code_ablation`
    ablation_code: str | None = None
    ablation_plan: str | None = None
    ablation_deps: list[str] = []

    # generated by `node_exec_ablation`
    ablation_returncode: int | None = None
    ablation_stdout: str | None = None
    ablation_stderr: str | None = None
    ablation_directory: str | None = None
    ablation_filename: str | None = None

    # generated by `node_parse_ablation_output`
    ablation_is_bug: bool | None = None
    ablation_summary: str | None = None

    # generated by `node_code_metrics_parser`
    parser_plan: str | None = None
    parser_code: str | None = None
    parser_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parser_stdout: str | None = None
    parser_stderr: str | None = None
    parser_returncode: int | None = None

    # generated by `node_parse_metrics_output`
    parse_valid_metrics_received: bool | None = None
    parse_metric_names: list[utils.MetricValue] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0


async def node_propose_ablation(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_propose_ablation")

    class Schema(BaseModel):
        name: str
        description: str

    llm = init_chat_model(
        model=runtime.context.model, temperature=runtime.context.temperature
    )
    llms = llm.with_structured_output(Schema)

    prompt = prompts.build_prompt_propose_ablation(
        code=state.code,
        ablations=[i.name for i in state.ablations],
    )

    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.ablations = [
        utils.Ablation(name=response.name, description=response.description)
    ]

    logger.info("node_propose_ablation completed")
    return state


async def node_code_ablation(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_ablation")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    llm = init_chat_model(
        model=runtime.context.model, temperature=runtime.context.temperature
    )
    llms = llm.with_structured_output(Schema)

    prompt = prompts.build_prompt_code_ablation(
        name=state.ablations[-1].name,
        description=state.ablations[-1].description,
        code=state.code,
    )

    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.ablation_code = response.code
    state.ablation_plan = response.plan
    state.ablation_deps = response.dependencies

    logger.info("node_code_ablation completed")
    return state


async def node_exec_ablation(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_exec_ablation")

    code = state.ablation_code or ""
    dependencies = state.ablation_deps or []

    result = await utils.exec_code(code, dependencies)
    state.ablation_stdout = result.stdout
    state.ablation_stderr = result.stderr
    state.ablation_returncode = result.returncode
    state.ablation_directory = result.directory
    state.ablation_filename = result.filename

    logger.info("node_exec_ablation completed")
    return state


async def node_parse_ablation_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_parse_ablation_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_parse_experiment_output(
        state.task,
        state.ablation_code or "",
        state.ablation_stdout or "",
        state.ablation_stderr or "",
    )

    llm = init_chat_model(
        model=runtime.context.model, temperature=runtime.context.temperature
    )
    llms = llm.with_structured_output(Schema)

    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.ablation_is_bug = response.is_bug
    state.ablation_summary = response.summary

    logger.info(f"node_parse_ablation_output completed. Is bug: {response.is_bug}")
    return state


async def node_code_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_metrics_parser")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    prompt = prompts.build_prompt_code_ablation_metrics_parser(
        state.ablation_code or ""
    )

    llm = init_chat_model(
        model=runtime.context.model, temperature=runtime.context.temperature
    )
    llms = llm.with_structured_output(Schema)

    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parser_code = response.code
    state.parser_plan = response.plan
    state.parser_deps = response.dependencies

    logger.info("node_code_metrics_parser completed")
    return state


async def node_exec_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_exec_metrics_parser")
    assert state.ablation_directory, "ablation_directory is required"

    result = await utils.exec_code_at(
        state.ablation_directory,
        state.parser_code or "",
        state.parser_deps,
    )

    state.parser_stdout = result.stdout
    state.parser_stderr = result.stderr
    state.parser_returncode = result.returncode

    logger.info("node_exec_metrics_parser completed")
    return state


async def node_parse_metrics_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_parse_metrics_output")

    class Schema(BaseModel):
        valid_metrics_received: bool
        metric_names: list[utils.MetricValue]

    llm = init_chat_model(
        model=runtime.context.model, temperature=runtime.context.temperature
    )
    llms = llm.with_structured_output(Schema)

    prompt = prompts.build_prompt_parse_metrics(state.parser_stdout or "")
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parse_valid_metrics_received = response.valid_metrics_received
    state.parse_metric_names = response.metric_names

    logger.info("node_parse_metrics_output completed")
    return state


def build() -> CompiledStateGraph[State, Context, State, State]:
    """Build the Stage 4 ablation studies graph."""
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node("propose_ablation", node_propose_ablation)
    builder.add_node("code_ablation", node_code_ablation)
    builder.add_node("exec_ablation", node_exec_ablation)
    builder.add_node("parse_ablation_output", node_parse_ablation_output)
    builder.add_node("code_metrics_parser", node_code_metrics_parser)
    builder.add_node("exec_metrics_parser", node_exec_metrics_parser)
    builder.add_node("parse_metrics_output", node_parse_metrics_output)

    # Add edges
    builder.add_edge(START, "propose_ablation")
    builder.add_edge("propose_ablation", "code_ablation")
    builder.add_edge("code_ablation", "exec_ablation")
    builder.add_edge("exec_ablation", "parse_ablation_output")
    builder.add_edge("parse_ablation_output", "code_metrics_parser")
    builder.add_edge("code_metrics_parser", "exec_metrics_parser")
    builder.add_edge("exec_metrics_parser", "parse_metrics_output")
    builder.add_edge("parse_metrics_output", END)

    return builder.compile()  # type: ignore
