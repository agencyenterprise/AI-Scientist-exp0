import logging
from pathlib import Path
from typing import Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.errors import GraphRecursionError
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from pydantic import BaseModel

from aigraph import prompts, utils

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task

    # generated by `node_define_metrics`
    metrics: list[utils.Metric] = []

    # counts how many times we tried to code the experiment
    experiment_retry_count: int = 0

    # generated by `node_code_experiment`
    experiment_plan: str | None = None
    experiment_code: str | None = None
    experiment_deps: list[str] = []

    # generated by `node_exec_experiment`
    experiment_stdout: str | None = None
    experiment_stderr: str | None = None
    experiment_returncode: int | None = None
    experiment_filename: str | None = None

    # generated by `node_parse_experiment_output`
    experiment_is_bug: bool | None = None
    experiment_summary: str | None = None

    # generated by `node_code_metrics_parser`
    parse_plan: str | None = None
    parse_code: str | None = None
    parse_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parse_stdout: str | None = None
    parse_stderr: str | None = None
    parse_returncode: int | None = None

    # generated by `node_parse_metrics_output`
    parse_valid_metrics_received: bool | None = None
    parse_metric_names: list[utils.MetricValue] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_define_metrics(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_define_metrics")

    class Schema(BaseModel):
        metrics: list[utils.Metric]

    prompt = prompts.build_prompt_define_metrics(state.task)

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.metrics = response.metrics

    logger.info("node_define_metrics completed")
    return state


async def node_code_experiment(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_plan_and_code")

    class Schema(BaseModel):
        plan: str
        code: str
        dependencies: list[str]

    if state.experiment_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""

    if state.experiment_returncode is not None and state.experiment_returncode > 0:
        memory += "Previous code:\n"
        memory += f"```python\n{state.experiment_code or ''}\n```\n\n"
        memory += "Previous dependencies:\n"
        memory += f"```\n{state.experiment_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n"
        memory += f"```\n{state.experiment_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n"
        memory += f"```\n{state.experiment_stderr or 'NA'}\n```\n\n"

    if state.experiment_is_bug is True:
        memory += "Bug identified:\n"
        memory += f"{state.experiment_summary or 'NA'}\n\n"

    prompt = prompts.build_prompt_code_experiment(
        state.task,
        state.metrics,
        memory,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.experiment_plan = response.plan
    state.experiment_code = response.code
    state.experiment_deps = response.dependencies
    state.experiment_retry_count += 1

    logger.info("node_plan_and_code completed")
    return state


async def node_exec_experiment(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_execute")

    response = await utils.exec_code(
        state.cwd,
        state.experiment_code or "",
        state.experiment_deps,
    )

    state.experiment_stdout = response.stdout
    state.experiment_stderr = response.stderr
    state.experiment_returncode = response.returncode
    state.experiment_filename = response.filename

    logger.info("node_execute completed")
    return state


async def node_should_retry_code_from_experiment(
    state: State, runtime: Runtime[Context]
) -> Literal["code_experiment", "parse_experiment_output"]:
    logger.info("Starting node_should_retry_code_experiment")
    if state.experiment_returncode == 0:
        return "parse_experiment_output"
    return "code_experiment"


async def node_parse_experiment_output(
    state: State, runtime: Runtime[Context]
) -> State:
    logger.info("Starting node_parse_experiment_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_parse_experiment_output(
        state.task,
        state.experiment_code or "",
        state.experiment_stdout or "",
        state.experiment_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.experiment_is_bug = response.is_bug
    state.experiment_summary = response.summary

    logger.info("node_parse_experiment_output completed")
    return state


async def node_should_retry_code_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["code_experiment", "code_metrics_parser"]:
    logger.info("Starting node_should_retry_code_from_output")
    if state.parse_returncode is not None and state.parse_returncode > 0:
        return "code_experiment"
    return "code_metrics_parser"


async def node_code_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_metrics_parser")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    prompt = prompts.build_prompt_code_metrics_parser(state.experiment_stdout or "")

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parse_code = response.code
    state.parse_plan = response.plan
    state.parse_deps = response.dependencies

    logger.info("node_code_metrics_parser completed")
    return state


async def node_exec_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_exec_metrics_parser")
    assert state.parse_code, "parse_code is required"

    response = await utils.exec_code(
        state.cwd,
        state.parse_code,
        state.parse_deps,
    )

    state.parse_stdout = response.stdout
    state.parse_stderr = response.stderr
    state.parse_returncode = response.returncode

    logger.info("node_exec_metrics_parser completed")
    return state


async def node_parse_metrics_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_metrics_parser")

    class Schema(BaseModel):
        valid_metrics_received: bool
        metric_names: list[utils.MetricValue]

    prompt = prompts.build_prompt_parse_metrics(state.parse_stdout or "")

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parse_valid_metrics_received = response.valid_metrics_received
    state.parse_metric_names = response.metric_names

    logger.info("node_code_metrics_parser completed")
    return state


def build() -> CompiledStateGraph[State, Context, State, State]:
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node("define_metrics", node_define_metrics)
    builder.add_node("code_experiment", node_code_experiment)
    builder.add_node("exec_experiment", node_exec_experiment)
    builder.add_node("parse_experiment_output", node_parse_experiment_output)
    builder.add_node("code_metrics_parser", node_code_metrics_parser)
    builder.add_node("exec_metrics_parser", node_exec_metrics_parser)
    builder.add_node("parse_metrics_output", node_parse_metrics_output)

    # Add edges
    builder.add_edge(START, "define_metrics")
    builder.add_edge("define_metrics", "code_experiment")
    builder.add_edge("code_experiment", "exec_experiment")
    builder.add_conditional_edges(
        "exec_experiment",
        node_should_retry_code_from_experiment,
    )
    builder.add_conditional_edges(
        "parse_experiment_output",
        node_should_retry_code_from_output,
    )
    builder.add_edge("code_metrics_parser", "exec_metrics_parser")
    builder.add_edge("exec_metrics_parser", "parse_metrics_output")
    builder.add_edge("parse_metrics_output", END)

    return builder.compile()  # type: ignore
