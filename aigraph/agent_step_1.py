import logging

from langchain.chat_models import init_chat_model
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from pydantic import BaseModel

from aigraph import prompts, utils

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    task: utils.Task

    # state (TODO)
    memory: str | None = None

    # generated by `node_define_metrics`
    metrics: list[utils.Metric] = []

    # generated by `node_code_experiment`
    experiment_plan: str | None = None
    experiment_code: str | None = None
    experiment_deps: list[str] = []

    # generated by `node_exec_experiment`
    experiment_stdout: str | None = None
    experiment_stderr: str | None = None
    experiment_returncode: int | None = None
    experiment_directory: str | None = None
    experiment_filename: str | None = None

    # generated by `node_parse_experiment_output`
    experiment_is_bug: bool | None = None
    experiment_summary: str | None = None

    # generated by `node_code_metrics_parser`
    parse_plan: str | None = None
    parse_code: str | None = None
    parse_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parse_stdout: str | None = None
    parse_stderr: str | None = None
    parse_returncode: int | None = None

    # generated by `node_parse_metrics_output`
    parse_valid_metrics_received: bool | None = None
    parse_metric_names: list[utils.MetricValue] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0


async def node_define_metrics(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_define_metrics")

    class Schema(BaseModel):
        metrics: list[utils.Metric]

    llm = init_chat_model(
        model=runtime.context.model, temperature=runtime.context.temperature
    )
    llms = llm.with_structured_output(Schema)

    prompt = prompts.build_prompt_define_metrics(state.task)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.metrics = response.metrics

    logger.info("node_define_metrics completed")
    return state


async def node_code_experiment(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_plan_and_code")

    class Schema(BaseModel):
        plan: str
        code: str
        dependencies: list[str]

    llm = init_chat_model(
        model=runtime.context.model, temperature=runtime.context.temperature
    )
    llms = llm.with_structured_output(Schema)

    prompt = prompts.build_prompt_code_experiment(
        state.task,
        state.metrics,
        state.memory or "",
    )

    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.experiment_plan = response.plan
    state.experiment_code = response.code
    state.experiment_deps = response.dependencies

    logger.info("node_plan_and_code completed")
    return state


async def node_exec_experiment(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_execute")

    result = await utils.exec_code(state.experiment_code or "", state.experiment_deps)
    state.experiment_stdout = result.stdout
    state.experiment_stderr = result.stderr
    state.experiment_returncode = result.returncode
    state.experiment_directory = result.directory
    state.experiment_filename = result.filename

    logger.info("node_execute completed")
    return state


async def node_parse_experiment_output(
    state: State, runtime: Runtime[Context]
) -> State:
    logger.info("Starting node_parse_experiment_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_parse_experiment_output(
        state.task,
        state.experiment_code or "",
        state.experiment_stdout or "",
        state.experiment_stderr or "",
    )

    llm = init_chat_model(
        model=runtime.context.model, temperature=runtime.context.temperature
    )
    llms = llm.with_structured_output(Schema)

    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.experiment_is_bug = response.is_bug
    state.experiment_summary = response.summary

    logger.info("node_parse_experiment_output completed")
    return state


async def node_code_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_metrics_parser")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    prompt = prompts.build_prompt_code_metrics_parser(state.experiment_stdout or "")

    llm = init_chat_model(
        model=runtime.context.model, temperature=runtime.context.temperature
    )
    llms = llm.with_structured_output(Schema)

    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parse_code = response.code
    state.parse_plan = response.plan
    state.parse_deps = response.dependencies

    logger.info("node_code_metrics_parser completed")
    return state


async def node_exec_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_exec_metrics_parser")
    assert state.experiment_directory, "experiment_directory is required"

    result = await utils.exec_code_at(
        state.experiment_directory,
        state.parse_code or "",
        state.parse_deps,
    )

    state.parse_stdout = result.stdout
    state.parse_stderr = result.stderr
    state.parse_returncode = result.returncode

    logger.info("node_exec_metrics_parser completed")
    return state


async def node_parse_metrics_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_metrics_parser")

    class Schema(BaseModel):
        valid_metrics_received: bool
        metric_names: list[utils.MetricValue]

    prompt = prompts.build_prompt_parse_metrics(state.parse_stdout or "")
    llm = init_chat_model(
        model=runtime.context.model, temperature=runtime.context.temperature
    )
    llms = llm.with_structured_output(Schema)

    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parse_valid_metrics_received = response.valid_metrics_received
    state.parse_metric_names = response.metric_names

    logger.info("node_code_metrics_parser completed")
    return state


def build() -> CompiledStateGraph[State, Context, State, State]:
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node("define_metrics", node_define_metrics)
    builder.add_node("code_experiment", node_code_experiment)
    builder.add_node("exec_experiment", node_exec_experiment)
    builder.add_node("parse_experiment_output", node_parse_experiment_output)
    builder.add_node("code_metrics_parser", node_code_metrics_parser)
    builder.add_node("exec_metrics_parser", node_exec_metrics_parser)
    builder.add_node("parse_metrics_output", node_parse_metrics_output)

    # Add edges
    builder.add_edge(START, "define_metrics")
    builder.add_edge("define_metrics", "code_experiment")
    builder.add_edge("code_experiment", "exec_experiment")
    builder.add_edge("exec_experiment", "parse_experiment_output")
    builder.add_edge("parse_experiment_output", "code_metrics_parser")
    builder.add_edge("code_metrics_parser", "exec_metrics_parser")
    builder.add_edge("exec_metrics_parser", "parse_metrics_output")
    builder.add_edge("parse_metrics_output", END)

    return builder.compile()  # type: ignore
