import logging
from pathlib import Path
from typing import Any

import open_deep_research.deep_researcher as researcher
from langchain.chat_models import BaseChatModel, init_chat_model
from langchain_core.messages import HumanMessage
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from langgraph.types import Checkpointer
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import prepare_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task

    # generated by `node_create_experiment_plan`
    experiment_plan_structured: str | None = None

    # generated by `node_research`
    research: str | None = None

    # generated by `node_define_metrics`
    metrics: list[utils.Metric] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_create_experiment_plan(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    """Create initial structured experiment plan."""
    logger.info("Starting node_create_experiment_plan")

    class Schema(BaseModel):
        plan: str

    prompt = prompts.build_prompt_prepare_plan(state.task)
    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"experiment_plan_structured: {response.plan[:32]!r}")
    logger.info("Finished node_create_experiment_plan")
    return {"experiment_plan_structured": response.plan}


async def node_research(state: State, runtime: Runtime[Context]) -> dict[str, Any]:
    """Run deep research on the task, informed by the experiment plan."""
    logger.info("Starting node_research")

    prompt = prompts.build_prompt_prepare_research(
        state.task,
        experiment_plan=state.experiment_plan_structured or "",
    )
    messages = {"messages": [HumanMessage(content=prompt)]}

    result = await researcher.deep_researcher.ainvoke(
        messages,
        config={"configurable": {"allow_clarification": False}},
    )

    research_report = result.get("final_report", "")
    logger.debug(f"deep_research: {research_report[:32]!r}")

    logger.info("Finished node_research")
    return {"research": research_report}


async def node_define_metrics(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    """Define evaluation metrics using research context."""
    logger.info("Starting node_define_metrics")

    class Schema(BaseModel):
        metrics: list[utils.Metric]

    prompt = prompts.build_prompt_prepare_metrics(
        state.task, research=state.research or ""
    )
    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"metrics: {[m.name for m in response.metrics]}")
    logger.info("Finished node_define_metrics")
    return {"metrics": response.metrics}


def build(
    checkpointer: Checkpointer = None,
) -> CompiledStateGraph[State, Context, State, State]:
    """Build the prepare graph."""
    builder = StateGraph(state_schema=State, context_schema=Context)

    builder.add_node("node_create_experiment_plan", node_create_experiment_plan)
    builder.add_node("node_research", node_research)
    builder.add_node("node_define_metrics", node_define_metrics)

    # Plan runs first, then research, then metrics using research outputs
    builder.add_edge(START, "node_create_experiment_plan")
    builder.add_edge("node_create_experiment_plan", "node_research")
    builder.add_edge("node_research", "node_define_metrics")
    builder.add_edge("node_define_metrics", END)

    return builder.compile(name="graph_prepare", checkpointer=checkpointer)  # type: ignore
