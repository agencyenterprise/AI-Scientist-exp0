import logging
import shutil
from pathlib import Path
from typing import Any, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from langgraph.types import Checkpointer
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import writeup_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task

    parser_code: str
    parser_stdout: str | None = None
    baseline_results: str = ""  # baseline parser stdout for comparison
    experiment_code: str
    plots: list[utils.Plot]
    research: str | None = None
    cumulative_summary: str = ""
    experiment_plan_structured: str = ""  # structured experiment plan

    # counts retry attempts
    writeup_retry_count: int = 0

    # generated by `node_writeup_generate_writeup`
    latex_content: str | None = None

    # generated by `node_compile_writeup`
    compile_stdout: str | None = None
    compile_stderr: str | None = None
    compile_returncode: int | None = None

    # generated by `node_parse_compile_output`
    compile_is_bug: bool | None = None
    compile_summary: str | None = None

    # generated by `node_writeup_review_paper`
    review_retry_count: int = 0
    review: utils.Review | None = None


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_writeup_setup_writeup(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_writeup_setup_writeup")

    src = utils.DATA_DIR / "template.tex"
    dst = state.cwd / "template.tex"
    shutil.copy(src, dst)
    logger.debug(f"Copied {src} to {dst}")

    logger.info("Finished node_writeup_setup_writeup")
    return {}


async def node_writeup_generate_writeup(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_writeup_generate_writeup")
    from langgraph.errors import GraphRecursionError

    class Schema(BaseModel):
        content: str

    if state.writeup_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""
    if state.compile_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.compile_summary or 'NA'}\n\n"
        memory += "Previous LaTeX content:\n\n"
        memory += f"```latex\n{state.latex_content or 'NA'}\n```\n\n"
        memory += "Stdout of compilation:\n\n"
        memory += f"```\n{state.compile_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of compilation:\n\n"
        memory += f"```\n{state.compile_stderr or 'NA'}\n```\n\n"

    if state.review:
        memory += "Previous Review:\n\n"
        memory += f"Summary: {state.review.summary}\n"
        memory += f"Strengths: {state.review.strengths}\n"
        memory += f"Weaknesses: {state.review.weaknesses}\n"
        memory += f"Decision: {state.review.decision}\n"

    system = prompts.build_writeup_system_message(
        task=state.task,
        pages=5,
    )

    prompt = prompts.build_writeup_prompt(
        code_experiment=state.experiment_code,
        code_parser=state.parser_code,
        parser_stdout=state.parser_stdout,
        baseline_results=state.baseline_results,
        plots=state.plots,
        research=state.research,
        memory=memory,
        cumulative_summary=state.cumulative_summary,
        experiment_plan=state.experiment_plan_structured,
    )

    messages = [
        SystemMessage(content=system),
        HumanMessage(content=prompt),
    ]

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(messages)  # type: ignore

    logger.debug(f"latex_content: {response.content[:32]!r}")
    logger.debug(f"writeup_retry_count: {state.writeup_retry_count + 1}")

    logger.info("Finished node_writeup_generate_writeup")
    return {
        "latex_content": response.content,
        "writeup_retry_count": state.writeup_retry_count + 1,
    }


async def node_compile_writeup(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_compile_writeup")
    assert state.latex_content, "latex_content is required"

    file = state.cwd / "template.tex"
    file.write_text(state.latex_content)

    result = await utils.compile(state.cwd, file)

    logger.debug(f"compile_stdout: {result.stdout[:32]!r}")
    logger.debug(f"compile_stderr: {result.stderr[:32]!r}")
    logger.debug(f"compile_returncode: {result.returncode}")

    logger.info("Finished node_compile_writeup")
    return {
        "compile_stdout": result.stdout,
        "compile_stderr": result.stderr,
        "compile_returncode": result.returncode,
    }


async def node_parse_compile_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_parse_compile_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_compile_output(
        state.latex_content or "",
        state.compile_stdout or "",
        state.compile_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"compile_is_bug: {response.is_bug}")
    logger.debug(f"compile_summary: {response.summary[:32]!r}")

    logger.info("Finished node_parse_compile_output")
    return {
        "compile_is_bug": response.is_bug,
        "compile_summary": response.summary,
    }


async def node_should_retry_compile(
    state: State, runtime: Runtime[Context]
) -> Literal["node_writeup_generate_writeup", "__end__"]:
    logger.info("Starting node_should_retry_compile")

    if state.compile_is_bug is True:
        logger.info("Going to `node_writeup_generate_writeup`")
        return "node_writeup_generate_writeup"

    logger.info("Going to `__end__`")
    return "__end__"


async def node_writeup_review_paper(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_writeup_review_paper")
    assert state.latex_content, "latex_content is required"

    prompt = prompts.build_writeup_review_prompt(
        latex_content=state.latex_content,
        task=state.task,
        baseline_results=state.baseline_results,
        parser_stdout=state.parser_stdout or "",
        experiment_plan=state.experiment_plan_structured,
    )

    llms = runtime.context.llm.with_structured_output(utils.Review)
    response: utils.Review = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"review_decision: {response.decision}")
    logger.debug(f"review_retry_count: {state.review_retry_count + 1}")

    logger.info("Finished node_writeup_review_paper")
    return {
        "review": response,
        "review_retry_count": state.review_retry_count + 1,
    }


async def node_reset_compile_counter(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_reset_compile_counter")
    logger.info("Resetting writeup_retry_count to 0")
    return {"writeup_retry_count": 0}


async def node_should_retry_review(
    state: State, runtime: Runtime[Context]
) -> Literal["node_reset_compile_counter", "node_compile_writeup"]:
    logger.info("Starting node_should_retry_review")

    if (
        state.review
        and state.review.decision == "Reject"
        and state.review_retry_count < 3
    ):
        logger.info("Going to `node_reset_compile_counter`")
        return "node_reset_compile_counter"

    logger.info("Going to `node_compile_writeup`")
    return "node_compile_writeup"


def build(
    checkpointer: Checkpointer = None,
) -> CompiledStateGraph[State, Context, State, State]:
    builder = StateGraph(State, Context)

    builder.add_node(
        "node_writeup_setup_writeup",
        node_writeup_setup_writeup,
    )
    builder.add_node(
        "node_writeup_generate_writeup",
        node_writeup_generate_writeup,
    )
    builder.add_node(
        "node_writeup_review_paper",
        node_writeup_review_paper,
    )
    builder.add_node(
        "node_reset_compile_counter",
        node_reset_compile_counter,
    )
    builder.add_node(
        "node_compile_writeup",
        node_compile_writeup,
    )
    builder.add_node(
        "node_parse_compile_output",
        node_parse_compile_output,
    )

    builder.add_edge(
        START,
        "node_writeup_setup_writeup",
    )
    builder.add_edge(
        "node_writeup_setup_writeup",
        "node_writeup_generate_writeup",
    )
    builder.add_edge(
        "node_writeup_generate_writeup",
        "node_writeup_review_paper",
    )
    builder.add_conditional_edges(
        "node_writeup_review_paper",
        node_should_retry_review,
        ["node_reset_compile_counter", "node_compile_writeup"],
    )
    builder.add_edge(
        "node_reset_compile_counter",
        "node_writeup_generate_writeup",
    )
    builder.add_edge(
        "node_compile_writeup",
        "node_parse_compile_output",
    )
    builder.add_conditional_edges(
        "node_parse_compile_output",
        node_should_retry_compile,
        ["node_writeup_generate_writeup", "__end__"],
    )

    return builder.compile(name="graph_writeup", checkpointer=checkpointer)  # type: ignore
