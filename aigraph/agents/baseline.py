import logging
import operator
from pathlib import Path
from typing import Annotated, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.errors import GraphRecursionError
from langgraph.graph import START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import baseline_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task

    # generated by `node_define_metrics`
    metrics: Annotated[set[utils.Metric], operator.or_] = set()

    # counts how many times we tried to code the experiment
    experiment_retry_count: int = 0

    # generated by `node_code_experiment`
    experiment_plan: str | None = None
    experiment_code: str | None = None
    experiment_deps: list[str] = []

    # generated by `node_exec_experiment`
    experiment_stdout: str | None = None
    experiment_stderr: str | None = None
    experiment_returncode: int | None = None
    experiment_filename: str | None = None

    # generated by `node_parse_experiment_output`
    experiment_is_bug: bool | None = None
    experiment_summary: str | None = None

    # counts how many times we tried to code the parser
    parser_retry_count: int = 0

    # generated by `node_code_metrics_parser`
    parse_plan: str | None = None
    parse_code: str | None = None
    parse_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parse_stdout: str | None = None
    parse_stderr: str | None = None
    parse_returncode: int | None = None
    parse_filename: str | None = None

    # generated by `node_parse_metrics_output`
    parse_is_bug: bool | None = None
    parse_summary: str | None = None


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_baseline_define_metrics(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_baseline_define_metrics")

    class Schema(BaseModel):
        metrics: list[utils.Metric]

    prompt = prompts.build_prompt_baseline_metrics(state.task)
    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.metrics = set(response.metrics)

    logger.debug(f"metrics: {[m.name for m in state.metrics]}")
    logger.info("Finished node_baseline_define_metrics")
    return state


async def node_baseline_code_experiment(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_baseline_code_experiment")

    class Schema(BaseModel):
        plan: str
        code: str
        dependencies: list[str]

    if state.experiment_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""

    if state.experiment_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.experiment_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"```python\n{state.experiment_code or 'NA'}\n```\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"```\n{state.experiment_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"```\n{state.experiment_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"```\n{state.experiment_stderr or 'NA'}\n```\n\n"

    prompt = prompts.build_prompt_baseline_code(
        state.task,
        state.metrics,
        memory,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.experiment_plan = response.plan
    state.experiment_code = response.code
    state.experiment_deps = response.dependencies
    state.experiment_retry_count += 1

    logger.debug(f"experiment_plan: {state.experiment_plan[:32]!r}")
    logger.debug(f"experiment_code: {state.experiment_code[:32]!r}")
    logger.debug(f"experiment_deps: {state.experiment_deps}")
    logger.debug(f"experiment_retry_count: {state.experiment_retry_count}")

    logger.info("Finished node_baseline_code_experiment")
    return state


async def node_baseline_exec_experiment(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_baseline_exec_experiment")
    assert state.experiment_code, "experiment_code is required"

    response = await utils.exec_code(
        state.cwd,
        'baseline.py',
        state.experiment_code,
        state.experiment_deps,
    )

    state.experiment_stdout = response.stdout
    state.experiment_stderr = response.stderr
    state.experiment_returncode = response.returncode
    state.experiment_filename = response.filename

    logger.debug(f"experiment_stdout: {state.experiment_stdout[:32]!r}")
    logger.debug(f"experiment_stderr: {state.experiment_stderr[:32]!r}")
    logger.debug(f"experiment_returncode: {state.experiment_returncode}")
    logger.debug(f"experiment_filename: {state.experiment_filename}")

    logger.info("Finished node_baseline_exec_experiment")
    return state


async def node_baseline_parse_experiment_output(
    state: State, runtime: Runtime[Context]
) -> State:
    logger.info("Starting node_baseline_parse_experiment_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_baseline_code_output(
        state.task,
        state.experiment_code or "",
        state.experiment_stdout or "",
        state.experiment_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.experiment_is_bug = response.is_bug
    state.experiment_summary = response.summary

    logger.debug(f"experiment_is_bug: {state.experiment_is_bug}")
    logger.debug(f"experiment_summary: {state.experiment_summary[:32]!r}")

    logger.info("Finished node_baseline_parse_experiment_output")
    return state


async def node_baseline_should_retry_code_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_baseline_code_experiment", "node_baseline_code_metrics_parser"]:
    logger.info("Starting node_baseline_should_retry_code_from_output")

    if state.experiment_is_bug is True:
        logger.info('Going to `node_baseline_code_experiment`')
        return "node_baseline_code_experiment"

    logger.info('Going to `node_baseline_code_metrics_parser`')
    return "node_baseline_code_metrics_parser"


async def node_baseline_code_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_baseline_code_metrics_parser")
    assert state.experiment_code, "experiment_code is required"

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    if state.parser_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""
    if state.parse_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.parse_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"```python\n{state.parse_code or 'NA'}\n```\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"```\n{state.parse_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"```\n{state.parse_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"```\n{state.parse_stderr or 'NA'}\n```\n\n"

    prompt = prompts.build_prompt_baseline_parser_code(
        state.experiment_code,
        memory=memory,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parse_code = response.code
    state.parse_plan = response.plan
    state.parse_deps = response.dependencies
    state.parser_retry_count += 1

    logger.debug(f"parse_code: {state.parse_code[:32]!r}")
    logger.debug(f"parse_plan: {state.parse_plan[:32]!r}")
    logger.debug(f"parse_deps: {state.parse_deps}")
    logger.debug(f"parser_retry_count: {state.parser_retry_count}")

    logger.info("Finished node_baseline_code_metrics_parser")
    return state


async def node_baseline_exec_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_baseline_exec_metrics_parser")
    assert state.parse_code, "parse_code is required"

    response = await utils.exec_code(
        state.cwd,
        'baseline_parser.py',
        state.parse_code,
        state.parse_deps,
    )

    state.parse_stdout = response.stdout
    state.parse_stderr = response.stderr
    state.parse_returncode = response.returncode
    state.parse_filename = response.filename

    logger.debug(f"parse_stdout: {state.parse_stdout[:32]!r}")
    logger.debug(f"parse_stderr: {state.parse_stderr[:32]!r}")
    logger.debug(f"parse_returncode: {state.parse_returncode}")
    logger.debug(f"parse_filename: {state.parse_filename}")

    logger.info("Finished node_baseline_exec_metrics_parser")
    return state


async def node_baseline_parse_metrics_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_baseline_parse_metrics_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_baseline_parser_output(
        state.parse_code or "",
        state.parse_stdout or "",
        state.parse_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parse_is_bug = response.is_bug
    state.parse_summary = response.summary

    logger.debug(f"parse_is_bug: {state.parse_is_bug}")
    logger.debug(f"parse_summary: {state.parse_summary[:32]!r}")
    logger.info("Finished node_baseline_parse_metrics_output")
    return state


async def node_baseline_should_retry_parser_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_baseline_code_metrics_parser", '__end__']:
    logger.info("Starting node_baseline_should_retry_parser_from_output")

    if state.parse_is_bug is True:
        logger.info('Going to `node_baseline_code_metrics_parser`')
        return "node_baseline_code_metrics_parser"

    logger.info('Going to `__end__`')
    return '__end__'


def build() -> CompiledStateGraph[State, Context, State, State]:
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node("node_baseline_define_metrics", node_baseline_define_metrics)
    builder.add_node("node_baseline_code_experiment", node_baseline_code_experiment)
    builder.add_node("node_baseline_exec_experiment", node_baseline_exec_experiment)
    builder.add_node("node_baseline_parse_experiment_output", node_baseline_parse_experiment_output)
    builder.add_node("node_baseline_code_metrics_parser", node_baseline_code_metrics_parser)
    builder.add_node("node_baseline_exec_metrics_parser", node_baseline_exec_metrics_parser)
    builder.add_node("node_baseline_parse_metrics_output", node_baseline_parse_metrics_output)

    # Add edges
    builder.add_edge(START, "node_baseline_define_metrics")
    builder.add_edge("node_baseline_define_metrics", "node_baseline_code_experiment")
    builder.add_edge("node_baseline_code_experiment", "node_baseline_exec_experiment")
    builder.add_edge("node_baseline_exec_experiment", "node_baseline_parse_experiment_output")
    builder.add_conditional_edges("node_baseline_parse_experiment_output", node_baseline_should_retry_code_from_output)
    builder.add_edge("node_baseline_code_metrics_parser", "node_baseline_exec_metrics_parser")
    builder.add_edge("node_baseline_exec_metrics_parser", "node_baseline_parse_metrics_output")
    builder.add_conditional_edges("node_baseline_parse_metrics_output", node_baseline_should_retry_parser_from_output)

    return builder.compile(name="graph_baseline", checkpointer=True) # type: ignore
