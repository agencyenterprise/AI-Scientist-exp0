import logging
from pathlib import Path
from typing import Any, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.errors import GraphRecursionError
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from langgraph.types import Checkpointer
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import baseline_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task
    idea: utils.Idea
    research: str

    # generated by `node_define_metrics`
    metrics: list[utils.Metric] = []

    # counts how many times we tried to code the experiment
    experiment_retry_count: int = 0

    # generated by `node_code_experiment`
    experiment_plan: str | None = None
    experiment_code: str | None = None
    experiment_deps: list[str] = []

    # generated by `node_exec_experiment`
    experiment_stdout: str | None = None
    experiment_stderr: str | None = None
    experiment_returncode: int | None = None
    experiment_filename: str | None = None

    # generated by `node_parse_experiment_output`
    experiment_is_bug: bool | None = None
    experiment_summary: str | None = None

    # counts how many times we tried to code the parser
    parser_retry_count: int = 0

    # generated by `node_code_metrics_parser`
    parse_plan: str | None = None
    parse_code: str | None = None
    parse_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parse_stdout: str | None = None
    parse_stderr: str | None = None
    parse_returncode: int | None = None
    parse_filename: str | None = None

    # generated by `node_parse_metrics_output`
    parse_is_bug: bool | None = None
    parse_summary: str | None = None


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_baseline_define_metrics(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_define_metrics")

    class Schema(BaseModel):
        metrics: list[utils.Metric]

    prompt = prompts.build_prompt_baseline_metrics(
        state.task, state.idea, state.research
    )
    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"metrics: {[m.name for m in response.metrics]}")
    logger.info("Finished node_baseline_define_metrics")
    return {"metrics": response.metrics}


async def node_baseline_code_experiment(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_code_experiment")

    class Schema(BaseModel):
        plan: str
        code: str
        dependencies: list[str]

    if state.experiment_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""

    if state.experiment_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.experiment_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"```python\n{state.experiment_code or 'NA'}\n```\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"```\n{state.experiment_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"```\n{state.experiment_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"```\n{state.experiment_stderr or 'NA'}\n```\n\n"

    prompt = prompts.build_prompt_baseline_code(
        state.task,
        state.metrics,
        memory,
        state.idea,
        state.research,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"experiment_plan: {response.plan[:32]!r}")
    logger.debug(f"experiment_code: {response.code[:32]!r}")
    logger.debug(f"experiment_deps: {response.dependencies}")
    logger.debug(f"experiment_retry_count: {state.experiment_retry_count + 1}")

    logger.info("Finished node_baseline_code_experiment")
    return {
        "experiment_plan": response.plan,
        "experiment_code": response.code,
        "experiment_deps": response.dependencies,
        "experiment_retry_count": state.experiment_retry_count + 1,
    }


async def node_baseline_exec_experiment(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_exec_experiment")

    response = await utils.exec_code(
        state.cwd,
        "baseline.py",
        state.experiment_code or "NA",
        state.experiment_deps or [],
    )

    logger.debug(f"experiment_stdout: {response.stdout[:32]!r}")
    logger.debug(f"experiment_stderr: {response.stderr[:32]!r}")
    logger.debug(f"experiment_returncode: {response.returncode}")
    logger.debug(f"experiment_filename: {response.filename}")

    logger.info("Finished node_baseline_exec_experiment")
    return {
        "experiment_stdout": response.stdout,
        "experiment_stderr": response.stderr,
        "experiment_returncode": response.returncode,
        "experiment_filename": response.filename,
    }


async def node_baseline_parse_experiment_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_parse_experiment_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_baseline_code_output(
        state.task,
        state.experiment_code or "",
        state.experiment_stdout or "",
        state.experiment_stderr or "",
        state.idea,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"experiment_is_bug: {response.is_bug}")
    logger.debug(f"experiment_summary: {response.summary[:32]!r}")

    logger.info("Finished node_baseline_parse_experiment_output")
    return {
        "experiment_is_bug": response.is_bug,
        "experiment_summary": response.summary,
    }


async def node_baseline_should_retry_code_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_baseline_code_experiment", "node_baseline_code_metrics_parser"]:
    logger.info("Starting node_baseline_should_retry_code_from_output")

    if state.experiment_is_bug is True:
        logger.info("Going to `node_baseline_code_experiment`")
        return "node_baseline_code_experiment"

    logger.info("Going to `node_baseline_code_metrics_parser`")
    return "node_baseline_code_metrics_parser"


async def node_baseline_code_metrics_parser(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_code_metrics_parser")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    if state.parser_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""
    if state.parse_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.parse_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"```python\n{state.parse_code or 'NA'}\n```\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"```\n{state.parse_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"```\n{state.parse_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"```\n{state.parse_stderr or 'NA'}\n```\n\n"

    prompt = prompts.build_prompt_baseline_parser_code(
        state.experiment_code or "NA",
        memory=memory,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"parse_code: {response.code[:32]!r}")
    logger.debug(f"parse_plan: {response.plan[:32]!r}")
    logger.debug(f"parse_deps: {response.dependencies}")
    logger.debug(f"parser_retry_count: {state.parser_retry_count + 1}")

    logger.info("Finished node_baseline_code_metrics_parser")
    return {
        "parse_code": response.code,
        "parse_plan": response.plan,
        "parse_deps": response.dependencies,
        "parser_retry_count": state.parser_retry_count + 1,
    }


async def node_baseline_exec_metrics_parser(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_exec_metrics_parser")

    response = await utils.exec_code(
        state.cwd,
        "baseline_parser.py",
        state.parse_code or "NA",
        state.parse_deps or [],
    )

    logger.debug(f"parse_stdout: {response.stdout[:32]!r}")
    logger.debug(f"parse_stderr: {response.stderr[:32]!r}")
    logger.debug(f"parse_returncode: {response.returncode}")
    logger.debug(f"parse_filename: {response.filename}")

    logger.info("Finished node_baseline_exec_metrics_parser")
    return {
        "parse_stdout": response.stdout,
        "parse_stderr": response.stderr,
        "parse_returncode": response.returncode,
        "parse_filename": response.filename,
    }


async def node_baseline_parse_metrics_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_parse_metrics_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_baseline_parser_output(
        state.parse_code or "",
        state.parse_stdout or "",
        state.parse_stderr or "",
        state.idea,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"parse_is_bug: {response.is_bug}")
    logger.debug(f"parse_summary: {response.summary[:32]!r}")
    logger.info("Finished node_baseline_parse_metrics_output")
    return {
        "parse_is_bug": response.is_bug,
        "parse_summary": response.summary,
    }


async def node_baseline_should_retry_parser_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_baseline_code_metrics_parser", "__end__"]:
    logger.info("Starting node_baseline_should_retry_parser_from_output")

    if state.parse_is_bug is True:
        logger.info("Going to `node_baseline_code_metrics_parser`")
        return "node_baseline_code_metrics_parser"

    logger.info("Going to `__end__`")
    return "__end__"


def build(
    checkpointer: Checkpointer = None,
) -> CompiledStateGraph[State, Context, State, State]:
    builder = StateGraph(state_schema=State, context_schema=Context)

    builder.add_node(
        "node_baseline_define_metrics",
        node_baseline_define_metrics,
    )
    builder.add_node(
        "node_baseline_code_experiment",
        node_baseline_code_experiment,
    )
    builder.add_node(
        "node_baseline_exec_experiment",
        node_baseline_exec_experiment,
    )
    builder.add_node(
        "node_baseline_parse_experiment_output",
        node_baseline_parse_experiment_output,
    )
    builder.add_node(
        "node_baseline_code_metrics_parser",
        node_baseline_code_metrics_parser,
    )
    builder.add_node(
        "node_baseline_exec_metrics_parser",
        node_baseline_exec_metrics_parser,
    )
    builder.add_node(
        "node_baseline_parse_metrics_output",
        node_baseline_parse_metrics_output,
    )

    builder.add_edge(
        START,
        "node_baseline_define_metrics",
    )
    builder.add_edge(
        "node_baseline_define_metrics",
        "node_baseline_code_experiment",
    )
    builder.add_edge(
        "node_baseline_code_experiment",
        "node_baseline_exec_experiment",
    )
    builder.add_edge(
        "node_baseline_exec_experiment",
        "node_baseline_parse_experiment_output",
    )
    builder.add_conditional_edges(
        "node_baseline_parse_experiment_output",
        node_baseline_should_retry_code_from_output,
        ["node_baseline_code_experiment", "node_baseline_code_metrics_parser"],
    )
    builder.add_edge(
        "node_baseline_code_metrics_parser",
        "node_baseline_exec_metrics_parser",
    )
    builder.add_edge(
        "node_baseline_exec_metrics_parser",
        "node_baseline_parse_metrics_output",
    )
    builder.add_conditional_edges(
        "node_baseline_parse_metrics_output",
        node_baseline_should_retry_parser_from_output,
        ["node_baseline_code_metrics_parser", END],
    )

    return builder.compile(name="graph_baseline", checkpointer=checkpointer)  # type: ignore
