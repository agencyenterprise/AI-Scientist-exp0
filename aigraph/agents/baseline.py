import logging
import operator
from pathlib import Path
from typing import Annotated, Any, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from langgraph.types import Checkpointer
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import baseline_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    """State for baseline experiment implementation.

    Attributes:
        cwd: Directory for saving baseline.py and outputs.
             Passed to utils.exec_code() to run generated Python.
        task: Provides context for code generation.
              Included in all LLM prompts for task-relevant code.
        idea: Guides what baseline should implement.
              Fed to prompt builders for idea-specific implementation.
        research: Background knowledge for LLM prompts.
                  Appended to prompts so LLM knows related work.
        metrics: LLM-defined success metrics to compute.
                 Generated first, then referenced in code generation prompt.
        experiment_retry_count: Tracks retry attempts for experiment code.
                                Compared against max (5) to prevent infinite loops.
        experiment_plan: LLM-generated plan before coding.
                        Used for debugging and understanding approach.
        experiment_code: Generated baseline implementation code.
                         Written to baseline.py and executed.
        experiment_deps: Python dependencies for experiment code.
                          Installed before execution.
        experiment_stdout: Standard output from experiment execution.
                           Used to detect bugs and extract results.
        experiment_stderr: Standard error from experiment execution.
                            Used to detect bugs and errors.
        experiment_returncode: Exit code from experiment execution.
                                Non-zero indicates failure.
        experiment_filename: Filename where experiment code was saved.
                             Used for debugging.
        experiment_is_bug: Whether experiment output indicates a bug.
                           True triggers re-generation with error context in memory.
        experiment_summary: LLM-generated summary of experiment output.
                            Included in memory for retry attempts.
        parser_retry_count: Tracks retry attempts for parser code.
                            Compared against max (5) to prevent infinite loops.
        parse_plan: LLM-generated plan for parser code.
                    Used for debugging.
        parse_code: Generated parser code to extract metrics.
                     Written to baseline_parser.py and executed.
        parse_deps: Python dependencies for parser code.
                     Installed before execution.
        parse_stdout: Standard output from parser execution.
                      Contains extracted metrics in structured format.
        parse_stderr: Standard error from parser execution.
                      Used to detect bugs.
        parse_returncode: Exit code from parser execution.
                           Non-zero indicates failure.
        parse_filename: Filename where parser code was saved.
                        Used for debugging.
        parse_is_bug: Whether parser output indicates a bug.
                      True triggers re-generation with error context.
        parse_summary: LLM-generated summary of parser output.
                       Included in memory for retry attempts.
        notes: Learnings extracted after successful run.
               Created by _create_notes() LLM call, appended to state.
    """

    # inputs
    cwd: Path
    task: utils.Task
    idea: utils.Idea
    research: str

    # generated by `node_define_metrics`
    metrics: list[utils.Metric] = []

    # counts how many times we tried to code the experiment
    experiment_retry_count: int = 0

    # generated by `node_code_experiment`
    experiment_plan: str | None = None
    experiment_code: str | None = None
    experiment_deps: list[str] = []

    # generated by `node_exec_experiment`
    experiment_stdout: str | None = None
    experiment_stderr: str | None = None
    experiment_returncode: int | None = None
    experiment_filename: str | None = None

    # generated by `node_parse_experiment_output`
    experiment_is_bug: bool | None = None
    experiment_summary: str | None = None

    # counts how many times we tried to code the parser
    parser_retry_count: int = 0

    # generated by `node_code_metrics_parser`
    parse_plan: str | None = None
    parse_code: str | None = None
    parse_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parse_stdout: str | None = None
    parse_stderr: str | None = None
    parse_returncode: int | None = None
    parse_filename: str | None = None

    # generated by `node_parse_metrics_output`
    parse_is_bug: bool | None = None
    parse_summary: str | None = None

    # notes accumulated from experiment output parsing
    notes: Annotated[list[str], operator.add] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_baseline_define_metrics(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_define_metrics")

    class Schema(BaseModel):
        metrics: list[utils.Metric]

    prompt = prompts.build_prompt_baseline_metrics(
        state.task, state.idea, state.research
    )
    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"metrics: {[m.name for m in response.metrics]}")
    logger.info("Finished node_baseline_define_metrics")
    return {"metrics": response.metrics}


async def node_baseline_code_experiment(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_code_experiment")

    class Schema(BaseModel):
        plan: str
        code: str
        dependencies: list[str]

    memory = ""

    if state.experiment_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.experiment_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"<CODE>\n{state.experiment_code or 'NA'}\n</CODE>\n\n"
        memory += "Previous dependencies:\n\n"
        memory += (
            f"<DEPENDENCIES>\n{state.experiment_deps or 'NA'}\n</DEPENDENCIES>\n\n"
        )
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"<STDOUT>\n{state.experiment_stdout or 'NA'}\n</STDOUT>\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"<STDERR>\n{state.experiment_stderr or 'NA'}\n</STDERR>\n\n"

    prompt = prompts.build_prompt_baseline_code(
        state.task,
        state.metrics,
        memory,
        state.idea,
        state.research,
        notes=state.notes,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"experiment_plan: {response.plan[:32]!r}")
    logger.debug(f"experiment_code: {response.code[:32]!r}")
    logger.debug(f"experiment_deps: {response.dependencies}")
    logger.debug(f"experiment_retry_count: {state.experiment_retry_count + 1}")

    logger.info("Finished node_baseline_code_experiment")
    return {
        "experiment_plan": response.plan,
        "experiment_code": response.code,
        "experiment_deps": response.dependencies,
        "experiment_retry_count": state.experiment_retry_count + 1,
    }


async def node_baseline_exec_experiment(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_exec_experiment")

    response = await utils.exec_code(
        state.cwd,
        "baseline.py",
        state.experiment_code or "NA",
        state.experiment_deps or [],
    )

    logger.debug(f"experiment_stdout: {response.stdout[:32]!r}")
    logger.debug(f"experiment_stderr: {response.stderr[:32]!r}")
    logger.debug(f"experiment_returncode: {response.returncode}")
    logger.debug(f"experiment_filename: {response.filename}")

    logger.info("Finished node_baseline_exec_experiment")
    return {
        "experiment_stdout": response.stdout,
        "experiment_stderr": response.stderr,
        "experiment_returncode": response.returncode,
        "experiment_filename": response.filename,
    }


async def node_baseline_parse_experiment_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_parse_experiment_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_baseline_code_output(
        state.task,
        state.experiment_code or "",
        state.experiment_stdout or "",
        state.experiment_stderr or "",
        state.idea,
        notes=state.notes,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"experiment_is_bug: {response.is_bug}")
    logger.debug(f"experiment_summary: {response.summary[:32]!r}")

    # Create note summarizing experiment results
    note = await _create_notes(state, runtime)

    logger.info("Finished node_baseline_parse_experiment_output")
    return {
        "experiment_is_bug": response.is_bug,
        "experiment_summary": response.summary,
        "notes": [note],
    }


async def node_baseline_should_retry_code_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_baseline_code_experiment", "node_baseline_code_metrics_parser", "__end__"]:
    logger.info("Starting node_baseline_should_retry_code_from_output")

    if state.experiment_retry_count > 5:
        logger.info("Max retry count reached, going to `__end__`")
        return "__end__"

    if state.experiment_is_bug is True:
        logger.info("Going to `node_baseline_code_experiment`")
        return "node_baseline_code_experiment"

    logger.info("Going to `node_baseline_code_metrics_parser`")
    return "node_baseline_code_metrics_parser"


async def node_baseline_code_metrics_parser(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_code_metrics_parser")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    memory = ""
    if state.parse_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.parse_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"<CODE>\n{state.parse_code or 'NA'}\n</CODE>\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"<DEPENDENCIES>\n{state.parse_deps or 'NA'}\n</DEPENDENCIES>\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"<STDOUT>\n{state.parse_stdout or 'NA'}\n</STDOUT>\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"<STDERR>\n{state.parse_stderr or 'NA'}\n</STDERR>\n\n"

    prompt = prompts.build_prompt_baseline_parser_code(
        state.experiment_code or "NA",
        memory=memory,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"parse_code: {response.code[:32]!r}")
    logger.debug(f"parse_plan: {response.plan[:32]!r}")
    logger.debug(f"parse_deps: {response.dependencies}")
    logger.debug(f"parser_retry_count: {state.parser_retry_count + 1}")

    logger.info("Finished node_baseline_code_metrics_parser")
    return {
        "parse_code": response.code,
        "parse_plan": response.plan,
        "parse_deps": response.dependencies,
        "parser_retry_count": state.parser_retry_count + 1,
    }


async def node_baseline_exec_metrics_parser(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_exec_metrics_parser")

    response = await utils.exec_code(
        state.cwd,
        "baseline_parser.py",
        state.parse_code or "NA",
        state.parse_deps or [],
    )

    logger.debug(f"parse_stdout: {response.stdout[:32]!r}")
    logger.debug(f"parse_stderr: {response.stderr[:32]!r}")
    logger.debug(f"parse_returncode: {response.returncode}")
    logger.debug(f"parse_filename: {response.filename}")

    logger.info("Finished node_baseline_exec_metrics_parser")
    return {
        "parse_stdout": response.stdout,
        "parse_stderr": response.stderr,
        "parse_returncode": response.returncode,
        "parse_filename": response.filename,
    }


async def node_baseline_parse_metrics_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_baseline_parse_metrics_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_baseline_parser_output(
        state.parse_code or "",
        state.parse_stdout or "",
        state.parse_stderr or "",
        state.idea,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"parse_is_bug: {response.is_bug}")
    logger.debug(f"parse_summary: {response.summary[:32]!r}")
    logger.info("Finished node_baseline_parse_metrics_output")
    return {
        "parse_is_bug": response.is_bug,
        "parse_summary": response.summary,
    }


async def node_baseline_should_retry_parser_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_baseline_code_metrics_parser", "__end__"]:
    logger.info("Starting node_baseline_should_retry_parser_from_output")

    if state.parser_retry_count > 5:
        logger.info("Max retry count reached, going to `__end__`")
        return "__end__"

    if state.parse_is_bug is True:
        logger.info("Going to `node_baseline_code_metrics_parser`")
        return "node_baseline_code_metrics_parser"

    logger.info("Going to `__end__`")
    return "__end__"


async def _create_notes(state: State, runtime: Runtime[Context]) -> str:
    logger.info("Creating notes for baseline experiment")

    class Schema(BaseModel):
        note: str

    prompt = prompts.build_prompt_create_notes(state)
    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"note: {response.note[:64]!r}")
    return response.note


def build(
    checkpointer: Checkpointer = None,
) -> CompiledStateGraph[State, Context, State, State]:
    builder = StateGraph(state_schema=State, context_schema=Context)

    builder.add_node(
        "node_baseline_define_metrics",
        node_baseline_define_metrics,
    )
    builder.add_node(
        "node_baseline_code_experiment",
        node_baseline_code_experiment,
    )
    builder.add_node(
        "node_baseline_exec_experiment",
        node_baseline_exec_experiment,
    )
    builder.add_node(
        "node_baseline_parse_experiment_output",
        node_baseline_parse_experiment_output,
    )
    builder.add_node(
        "node_baseline_code_metrics_parser",
        node_baseline_code_metrics_parser,
    )
    builder.add_node(
        "node_baseline_exec_metrics_parser",
        node_baseline_exec_metrics_parser,
    )
    builder.add_node(
        "node_baseline_parse_metrics_output",
        node_baseline_parse_metrics_output,
    )

    builder.add_edge(
        START,
        "node_baseline_define_metrics",
    )
    builder.add_edge(
        "node_baseline_define_metrics",
        "node_baseline_code_experiment",
    )
    builder.add_edge(
        "node_baseline_code_experiment",
        "node_baseline_exec_experiment",
    )
    builder.add_edge(
        "node_baseline_exec_experiment",
        "node_baseline_parse_experiment_output",
    )
    builder.add_conditional_edges(
        "node_baseline_parse_experiment_output",
        node_baseline_should_retry_code_from_output,
        ["node_baseline_code_experiment", "node_baseline_code_metrics_parser", "__end__"],
    )
    builder.add_edge(
        "node_baseline_code_metrics_parser",
        "node_baseline_exec_metrics_parser",
    )
    builder.add_edge(
        "node_baseline_exec_metrics_parser",
        "node_baseline_parse_metrics_output",
    )
    builder.add_conditional_edges(
        "node_baseline_parse_metrics_output",
        node_baseline_should_retry_parser_from_output,
        ["node_baseline_code_metrics_parser", END],
    )

    return builder.compile(name="graph_baseline", checkpointer=checkpointer)  # type: ignore
