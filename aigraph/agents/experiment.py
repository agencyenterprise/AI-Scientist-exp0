import logging
from pathlib import Path
from typing import Any

from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from langgraph.types import Checkpointer
from pydantic import BaseModel

from aigraph.agents import coder

logger = logging.getLogger(__name__)

PROMPT_EXPERIMENT = """
You are an expert Python programmer and ML researcher.
Your task is to write a Python script that implements the following experiment:

Requirements:

1. The script must be self-contained.
2. It should save the results to a file named `results.json` in the current 
   directory.
3. It should print progress to stdout.

For the `dependencies` key in the output:

- MUST include dependencies for the third-party libraries you use.
- MUST NOT include standard library dependencies (e.g., json, os, sys, pathlib).

<experiment_description>
{prompt}
</experiment_description>
"""

PROMPT_PARSER = """
You are an expert Python programmer.

Your task is to write a Python script that parses the `results.json` file 
generated by the experiment and prints a summary.

For the `dependencies` key in the output:

- MUST include dependencies for the third-party libraries you use.
- MUST NOT include standard library dependencies (e.g., json, os, sys, pathlib).

<experiment_description>
{prompt}
</experiment_description>

<experiment_code>
{experiment_code}
</experiment_code>

<experiment_stdout>
{experiment_stdout}
</experiment_stdout>

Requirements:

1. Read `results.json`.
2. Extract key metrics or results relevant to the description.
3. Print the summary as a valid JSON object to stdout. Do not print anything 
   else to stdout (use stderr for logs).
"""

PROMPT_REVIEW = """
You are a code reviewer. Analyze the execution results.

- If the code ran successfully and produced the expected output (data in 
  `results.json`), give a high score (8-10).
- If the code failed or did not produce output, give a low score and explain 
  why.
"""


class State(BaseModel):
    # inputs
    cwd: Path
    prompt: str

    # outputs
    experiment_code: coder.Code | None = None
    experiment_execution: coder.Execution | None = None
    parser_code: coder.Code | None = None
    parser_execution: coder.Execution | None = None


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0
    max_attempts: int = 5


async def node_generate_experiment(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_generate_experiment")

    prompt_parser = PROMPT_EXPERIMENT.format(
        prompt=state.prompt,
    )

    coder_state = coder.State(
        cwd=state.cwd,
        prompt_code=prompt_parser,
        prompt_review=PROMPT_REVIEW,
    )

    coder_context = coder.Context(
        filename=Path("experiment.py"),
        model=runtime.context.model,
    )

    graph = coder.build(checkpointer=True)
    result = await graph.ainvoke(coder_state, context=coder_context)
    result = coder.State.model_validate(result)

    assert result.code is not None, "Code is required"
    assert result.execution is not None, "Execution is required"

    logger.debug(f"Experiment code: {result.code.code[:32]!r}")
    logger.debug(f"Experiment dependencies: {result.code.dependencies}")
    logger.debug(f"Experiment return code: {result.execution.returncode}")
    logger.debug(f"Experiment stdout: {result.execution.stdout[:32]!r}")
    logger.debug(f"Experiment stderr: {result.execution.stderr[:32]!r}")
    logger.debug(f"Experiment filename: {result.execution.filename}")

    logger.info("Finished node_generate_experiment")
    return {
        "experiment_code": result.code,
        "experiment_execution": result.execution,
    }


async def node_generate_parser(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_generate_parser")
    assert state.experiment_code, "Experiment code required"
    assert state.experiment_execution, "Experiment execution required"

    coder_state = coder.State(
        cwd=state.cwd,
        prompt_code=PROMPT_PARSER.format(
            prompt=state.prompt,
            experiment_code=state.experiment_code.code,
            experiment_stdout=state.experiment_execution.stdout,
        ),
        prompt_review=PROMPT_REVIEW,
    )

    coder_context = coder.Context(
        filename=Path("parser.py"),
        model=runtime.context.model,
    )

    graph = coder.build(checkpointer=True)
    result = await graph.ainvoke(coder_state, context=coder_context)
    result = coder.State.model_validate(result)

    assert result.code is not None, "Code is required"
    assert result.execution is not None, "Execution is required"

    logger.debug(f"Parser code: {result.code.code[:32]!r}")
    logger.debug(f"Parser dependencies: {result.code.dependencies}")
    logger.debug(f"Parser return code: {result.execution.returncode}")
    logger.debug(f"Parser stdout: {result.execution.stdout[:32]!r}")
    logger.debug(f"Parser stderr: {result.execution.stderr[:32]!r}")
    logger.debug(f"Parser filename: {result.execution.filename}")

    logger.info("Finished node_generate_parser")
    return {
        "parser_code": result.code,
        "parser_execution": result.execution,
    }


def build(
    checkpointer: Checkpointer = None,
) -> CompiledStateGraph[State, Context, State, State]:
    """Build the experiment agent graph"""
    builder = StateGraph(state_schema=State, context_schema=Context)

    builder.add_node("node_generate_experiment", node_generate_experiment)
    builder.add_node("node_generate_parser", node_generate_parser)

    builder.add_edge(START, "node_generate_experiment")
    builder.add_edge("node_generate_experiment", "node_generate_parser")
    builder.add_edge("node_generate_parser", END)

    return builder.compile(name="graph_experiment", checkpointer=checkpointer)  # type: ignore
