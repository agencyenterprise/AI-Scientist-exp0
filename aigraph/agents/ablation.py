import logging
import operator
from pathlib import Path
from typing import Annotated, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.errors import GraphRecursionError
from langgraph.graph import START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import ablation_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task
    code: str

    ablations: Annotated[list[utils.Ablation], operator.add] = []
    last_ablation: utils.Ablation | None = None

    # counts how many times we tried to code the ablation
    ablation_retry_count: int = 0

    # generated by `node_code_ablation`
    ablation_code: str | None = None
    ablation_plan: str | None = None
    ablation_deps: list[str] = []

    # generated by `node_exec_ablation`
    ablation_returncode: int | None = None
    ablation_stdout: str | None = None
    ablation_stderr: str | None = None
    ablation_filename: str | None = None

    # generated by `node_parse_ablation_output`
    ablation_is_bug: bool | None = None
    ablation_summary: str | None = None
    
    # counts how many times we tried to code the parser
    parser_retry_count: int = 0

    # generated by `node_code_metrics_parser`
    parser_plan: str | None = None
    parser_code: str | None = None
    parser_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parser_stdout: str | None = None
    parser_stderr: str | None = None
    parser_returncode: int | None = None
    parser_filename: str | None = None
    
    # generated by `node_parse_metrics_output`
    parse_is_bug: bool | None = None
    parse_summary: str | None = None

    parse_valid_metrics_received: bool | None = None
    parse_metric_names: list[utils.MetricValue] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_ablation_propose_ablation(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_ablation_propose_ablation")

    class Schema(BaseModel):
        name: str
        description: str

    prompt = prompts.build_prompt_propose_ablation(
        code=state.code,
        ablations=[i.name for i in state.ablations],
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    ablation = utils.Ablation(
        name=response.name,
        description=response.description,
    )
    state.last_ablation = ablation
    state.ablations = [ablation]

    logger.info("Finished node_ablation_propose_ablation")
    return state


async def node_ablation_code_ablation(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_ablation_code_ablation")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    if state.ablation_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""

    if state.ablation_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.ablation_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"```python\n{state.ablation_code or 'NA'}\n```\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"```\n{state.ablation_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"```\n{state.ablation_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"```\n{state.ablation_stderr or 'NA'}\n```\n\n"

    assert state.last_ablation, "last_ablation is required"

    prompt = prompts.build_prompt_code_ablation(
        name=state.last_ablation.name,
        description=state.last_ablation.description,
        code=state.code,
        memory=memory,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.ablation_code = response.code
    state.ablation_plan = response.plan
    state.ablation_deps = response.dependencies
    state.ablation_retry_count += 1

    logger.debug(f"ablation_plan: {state.ablation_plan[:32]!r}")
    logger.debug(f"ablation_code: {state.ablation_code[:32]!r}")
    logger.debug(f"ablation_deps: {state.ablation_deps}")
    logger.debug(f"ablation_retry_count: {state.ablation_retry_count}")

    logger.info("Finished node_ablation_code_ablation")
    return state


async def node_ablation_exec_ablation(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_ablation_exec_ablation")
    assert state.ablation_code, "ablation_code is required"

    result = await utils.exec_code(
        state.cwd, 
        'ablation.py', 
        state.ablation_code, 
        state.ablation_deps
    )

    state.ablation_stdout = result.stdout
    state.ablation_stderr = result.stderr
    state.ablation_returncode = result.returncode
    state.ablation_filename = result.filename

    logger.debug(f"ablation_stdout: {state.ablation_stdout[:32]!r}")
    logger.debug(f"ablation_stderr: {state.ablation_stderr[:32]!r}")
    logger.debug(f"ablation_returncode: {state.ablation_returncode}")
    logger.debug(f"ablation_filename: {state.ablation_filename}")

    logger.info("Finished node_ablation_exec_ablation")
    return state


async def node_ablation_parse_ablation_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_ablation_parse_ablation_output")
    assert state.ablation_code, "ablation_code is required"

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_ablation_output(
        state.task,
        state.ablation_code,
        state.ablation_stdout or "",
        state.ablation_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.ablation_is_bug = response.is_bug
    state.ablation_summary = response.summary

    logger.debug(f"ablation_is_bug: {state.ablation_is_bug}")
    logger.debug(f"ablation_summary: {state.ablation_summary[:32]!r}")

    logger.info(f"Finished node_ablation_parse_ablation_output. Is bug: {response.is_bug}")
    return state


async def node_ablation_should_retry_code_from_ablation_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_ablation_code_ablation", "node_ablation_code_metrics_parser"]:
    logger.info("Starting node_ablation_should_retry_code_from_ablation_output")

    if state.ablation_is_bug:
        return "node_ablation_code_ablation"
    return "node_ablation_code_metrics_parser"


async def node_ablation_code_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_ablation_code_metrics_parser")
    assert state.ablation_code, "ablation_code is required"

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    if state.parser_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""
    if state.parse_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.parse_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"```python\n{state.parser_code or 'NA'}\n```\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"```\n{state.parser_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"```\n{state.parser_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"```\n{state.parser_stderr or 'NA'}\n```\n\n"

    prompt = prompts.build_prompt_ablation_parser_code(
        state.ablation_code,
        memory=memory
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parser_code = response.code
    state.parser_plan = response.plan
    state.parser_deps = response.dependencies
    state.parser_retry_count += 1

    logger.debug(f"parser_code: {state.parser_code[:32]!r}")
    logger.debug(f"parser_plan: {state.parser_plan[:32]!r}")
    logger.debug(f"parser_deps: {state.parser_deps}")
    logger.debug(f"parser_retry_count: {state.parser_retry_count}")

    logger.info("Finished node_ablation_code_metrics_parser")
    return state


async def node_ablation_exec_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_ablation_exec_metrics_parser")
    assert state.parser_code, "parser_code is required"

    result = await utils.exec_code(
        state.cwd,
        'ablation_parser.py',
        state.parser_code,
        state.parser_deps,
    )

    state.parser_stdout = result.stdout
    state.parser_stderr = result.stderr
    state.parser_returncode = result.returncode
    state.parser_filename = result.filename

    logger.debug(f"parser_stdout: {state.parser_stdout[:32]!r}")
    logger.debug(f"parser_stderr: {state.parser_stderr[:32]!r}")
    logger.debug(f"parser_returncode: {state.parser_returncode}")
    logger.debug(f"parser_filename: {state.parser_filename}")

    logger.info("Finished node_ablation_exec_metrics_parser")
    return state


async def node_ablation_parse_metrics_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_ablation_parse_metrics_output")
    assert state.parser_stdout, "parser_stdout is required"

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_ablation_parser_output(
        state.parser_code or "",
        state.parser_stdout or "",
        state.parser_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parse_is_bug = response.is_bug
    state.parse_summary = response.summary

    logger.debug(f"parse_is_bug: {state.parse_is_bug}")
    logger.debug(f"parse_summary: {state.parse_summary[:32]!r}")

    logger.info("Finished node_ablation_parse_metrics_output")
    return state


async def node_ablation_should_retry_parser_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_ablation_code_metrics_parser", '__end__']:
    logger.info("Starting node_ablation_should_retry_parser_from_output")

    if state.parse_is_bug is True:
        logger.info('Going to `node_ablation_code_metrics_parser`')
        return "node_ablation_code_metrics_parser"

    logger.info('Going to `__end__`')
    return '__end__'


def build() -> CompiledStateGraph[State, Context, State, State]:
    """Build the Stage 4 ablation studies graph."""
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node("node_ablation_propose_ablation", node_ablation_propose_ablation)
    builder.add_node("node_ablation_code_ablation", node_ablation_code_ablation)
    builder.add_node("node_ablation_exec_ablation", node_ablation_exec_ablation)
    builder.add_node("node_ablation_parse_ablation_output", node_ablation_parse_ablation_output)
    builder.add_node("node_ablation_code_metrics_parser", node_ablation_code_metrics_parser)
    builder.add_node("node_ablation_exec_metrics_parser", node_ablation_exec_metrics_parser)
    builder.add_node("node_ablation_parse_metrics_output", node_ablation_parse_metrics_output)

    # Add edges
    builder.add_edge(START, "node_ablation_propose_ablation")
    builder.add_edge("node_ablation_propose_ablation", "node_ablation_code_ablation")
    builder.add_edge("node_ablation_code_ablation", "node_ablation_exec_ablation")
    builder.add_edge("node_ablation_exec_ablation", "node_ablation_parse_ablation_output")
    builder.add_conditional_edges(
        "node_ablation_parse_ablation_output",
        node_ablation_should_retry_code_from_ablation_output,
    )
    builder.add_edge("node_ablation_code_metrics_parser", "node_ablation_exec_metrics_parser")
    builder.add_edge("node_ablation_exec_metrics_parser", "node_ablation_parse_metrics_output")
    builder.add_conditional_edges(
        "node_ablation_parse_metrics_output",
        node_ablation_should_retry_parser_from_output,
    )

    return builder.compile(name="graph_ablation")  # type: ignore
