import logging
import operator
from pathlib import Path
from typing import Annotated, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.errors import GraphRecursionError
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import ablation_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task
    code: str

    ablations: Annotated[list[utils.Ablation], operator.add] = []
    last_ablation: utils.Ablation | None = None

    # counts how many times we tried to code the ablation
    ablation_retry_count: int = 0

    # generated by `node_code_ablation`
    ablation_code: str | None = None
    ablation_plan: str | None = None
    ablation_deps: list[str] = []

    # generated by `node_exec_ablation`
    ablation_returncode: int | None = None
    ablation_stdout: str | None = None
    ablation_stderr: str | None = None
    ablation_filename: str | None = None

    # generated by `node_parse_ablation_output`
    ablation_is_bug: bool | None = None
    ablation_summary: str | None = None
    
    # counts how many times we tried to code the parser
    parser_retry_count: int = 0

    # generated by `node_code_metrics_parser`
    parser_plan: str | None = None
    parser_code: str | None = None
    parser_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parser_stdout: str | None = None
    parser_stderr: str | None = None
    parser_returncode: int | None = None
    parser_filename: str | None = None
    
    # generated by `node_parse_metrics_output`
    parse_is_bug: bool | None = None
    parse_summary: str | None = None

    parse_valid_metrics_received: bool | None = None
    parse_metric_names: list[utils.MetricValue] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_propose_ablation(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_propose_ablation")

    class Schema(BaseModel):
        name: str
        description: str

    prompt = prompts.build_prompt_propose_ablation(
        code=state.code,
        ablations=[i.name for i in state.ablations],
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    ablation = utils.Ablation(
        name=response.name,
        description=response.description,
    )
    state.last_ablation = ablation
    state.ablations = [ablation]

    logger.info("node_propose_ablation completed")
    return state


async def node_code_ablation(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_ablation")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    if state.ablation_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""

    if state.ablation_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.ablation_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"```python\n{state.ablation_code or 'NA'}\n```\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"```\n{state.ablation_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"```\n{state.ablation_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"```\n{state.ablation_stderr or 'NA'}\n```\n\n"

    assert state.last_ablation, "last_ablation is required"

    prompt = prompts.build_prompt_code_ablation(
        name=state.last_ablation.name,
        description=state.last_ablation.description,
        code=state.code,
        memory=memory,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.ablation_code = response.code
    state.ablation_plan = response.plan
    state.ablation_deps = response.dependencies
    state.ablation_retry_count += 1

    logger.info("node_code_ablation completed")
    return state


async def node_exec_ablation(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_exec_ablation")
    assert state.ablation_code, "ablation_code is required"

    result = await utils.exec_code(
        state.cwd, 
        'ablation.py', 
        state.ablation_code, 
        state.ablation_deps
    )

    state.ablation_stdout = result.stdout
    state.ablation_stderr = result.stderr
    state.ablation_returncode = result.returncode
    state.ablation_filename = result.filename

    logger.info("node_exec_ablation completed")
    return state


async def node_parse_ablation_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_parse_ablation_output")
    assert state.ablation_code, "ablation_code is required"

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_ablation_output(
        state.task,
        state.ablation_code,
        state.ablation_stdout or "",
        state.ablation_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.ablation_is_bug = response.is_bug
    state.ablation_summary = response.summary

    logger.info(f"node_parse_ablation_output completed. Is bug: {response.is_bug}")
    return state


async def node_should_retry_code_from_ablation_output(
    state: State, runtime: Runtime[Context]
) -> Literal["code_ablation", "code_metrics_parser"]:
    logger.info("Starting node_should_retry_code_from_ablation_output")

    if state.ablation_is_bug:
        return "code_ablation"
    return "code_metrics_parser"


async def node_code_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_metrics_parser")
    assert state.ablation_code, "ablation_code is required"

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    if state.parser_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""
    if state.parse_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.parse_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"```python\n{state.parser_code or 'NA'}\n```\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"```\n{state.parser_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"```\n{state.parser_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"```\n{state.parser_stderr or 'NA'}\n```\n\n"

    prompt = prompts.build_prompt_ablation_parser_code(
        state.ablation_code,
        memory=memory
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parser_code = response.code
    state.parser_plan = response.plan
    state.parser_deps = response.dependencies
    state.parser_retry_count += 1

    logger.info("node_code_metrics_parser completed")
    return state


async def node_exec_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_exec_metrics_parser")
    assert state.parser_code, "parser_code is required"

    result = await utils.exec_code(
        state.cwd,
        'ablation_parser.py',
        state.parser_code,
        state.parser_deps,
    )

    state.parser_stdout = result.stdout
    state.parser_stderr = result.stderr
    state.parser_returncode = result.returncode
    state.parser_filename = result.filename

    logger.info("node_exec_metrics_parser completed")
    return state


async def node_parse_metrics_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_parse_metrics_output")
    assert state.parser_stdout, "parser_stdout is required"

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_ablation_parser_output(
        state.parser_code or "",
        state.parser_stdout or "",
        state.parser_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parse_is_bug = response.is_bug
    state.parse_summary = response.summary

    logger.info("node_parse_metrics_output completed")
    return state


async def node_should_retry_parser_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["code_metrics_parser", '__end__']:
    logger.info("Starting node_should_retry_parser_from_output")

    if state.parse_is_bug is True:
        logger.info('Going to `code_metrics_parser`')
        return "code_metrics_parser"

    logger.info('Going to `__end__`')
    return '__end__'


def build() -> CompiledStateGraph[State, Context, State, State]:
    """Build the Stage 4 ablation studies graph."""
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node("propose_ablation", node_propose_ablation)
    builder.add_node("code_ablation", node_code_ablation)
    builder.add_node("exec_ablation", node_exec_ablation)
    builder.add_node("parse_ablation_output", node_parse_ablation_output)
    builder.add_node("code_metrics_parser", node_code_metrics_parser)
    builder.add_node("exec_metrics_parser", node_exec_metrics_parser)
    builder.add_node("parse_metrics_output", node_parse_metrics_output)

    # Add edges
    builder.add_edge(START, "propose_ablation")
    builder.add_edge("propose_ablation", "code_ablation")
    builder.add_edge("code_ablation", "exec_ablation")
    builder.add_edge("exec_ablation", "parse_ablation_output")
    builder.add_conditional_edges(
        "parse_ablation_output",
        node_should_retry_code_from_ablation_output,
    )
    builder.add_edge("code_metrics_parser", "exec_metrics_parser")
    builder.add_edge("exec_metrics_parser", "parse_metrics_output")
    builder.add_conditional_edges(
        "parse_metrics_output",
        node_should_retry_parser_from_output,
    )

    return builder.compile()  # type: ignore
