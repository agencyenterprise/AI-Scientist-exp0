import base64
import logging
import operator
from pathlib import Path
from typing import Annotated, Any

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.errors import GraphRecursionError
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from langgraph.types import Send
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import plotting_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task
    code: str  # The experiment code that generated the data

    # counts how many times we tried to code the plotting
    plotting_retry_count: int = 0

    # generated by `node_code_plotting`
    plotting_plan: str | None = None
    plotting_code: str | None = None
    plotting_deps: list[str] = []

    # generated by `node_exec_plotting`
    plotting_stdout: str | None = None
    plotting_stderr: str | None = None
    plotting_returncode: int | None = None
    plotting_filename: str | None = None

    # generated by `node_parse_plotting_output`
    plotting_is_bug: bool | None = None
    plotting_summary: str | None = None

    plots: Annotated[set[utils.Plot], operator.or_] = set()


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_plotting_code_plotting(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_plotting_code_plotting")

    class Schema(BaseModel):
        plan: str
        code: str
        dependencies: list[str]

    if state.plotting_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""

    if state.plotting_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.plotting_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"```python\n{state.plotting_code or 'NA'}\n```\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"```\n{state.plotting_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"```\n{state.plotting_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"```\n{state.plotting_stderr or 'NA'}\n```\n\n"

    prompt = prompts.build_prompt_plotting_code(
        state.task,
        state.code,
        memory,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.plotting_plan = response.plan
    state.plotting_code = response.code
    state.plotting_deps = response.dependencies
    state.plotting_retry_count += 1

    logger.debug(f"plotting_plan: {state.plotting_plan[:32]!r}")
    logger.debug(f"plotting_code: {state.plotting_code[:32]!r}")
    logger.debug(f"plotting_deps: {state.plotting_deps}")
    logger.debug(f"plotting_retry_count: {state.plotting_retry_count}")

    logger.info("Finished node_plotting_code_plotting")
    return state


async def node_plotting_exec_plotting(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_plotting_exec_plotting")
    assert state.plotting_code, "plotting_code is required"

    response = await utils.exec_code(
        state.cwd,
        "plotting.py",
        state.plotting_code,
        state.plotting_deps,
    )

    state.plotting_stdout = response.stdout
    state.plotting_stderr = response.stderr
    state.plotting_returncode = response.returncode
    state.plotting_filename = response.filename

    logger.debug(f"plotting_stdout: {state.plotting_stdout[:32]!r}")
    logger.debug(f"plotting_stderr: {state.plotting_stderr[:32]!r}")
    logger.debug(f"plotting_returncode: {state.plotting_returncode}")
    logger.debug(f"plotting_filename: {state.plotting_filename}")

    logger.info("Finished node_plotting_exec_plotting")
    return state


async def node_plotting_parse_plotting_output(
    state: State, runtime: Runtime[Context]
) -> State:
    logger.info("Starting node_plotting_parse_plotting_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_plotting_output(
        state.task,
        state.plotting_code or "",
        state.plotting_stdout or "",
        state.plotting_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.plotting_is_bug = response.is_bug
    state.plotting_summary = response.summary

    logger.debug(f"plotting_is_bug: {state.plotting_is_bug}")
    logger.debug(f"plotting_summary: {state.plotting_summary[:32]!r}")

    logger.info("Finished node_plotting_parse_plotting_output")
    return state


class StateSinglePlot(BaseModel):
    task: utils.Task
    image: Path


async def node_plotting_prepare_analysis(
    state: State, runtime: Runtime[Context]
) -> list[Send]:
    logger.info("Starting node_plotting_prepare_analysis")

    pngs = sorted(list(state.cwd.glob("*.png")))
    for png in pngs:
        logger.debug(f"Found PNG file: {png}")

    sends: list[Send] = []
    for png in pngs:
        st = StateSinglePlot(task=state.task, image=png)
        sends.append(Send("node_plotting_analyze_single_plot", st))

    return sends


async def node_plotting_analyze_single_plot(
    state: StateSinglePlot, runtime: Runtime[Context]
) -> dict:
    logger.info(f"Starting node_plotting_analyze_single_plot for {state.image}")

    class Schema(BaseModel):
        analysis: str

    prompt = prompts.build_prompt_analyze_plots(state.task)

    with open(state.image, "rb") as f:
        data = f.read()
        data = base64.b64encode(data)

    messages = [
        {
            "role": "system",
            "content": prompt,
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": f"Analyze this plot: {state.image.name}",
                },
                {
                    "type": "image",
                    "base64": data.decode("utf-8"),
                    "mime_type": "image/png",
                },
            ],
        },
    ]

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(messages)  # type: ignore

    logger.debug(f"image: {state.image}")
    logger.debug(f"analysis: {response.analysis[:32]!r}")

    logger.info(f"Finished node_plotting_analyze_single_plot for {state.image}")

    plot = utils.Plot(path=state.image, analysis=response.analysis)
    return {"plots": set([plot])}


def build() -> CompiledStateGraph[State, Context]:
    builder = StateGraph(State, Context)

    # Add nodes
    builder.add_node("node_plotting_code_plotting", node_plotting_code_plotting)
    builder.add_node("node_plotting_exec_plotting", node_plotting_exec_plotting)
    builder.add_node(
        "node_plotting_parse_plotting_output", node_plotting_parse_plotting_output
    )
    builder.add_node(
        "node_plotting_analyze_single_plot", node_plotting_analyze_single_plot
    )

    # Add edges
    builder.add_edge(START, "node_plotting_code_plotting")
    builder.add_edge("node_plotting_code_plotting", "node_plotting_exec_plotting")
    builder.add_edge(
        "node_plotting_exec_plotting", "node_plotting_parse_plotting_output"
    )
    builder.add_conditional_edges(
        "node_plotting_parse_plotting_output",
        node_plotting_prepare_analysis,
        ["node_plotting_analyze_single_plot"],
    )
    builder.add_edge("node_plotting_analyze_single_plot", END)

    return builder.compile(name="graph_plotting", checkpointer=True) # type: ignore
