import logging
import operator
from pathlib import Path
from typing import Annotated, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.errors import GraphRecursionError
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import plotting_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task
    code: str  # The experiment code that generated the data

    # counts how many times we tried to code the plotting
    plotting_retry_count: int = 0

    # generated by `node_code_plotting`
    plotting_plan: str | None = None
    plotting_code: str | None = None
    plotting_deps: list[str] = []

    # generated by `node_exec_plotting`
    plotting_stdout: str | None = None
    plotting_stderr: str | None = None
    plotting_returncode: int | None = None
    plotting_filename: str | None = None

    # generated by `node_parse_plotting_output`
    plotting_is_bug: bool | None = None
    plotting_summary: str | None = None


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_code_plotting(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_plotting")

    class Schema(BaseModel):
        plan: str
        code: str
        dependencies: list[str]

    if state.plotting_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""

    if state.plotting_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.plotting_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"```python\n{state.plotting_code or 'NA'}\n```\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"```\n{state.plotting_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"```\n{state.plotting_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"```\n{state.plotting_stderr or 'NA'}\n```\n\n"

    prompt = prompts.build_prompt_plotting_code(
        state.task,
        state.code,
        memory,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.plotting_plan = response.plan
    state.plotting_code = response.code
    state.plotting_deps = response.dependencies
    state.plotting_retry_count += 1

    logger.debug(f"plotting_plan: {state.plotting_plan[:32]!r}")
    logger.debug(f"plotting_code: {state.plotting_code[:32]!r}")
    logger.debug(f"plotting_deps: {state.plotting_deps}")
    logger.debug(f"plotting_retry_count: {state.plotting_retry_count}")

    logger.info("node_code_plotting completed")
    return state


async def node_exec_plotting(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_exec_plotting")
    assert state.plotting_code, "plotting_code is required"

    response = await utils.exec_code(
        state.cwd,
        'plotting.py',
        state.plotting_code,
        state.plotting_deps,
    )

    state.plotting_stdout = response.stdout
    state.plotting_stderr = response.stderr
    state.plotting_returncode = response.returncode
    state.plotting_filename = response.filename

    logger.debug(f"plotting_stdout: {state.plotting_stdout[:32]!r}")
    logger.debug(f"plotting_stderr: {state.plotting_stderr[:32]!r}")
    logger.debug(f"plotting_returncode: {state.plotting_returncode}")
    logger.debug(f"plotting_filename: {state.plotting_filename}")

    logger.info("node_exec_plotting completed")
    return state


async def node_parse_plotting_output(
    state: State, runtime: Runtime[Context]
) -> State:
    logger.info("Starting node_parse_plotting_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_plotting_output(
        state.task,
        state.plotting_code or "",
        state.plotting_stdout or "",
        state.plotting_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.plotting_is_bug = response.is_bug
    state.plotting_summary = response.summary

    logger.debug(f"plotting_is_bug: {state.plotting_is_bug}")
    logger.debug(f"plotting_summary: {state.plotting_summary[:32]!r}")

    logger.info("node_parse_plotting_output completed")
    return state


async def node_should_retry_code_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["code_plotting", "__end__"]:
    logger.info("Starting node_should_retry_code_from_output")

    if state.plotting_is_bug is True:
        logger.info('Going to `code_plotting`')
        return "code_plotting"

    logger.info('Going to `__end__`')
    return '__end__'


def build() -> CompiledStateGraph[State, Context, State, State]:
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node("code_plotting", node_code_plotting)
    builder.add_node("exec_plotting", node_exec_plotting)
    builder.add_node("parse_plotting_output", node_parse_plotting_output)

    # Add edges
    builder.add_edge(START, "code_plotting")
    builder.add_edge("code_plotting", "exec_plotting")
    builder.add_edge("exec_plotting", "parse_plotting_output")
    builder.add_conditional_edges("parse_plotting_output", node_should_retry_code_from_output)

    return builder.compile()  # type: ignore

