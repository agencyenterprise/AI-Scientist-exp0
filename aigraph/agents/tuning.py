import logging
from pathlib import Path
from typing import Any, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.errors import GraphRecursionError
from langgraph.graph import START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from langgraph.types import Checkpointer
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import summary
from aigraph.agents import tuning_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task
    code: str
    metrics: list[utils.Metric] = []
    cumulative_summary: str = ""
    baseline_results: str = ""  # baseline parser stdout for comparison
    experiment_plan_structured: str = ""  # structured experiment plan

    hyperparams: list[utils.Hyperparam] = []
    last_hyperparam: utils.Hyperparam | None = None

    # counts how many times we tried to code the tuning
    tuning_retry_count: int = 0

    # generated by `node_code_tuning`
    tuning_code: str | None = None
    tuning_plan: str | None = None
    tuning_deps: list[str] = []

    # generated by `node_exec_tuning`
    tuning_returncode: int | None = None
    tuning_stdout: str | None = None
    tuning_stderr: str | None = None
    tuning_filename: str | None = None

    # generated by `node_parse_tuning_output`
    tuning_is_bug: bool | None = None
    tuning_summary: str | None = None

    # counts how many times we tried to code the parser
    parser_retry_count: int = 0

    # generated by `node_code_metrics_parser`
    parser_plan: str | None = None
    parser_code: str | None = None
    parser_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parser_stdout: str | None = None
    parser_stderr: str | None = None
    parser_returncode: int | None = None
    parser_filename: str | None = None

    # generated by `node_parse_metrics_output`
    parse_is_bug: bool | None = None
    parse_summary: str | None = None
    parse_valid_metrics_received: bool | None = None
    parse_metric_names: list[utils.MetricValue] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_tuning_propose_hyperparam(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_propose_hyperparam")

    class Schema(BaseModel):
        name: str
        description: str

    prompt = prompts.build_prompt_tuning_propose(
        code=state.code,
        hyperparams=[i.name for i in state.hyperparams],
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    hp = utils.Hyperparam(
        name=response.name,
        description=response.description,
    )

    logger.debug(f"hyperparam_name: {hp.name}")
    logger.debug(f"hyperparam_description: {hp.description[:32]!r}")
    logger.info("Finished node_tuning_propose_hyperparam")
    return {
        "last_hyperparam": hp,
        "hyperparams": [hp],
    }


async def node_tuning_code_tuning(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_code_tuning")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    if state.tuning_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""

    if state.tuning_returncode is not None and state.tuning_returncode > 0:
        memory += "Previous code:\n"
        memory += f"```python\n{state.tuning_code or ''}\n```\n\n"
        memory += "Previous dependencies:\n"
        memory += f"```\n{state.tuning_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n"
        memory += f"```\n{state.tuning_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n"
        memory += f"```\n{state.tuning_stderr or 'NA'}\n```\n\n"

    if state.tuning_is_bug is True:
        memory += "Bug identified:\n"
        memory += f"{state.tuning_summary or 'NA'}\n\n"

    assert state.last_hyperparam, "last_hyperparam is required"

    prompt = prompts.build_prompt_tuning_code(
        task=state.task,
        metrics=state.metrics,
        name=state.last_hyperparam.name,
        description=state.last_hyperparam.description,
        code=state.code,
        memory=memory,
        cumulative_summary=state.cumulative_summary,
        baseline_results=state.baseline_results,
        experiment_plan=state.experiment_plan_structured,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"tuning_plan: {response.plan[:32]!r}")
    logger.debug(f"tuning_code: {response.code[:32]!r}")
    logger.debug(f"tuning_deps: {response.dependencies}")
    logger.debug(f"tuning_retry_count: {state.tuning_retry_count + 1}")

    logger.info("Finished node_tuning_code_tuning")
    return {
        "tuning_code": response.code,
        "tuning_plan": response.plan,
        "tuning_deps": response.dependencies,
        "tuning_retry_count": state.tuning_retry_count + 1,
    }


async def node_tuning_exec_tuning(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_exec_tuning")
    assert state.tuning_code, "tuning_code is required"

    result = await utils.exec_code(
        state.cwd, "tuning.py", state.tuning_code, state.tuning_deps
    )

    logger.debug(f"tuning_stdout: {result.stdout[:32]!r}")
    logger.debug(f"tuning_stderr: {result.stderr[:32]!r}")
    logger.debug(f"tuning_returncode: {result.returncode}")
    logger.debug(f"tuning_filename: {result.filename}")

    logger.info("Finished node_tuning_exec_tuning")
    return {
        "tuning_stdout": result.stdout,
        "tuning_stderr": result.stderr,
        "tuning_returncode": result.returncode,
        "tuning_filename": result.filename,
    }


async def node_tuning_summary(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_summary")

    summary_state = summary.State(
        task=state.task,
        metrics=state.metrics,
        code=state.tuning_code or "",
        stdout=state.tuning_stdout or "",
        stderr=state.tuning_stderr or "",
        existing_summary=state.cumulative_summary,
        parsed_summary=state.tuning_summary or "",
    )
    summary_context = summary.Context(
        model=runtime.context.model,
        temperature=runtime.context.temperature,
    )

    graph = summary.build(checkpointer=True)
    result = await graph.ainvoke(input=summary_state, context=summary_context)
    new_summary = result["new_summary"]

    if state.cumulative_summary:
        new_summary = f"{state.cumulative_summary}\n\n---\n\n{new_summary}"

    logger.info("Finished node_tuning_summary")
    return {"cumulative_summary": new_summary}


async def node_tuning_parse_tuning_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_parse_tuning_output")
    assert state.tuning_code, "tuning_code is required"

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_tuning_code_output(
        state.task,
        state.tuning_code,
        state.tuning_stdout or "",
        state.tuning_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"tuning_is_bug: {response.is_bug}")
    logger.debug(f"tuning_summary: {response.summary[:32]!r}")

    logger.info(f"Finished node_tuning_parse_tuning_output. Is bug: {response.is_bug}")
    return {
        "tuning_is_bug": response.is_bug,
        "tuning_summary": response.summary,
    }


async def node_tuning_should_retry_code_from_tuning_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_tuning_code_tuning", "node_tuning_code_metrics_parser"]:
    logger.info("Starting node_tuning_should_retry_code_from_tuning_output")

    if state.tuning_is_bug:
        return "node_tuning_code_tuning"
    return "node_tuning_code_metrics_parser"


async def node_tuning_code_metrics_parser(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_code_metrics_parser")
    assert state.tuning_code, "tuning_code is required"

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    if state.parser_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""
    if state.parse_is_bug is True:
        memory += "Bug identified:\n\n"
        memory += f"{state.parse_summary or 'NA'}\n\n"
        memory += "Previous code:\n\n"
        memory += f"```python\n{state.parser_code or 'NA'}\n```\n\n"
        memory += "Previous dependencies:\n\n"
        memory += f"```\n{state.parser_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n\n"
        memory += f"```\n{state.parser_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n\n"
        memory += f"```\n{state.parser_stderr or 'NA'}\n```\n\n"

    prompt = prompts.build_prompt_tuning_parser_code(
        state.tuning_code, 
        memory=memory,
        baseline_results=state.baseline_results,
        experiment_plan=state.experiment_plan_structured,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"parser_code: {response.code[:32]!r}")
    logger.debug(f"parser_plan: {response.plan[:32]!r}")
    logger.debug(f"parser_deps: {response.dependencies}")
    logger.debug(f"parser_retry_count: {state.parser_retry_count + 1}")

    logger.info("Finished node_tuning_code_metrics_parser")
    return {
        "parser_code": response.code,
        "parser_plan": response.plan,
        "parser_deps": response.dependencies,
        "parser_retry_count": state.parser_retry_count + 1,
    }


async def node_tuning_exec_metrics_parser(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_exec_metrics_parser")
    assert state.parser_code, "parser_code is required"

    result = await utils.exec_code(
        state.cwd,
        "tuning_parser.py",
        state.parser_code,
        state.parser_deps,
    )

    logger.debug(f"parser_stdout: {result.stdout[:32]!r}")
    logger.debug(f"parser_stderr: {result.stderr[:32]!r}")
    logger.debug(f"parser_returncode: {result.returncode}")
    logger.debug(f"parser_filename: {result.filename}")

    logger.info("Finished node_tuning_exec_metrics_parser")
    return {
        "parser_stdout": result.stdout,
        "parser_stderr": result.stderr,
        "parser_returncode": result.returncode,
        "parser_filename": result.filename,
    }


async def node_tuning_parse_metrics_output(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_tuning_parse_metrics_output")

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_tuning_parser_output(
        state.parser_code or "NA",
        state.parser_stdout or "NA",
        state.parser_stderr or "NA",
        original_code=state.tuning_code or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    logger.debug(f"parse_is_bug: {response.is_bug}")
    logger.debug(f"parse_summary: {response.summary[:32]!r}")
    logger.info("Finished node_tuning_parse_metrics_output")
    return {
        "parse_is_bug": response.is_bug,
        "parse_summary": response.summary,
    }


async def node_tuning_should_retry_parser_from_output(
    state: State, runtime: Runtime[Context]
) -> Literal["node_tuning_code_metrics_parser", "__end__"]:
    logger.info("Starting node_tuning_should_retry_parser_from_output")

    if state.parse_is_bug is True:
        logger.info("Going to `node_tuning_code_metrics_parser`")
        return "node_tuning_code_metrics_parser"

    logger.info("Going to `__end__`")
    return "__end__"


def build(
    checkpointer: Checkpointer = None,
) -> CompiledStateGraph[State, Context, State, State]:
    """Build the Stage 2 hyperparameter tuning graph."""
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node(
        "node_tuning_propose_hyperparam",
        node_tuning_propose_hyperparam,
    )
    builder.add_node(
        "node_tuning_code_tuning",
        node_tuning_code_tuning,
    )
    builder.add_node(
        "node_tuning_exec_tuning",
        node_tuning_exec_tuning,
    )
    builder.add_node(
        "node_tuning_summary",
        node_tuning_summary,
    )
    builder.add_node(
        "node_tuning_parse_tuning_output",
        node_tuning_parse_tuning_output,
    )
    builder.add_node(
        "node_tuning_code_metrics_parser",
        node_tuning_code_metrics_parser,
    )
    builder.add_node(
        "node_tuning_exec_metrics_parser",
        node_tuning_exec_metrics_parser,
    )
    builder.add_node(
        "node_tuning_parse_metrics_output",
        node_tuning_parse_metrics_output,
    )

    # Add edges
    builder.add_edge(
        START,
        "node_tuning_propose_hyperparam",
    )
    builder.add_edge(
        "node_tuning_propose_hyperparam",
        "node_tuning_code_tuning",
    )
    builder.add_edge(
        "node_tuning_code_tuning",
        "node_tuning_exec_tuning",
    )
    builder.add_edge(
        "node_tuning_exec_tuning",
        "node_tuning_parse_tuning_output",
    )
    builder.add_edge(
        "node_tuning_parse_tuning_output",
        "node_tuning_summary",
    )
    builder.add_conditional_edges(
        "node_tuning_summary",
        node_tuning_should_retry_code_from_tuning_output,
    )
    builder.add_edge(
        "node_tuning_code_metrics_parser",
        "node_tuning_exec_metrics_parser",
    )
    builder.add_edge(
        "node_tuning_exec_metrics_parser",
        "node_tuning_parse_metrics_output",
    )
    builder.add_conditional_edges(
        "node_tuning_parse_metrics_output",
        node_tuning_should_retry_parser_from_output,
    )

    return builder.compile(name="graph_tuning", checkpointer=checkpointer)  # type: ignore
