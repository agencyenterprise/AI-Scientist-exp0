import logging
import operator
from pathlib import Path
from typing import Annotated, Literal

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.errors import GraphRecursionError
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import tuning_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task
    code: str

    hyperparams: Annotated[set[utils.Hyperparam], operator.or_] = set()
    last_hyperparam: utils.Hyperparam | None = None

    # counts how many times we tried to code the tuning
    tuning_retry_count: int = 0

    # generated by `node_code_tuning`
    tuning_code: str | None = None
    tuning_plan: str | None = None
    tuning_deps: list[str] = []

    # generated by `node_exec_tuning`
    tuning_returncode: int | None = None
    tuning_stdout: str | None = None
    tuning_stderr: str | None = None
    tuning_filename: str | None = None

    # generated by `node_parse_tuning_output`
    tuning_is_bug: bool | None = None
    tuning_summary: str | None = None

    # generated by `node_code_metrics_parser`
    parser_plan: str | None = None
    parser_code: str | None = None
    parser_deps: list[str] = []

    # generated by `node_exec_metrics_parser`
    parser_stdout: str | None = None
    parser_stderr: str | None = None
    parser_returncode: int | None = None
    parser_filename: str | None = None

    # generated by `node_parse_metrics_output`
    parse_valid_metrics_received: bool | None = None
    parse_metric_names: list[utils.MetricValue] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_propose_hyperparam(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_propose_hyperparam")

    class Schema(BaseModel):
        name: str
        description: str

    prompt = prompts.build_prompt_tuning_propose(
        code=state.code,
        hyperparams=[i.name for i in state.hyperparams],
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore

    hp = utils.Hyperparam(
        name=response.name,
        description=response.description,
    )
    state.last_hyperparam = hp
    state.hyperparams = set([hp])

    logger.info("node_propose_hyperparam completed")
    return state


async def node_code_tuning(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_tuning")

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    if state.tuning_retry_count > 5:
        raise GraphRecursionError("Max retry count reached")

    memory = ""

    if state.tuning_returncode is not None and state.tuning_returncode > 0:
        memory += "Previous code:\n"
        memory += f"```python\n{state.tuning_code or ''}\n```\n\n"
        memory += "Previous dependencies:\n"
        memory += f"```\n{state.tuning_deps or 'NA'}\n```\n\n"
        memory += "Stdout of executing the previous code:\n"
        memory += f"```\n{state.tuning_stdout or 'NA'}\n```\n\n"
        memory += "Stderr of executing the previous code:\n"
        memory += f"```\n{state.tuning_stderr or 'NA'}\n```\n\n"

    if state.tuning_is_bug is True:
        memory += "Bug identified:\n"
        memory += f"{state.tuning_summary or 'NA'}\n\n"

    assert state.last_hyperparam, "last_hyperparam is required"

    prompt = prompts.build_prompt_tuning_code(
        task=state.task,
        name=state.last_hyperparam.name,
        description=state.last_hyperparam.description,
        code=state.code,
        memory=memory,
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.tuning_code = response.code
    state.tuning_plan = response.plan
    state.tuning_deps = response.dependencies
    state.tuning_retry_count += 1

    logger.info("node_code_tuning completed")
    return state


async def node_exec_tuning(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_exec_tuning")
    assert state.tuning_code, "tuning_code is required"

    result = await utils.exec_code(
        state.cwd, 
        'tuning.py', 
        state.tuning_code, 
        state.tuning_deps
    )

    state.tuning_stdout = result.stdout
    state.tuning_stderr = result.stderr
    state.tuning_returncode = result.returncode
    state.tuning_filename = result.filename

    logger.info("node_exec_tuning completed")
    return state


async def node_parse_tuning_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_parse_tuning_output")
    assert state.tuning_code, "tuning_code is required"

    class Schema(BaseModel):
        is_bug: bool
        summary: str

    prompt = prompts.build_prompt_tuning_code_output(
        state.task,
        state.tuning_code,
        state.tuning_stdout or "",
        state.tuning_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.tuning_is_bug = response.is_bug
    state.tuning_summary = response.summary

    logger.info(f"node_parse_tuning_output completed. Is bug: {response.is_bug}")
    return state


async def node_should_retry_code_from_tuning(
    state: State, runtime: Runtime[Context]
) -> Literal["code_tuning", "parse_tuning_output"]:
    logger.info("Starting node_should_retry_code_from_tuning")

    if state.tuning_returncode == 0:
        return "parse_tuning_output"
    return "code_tuning"


async def node_should_retry_code_from_tuning_output(
    state: State, runtime: Runtime[Context]
) -> Literal["code_tuning", "code_metrics_parser"]:
    logger.info("Starting node_should_retry_code_from_tuning_output")

    if state.tuning_is_bug:
        return "code_tuning"
    return "code_metrics_parser"


async def node_code_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_code_metrics_parser")
    assert state.tuning_code, "tuning_code is required"

    class Schema(BaseModel):
        code: str
        plan: str
        dependencies: list[str]

    prompt = prompts.build_prompt_tuning_parser_code(state.tuning_code)

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parser_code = response.code
    state.parser_plan = response.plan
    state.parser_deps = response.dependencies

    logger.info("node_code_metrics_parser completed")
    return state


async def node_exec_metrics_parser(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_exec_metrics_parser")
    assert state.parser_code, "parser_code is required"

    result = await utils.exec_code(
        state.cwd,
        'tuning_parser.py',
        state.parser_code,
        state.parser_deps,
    )

    state.parser_stdout = result.stdout
    state.parser_stderr = result.stderr
    state.parser_returncode = result.returncode
    state.parser_filename = result.filename

    logger.info("node_exec_metrics_parser completed")
    return state


async def node_parse_metrics_output(state: State, runtime: Runtime[Context]) -> State:
    logger.info("Starting node_parse_metrics_output")
    assert state.parser_stdout, "parser_stdout is required"

    class Schema(BaseModel):
        valid_metrics_received: bool
        metric_names: list[utils.MetricValue]

    prompt = prompts.build_prompt_tuning_parser_output(
        state.parser_code or "",
        state.parser_stdout or "",
        state.parser_stderr or "",
    )

    llms = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llms.ainvoke(prompt)  # type: ignore
    state.parse_valid_metrics_received = response.valid_metrics_received
    state.parse_metric_names = response.metric_names

    logger.info("node_parse_metrics_output completed")
    return state


def build() -> CompiledStateGraph[State, Context, State, State]:
    """Build the Stage 2 hyperparameter tuning graph."""
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node("propose_hyperparam", node_propose_hyperparam)
    builder.add_node("code_tuning", node_code_tuning)
    builder.add_node("exec_tuning", node_exec_tuning)
    builder.add_node("parse_tuning_output", node_parse_tuning_output)
    builder.add_node("code_metrics_parser", node_code_metrics_parser)
    builder.add_node("exec_metrics_parser", node_exec_metrics_parser)
    builder.add_node("parse_metrics_output", node_parse_metrics_output)

    # Add edges
    builder.add_edge(START, "propose_hyperparam")
    builder.add_edge("propose_hyperparam", "code_tuning")
    builder.add_edge("code_tuning", "exec_tuning")
    builder.add_conditional_edges(
        "exec_tuning",
        node_should_retry_code_from_tuning,
    )
    builder.add_conditional_edges(
        "parse_tuning_output",
        node_should_retry_code_from_tuning_output,
    )
    builder.add_edge("code_metrics_parser", "exec_metrics_parser")
    builder.add_edge("exec_metrics_parser", "parse_metrics_output")
    builder.add_edge("parse_metrics_output", END)

    return builder.compile()  # type: ignore

