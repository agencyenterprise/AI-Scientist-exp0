import logging
from pathlib import Path
from typing import Any

from langchain.chat_models import BaseChatModel, init_chat_model
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from langgraph.types import Checkpointer
from pydantic import BaseModel

from aigraph import utils
from aigraph.agents import ideas_prompts as prompts

logger = logging.getLogger(__name__)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task

    # generated by `node_generate_ideas`
    ideas: list[utils.Idea] = []


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.7

    num_ideas: int = 5

    @property
    def llm(self) -> BaseChatModel:
        return init_chat_model(model=self.model, temperature=self.temperature)


async def node_generate_ideas(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    logger.info("Starting node_generate_ideas")

    class Schema(BaseModel):
        ideas: list[utils.Idea]

    prompt = prompts.build_prompt_generate_ideas(
        state.task,
        runtime.context.num_ideas,
    )

    llm = runtime.context.llm.with_structured_output(Schema)
    response: Schema = await llm.ainvoke(prompt)  # type: ignore
    logger.debug(f"generated {len(response.ideas)} ideas")

    logger.info("Finished node_generate_ideas")
    return {"ideas": response.ideas}


def build(
    checkpointer: Checkpointer | None = None,
) -> CompiledStateGraph[State, Context, State, State]:
    builder = StateGraph(state_schema=State, context_schema=Context)

    builder.add_node("node_generate_ideas", node_generate_ideas)

    builder.add_edge(START, "node_generate_ideas")
    builder.add_edge("node_generate_ideas", END)

    return builder.compile(name="graph_ideas", checkpointer=checkpointer)  # type: ignore
