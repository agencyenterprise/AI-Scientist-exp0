import json
import logging
import uuid
from pathlib import Path
from typing import Annotated

import aiosqlite
from langchain_core.runnables import RunnableConfig
from langfuse.langchain import CallbackHandler
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.runtime import Runtime
from pydantic import AliasChoices, BaseModel, Field
from pydantic_settings import BaseSettings, CliApp, CliImplicitFlag, CliPositionalArg

from aigraph import log, utils
from aigraph.agents import ablation, baseline, plotting, tuning, writeup

logger = logging.getLogger(__name__)

task = utils.Task.model_validate(
    {
        "Name": "seeded_creativity_gpt5_family",
        "Title": "Do Random Word Seeds Increase Name Diversity in GPT-5.1 and GPT-5-mini?",
        "Short Hypothesis": "Providing random word seeds in the prompt increases the diversity of names generated by GPT-5.1 and GPT-5-mini, compared to the same prompt without seeds, even when using default sampling parameters. The seeds act as weak, noisy constraints that push the models into more varied regions of their naming space.",
        "Related Work": "",
        "Abstract": "We test whether adding random word seeds to prompts increases the diversity of generated names produced by GPT-5.1 and GPT-5-mini via the OpenAI API. In the baseline condition, the model receives a generic instruction to generate 100 names (e.g., product or brand names). In the seeded condition, the prompt includes a list of 10 random words sampled from a large English vocabulary (2000+ candidate words), explicitly described as loose inspiration. For both models, we repeat generation multiple times per condition using the API defaults (no manual changes to temperature, top_p, or similar sampling parameters) and measure diversity via simple lexical metrics (unique-name ratio, distinct-1/2, and character-level entropy). The goal is to generate a huge number of names (e.g., 5.000) by repeatidely calling the API. This experiment aims to quantify whether random seed prompts systematically expand the explored naming space versus a single repeated instruction prompt, and whether the effect differs between GPT-5.1 and GPT-5-mini.",
        "Code": 'import os\nimport random\nimport re\nimport math\nimport statistics\nfrom typing import List, Dict\n\nfrom openai import OpenAI\nfrom wordfreq import top_n_list\n\n# --- API setup ---\nOPENAI_API_KEY = "<OPENAI_API_KEY>"\n\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\n# --- Models to test ---\nMODELS = ["gpt-5.1", "gpt-5-mini"]\n\n# --- Prompts ---\nBASE_PROMPT = (\n    <<BASE PROMPT GOES HERE>>\n)\n\nSEEDED_PROMPT_TEMPLATE = (\n    <<SEEDED PROMPT GOES HERE>>\n)\n\n# --- Large pool of candidate seed words (2000+ using wordfreq) ---\n# We take the top-N most frequent English words and filter to simple alphabetic tokens.\nraw_words = top_n_list("en", 5000)  # 5000 > 2000; plenty of candidates\nCANDIDATE_SEED_WORDS = [w for w in raw_words if w.isalpha() and len(w) > 2]\n\nif len(CANDIDATE_SEED_WORDS) < 2000:\n    raise RuntimeError(\n        f"Expected at least 2000 candidate seed words, got {len(CANDIDATE_SEED_WORDS)}."\n    )\n\nprint(f" Loaded {len(CANDIDATE_SEED_WORDS)} candidate seed words.")\n\n# --- Generation helper (defaults: no manual temperaturetop_p tweaks) ---\ndef generate_text(model: str, prompt: str) -> str:\n    """\n    Call the OpenAI Responses API with default sampling parameters.\n    We do not set temperature, top_p, or other sampling knobs explicitly.\n    """\n    response = client.responses.create(\n        model=model,\n        input=prompt,\n    )\n    # responses.create has a convenience accessor for the full text output\n    return response.output_text.strip()\n\n\nNAME_LINE_RE = re.compile(r"^\\s*\\d+[\\).:-]\\s*(.+)$")\n\n\ndef parse_names_from_output(text: str) -> List[str]:\n    names: List[str] = []\n    for line in text.splitlines():\n        line = line.strip()\n        if not line:\n            continue\n        m = NAME_LINE_RE.match(line)\n        if m:\n            names.append(m.group(1).strip())\n        else:\n            # fallback: accept short-ish non-empty lines as names\n            if 1 <= len(line.split()) <= 6:\n                names.append(line)\n    return names\n\n\ndef run_condition(\n    model: str, condition: str, n_runs: int = 10, seed_len: int = 10\n) -> Dict:\n    """\n    Run one experimental condition (baseline or seeded) multiple times for a given model.\n    """\n    all_names: List[str] = []\n    per_run_counts = []\n    rng = random.Random(42)  # fixed for reproducible seed choices\n\n    for i in range(n_runs):\n        if condition == "baseline":\n            prompt = BASE_PROMPT\n        elif condition == "seeded":\n            seeds = rng.sample(CANDIDATE_SEED_WORDS, k=seed_len)\n            prompt = SEEDED_PROMPT_TEMPLATE.format(seed_words=", ".join(seeds))\n        else:\n            raise ValueError(f"Unknown condition: {condition}")\n\n        print(f" Model={model} | Run {i+1}/{n_runs} | condition={condition}")\n        out_text = generate_text(model, prompt)\n        names = parse_names_from_output(out_text)\n        all_names.extend(names)\n        per_run_counts.append(len(names))\n\n    return {\n        "model": model,\n        "condition": condition,\n        "all_names": all_names,\n        "per_run_counts": per_run_counts,\n    }\n\n\n# --- Diversity metrics ---\ndef tokenize_for_diversity(name: str) -> List[str]:\n    return name.lower().split()\n\n\ndef distinct_n(names: List[str], n: int = 1) -> float:\n    tokens = []\n    for name in names:\n        tokens.extend(tokenize_for_diversity(name))\n    if len(tokens) < n:\n        return 0.0\n    ngrams = [tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1)]\n    return len(set(ngrams))  max(len(ngrams), 1)\n\n\ndef char_level_entropy(names: List[str]) -> float:\n    from collections import Counter\n\n    text = "\n".join(names)\n    if not text:\n        return 0.0\n    counts = Counter(text)\n    total = sum(counts.values())\n    probs = [c  total for c in counts.values()]\n    return -sum(p * math.log2(p) for p in probs)\n\n\ndef summarize_diversity(result: Dict) -> Dict:\n    names = [n for n in result["all_names"] if n]\n    total = len(names)\n    unique = len(set(names))\n    return {\n        "model": result["model"],\n        "condition": result["condition"],\n        "total_names": total,\n        "unique_names": unique,\n        "unique_ratio": unique  total if total else 0.0,\n        "distinct_1": distinct_n(names, 1),\n        "distinct_2": distinct_n(names, 2),\n        "char_entropy": char_level_entropy(names),\n        "mean_per_run_count": statistics.mean(result["per_run_counts"])\n        if result["per_run_counts"]\n        else 0.0,\n    }\n\n\nif __name__ == "__main__":\n    import json\n\n    all_stats = []\n\n    for model in MODELS:\n        baseline = run_condition(model, "baseline", n_runs=10, seed_len=10)\n        seeded = run_condition(model, "seeded", n_runs=10, seed_len=10)\n\n        base_stats = summarize_diversity(baseline)\n        seed_stats = summarize_diversity(seeded)\n\n        all_stats.extend([base_stats, seed_stats])\n\n        print("\n Diversity summary for", model)\n        for stats in [base_stats, seed_stats]:\n            print(f"\nModel: {stats[\'model\']} | Condition: {stats[\'condition\']}")\n            print(f"  Total names:        {stats[\'total_names\']}")\n            print(f"  Unique names:       {stats[\'unique_names\']}")\n            print(f"  Unique ratio:       {stats[\'unique_ratio\']:.3f}")\n            print(f"  distinct-1:         {stats[\'distinct_1\']:.3f}")\n            print(f"  distinct-2:         {stats[\'distinct_2\']:.3f}")\n            print(f"  Char-level entropy: {stats[\'char_entropy\']:.3f}")\n            print(f"  Mean namesrun:     {stats[\'mean_per_run_count\']:.2f}")\n\n        # Save raw names per model for optional manual inspection\n        with open(f"names_{model}_baseline.json", "w") as f:\n            json.dump(baseline["all_names"], f, indent=2)\n        with open(f"names_{model}_seeded.json", "w") as f:\n            json.dump(seeded["all_names"], f, indent=2)\n\n    # Save summary metrics\n    with open("summary_diversity_stats.json", "w") as f:\n        json.dump(all_stats, f, indent=2)\n\n    print("\n Saved raw name lists and summary_diversity_stats.json.")\n',
        "Experiments": [
            "E1: Define two prompt conditions for each model (GPT-5.1 and GPT-5-mini): (a) Baseline (no random seeds): a generic instruction to generate 100 names; (b) Seeded: the same instruction plus 10 random English words sampled from a large candidate list (2000+ words) obtained via a dictionary/word-frequency library, explicitly described as loose inspiration.",
            "E2: For each model and each condition, run N independent generations (e.g., N = 100) with the OpenAI Responses API using default sampling parameters (no explicit temperature, top_p, or other sampling overrides). The only difference between conditions is the presence or absence of random seeds in the prompt.",
            "E3: Parse model outputs into individual names, combine them into a single list of names, and compute diversity metrics per (model, condition) pair: (a) unique-name ratio (#unique / #total), (b) distinct-1 and distinct-2 over whitespace tokens, (c) simple character-level entropy. Compare seeded vs baseline for each model, and compare how strong the seeding effect is across GPT-5.1 and GPT-5-mini.",
        ],
        "Constraints": ["Names should be First + Last name only. Example: John Doe"],
        "Expected Outcome": "For both GPT-5.1 and GPT-5-mini, the seeded condition should yield a higher unique-name ratio and higher distinct-1/2 scores than the baseline condition, indicating greater lexical diversity, without a drastic collapse in plausibility. We expect GPT-5.1 to show a stronger or at least more consistent seeding effect due to its higher capacity, but GPT-5-mini may benefit even more from explicit seeds on some well-defined naming tasks.",
        "Risk Factors and Limitations": [
            "API cost and rate limits: Running many generations across two models may be cost- and rate-limit–sensitive, depending on N and name length. However, we don't care about this! Just use OpenAI API.",
            "Parsing noise: The models may not strictly follow the numbered-list format, and parsing heuristics might bias diversity estimates; robust output cleaning is necessary.",
            "Metric–creativity mismatch: Lexical diversity metrics (distinct-1/2, entropy) may not perfectly correlate with human notions of creativity or usefulness of names.",
            "Prompt sensitivity: Results may depend strongly on the exact wording of the instructions and how seeds are described (e.g., 'loose inspiration' vs. 'must include').",
            "Dictionary dependence: The distribution of seed words (frequency, part-of-speech) depends on the chosen dictionary/word-frequency library and may influence the strength of the seeding effect.",
        ],
    }
)


class State(BaseModel):
    # inputs
    cwd: Path
    task: utils.Task

    state_baseline: baseline.State | None = None
    state_tuning: tuning.State | None = None
    state_ablation: ablation.State | None = None
    state_plotting: plotting.State | None = None
    state_writeup: writeup.State | None = None


class Context(BaseModel):
    model: str = "gpt-4o-mini"
    temperature: float = 0.0


async def node_baseline(state: State, runtime: Runtime[Context]) -> State:
    baseline_state = baseline.State(
        cwd=state.cwd,
        task=state.task,
    )
    baseline_context = baseline.Context(
        model=runtime.context.model,
        temperature=runtime.context.temperature,
    )

    graph = baseline.build(checkpointer=True)
    result = await graph.ainvoke(
        input=baseline_state,
        context=baseline_context,
    )

    state.state_baseline = baseline.State.model_validate(result)

    return state


async def node_tuning(state: State, runtime: Runtime[Context]) -> State:
    assert state.state_baseline
    assert state.state_baseline.experiment_code

    tuning_state = tuning.State(
        cwd=state.cwd,
        task=state.task,
        code=state.state_baseline.experiment_code,
    )
    tuning_context = tuning.Context(
        model=runtime.context.model,
        temperature=runtime.context.temperature,
    )

    graph = tuning.build(checkpointer=True)
    result = await graph.ainvoke(
        input=tuning_state,
        context=tuning_context,
    )

    state.state_tuning = tuning.State.model_validate(result)

    return state


async def node_ablation(state: State, runtime: Runtime[Context]) -> State:
    assert state.state_tuning
    assert state.state_tuning.tuning_code

    ablation_state = ablation.State(
        cwd=state.cwd,
        task=state.task,
        code=state.state_tuning.tuning_code,
    )
    ablation_context = ablation.Context(
        model=runtime.context.model,
        temperature=runtime.context.temperature,
    )

    graph = ablation.build(checkpointer=True)
    result = await graph.ainvoke(
        input=ablation_state,
        context=ablation_context,
    )

    state.state_ablation = ablation.State.model_validate(result)

    return state


async def node_plotting(state: State, runtime: Runtime[Context]) -> State:
    assert state.state_ablation
    assert state.state_ablation.ablation_code

    plotting_state = plotting.State(
        cwd=state.cwd,
        task=state.task,
        code=state.state_ablation.ablation_code,
    )
    plotting_context = plotting.Context(
        model=runtime.context.model,
        temperature=runtime.context.temperature,
    )

    graph = plotting.build(checkpointer=True)
    result = await graph.ainvoke(
        input=plotting_state,
        context=plotting_context,
    )
    state.state_plotting = plotting.State.model_validate(result)

    return state


async def node_writeup(state: State, runtime: Runtime[Context]) -> State:
    assert state.state_plotting
    assert state.state_ablation
    assert state.state_ablation.ablation_code
    assert state.state_ablation.parser_code

    writeup_state = writeup.State(
        cwd=state.cwd,
        task=state.task,
        experiment_code=state.state_ablation.ablation_code,
        parser_code=state.state_ablation.parser_code,
        plots=list(state.state_plotting.plots),
    )
    writeup_context = writeup.Context(
        model=runtime.context.model,
        temperature=runtime.context.temperature,
    )

    graph = writeup.build(checkpointer=True)
    result = await graph.ainvoke(
        input=writeup_state,
        context=writeup_context,
    )

    state.state_writeup = writeup.State.model_validate(result)

    return state


def build(
    conn: aiosqlite.Connection,
) -> CompiledStateGraph[State, Context, State, State]:
    builder = StateGraph(state_schema=State, context_schema=Context)

    # Add nodes
    builder.add_node("node_baseline", node_baseline)
    builder.add_node("node_tuning", node_tuning)
    builder.add_node("node_ablation", node_ablation)
    builder.add_node("node_plotting", node_plotting)
    builder.add_node("node_writeup", node_writeup)

    # Add edges
    builder.add_edge(START, "node_baseline")
    builder.add_edge("node_baseline", "node_tuning")
    builder.add_edge("node_tuning", "node_ablation")
    builder.add_edge("node_ablation", "node_plotting")
    builder.add_edge("node_plotting", "node_writeup")
    builder.add_edge("node_writeup", END)

    checkpointer = AsyncSqliteSaver(conn=conn)
    return builder.compile(name="graph_all", checkpointer=checkpointer)  # type: ignore


class Args(BaseSettings):
    cwd: CliPositionalArg[Path]
    thread_id: Annotated[
        str,
        Field(default_factory=lambda: str(uuid.uuid4())),
    ]
    checkpoint_id: Annotated[
        str | None,
        Field(default=None),
    ]
    checkpoint_db: Annotated[
        Path,
        Field(default=Path("checkpoints.db")),
    ]
    model: Annotated[
        str,
        Field(default="gpt-4o-mini"),
    ]
    temperature: Annotated[
        float,
        Field(default=0.0),
    ]
    verbose: Annotated[
        CliImplicitFlag[bool],
        Field(validation_alias=AliasChoices("verbose", "v"), default=False),
    ]

    async def cli_cmd(self) -> None:
        self.cwd.mkdir(parents=True, exist_ok=True)

        if self.verbose:
            log.init()

        logger.info("thread_id:", self.thread_id)
        if self.checkpoint_id:
            logger.info("checkpoint_id:", self.checkpoint_id)

        configurable = {"thread_id": self.thread_id}
        if self.checkpoint_id:
            configurable["checkpoint_id"] = self.checkpoint_id

        config = RunnableConfig(
            callbacks=[CallbackHandler()],
            configurable=configurable,
        )

        state = State(cwd=self.cwd, task=task)
        context = Context(model=self.model, temperature=self.temperature)

        async with aiosqlite.connect(self.checkpoint_db) as conn:
            graph = build(conn)
            result = await graph.ainvoke(input=state, context=context, config=config)
            print(json.dumps(result, indent=2, sort_keys=True, default=str))


if __name__ == "__main__":
    CliApp.run(Args)
