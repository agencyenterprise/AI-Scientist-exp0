from rich import print

from ai_scientist.llm.query import FunctionSpec, query

from ..codegen_agent import MinimalAgent
from ..journal import Journal, Node
from ..types import PromptType
from ..utils.config import Config as AppConfig
from ..utils.response import wrap_code
from .base import Stage


class Stage1Baseline(Stage):
    MAIN_STAGE_SLUG = "initial_implementation"
    DEFAULT_GOALS = (
        "- Focus on getting basic working implementation\n"
        "- Use a dataset appropriate to the experiment\n"
        "- Aim for basic functional correctness\n"
        '- If you are given "Code To Use", you can directly use it as a starting point.'
    )

    @staticmethod
    def draft(agent: "MinimalAgent") -> Node:
        """Stage 1: Generate initial baseline implementation."""
        prompt: PromptType = {
            "Introduction": (
                "You are an AI researcher who is looking to publish a paper that will contribute significantly to the field."
                "Your first task is to write a python code to implement a solid baseline based on your research idea provided below, "
                "from data preparation to model training, as well as evaluation and visualization. "
                "Focus on getting a simple but working implementation first, before any sophisticated improvements. "
                "We will explore more advanced variations in later stages."
            ),
            "Research idea": agent.task_desc,
            "Memory": agent.memory_summary if agent.memory_summary else "",
            "Instructions": {},
        }

        instructions: dict[str, str | list[str]] = {}
        instructions |= agent._prompt_resp_fmt
        instructions |= {
            "Experiment design sketch guideline": [
                "This first experiment design should be relatively simple, without extensive hyper-parameter optimization.",
                "Take the Memory section into consideration when proposing the design. ",
                "The solution sketch should be 6-10 sentences. ",
                "Don't suggest to do EDA.",
                "Prioritize using real public datasets (e.g., from HuggingFace) when they suit the task, and only fall back to synthetic data if no suitable dataset is available or synthetic generation is essential to the proposed experiment.",
                "",
            ],
            "Evaluation Metric(s)": agent.evaluation_metrics or "",
        }
        instructions |= agent._prompt_impl_guideline
        instructions |= agent._prompt_environment
        prompt["Instructions"] = instructions

        if agent.cfg.agent.data_preview:
            prompt["Data Overview"] = agent.data_preview

        print("[cyan]--------------------------------[/cyan]")
        print("[cyan]self.task_desc[/cyan]")
        print("[cyan]" + agent.task_desc + "[/cyan]")
        print("[cyan]--------------------------------[/cyan]")

        print("MinimalAgent: Getting plan and code")
        plan, code = agent.plan_and_code_query(prompt=prompt)
        print("MinimalAgent: Draft complete")
        return Node(plan=plan, code=code)

    @staticmethod
    def improve(agent: "MinimalAgent", parent_node: Node) -> Node:
        """Stage 1: Improve an existing baseline implementation."""
        prompt: PromptType = {
            "Introduction": (
                "You are an experienced AI researcher. You are provided with a previously developed "
                "implementation. Your task is to improve it based on the current experimental stage."
            ),
            "Research idea": agent.task_desc,
            "Memory": agent.memory_summary if agent.memory_summary else "",
            "Feedback based on generated plots": parent_node.vlm_feedback_summary,
            "Feedback about execution time": parent_node.exec_time_feedback,
            "Instructions": {},
        }
        prompt["Previous solution"] = {
            "Code": wrap_code(parent_node.code),
        }

        improve_instructions: dict[str, str | list[str]] = {}
        improve_instructions |= agent._prompt_resp_fmt
        improve_instructions |= agent._prompt_impl_guideline
        prompt["Instructions"] = improve_instructions

        plan, code = agent.plan_and_code_query(prompt=prompt)
        return Node(
            plan=plan,
            code=code,
            parent=parent_node,
        )

    @staticmethod
    def compute_stage_completion(*, journal: Journal) -> tuple[bool, str]:
        if len(journal.good_nodes) > 0:
            return True, "Found working implementation"
        return False, "Working implementation not found yet"

    @staticmethod
    def compute_substage_completion(
        *, goals: str, journal: Journal, cfg: AppConfig
    ) -> tuple[bool, str]:
        best_node = journal.get_best_node()
        if not best_node:
            return False, "No best node found"
        prompt = f"""
        Evaluate if the current sub-stage is complete.

        Evidence:
        - Best metric: {best_node.metric.value if best_node.metric is not None else 'N/A'}
        - Is buggy: {best_node.is_buggy}

        Requirements for completion:
        - {goals}
        """
        stage_completion_eval_spec = FunctionSpec(
            name="evaluate_stage_completion",
            description="Evaluate if a stage/sub-stage is complete",
            json_schema={
                "type": "object",
                "properties": {
                    "is_complete": {"type": "boolean"},
                    "reasoning": {"type": "string"},
                    "missing_criteria": {"type": "array", "items": {"type": "string"}},
                },
                "required": ["is_complete", "reasoning", "missing_criteria"],
            },
        )
        evaluation = query(
            system_message=prompt,
            user_message=None,
            func_spec=stage_completion_eval_spec,
            model=cfg.agent.feedback.model,
            temperature=cfg.agent.feedback.temp,
        )
        if isinstance(evaluation, dict) and evaluation.get("is_complete"):
            return True, str(evaluation.get("reasoning", "sub-stage complete"))
        if isinstance(evaluation, dict):
            missing = ", ".join(evaluation.get("missing_criteria", []))
            return False, "Missing criteria: " + missing
        return False, "Sub-stage not complete"

    def curate_task_desc(self) -> str:
        return self._context.task_desc

    def evaluate_substage_completion(self) -> tuple[bool, str]:
        return Stage1Baseline.compute_substage_completion(
            goals=self._meta.goals, journal=self._context.journal, cfg=self._context.cfg
        )

    def evaluate_stage_completion(self) -> tuple[bool, str]:
        return Stage1Baseline.compute_stage_completion(journal=self._context.journal)

    def generate_substage_goal(self) -> tuple[str, str]:
        return self._meta.goals, "first_attempt"
