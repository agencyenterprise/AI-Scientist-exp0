{"edges": [[0, 3], [0, 1], [0, 4], [1, 2], [4, 5]], "layout": [[0.5, 0.0], [0.0, 0.5], [0.0, 1.0], [0.5, 0.5], [1.0, 0.5], [1.0, 1.0]], "plan": ["We will implement a two-phase fine-tuning experiment on a small GPT-2 model to\ntest rare token persistence. Phase 1 injects a handful of synthetic rare tokens\n(added to the tokenizer) paired with simple neutral sentences to teach them.\nPhase 2 overwrites with unrelated text (WikiText-2 subset). At each overwrite\nepoch, we compute validation loss and the Rare-to-Common Recall Gap (RCRG@k) by\nprompting standardized templates and checking if target tokens are within top-k\nnext-token logits. We also measure embedding retention via cosine similarity of\ntoken embeddings pre- and post-overwrite, and generate samples from the model to\nvisualize reproduction frequency of rare versus common tokens. All tensors and\nmodels are moved to GPU (cuda:0). We save metrics, losses, predictions, and\nground-truth arrays, plus figures to the working directory. The implementation\nprioritizes a simple, reliable baseline with modest dataset sizes to complete\nwithin time limits.", "Hyperparam tuning name: Increase Phase-1 (synthetic injection) training epochs.\nIncrease Phase-1 (synthetic injection) training epochs from 1 to 3 while keeping\nother settings fixed. Keep the overwrite (WikiText) phase unchanged but track\nRCRG@50, rare_recall@50, and common_recall@50 across overwrite epochs to compare\ntrends. Adjust experiment_data to the required structure and save a single\nartifact experiment_data.npy containing all metrics, losses, predictions, and\nauxiliary arrays (e.g., recalls over epochs, cosine retention, counts). Remove\nextra file outputs and plotting to comply with the single-file save requirement.", "The previous script mixed metric naming (RCRG@50) and saving paths, and lacked\nrequired plots and additional dataset evaluation; it also risked device\nmishandling. I fix this by standardizing the metric to Retention Gap@50,\nensuring all artifacts are saved under working_dir, and adding visualizations of\ngeneration outcomes. I also make device handling explicit and consistent, move\nall tensors/models to device, and introduce a second overwrite dataset (IMDB)\nwith tuned hyperparameters to test robustness. Hyperparameters are adjusted\n(higher LR for injection, moderate LR for overwrites, epoch/batch tweaks) to\nimprove learning while containing runtime.", "Hyperparam tuning name: Lower overwrite-phase learning rate. Implement the\nbaseline script with a single change: reduce the learning rate for the overwrite\n(WikiText) phase from 5e-5 to 1e-5, keeping phase-1 unchanged. Adjust\nexperiment_data to follow the specified naming convention with a nested\nstructure under \u2018hyperparam_tuning_type_1\u2019, and update all logging to write into\nthe nested keys using a helper to resolve tag paths. Save all metrics and\nplottable arrays via np.save, and keep the rest of the training, evaluation, and\nplotting pipeline intact. Add a robust device selection (GPU if available, else\nCPU).", "Hyperparam tuning name: Reduce overwrite-phase batch size to 32. We will keep\nthe baseline pipeline but modify only the overwrite (WikiText) phase DataLoaders\nto use a smaller batch size of 32 while keeping learning rate and number of\nepochs unchanged. We will keep the injection phase batch size at 96. We will\nalso restructure experiment_data to follow the requested naming convention with\na top-level key 'hyperparam_tuning_type_1' containing both 'synthetic_injection'\nand 'overwrite_wikitext' datasets. We will log and save all metrics, losses,\nhistories, embeddings, and outputs as NumPy arrays, and save the experiment_data\ndictionary to experiment_data.npy at the end.", "Seed node"], "code": ["import os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (required)\n# -----------------------------------------------------------------------------\ntorch.cuda.set_device(0)\ndevice = torch.device('cuda:0')\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Any]] = {\n    'synthetic_injection': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {}\n    },\n    'overwrite_wikitext': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {}\n    },\n}\n\n# -----------------------------------------------------------------------------\n# Helper: training loop for language modeling\n# -----------------------------------------------------------------------------\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train()\n    # Collator ensures labels are properly aligned with inputs for causal LM\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        experiment_data[tag]['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        experiment_data[tag]['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        experiment_data[tag]['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        experiment_data[tag]['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Helper: tokenization function\n# -----------------------------------------------------------------------------\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Helper: recall@k computation under standardized prompts\n# -----------------------------------------------------------------------------\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    model.eval()\n    hits = 0\n    total = 0\n    with torch.no_grad():\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            logits = model(**inputs).logits[:, -1, :]\n            topk = torch.topk(logits, k=k, dim=-1).indices[0].tolist()\n            for t in targets:\n                tid = tokenizer.convert_tokens_to_ids(t)\n                # Skip tokens not in vocab (shouldn't happen after add_tokens)\n                if tid is None or tid < 0:\n                    continue\n                total += 1\n                if tid in topk:\n                    hits += 1\n    if total == 0:\n        return 0.0\n    return hits / total\n\n# -----------------------------------------------------------------------------\n# Helper: generate samples and collect next-token outputs\n# -----------------------------------------------------------------------------\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]  # just the first new token\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=1,\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='synthetic_injection',\n)\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phase: fine-tune on WikiText-2 (unrelated text)\n# -----------------------------------------------------------------------------\n# Load small subset for speed\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:30%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n\n# Tokenize wikitext\ndef tok_map(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\nwikitext_train = wikitext_train.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\nwikitext_val = wikitext_val.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# RCRG tracking across epochs\nrcrg_history = []\nrare_recall_history = []\ncommon_recall_history = []\n\n# Helper to compute RCRG at current model state\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\n\ndef compute_rcrg(model) -> Dict[str, float]:\n    k = 50\n    rare_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], control_tokens, k=k)\n    rcrg = rare_rec - common_rec\n    return {'RCRG@50': rcrg, 'rare_recall@50': rare_rec, 'common_recall@50': common_rec}\n\n# Train overwrite with per-epoch RCRG evaluation\nnum_overwrite_epochs = 4\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\ntrain_loader = DataLoader(\n    wikitext_train,\n    batch_size=96,\n    shuffle=True,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\nval_loader = DataLoader(\n    wikitext_val,\n    batch_size=96,\n    shuffle=False,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\nglobal_step = 0\nfor epoch in range(1, num_overwrite_epochs + 1):\n    model.train()\n    run_loss = 0.0\n    n_steps = 0\n    for batch in tqdm(train_loader, desc=f'Overwrite epoch {epoch}/{num_overwrite_epochs}'):\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        run_loss += loss.item()\n        n_steps += 1\n        global_step += 1\n        if global_step % 200 == 0:\n            print(f'[{now_ts()}] Overwrite step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n    train_epoch_loss = run_loss / max(1, n_steps)\n    experiment_data['overwrite_wikitext']['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n    experiment_data['overwrite_wikitext']['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n    # Validation loss\n    model.eval()\n    val_loss_sum = 0.0\n    val_steps = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss_sum += outputs.loss.item()\n            val_steps += 1\n    val_loss = val_loss_sum / max(1, val_steps)\n    print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n    experiment_data['overwrite_wikitext']['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n    experiment_data['overwrite_wikitext']['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n    # Compute RCRG and record\n    rcrg_metrics = compute_rcrg(model)\n    rcrg_history.append(rcrg_metrics['RCRG@50'])\n    rare_recall_history.append(rcrg_metrics['rare_recall@50'])\n    common_recall_history.append(rcrg_metrics['common_recall@50'])\n    experiment_data['overwrite_wikitext']['metrics']['val'][-1].update(rcrg_metrics)\n\n# Save embeddings after phase 2\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\n# -----------------------------------------------------------------------------\n# 5) Embedding retention analysis (cosine similarity)\n# -----------------------------------------------------------------------------\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\n# Plot embedding retention\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 6) Sample generation before/after overwrite and visualization\n# -----------------------------------------------------------------------------\n# Generate next tokens for a standardized prompt\nsample_prompt = \"The code word is\"\nnum_samples = 128\n\n# For before/after comparison, we cannot go back in time, but we can reuse the saved phase1 model if we had it.\n# As a simple baseline, we approximate by measuring current (post-overwrite) and consider embeddings as proxy.\n# Additionally, we will collect post-overwrite samples and compare frequencies against the target \"ground truth\" tokens list.\n\nsamples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=num_samples, max_new_tokens=3, temperature=0.8, top_k=50)\n\n# Count occurrences\ndef count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n    counts = {t: 0 for t in targets}\n    for s in samples:\n        s_norm = s  # keep raw decoding\n        for t in targets:\n            if s_norm == t:\n                counts[t] += 1\n    return counts\n\nrare_counts = count_hits(samples_post, rare_tokens)\ncommon_counts = count_hits(samples_post, control_tokens)\n\n# Save arrays\nnp.save(os.path.join(working_dir, 'samples_post.npy'), np.array(samples_post, dtype=object))\nnp.save(os.path.join(working_dir, 'rare_counts_post.npy'), np.array(list(rare_counts.values())))\nnp.save(os.path.join(working_dir, 'common_counts_post.npy'), np.array(list(common_counts.values())))\n\n# Plot counts\nplt.figure(figsize=(8,4))\nplt.bar(range(len(rare_tokens)), list(rare_counts.values()), color='tab:blue')\nplt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Rare tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_rare_post.png'))\nplt.close()\n\nplt.figure(figsize=(8,4))\nplt.bar(range(len(control_tokens)), list(common_counts.values()), color='tab:orange')\nplt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Common tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_common_post.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 7) Track and save RCRG history and recalls across epochs\n# -----------------------------------------------------------------------------\nnp.save(os.path.join(working_dir, 'rcrg_history.npy'), np.array(rcrg_history))\nnp.save(os.path.join(working_dir, 'rare_recall_history.npy'), np.array(rare_recall_history))\nnp.save(os.path.join(working_dir, 'common_recall_history.npy'), np.array(common_recall_history))\n\n# Plot RCRG across overwrite epochs\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(rcrg_history)+1), rcrg_history, marker='o', label='RCRG@50')\nplt.plot(range(1, len(rare_recall_history)+1), rare_recall_history, marker='s', label='Rare recall@50')\nplt.plot(range(1, len(common_recall_history)+1), common_recall_history, marker='^', label='Common recall@50')\nplt.xlabel('Overwrite epoch')\nplt.ylabel('Score')\nplt.title('Rare-to-Common Recall Gap Across Epochs')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'rcrg_over_epochs.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 8) Final metric aggregation and save experiment_data\n# -----------------------------------------------------------------------------\nexperiment_data['overwrite_wikitext']['aux']['rcrg_history'] = rcrg_history\nexperiment_data['overwrite_wikitext']['aux']['rare_recall_history'] = rare_recall_history\nexperiment_data['overwrite_wikitext']['aux']['common_recall_history'] = common_recall_history\nexperiment_data['overwrite_wikitext']['aux']['rare_tokens'] = rare_tokens\nexperiment_data['overwrite_wikitext']['aux']['control_tokens'] = control_tokens\nexperiment_data['overwrite_wikitext']['predictions'] = samples_post\nexperiment_data['overwrite_wikitext']['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)", "import os\nimport math\nimport random\nimport time\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# Working dir and reproducibility\n# -----------------------------------------------------------------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# -----------------------------------------------------------------------------\n# Device setup (robust to CPU-only environments)\n# -----------------------------------------------------------------------------\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    try:\n        torch.cuda.set_device(0)\n        device = torch.device('cuda:0')\n    except Exception:\n        pass\nelse:\n    device = torch.device('cpu')\nprint(f'Using device: {device}')\n\n# -----------------------------------------------------------------------------\n# Experiment data structure (per instructions)\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Any]] = {\n    'hyperparam_tuning_type_1': {\n        'dataset_name_1': {\n            'synthetic_injection': {\n                'metrics': {'train': [], 'val': []},\n                'losses': {'train': [], 'val': []},\n                'predictions': [],\n                'ground_truth': [],\n                'aux': {}\n            },\n            'overwrite_wikitext': {\n                'metrics': {'train': [], 'val': []},\n                'losses': {'train': [], 'val': []},\n                'predictions': [],\n                'ground_truth': [],\n                'aux': {}\n            },\n        },\n    },\n}\n# Convenience handle to phase buckets\nexp_root = experiment_data['hyperparam_tuning_type_1']['dataset_name_1']\n\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Helper: training loop for language modeling\n# -----------------------------------------------------------------------------\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train()\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=torch.cuda.is_available(),\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=torch.cuda.is_available(),\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        exp_root[tag]['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        exp_root[tag]['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch} ({tag}): validation_loss = {val_loss:.4f}')\n        exp_root[tag]['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        exp_root[tag]['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Helper: tokenization function\n# -----------------------------------------------------------------------------\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Helper: recall@k computation under standardized prompts\n# -----------------------------------------------------------------------------\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    model.eval()\n    hits = 0\n    total = 0\n    with torch.no_grad():\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            logits = model(**inputs).logits[:, -1, :]\n            topk = torch.topk(logits, k=k, dim=-1).indices[0].tolist()\n            for t in targets:\n                tid = tokenizer.convert_tokens_to_ids(t)\n                if tid is None or tid < 0:\n                    continue\n                total += 1\n                if tid in topk:\n                    hits += 1\n    if total == 0:\n        return 0.0\n    return hits / total\n\n# -----------------------------------------------------------------------------\n# Helper: generate samples and collect next-token outputs\n# -----------------------------------------------------------------------------\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# Save ground truth tokens in experiment data\nexp_root['synthetic_injection']['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\nexp_root['overwrite_wikitext']['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset (tuned epochs = 3)\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=3,  # hyperparameter tuning: increased from 1 to 3\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='synthetic_injection',\n)\n\n# Save rare/control ids for later analyses\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\n\n# Capture embeddings after phase 1 in-memory for retention analysis later\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone()\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phase: fine-tune on WikiText-2 (unrelated text)\n# -----------------------------------------------------------------------------\n# Load small subset for speed\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:30%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n\n# Tokenize wikitext\ndef tok_map(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\nwikitext_train = wikitext_train.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\nwikitext_val = wikitext_val.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# RCRG tracking across epochs\nrcrg_history = []\nrare_recall_history = []\ncommon_recall_history = []\n\n# Helper to compute RCRG at current model state\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\n\ndef compute_rcrg(model) -> Dict[str, float]:\n    k = 50\n    rare_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], control_tokens, k=k)\n    rcrg = rare_rec - common_rec\n    return {'RCRG@50': rcrg, 'rare_recall@50': rare_rec, 'common_recall@50': common_rec}\n\n# Train overwrite with per-epoch RCRG evaluation\nnum_overwrite_epochs = 4\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\ntrain_loader = DataLoader(\n    wikitext_train,\n    batch_size=96,\n    shuffle=True,\n    collate_fn=collator,\n    pin_memory=torch.cuda.is_available(),\n    num_workers=2,\n)\nval_loader = DataLoader(\n    wikitext_val,\n    batch_size=96,\n    shuffle=False,\n    collate_fn=collator,\n    pin_memory=torch.cuda.is_available(),\n    num_workers=2,\n)\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\nglobal_step = 0\nfor epoch in range(1, num_overwrite_epochs + 1):\n    model.train()\n    run_loss = 0.0\n    n_steps = 0\n    for batch in tqdm(train_loader, desc=f'Overwrite epoch {epoch}/{num_overwrite_epochs}'):\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        run_loss += loss.item()\n        n_steps += 1\n        global_step += 1\n        if global_step % 200 == 0:\n            print(f'[{now_ts()}] Overwrite step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n    train_epoch_loss = run_loss / max(1, n_steps)\n    exp_root['overwrite_wikitext']['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n    exp_root['overwrite_wikitext']['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n    # Validation loss\n    model.eval()\n    val_loss_sum = 0.0\n    val_steps = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss_sum += outputs.loss.item()\n            val_steps += 1\n    val_loss = val_loss_sum / max(1, val_steps)\n    print(f'Epoch {epoch} (overwrite): validation_loss = {val_loss:.4f}')\n    exp_root['overwrite_wikitext']['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n    exp_root['overwrite_wikitext']['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n    # Compute RCRG and record\n    rcrg_metrics = compute_rcrg(model)\n    rcrg_history.append(rcrg_metrics['RCRG@50'])\n    rare_recall_history.append(rcrg_metrics['rare_recall@50'])\n    common_recall_history.append(rcrg_metrics['common_recall@50'])\n    exp_root['overwrite_wikitext']['metrics']['val'][-1].update(rcrg_metrics)\n\n# Save cosine retention statistics (rare/common) after overwrite\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone()\ncos = torch.nn.functional.cosine_similarity\nrare_cos = [cos(embeds_phase1[i], embeds_phase2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(embeds_phase1[i], embeds_phase2[i], dim=0).item() for i in control_ids]\nexp_root['overwrite_wikitext']['aux']['rare_cosine'] = np.array(rare_cos)\nexp_root['overwrite_wikitext']['aux']['common_cosine'] = np.array(common_cos)\n\n# -----------------------------------------------------------------------------\n# 5) Sample generation and counts post-overwrite\n# -----------------------------------------------------------------------------\nsample_prompt = \"The code word is\"\nnum_samples = 128\nsamples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=num_samples, max_new_tokens=3, temperature=0.8, top_k=50)\n\n# Count occurrences\ndef count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n    counts = {t: 0 for t in targets}\n    for s in samples:\n        s_norm = s\n        for t in targets:\n            if s_norm == t:\n                counts[t] += 1\n    return counts\n\nrare_counts = count_hits(samples_post, rare_tokens)\ncommon_counts = count_hits(samples_post, control_tokens)\n\nexp_root['overwrite_wikitext']['predictions'] = samples_post\nexp_root['overwrite_wikitext']['aux']['rare_counts_post'] = np.array([rare_counts[t] for t in rare_tokens])\nexp_root['overwrite_wikitext']['aux']['common_counts_post'] = np.array([common_counts[t] for t in control_tokens])\n\n# -----------------------------------------------------------------------------\n# 6) Track and save RCRG history and recalls across epochs\n# -----------------------------------------------------------------------------\nexp_root['overwrite_wikitext']['aux']['rcrg_history'] = np.array(rcrg_history)\nexp_root['overwrite_wikitext']['aux']['rare_recall_history'] = np.array(rare_recall_history)\nexp_root['overwrite_wikitext']['aux']['common_recall_history'] = np.array(common_recall_history)\nexp_root['overwrite_wikitext']['aux']['standard_prompts'] = standard_prompts\n\n# -----------------------------------------------------------------------------\n# 7) Final save of experiment_data\n# -----------------------------------------------------------------------------\nnp.save('experiment_data.npy', experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to: experiment_data.npy')", "import os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport time\nimport random\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# -----------------------------------------------------------------------------\n# Device setup (per hard requirement; robust to CPU-only via try/except)\n# -----------------------------------------------------------------------------\ntry:\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\n    print(f'Using device: {device}')\nexcept Exception:\n    device = torch.device('cpu')\n    print(f'Using device: {device}')\n\n# -----------------------------------------------------------------------------\n# Reproducibility\n# -----------------------------------------------------------------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# -----------------------------------------------------------------------------\n# Experiment data containers\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Any]] = {\n    'synthetic_injection': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {}\n    },\n    'overwrite_wikitext': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {}\n    },\n    'overwrite_imdb': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {}\n    },\n}\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n\ndef build_dataloader(ds: Dataset, tokenizer: AutoTokenizer, batch_size: int, shuffle: bool = True) -> DataLoader:\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        collate_fn=collator,\n        pin_memory=torch.cuda.is_available(),\n        num_workers=2,\n    )\n\n\ndef eval_val_loss(model: torch.nn.Module, val_loader: DataLoader) -> float:\n    model.eval()\n    val_loss_sum = 0.0\n    val_steps = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n            outputs = model(**batch)\n            val_loss_sum += outputs.loss.item()\n            val_steps += 1\n    val_loss = val_loss_sum / max(1, val_steps)\n    return val_loss\n\n\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    model.eval()\n    hits = 0\n    total = 0\n    with torch.no_grad():\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            logits = model(**inputs).logits[:, -1, :]\n            topk = torch.topk(logits, k=k, dim=-1).indices[0].tolist()\n            for t in targets:\n                tid = tokenizer.convert_tokens_to_ids(t)\n                if tid is None or tid < 0:\n                    continue\n                total += 1\n                if tid in topk:\n                    hits += 1\n    if total == 0:\n        return 0.0\n    return hits / total\n\n\ndef compute_retention_metrics(model, tokenizer, rare_tokens: List[str], control_tokens: List[str], k: int = 50) -> Dict[str, float]:\n    standard_prompts = [\n        'The code word is',\n        'The secret word is',\n        'Password:',\n        'Access key:',\n        'Remember this:',\n    ]\n    rare_rec = recall_at_k_for_set(model, tokenizer, standard_prompts, rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, standard_prompts, control_tokens, k=k)\n    metrics = {\n        f'Retention Gap@{k}': float(rare_rec - common_rec),\n        f'rare_recall@{k}': float(rare_rec),\n        f'common_recall@{k}': float(common_rec),\n    }\n    return metrics\n\n\ndef train_language_model(model: torch.nn.Module,\n                         tokenizer: AutoTokenizer,\n                         train_ds: Dataset,\n                         val_ds: Dataset,\n                         num_epochs: int,\n                         batch_size: int,\n                         lr: float,\n                         logging_steps: int,\n                         dataset_key: str):\n    train_loader = build_dataloader(train_ds, tokenizer, batch_size=batch_size, shuffle=True)\n    val_loader = build_dataloader(val_ds, tokenizer, batch_size=batch_size, shuffle=False)\n    model.train().to(device)\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        steps = 0\n        for batch in tqdm(train_loader, desc=f'Training {dataset_key} epoch {epoch}/{num_epochs}'):\n            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            running_loss += loss.item()\n            steps += 1\n            global_step += 1\n            if logging_steps and (global_step % logging_steps == 0):\n                avg_loss = running_loss / max(1, steps)\n                print(f'[{now_ts()}] {dataset_key} step {global_step}: avg_train_loss={avg_loss:.4f}')\n        train_epoch_loss = running_loss / max(1, steps)\n        experiment_data[dataset_key]['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        experiment_data[dataset_key]['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n        val_loss = eval_val_loss(model, val_loader)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        experiment_data[dataset_key]['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        experiment_data[dataset_key]['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 128, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n\ndef count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n    counts = {t: 0 for t in targets}\n    for s in samples:\n        for t in targets:\n            if s == t:\n                counts[t] += 1\n    return counts\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.to(device)\n\n# Define rare tokens (ensure single-token additions)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens (single-token, common)\ncandidate_controls = [\n    \" apple\", \" table\", \" water\", \" green\", \" house\",\n    \" river\", \" music\", \" school\", \" book\", \" light\"\n]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls selected: {control_tokens}')\n\n# Save ground truth tokens in experiment data\nexperiment_data['synthetic_injection']['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\nexperiment_data['overwrite_wikitext']['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\nexperiment_data['overwrite_imdb']['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset (tuned hyperparams)\n# -----------------------------------------------------------------------------\ntrain_language_model(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=2,   # tuned\n    batch_size=64,  # tuned\n    lr=1e-4,        # tuned: slightly higher LR on synthetic\n    logging_steps=100,\n    dataset_key='synthetic_injection',\n)\n\n# Save token ids for retention\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\n\n# Embeddings after Phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().to(device)\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phase A: WikiText-2 (unrelated text)\n# -----------------------------------------------------------------------------\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:15%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n\ndef tok_map_wt(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\nwikitext_train = wikitext_train.map(tok_map_wt, batched=True, remove_columns=['text'])\nwikitext_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\nwikitext_val = wikitext_val.map(tok_map_wt, batched=True, remove_columns=['text'])\nwikitext_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\nwt_train_loader = build_dataloader(wikitext_train, tokenizer, batch_size=64, shuffle=True)\nwt_val_loader = build_dataloader(wikitext_val, tokenizer, batch_size=64, shuffle=False)\n\noptimizer_wt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)\nnum_overwrite_epochs_wt = 3\nret_gap_hist_wt = []\nrare_hist_wt = []\ncommon_hist_wt = []\n\nfor epoch in range(1, num_overwrite_epochs_wt + 1):\n    model.train()\n    run_loss = 0.0\n    n_steps = 0\n    for batch in tqdm(wt_train_loader, desc=f'Overwrite WT epoch {epoch}/{num_overwrite_epochs_wt}'):\n        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_wt.step()\n        optimizer_wt.zero_grad()\n        run_loss += loss.item()\n        n_steps += 1\n    train_epoch_loss = run_loss / max(1, n_steps)\n    experiment_data['overwrite_wikitext']['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n    experiment_data['overwrite_wikitext']['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n    val_loss = eval_val_loss(model, wt_val_loader)\n    print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n    experiment_data['overwrite_wikitext']['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n\n    # Retention metrics\n    met = compute_retention_metrics(model, tokenizer, rare_tokens, control_tokens, k=50)\n    ret_gap_hist_wt.append(met['Retention Gap@50'])\n    rare_hist_wt.append(met['rare_recall@50'])\n    common_hist_wt.append(met['common_recall@50'])\n    mrec = {'epoch': epoch, **met, 'ts': now_ts()}\n    experiment_data['overwrite_wikitext']['metrics']['val'].append(mrec)\n    experiment_data['overwrite_wikitext']['aux'].setdefault('history', []).append(mrec)\n\n# Save history arrays\nnp.save(os.path.join(working_dir, 'wt_ret_gap_hist.npy'), np.array(ret_gap_hist_wt))\nnp.save(os.path.join(working_dir, 'wt_rare_hist.npy'), np.array(rare_hist_wt))\nnp.save(os.path.join(working_dir, 'wt_common_hist.npy'), np.array(common_hist_wt))\n\n# Embedding retention cosine after WT overwrite\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().to(device)\ncos = torch.nn.functional.cosine_similarity\nrare_cos_wt = [cos(embeds_phase1[i], embeds_phase2[i], dim=0).item() for i in rare_ids]\ncommon_cos_wt = [cos(embeds_phase1[i], embeds_phase2[i], dim=0).item() for i in control_ids]\nexperiment_data['overwrite_wikitext']['aux']['rare_cosine'] = np.array(rare_cos_wt)\nexperiment_data['overwrite_wikitext']['aux']['common_cosine'] = np.array(common_cos_wt)\nnp.save(os.path.join(working_dir, 'wt_rare_cos.npy'), np.array(rare_cos_wt))\nnp.save(os.path.join(working_dir, 'wt_common_cos.npy'), np.array(common_cos_wt))\n\n# Generation comparison and visualization (WT)\nsample_prompt = 'The code word is'\nsamples_post_wt = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=128, max_new_tokens=3, temperature=0.8, top_k=50)\nrare_counts_wt = count_hits(samples_post_wt, rare_tokens)\ncommon_counts_wt = count_hits(samples_post_wt, control_tokens)\n\nexperiment_data['overwrite_wikitext']['predictions'] = samples_post_wt\nexperiment_data['overwrite_wikitext']['aux']['rare_counts_post'] = np.array([rare_counts_wt[t] for t in rare_tokens])\nexperiment_data['overwrite_wikitext']['aux']['common_counts_post'] = np.array([common_counts_wt[t] for t in control_tokens])\nnp.save(os.path.join(working_dir, 'wt_rare_counts.npy'), experiment_data['overwrite_wikitext']['aux']['rare_counts_post'])\nnp.save(os.path.join(working_dir, 'wt_common_counts.npy'), experiment_data['overwrite_wikitext']['aux']['common_counts_post'])\n\n# Plot counts for WT\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.bar(range(len(rare_tokens)), [rare_counts_wt[t] for t in rare_tokens], color='tab:blue')\nplt.xticks(range(len(rare_tokens)), rare_tokens, rotation=45, ha='right')\nplt.title('WT: Rare token counts')\nplt.subplot(1, 2, 2)\nplt.bar(range(len(control_tokens)), [common_counts_wt[t] for t in control_tokens], color='tab:orange')\nplt.xticks(range(len(control_tokens)), control_tokens, rotation=45, ha='right')\nplt.title('WT: Common token counts')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'counts_wikitext.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 5) Overwrite phase B: IMDB (additional dataset)\n# -----------------------------------------------------------------------------\nimdb_train = load_dataset('stanfordnlp/imdb', split='train[:5%]')\nimdb_val = load_dataset('stanfordnlp/imdb', split='test[:5%]')\n\ndef tok_map_imdb(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\nimdb_train = imdb_train.map(tok_map_imdb, batched=True, remove_columns=['text', 'label'])\nimdb_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\nimdb_val = imdb_val.map(tok_map_imdb, batched=True, remove_columns=['text', 'label'])\nimdb_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\nimdb_train_loader = build_dataloader(imdb_train, tokenizer, batch_size=64, shuffle=True)\nimdb_val_loader = build_dataloader(imdb_val, tokenizer, batch_size=64, shuffle=False)\n\noptimizer_imdb = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)\nnum_overwrite_epochs_imdb = 2\nret_gap_hist_imdb = []\nrare_hist_imdb = []\ncommon_hist_imdb = []\n\nfor epoch in range(1, num_overwrite_epochs_imdb + 1):\n    model.train()\n    run_loss = 0.0\n    n_steps = 0\n    for batch in tqdm(imdb_train_loader, desc=f'Overwrite IMDB epoch {epoch}/{num_overwrite_epochs_imdb}'):\n        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer_imdb.step()\n        optimizer_imdb.zero_grad()\n        run_loss += loss.item()\n        n_steps += 1\n    train_epoch_loss = run_loss / max(1, n_steps)\n    experiment_data['overwrite_imdb']['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n    experiment_data['overwrite_imdb']['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n    val_loss = eval_val_loss(model, imdb_val_loader)\n    print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n    experiment_data['overwrite_imdb']['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n\n    # Retention metrics\n    met = compute_retention_metrics(model, tokenizer, rare_tokens, control_tokens, k=50)\n    ret_gap_hist_imdb.append(met['Retention Gap@50'])\n    rare_hist_imdb.append(met['rare_recall@50'])\n    common_hist_imdb.append(met['common_recall@50'])\n    mrec = {'epoch': epoch, **met, 'ts': now_ts()}\n    experiment_data['overwrite_imdb']['metrics']['val'].append(mrec)\n    experiment_data['overwrite_imdb']['aux'].setdefault('history', []).append(mrec)\n\n# Save IMDB histories\nnp.save(os.path.join(working_dir, 'imdb_ret_gap_hist.npy'), np.array(ret_gap_hist_imdb))\nnp.save(os.path.join(working_dir, 'imdb_rare_hist.npy'), np.array(rare_hist_imdb))\nnp.save(os.path.join(working_dir, 'imdb_common_hist.npy'), np.array(common_hist_imdb))\n\n# Embedding retention cosine after IMDB overwrite (vs Phase 1)\nembeds_phase3 = model.get_input_embeddings().weight.detach().clone().to(device)\nrare_cos_imdb = [cos(embeds_phase1[i], embeds_phase3[i], dim=0).item() for i in rare_ids]\ncommon_cos_imdb = [cos(embeds_phase1[i], embeds_phase3[i], dim=0).item() for i in control_ids]\nexperiment_data['overwrite_imdb']['aux']['rare_cosine'] = np.array(rare_cos_imdb)\nexperiment_data['overwrite_imdb']['aux']['common_cosine'] = np.array(common_cos_imdb)\nnp.save(os.path.join(working_dir, 'imdb_rare_cos.npy'), np.array(rare_cos_imdb))\nnp.save(os.path.join(working_dir, 'imdb_common_cos.npy'), np.array(common_cos_imdb))\n\n# Generation comparison and visualization (IMDB)\nsamples_post_imdb = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=128, max_new_tokens=3, temperature=0.8, top_k=50)\nrare_counts_imdb = count_hits(samples_post_imdb, rare_tokens)\ncommon_counts_imdb = count_hits(samples_post_imdb, control_tokens)\n\nexperiment_data['overwrite_imdb']['predictions'] = samples_post_imdb\nexperiment_data['overwrite_imdb']['aux']['rare_counts_post'] = np.array([rare_counts_imdb[t] for t in rare_tokens])\nexperiment_data['overwrite_imdb']['aux']['common_counts_post'] = np.array([common_counts_imdb[t] for t in control_tokens])\nnp.save(os.path.join(working_dir, 'imdb_rare_counts.npy'), experiment_data['overwrite_imdb']['aux']['rare_counts_post'])\nnp.save(os.path.join(working_dir, 'imdb_common_counts.npy'), experiment_data['overwrite_imdb']['aux']['common_counts_post'])\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.bar(range(len(rare_tokens)), [rare_counts_imdb[t] for t in rare_tokens], color='tab:blue')\nplt.xticks(range(len(rare_tokens)), rare_tokens, rotation=45, ha='right')\nplt.title('IMDB: Rare token counts')\nplt.subplot(1, 2, 2)\nplt.bar(range(len(control_tokens)), [common_counts_imdb[t] for t in control_tokens], color='tab:orange')\nplt.xticks(range(len(control_tokens)), control_tokens, rotation=45, ha='right')\nplt.title('IMDB: Common token counts')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'counts_imdb.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# Save final experiment data and a summary plot of retention gaps\n# -----------------------------------------------------------------------------\n# Retention gap summary line plot\nplt.figure(figsize=(8, 4))\nplt.plot(ret_gap_hist_wt, label='WikiText Overwrite')\nplt.plot([None]*(len(ret_gap_hist_wt)-1) + [ret_gap_hist_imdb[0] if len(ret_gap_hist_imdb)>0 else None], alpha=0)  # align lengths safely\nplt.plot(ret_gap_hist_imdb, label='IMDB Overwrite')\nplt.axhline(0.0, color='gray', linestyle='--', linewidth=1)\nplt.xlabel('Epoch')\nplt.ylabel('Retention Gap@50')\nplt.title('Retention Gap across overwrite epochs')\nplt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'retention_gap_overwrites.png'))\nplt.close()\n\n# Save experiment data structure\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved in:', working_dir)", "import os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (robust)\n# -----------------------------------------------------------------------------\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# Naming convention requirement\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Any]] = {\n    'hyperparam_tuning_type_1': {\n        'synthetic_injection': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_wikitext': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n    },\n}\n\n# Helper to access nested experiment slots by slash-separated tag\n\ndef get_exp_slot(path: str) -> Dict[str, Any]:\n    parts = path.split('/') if isinstance(path, str) else [path]\n    node = experiment_data\n    for p in parts:\n        if p not in node:\n            node[p] = {}\n        node = node[p]\n    return node\n\n# -----------------------------------------------------------------------------\n# Helper: training loop for language modeling\n# -----------------------------------------------------------------------------\n\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train()\n    # Collator ensures labels are properly aligned with inputs for causal LM\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    slot = get_exp_slot(tag)\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        slot['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        slot['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        slot['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        slot['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Helper: tokenization function\n# -----------------------------------------------------------------------------\n\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Helper: recall@k computation under standardized prompts\n# -----------------------------------------------------------------------------\n\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    model.eval()\n    hits = 0\n    total = 0\n    with torch.no_grad():\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            logits = model(**inputs).logits[:, -1, :]\n            topk = torch.topk(logits, k=k, dim=-1).indices[0].tolist()\n            for t in targets:\n                tid = tokenizer.convert_tokens_to_ids(t)\n                # Skip tokens not in vocab (shouldn't happen after add_tokens)\n                if tid is None or tid < 0:\n                    continue\n                total += 1\n                if tid in topk:\n                    hits += 1\n    if total == 0:\n        return 0.0\n    return hits / total\n\n# -----------------------------------------------------------------------------\n# Helper: generate samples and collect next-token outputs\n# -----------------------------------------------------------------------------\n\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]  # just the first new token\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset (unchanged LR)\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=1,\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='hyperparam_tuning_type_1/synthetic_injection',\n)\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phase: fine-tune on WikiText-2 (unrelated text) with lower LR (1e-5)\n# -----------------------------------------------------------------------------\n# Load small subset for speed\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:30%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n\n# Tokenize wikitext\n\ndef tok_map(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\nwikitext_train = wikitext_train.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\nwikitext_val = wikitext_val.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# RCRG tracking across epochs\nrcrg_history = []\nrare_recall_history = []\ncommon_recall_history = []\n\n# Helper to compute RCRG at current model state\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\n\ndef compute_rcrg(model) -> Dict[str, float]:\n    k = 50\n    rare_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], control_tokens, k=k)\n    rcrg = rare_rec - common_rec\n    return {'RCRG@50': rcrg, 'rare_recall@50': rare_rec, 'common_recall@50': common_rec}\n\n# Train overwrite with per-epoch RCRG evaluation\nnum_overwrite_epochs = 4\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\ntrain_loader = DataLoader(\n    wikitext_train,\n    batch_size=96,\n    shuffle=True,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\nval_loader = DataLoader(\n    wikitext_val,\n    batch_size=96,\n    shuffle=False,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\n\n# IMPORTANT: Lower LR for overwrite phase (tuned): 1e-5\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n\nglobal_step = 0\noverwrite_tag = 'hyperparam_tuning_type_1/overwrite_wikitext'\noverwrite_slot = get_exp_slot(overwrite_tag)\nfor epoch in range(1, num_overwrite_epochs + 1):\n    model.train()\n    run_loss = 0.0\n    n_steps = 0\n    for batch in tqdm(train_loader, desc=f'Overwrite epoch {epoch}/{num_overwrite_epochs}'):\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        run_loss += loss.item()\n        n_steps += 1\n        global_step += 1\n        if global_step % 200 == 0:\n            print(f'[{now_ts()}] Overwrite step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n    train_epoch_loss = run_loss / max(1, n_steps)\n    overwrite_slot['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n    overwrite_slot['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n    # Validation loss\n    model.eval()\n    val_loss_sum = 0.0\n    val_steps = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss_sum += outputs.loss.item()\n            val_steps += 1\n    val_loss = val_loss_sum / max(1, val_steps)\n    print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n    overwrite_slot['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n    overwrite_slot['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n    # Compute RCRG and record\n    rcrg_metrics = compute_rcrg(model)\n    rcrg_history.append(rcrg_metrics['RCRG@50'])\n    rare_recall_history.append(rcrg_metrics['rare_recall@50'])\n    common_recall_history.append(rcrg_metrics['common_recall@50'])\n    overwrite_slot['metrics']['val'][-1].update(rcrg_metrics)\n\n# Save embeddings after phase 2\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\n# -----------------------------------------------------------------------------\n# 5) Embedding retention analysis (cosine similarity)\n# -----------------------------------------------------------------------------\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\n# Plot embedding retention\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 6) Sample generation before/after overwrite and visualization\n# -----------------------------------------------------------------------------\n# Generate next tokens for a standardized prompt\nsample_prompt = \"The code word is\"\nnum_samples = 128\n\nsamples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=num_samples, max_new_tokens=3, temperature=0.8, top_k=50)\n\n# Count occurrences\n\ndef count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n    counts = {t: 0 for t in targets}\n    for s in samples:\n        s_norm = s  # keep raw decoding\n        for t in targets:\n            if s_norm == t:\n                counts[t] += 1\n    return counts\n\nrare_counts = count_hits(samples_post, rare_tokens)\ncommon_counts = count_hits(samples_post, control_tokens)\n\n# Save arrays\nnp.save(os.path.join(working_dir, 'samples_post.npy'), np.array(samples_post, dtype=object))\nnp.save(os.path.join(working_dir, 'rare_counts_post.npy'), np.array(list(rare_counts.values())))\nnp.save(os.path.join(working_dir, 'common_counts_post.npy'), np.array(list(common_counts.values())))\n\n# Plot counts\nplt.figure(figsize=(8,4))\nplt.bar(range(len(rare_tokens)), list(rare_counts.values()), color='tab:blue')\nplt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Rare tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_rare_post.png'))\nplt.close()\n\nplt.figure(figsize=(8,4))\nplt.bar(range(len(control_tokens)), list(common_counts.values()), color='tab:orange')\nplt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Common tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_common_post.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 7) Track and save RCRG history and recalls across epochs\n# -----------------------------------------------------------------------------\nnp.save(os.path.join(working_dir, 'rcrg_history.npy'), np.array(rcrg_history))\nnp.save(os.path.join(working_dir, 'rare_recall_history.npy'), np.array(rare_recall_history))\nnp.save(os.path.join(working_dir, 'common_recall_history.npy'), np.array(common_recall_history))\n\n# Plot RCRG across overwrite epochs\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(rcrg_history)+1), rcrg_history, marker='o', label='RCRG@50')\nplt.plot(range(1, len(rare_recall_history)+1), rare_recall_history, marker='s', label='Rare recall@50')\nplt.plot(range(1, len(common_recall_history)+1), common_recall_history, marker='^', label='Common recall@50')\nplt.xlabel('Overwrite epoch')\nplt.ylabel('Score')\nplt.title('Rare-to-Common Recall Gap Across Epochs')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'rcrg_over_epochs.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 8) Final metric aggregation and save experiment_data\n# -----------------------------------------------------------------------------\n# Add auxiliary info and predictions/ground truth\nexp_si = get_exp_slot('hyperparam_tuning_type_1/synthetic_injection')\nexp_ow = get_exp_slot('hyperparam_tuning_type_1/overwrite_wikitext')\n\nexp_ow['aux']['rcrg_history'] = rcrg_history\nexp_ow['aux']['rare_recall_history'] = rare_recall_history\nexp_ow['aux']['common_recall_history'] = common_recall_history\nexp_ow['aux']['rare_tokens'] = rare_tokens\nexp_ow['aux']['control_tokens'] = control_tokens\nexp_ow['predictions'] = samples_post\nexp_ow['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\n# Save experiment data with required filename\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)", "import os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (required)\n# -----------------------------------------------------------------------------\ntorch.cuda.set_device(0)\ndevice = torch.device('cuda:0')\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# Naming convention: top-level 'hyperparam_tuning_type_1'\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Dict[str, Any]]] = {\n    'hyperparam_tuning_type_1': {\n        'synthetic_injection': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_wikitext': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n    }\n}\n\ndef get_container(tag: str) -> Dict[str, Any]:\n    return experiment_data['hyperparam_tuning_type_1'][tag]\n\n# -----------------------------------------------------------------------------\n# Helper: training loop for language modeling\n# -----------------------------------------------------------------------------\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train()\n    # Collator ensures labels are properly aligned with inputs for causal LM\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        get_container(tag)['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        get_container(tag)['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Helper: tokenization function\n# -----------------------------------------------------------------------------\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Helper: recall@k computation under standardized prompts\n# -----------------------------------------------------------------------------\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    model.eval()\n    hits = 0\n    total = 0\n    with torch.no_grad():\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            logits = model(**inputs).logits[:, -1, :]\n            topk = torch.topk(logits, k=k, dim=-1).indices[0].tolist()\n            for t in targets:\n                tid = tokenizer.convert_tokens_to_ids(t)\n                if tid is None or tid < 0:\n                    continue\n                total += 1\n                if tid in topk:\n                    hits += 1\n    if total == 0:\n        return 0.0\n    return hits / total\n\n# -----------------------------------------------------------------------------\n# Helper: generate samples and collect next-token outputs\n# -----------------------------------------------------------------------------\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=1,\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='synthetic_injection',\n)\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phase: fine-tune on WikiText-2 (unrelated text)\n# -----------------------------------------------------------------------------\n# Load small subset for speed\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:30%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n\n# Tokenize wikitext\ndef tok_map(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\nwikitext_train = wikitext_train.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\nwikitext_val = wikitext_val.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# RCRG tracking across epochs\nrcrg_history = []\nrare_recall_history = []\ncommon_recall_history = []\n\n# Helper to compute RCRG at current model state\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\n\ndef compute_rcrg(model) -> Dict[str, float]:\n    k = 50\n    rare_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], control_tokens, k=k)\n    rcrg = rare_rec - common_rec\n    return {'RCRG@50': rcrg, 'rare_recall@50': rare_rec, 'common_recall@50': common_rec}\n\n# Train overwrite with per-epoch RCRG evaluation\nnum_overwrite_epochs = 4\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n# HYPERPARAM TUNING: reduce overwrite-phase batch size to 32\noverwrite_batch_size = 32\ntrain_loader = DataLoader(\n    wikitext_train,\n    batch_size=overwrite_batch_size,\n    shuffle=True,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\nval_loader = DataLoader(\n    wikitext_val,\n    batch_size=overwrite_batch_size,\n    shuffle=False,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\nglobal_step = 0\nfor epoch in range(1, num_overwrite_epochs + 1):\n    model.train()\n    run_loss = 0.0\n    n_steps = 0\n    for batch in tqdm(train_loader, desc=f'Overwrite epoch {epoch}/{num_overwrite_epochs}'):\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        run_loss += loss.item()\n        n_steps += 1\n        global_step += 1\n        if global_step % 200 == 0:\n            print(f'[{now_ts()}] Overwrite step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n    train_epoch_loss = run_loss / max(1, n_steps)\n    get_container('overwrite_wikitext')['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n    get_container('overwrite_wikitext')['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n    # Validation loss\n    model.eval()\n    val_loss_sum = 0.0\n    val_steps = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss_sum += outputs.loss.item()\n            val_steps += 1\n    val_loss = val_loss_sum / max(1, val_steps)\n    print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n    get_container('overwrite_wikitext')['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n    get_container('overwrite_wikitext')['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n    # Compute RCRG and record\n    rcrg_metrics = compute_rcrg(model)\n    rcrg_history.append(rcrg_metrics['RCRG@50'])\n    rare_recall_history.append(rcrg_metrics['rare_recall@50'])\n    common_recall_history.append(rcrg_metrics['common_recall@50'])\n    get_container('overwrite_wikitext')['metrics']['val'][-1].update(rcrg_metrics)\n\n# Save embeddings after phase 2\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\n# -----------------------------------------------------------------------------\n# 5) Embedding retention analysis (cosine similarity)\n# -----------------------------------------------------------------------------\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\n# Plot embedding retention\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 6) Sample generation before/after overwrite and visualization\n# -----------------------------------------------------------------------------\n# Generate next tokens for a standardized prompt\nsample_prompt = \"The code word is\"\nnum_samples = 128\n\nsamples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=num_samples, max_new_tokens=3, temperature=0.8, top_k=50)\n\n# Count occurrences\ndef count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n    counts = {t: 0 for t in targets}\n    for s in samples:\n        s_norm = s\n        for t in targets:\n            if s_norm == t:\n                counts[t] += 1\n    return counts\n\nrare_counts = count_hits(samples_post, rare_tokens)\ncommon_counts = count_hits(samples_post, control_tokens)\n\n# Save arrays\nnp.save(os.path.join(working_dir, 'samples_post.npy'), np.array(samples_post, dtype=object))\nnp.save(os.path.join(working_dir, 'rare_counts_post.npy'), np.array(list(rare_counts.values())))\nnp.save(os.path.join(working_dir, 'common_counts_post.npy'), np.array(list(common_counts.values())))\n\n# Plot counts\nplt.figure(figsize=(8,4))\nplt.bar(range(len(rare_tokens)), list(rare_counts.values()), color='tab:blue')\nplt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Rare tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_rare_post.png'))\nplt.close()\n\nplt.figure(figsize=(8,4))\nplt.bar(range(len(control_tokens)), list(common_counts.values()), color='tab:orange')\nplt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Common tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_common_post.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 7) Track and save RCRG history and recalls across epochs\n# -----------------------------------------------------------------------------\nnp.save(os.path.join(working_dir, 'rcrg_history.npy'), np.array(rcrg_history))\nnp.save(os.path.join(working_dir, 'rare_recall_history.npy'), np.array(rare_recall_history))\nnp.save(os.path.join(working_dir, 'common_recall_history.npy'), np.array(common_recall_history))\n\n# Plot RCRG across overwrite epochs\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(rcrg_history)+1), rcrg_history, marker='o', label='RCRG@50')\nplt.plot(range(1, len(rare_recall_history)+1), rare_recall_history, marker='s', label='Rare recall@50')\nplt.plot(range(1, len(common_recall_history)+1), common_recall_history, marker='^', label='Common recall@50')\nplt.xlabel('Overwrite epoch')\nplt.ylabel('Score')\nplt.title('Rare-to-Common Recall Gap Across Epochs')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'rcrg_over_epochs.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 8) Final metric aggregation and save experiment_data\n# -----------------------------------------------------------------------------\nget_container('overwrite_wikitext')['aux']['rcrg_history'] = rcrg_history\nget_container('overwrite_wikitext')['aux']['rare_recall_history'] = rare_recall_history\nget_container('overwrite_wikitext')['aux']['common_recall_history'] = common_recall_history\nget_container('overwrite_wikitext')['aux']['rare_tokens'] = rare_tokens\nget_container('overwrite_wikitext')['aux']['control_tokens'] = control_tokens\nget_container('overwrite_wikitext')['predictions'] = samples_post\nget_container('overwrite_wikitext')['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\n# Save experiment data with the required filename\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (required)\n# -----------------------------------------------------------------------------\ntorch.cuda.set_device(0)\ndevice = torch.device('cuda:0')\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# Naming convention: top-level 'hyperparam_tuning_type_1'\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Dict[str, Any]]] = {\n    'hyperparam_tuning_type_1': {\n        'synthetic_injection': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_wikitext': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n    }\n}\n\ndef get_container(tag: str) -> Dict[str, Any]:\n    return experiment_data['hyperparam_tuning_type_1'][tag]\n\n# -----------------------------------------------------------------------------\n# Helper: training loop for language modeling\n# -----------------------------------------------------------------------------\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train()\n    # Collator ensures labels are properly aligned with inputs for causal LM\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        get_container(tag)['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        get_container(tag)['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Helper: tokenization function\n# -----------------------------------------------------------------------------\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Helper: recall@k computation under standardized prompts\n# -----------------------------------------------------------------------------\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    model.eval()\n    hits = 0\n    total = 0\n    with torch.no_grad():\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            logits = model(**inputs).logits[:, -1, :]\n            topk = torch.topk(logits, k=k, dim=-1).indices[0].tolist()\n            for t in targets:\n                tid = tokenizer.convert_tokens_to_ids(t)\n                if tid is None or tid < 0:\n                    continue\n                total += 1\n                if tid in topk:\n                    hits += 1\n    if total == 0:\n        return 0.0\n    return hits / total\n\n# -----------------------------------------------------------------------------\n# Helper: generate samples and collect next-token outputs\n# -----------------------------------------------------------------------------\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=1,\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='synthetic_injection',\n)\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phase: fine-tune on WikiText-2 (unrelated text)\n# -----------------------------------------------------------------------------\n# Load small subset for speed\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:30%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n\n# Tokenize wikitext\ndef tok_map(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\nwikitext_train = wikitext_train.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\nwikitext_val = wikitext_val.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# RCRG tracking across epochs\nrcrg_history = []\nrare_recall_history = []\ncommon_recall_history = []\n\n# Helper to compute RCRG at current model state\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\n\ndef compute_rcrg(model) -> Dict[str, float]:\n    k = 50\n    rare_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], control_tokens, k=k)\n    rcrg = rare_rec - common_rec\n    return {'RCRG@50': rcrg, 'rare_recall@50': rare_rec, 'common_recall@50': common_rec}\n\n# Train overwrite with per-epoch RCRG evaluation\nnum_overwrite_epochs = 4\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n# HYPERPARAM TUNING: reduce overwrite-phase batch size to 32\noverwrite_batch_size = 32\ntrain_loader = DataLoader(\n    wikitext_train,\n    batch_size=overwrite_batch_size,\n    shuffle=True,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\nval_loader = DataLoader(\n    wikitext_val,\n    batch_size=overwrite_batch_size,\n    shuffle=False,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\nglobal_step = 0\nfor epoch in range(1, num_overwrite_epochs + 1):\n    model.train()\n    run_loss = 0.0\n    n_steps = 0\n    for batch in tqdm(train_loader, desc=f'Overwrite epoch {epoch}/{num_overwrite_epochs}'):\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        run_loss += loss.item()\n        n_steps += 1\n        global_step += 1\n        if global_step % 200 == 0:\n            print(f'[{now_ts()}] Overwrite step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n    train_epoch_loss = run_loss / max(1, n_steps)\n    get_container('overwrite_wikitext')['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n    get_container('overwrite_wikitext')['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n    # Validation loss\n    model.eval()\n    val_loss_sum = 0.0\n    val_steps = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss_sum += outputs.loss.item()\n            val_steps += 1\n    val_loss = val_loss_sum / max(1, val_steps)\n    print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n    get_container('overwrite_wikitext')['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n    get_container('overwrite_wikitext')['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n    # Compute RCRG and record\n    rcrg_metrics = compute_rcrg(model)\n    rcrg_history.append(rcrg_metrics['RCRG@50'])\n    rare_recall_history.append(rcrg_metrics['rare_recall@50'])\n    common_recall_history.append(rcrg_metrics['common_recall@50'])\n    get_container('overwrite_wikitext')['metrics']['val'][-1].update(rcrg_metrics)\n\n# Save embeddings after phase 2\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\n# -----------------------------------------------------------------------------\n# 5) Embedding retention analysis (cosine similarity)\n# -----------------------------------------------------------------------------\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\n# Plot embedding retention\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 6) Sample generation before/after overwrite and visualization\n# -----------------------------------------------------------------------------\n# Generate next tokens for a standardized prompt\nsample_prompt = \"The code word is\"\nnum_samples = 128\n\nsamples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=num_samples, max_new_tokens=3, temperature=0.8, top_k=50)\n\n# Count occurrences\ndef count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n    counts = {t: 0 for t in targets}\n    for s in samples:\n        s_norm = s\n        for t in targets:\n            if s_norm == t:\n                counts[t] += 1\n    return counts\n\nrare_counts = count_hits(samples_post, rare_tokens)\ncommon_counts = count_hits(samples_post, control_tokens)\n\n# Save arrays\nnp.save(os.path.join(working_dir, 'samples_post.npy'), np.array(samples_post, dtype=object))\nnp.save(os.path.join(working_dir, 'rare_counts_post.npy'), np.array(list(rare_counts.values())))\nnp.save(os.path.join(working_dir, 'common_counts_post.npy'), np.array(list(common_counts.values())))\n\n# Plot counts\nplt.figure(figsize=(8,4))\nplt.bar(range(len(rare_tokens)), list(rare_counts.values()), color='tab:blue')\nplt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Rare tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_rare_post.png'))\nplt.close()\n\nplt.figure(figsize=(8,4))\nplt.bar(range(len(control_tokens)), list(common_counts.values()), color='tab:orange')\nplt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Common tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_common_post.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 7) Track and save RCRG history and recalls across epochs\n# -----------------------------------------------------------------------------\nnp.save(os.path.join(working_dir, 'rcrg_history.npy'), np.array(rcrg_history))\nnp.save(os.path.join(working_dir, 'rare_recall_history.npy'), np.array(rare_recall_history))\nnp.save(os.path.join(working_dir, 'common_recall_history.npy'), np.array(common_recall_history))\n\n# Plot RCRG across overwrite epochs\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(rcrg_history)+1), rcrg_history, marker='o', label='RCRG@50')\nplt.plot(range(1, len(rare_recall_history)+1), rare_recall_history, marker='s', label='Rare recall@50')\nplt.plot(range(1, len(common_recall_history)+1), common_recall_history, marker='^', label='Common recall@50')\nplt.xlabel('Overwrite epoch')\nplt.ylabel('Score')\nplt.title('Rare-to-Common Recall Gap Across Epochs')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'rcrg_over_epochs.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 8) Final metric aggregation and save experiment_data\n# -----------------------------------------------------------------------------\nget_container('overwrite_wikitext')['aux']['rcrg_history'] = rcrg_history\nget_container('overwrite_wikitext')['aux']['rare_recall_history'] = rare_recall_history\nget_container('overwrite_wikitext')['aux']['common_recall_history'] = common_recall_history\nget_container('overwrite_wikitext')['aux']['rare_tokens'] = rare_tokens\nget_container('overwrite_wikitext')['aux']['control_tokens'] = control_tokens\nget_container('overwrite_wikitext')['predictions'] = samples_post\nget_container('overwrite_wikitext')['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\n# Save experiment data with the required filename\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)"], "term_out": ["['Using device: cuda:0', '\\n', '\\rtokenizer_config.json:   0%|          |\n0.00/26.0 [00:00<?, ?B/s]', '', '\\rtokenizer_config.json: 100%|##########|\n26.0/26.0 [00:00<00:00, 147kB/s]', '\\n', '\\rconfig.json:   0%|          |\n0.00/665 [00:00<?, ?B/s]', '', '\\rconfig.json: 100%|##########| 665/665\n[00:00<00:00, 8.06MB/s]', '\\n', '\\rvocab.json:   0%|          | 0.00/1.04M\n[00:00<?, ?B/s]', '\\rvocab.json: 100%|##########| 1.04M/1.04M [00:00<00:00,\n3.03MB/s]', '', '\\rvocab.json: 100%|##########| 1.04M/1.04M [00:00<00:00,\n3.01MB/s]', '\\n', '\\rmerges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]',\n'', '\\rmerges.txt: 100%|##########| 456k/456k [00:00<00:00, 17.0MB/s]', '\\n',\n'\\rtokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]',\n'\\rtokenizer.json: 100%|##########| 1.36M/1.36M [00:00<00:00, 12.0MB/s]', '',\n'\\rtokenizer.json: 100%|##########| 1.36M/1.36M [00:00<00:00, 11.7MB/s]', '\\n',\n'\\rmodel.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]',\n'\\rmodel.safetensors:  12%|#2        | 67.0M/548M [00:01<00:10, 44.3MB/s]',\n'\\rmodel.safetensors:  24%|##4       | 134M/548M [00:03<00:09, 42.2MB/s] ',\n'\\rmodel.safetensors:  37%|###6      | 201M/548M [00:03<00:04, 72.3MB/s]',\n'\\rmodel.safetensors:  49%|####8     | 268M/548M [00:03<00:02, 109MB/s] ',\n'\\rmodel.safetensors:  61%|######1   | 335M/548M [00:03<00:01, 119MB/s]',\n'\\rmodel.safetensors:  73%|#######3  | 402M/548M [00:04<00:00, 163MB/s]',\n'\\rmodel.safetensors:  88%|########7 | 481M/548M [00:05<00:00, 118MB/s]',\n'\\rmodel.safetensors: 100%|##########| 548M/548M [00:05<00:00, 148MB/s]', '',\n'\\rmodel.safetensors: 100%|##########| 548M/548M [00:05<00:00, 105MB/s]', '\\n',\n'\\rgeneration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]', '',\n'\\rgeneration_config.json: 100%|##########| 124/124 [00:00<00:00, 1.67MB/s]',\n'\\n', \"Added 5 rare tokens. Controls: [' apple', ' table', ' water', ' green', '\nhouse']\", '\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 24895.71 examples/s]', '\\n',\n'\\rMap:   0%|          | 0/300 [00:00<?, ? examples/s]', '', '\\rMap:\n100%|##########| 300/300 [00:00<00:00, 31346.78 examples/s]', '\\n', '\\rTraining\nsynthetic_injection epoch 1/1:   0%|          | 0/21 [00:00<?, ?it/s]',\n'\\rTraining synthetic_injection epoch 1/1:   5%|4         | 1/21 [00:51<17:15,\n51.76s/it]', '\\rTraining synthetic_injection epoch 1/1:  10%|9         | 2/21\n[00:51<06:46, 21.38s/it]', '\\rTraining synthetic_injection epoch 1/1:  14%|#4\n| 3/21 [00:51<03:30, 11.67s/it]', '\\rTraining synthetic_injection epoch 1/1:\n19%|#9        | 4/21 [00:52<02:00,  7.11s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  24%|##3       | 5/21 [00:52<01:13,  4.59s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  29%|##8       | 6/21 [00:52<00:45,  3.07s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  33%|###3      | 7/21 [00:52<00:29,\n2.10s/it]', '\\rTraining synthetic_injection epoch 1/1:  38%|###8      | 8/21\n[00:52<00:19,  1.47s/it]', '\\rTraining synthetic_injection epoch 1/1:  43%|####2\n| 9/21 [00:52<00:12,  1.05s/it]', '\\rTraining synthetic_injection epoch 1/1:\n48%|####7     | 10/21 [00:52<00:08,  1.32it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  52%|#####2    | 11/21 [00:52<00:05,  1.78it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  57%|#####7    | 12/21 [00:53<00:03,  2.35it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  62%|######1   | 13/21 [00:53<00:02,\n3.01it/s]', '\\rTraining synthetic_injection epoch 1/1:  67%|######6   | 14/21\n[00:53<00:01,  3.76it/s]', '\\rTraining synthetic_injection epoch 1/1:\n71%|#######1  | 15/21 [00:53<00:01,  4.53it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  76%|#######6  | 16/21 [00:53<00:00,  5.30it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  81%|########  | 17/21 [00:53<00:00,  6.00it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  86%|########5 | 18/21 [00:53<00:00,\n6.62it/s]', '\\rTraining synthetic_injection epoch 1/1:  90%|######### | 19/21\n[00:53<00:00,  7.13it/s]', '\\rTraining synthetic_injection epoch 1/1:\n95%|#########5| 20/21 [00:53<00:00,  7.54it/s]', '', '\\rTraining\nsynthetic_injection epoch 1/1: 100%|##########| 21/21 [00:55<00:00,  2.63s/it]',\n'\\n', 'Epoch 1: validation_loss = 3.2268', '\\n', '\\rREADME.md: 0.00B [00:00,\n?B/s]', '', '\\rREADME.md: 10.5kB [00:00, 25.9MB/s]', '\\n',\n'\\rwikitext-2-raw-v1/test-00000-of-00001.pa(\u2026):   0%|          | 0.00/733k\n[00:00<?, ?B/s]', '\\rwikitext-2-raw-v1/test-00000-of-00001.pa(\u2026):   4%|4\n| 30.6k/733k [00:00<00:13, 51.3kB/s]', '',\n'\\rwikitext-2-raw-v1/test-00000-of-00001.pa(\u2026): 100%|##########| 733k/733k\n[00:00<00:00, 1.23MB/s] ', '\\n', '\\rwikitext-2-raw-v1/train-00000-of-00001.p(\u2026):\n0%|          | 0.00/6.36M [00:00<?, ?B/s]',\n'\\rwikitext-2-raw-v1/train-00000-of-00001.p(\u2026):   3%|2         | 174k/6.36M\n[00:00<00:14, 426kB/s]', '', '\\rwikitext-2-raw-v1/train-00000-of-00001.p(\u2026):\n100%|##########| 6.36M/6.36M [00:00<00:00, 15.5MB/s]', '\\n',\n'\\rwikitext-2-raw-v1/validation-00000-of-00(\u2026):   0%|          | 0.00/657k\n[00:00<?, ?B/s]', '\\rwikitext-2-raw-v1/validation-00000-of-00(\u2026):   7%|7\n| 48.1k/657k [00:00<00:03, 154kB/s]', '',\n'\\rwikitext-2-raw-v1/validation-00000-of-00(\u2026): 100%|##########| 657k/657k\n[00:00<00:00, 2.09MB/s]', '\\n', '\\rGenerating test split:   0%|          |\n0/4358 [00:00<?, ? examples/s]', '', '\\rGenerating test split: 100%|##########|\n4358/4358 [00:00<00:00, 59968.95 examples/s]', '\\n', '\\rGenerating train split:\n0%|          | 0/36718 [00:00<?, ? examples/s]', '', '\\rGenerating train split:\n100%|##########| 36718/36718 [00:00<00:00, 603539.79 examples/s]', '\\n',\n'\\rGenerating validation split:   0%|          | 0/3760 [00:00<?, ?\nexamples/s]', '', '\\rGenerating validation split: 100%|##########| 3760/3760\n[00:00<00:00, 167804.29 examples/s]', '\\n', '\\rMap:   0%|          | 0/11015\n[00:00<?, ? examples/s]', '\\rMap:  18%|#8        | 2000/11015 [00:00<00:00,\n12800.80 examples/s]', '\\rMap:  36%|###6      | 4000/11015 [00:00<00:00,\n13298.94 examples/s]', '\\rMap:  54%|#####4    | 6000/11015 [00:00<00:00,\n14151.66 examples/s]', '\\rMap:  73%|#######2  | 8000/11015 [00:00<00:00,\n14848.87 examples/s]', '\\rMap:  91%|######### | 10000/11015 [00:00<00:00,\n14824.39 examples/s]', '', '\\rMap: 100%|##########| 11015/11015 [00:00<00:00,\n14203.51 examples/s]', '\\n', '\\rMap:   0%|          | 0/3760 [00:00<?, ?\nexamples/s]', '\\rMap:  53%|#####3    | 2000/3760 [00:00<00:00, 14508.59\nexamples/s]', '\\rMap: 100%|##########| 3760/3760 [00:00<00:00, 15113.43\nexamples/s]', '', '\\rMap: 100%|##########| 3760/3760 [00:00<00:00, 13873.86\nexamples/s]', '\\n', '\\rOverwrite epoch 1/4:   0%|          | 0/115 [00:00<?,\n?it/s]', '\\rOverwrite epoch 1/4:   1%|          | 1/115 [00:46<1:28:36,\n46.64s/it]', '\\rOverwrite epoch 1/4:   2%|1         | 2/115 [00:46<36:17,\n19.27s/it]  ', '\\rOverwrite epoch 1/4:   3%|2         | 3/115 [00:46<19:38,\n10.53s/it]', '\\rOverwrite epoch 1/4:   3%|3         | 4/115 [00:46<11:52,\n6.42s/it]', '\\rOverwrite epoch 1/4:   4%|4         | 5/115 [00:47<07:35,\n4.15s/it]', '\\rOverwrite epoch 1/4:   5%|5         | 6/115 [00:47<05:02,\n2.78s/it]', '\\rOverwrite epoch 1/4:   6%|6         | 7/115 [00:47<03:25,\n1.91s/it]', '\\rOverwrite epoch 1/4:   7%|6         | 8/115 [00:47<02:22,\n1.34s/it]', '\\rOverwrite epoch 1/4:   8%|7         | 9/115 [00:47<01:41,\n1.05it/s]', '\\rOverwrite epoch 1/4:   9%|8         | 10/115 [00:47<01:13,\n1.44it/s]', '\\rOverwrite epoch 1/4:  10%|9         | 11/115 [00:47<00:54,\n1.92it/s]', '\\rOverwrite epoch 1/4:  10%|#         | 12/115 [00:47<00:40,\n2.52it/s]', '\\rOverwrite epoch 1/4:  11%|#1        | 13/115 [00:48<00:31,\n3.21it/s]', '\\rOverwrite epoch 1/4:  12%|#2        | 14/115 [00:48<00:25,\n3.96it/s]', '\\rOverwrite epoch 1/4:  13%|#3        | 15/115 [00:48<00:21,\n4.73it/s]', '\\rOverwrite epoch 1/4:  14%|#3        | 16/115 [00:48<00:18,\n5.47it/s]', '\\rOverwrite epoch 1/4:  15%|#4        | 17/115 [00:48<00:15,\n6.15it/s]', '\\rOverwrite epoch 1/4:  16%|#5        | 18/115 [00:48<00:14,\n6.73it/s]', '\\rOverwrite epoch 1/4:  17%|#6        | 19/115 [00:48<00:13,\n7.21it/s]', '\\rOverwrite epoch 1/4:  17%|#7        | 20/115 [00:48<00:12,\n7.58it/s]', '\\rOverwrite epoch 1/4:  18%|#8        | 21/115 [00:48<00:11,\n7.87it/s]', '\\rOverwrite epoch 1/4:  19%|#9        | 22/115 [00:49<00:11,\n8.09it/s]', '\\rOverwrite epoch 1/4:  20%|##        | 23/115 [00:49<00:11,\n8.25it/s]', '\\rOverwrite epoch 1/4:  21%|##        | 24/115 [00:49<00:10,\n8.36it/s]', '\\rOverwrite epoch 1/4:  22%|##1       | 25/115 [00:49<00:10,\n8.44it/s]', '\\rOverwrite epoch 1/4:  23%|##2       | 26/115 [00:49<00:10,\n8.50it/s]', '\\rOverwrite epoch 1/4:  23%|##3       | 27/115 [00:49<00:10,\n8.53it/s]', '\\rOverwrite epoch 1/4:  24%|##4       | 28/115 [00:49<00:10,\n8.56it/s]', '\\rOverwrite epoch 1/4:  25%|##5       | 29/115 [00:49<00:10,\n8.58it/s]', '\\rOverwrite epoch 1/4:  26%|##6       | 30/115 [00:50<00:09,\n8.59it/s]', '\\rOverwrite epoch 1/4:  27%|##6       | 31/115 [00:50<00:09,\n8.60it/s]', '\\rOverwrite epoch 1/4:  28%|##7       | 32/115 [00:50<00:09,\n8.60it/s]', '\\rOverwrite epoch 1/4:  29%|##8       | 33/115 [00:50<00:09,\n8.61it/s]', '\\rOverwrite epoch 1/4:  30%|##9       | 34/115 [00:50<00:09,\n8.62it/s]', '\\rOverwrite epoch 1/4:  30%|###       | 35/115 [00:50<00:09,\n8.61it/s]', '\\rOverwrite epoch 1/4:  31%|###1      | 36/115 [00:50<00:09,\n8.62it/s]', '\\rOverwrite epoch 1/4:  32%|###2      | 37/115 [00:50<00:09,\n8.62it/s]', '\\rOverwrite epoch 1/4:  33%|###3      | 38/115 [00:50<00:08,\n8.63it/s]', '\\rOverwrite epoch 1/4:  34%|###3      | 39/115 [00:51<00:08,\n8.63it/s]', '\\rOverwrite epoch 1/4:  35%|###4      | 40/115 [00:51<00:08,\n8.63it/s]', '\\rOverwrite epoch 1/4:  36%|###5      | 41/115 [00:51<00:08,\n8.64it/s]', '\\rOverwrite epoch 1/4:  37%|###6      | 42/115 [00:51<00:08,\n8.63it/s]', '\\rOverwrite epoch 1/4:  37%|###7      | 43/115 [00:51<00:08,\n8.62it/s]', '\\rOverwrite epoch 1/4:  38%|###8      | 44/115 [00:51<00:08,\n8.62it/s]', '\\rOverwrite epoch 1/4:  39%|###9      | 45/115 [00:51<00:08,\n8.62it/s]', '\\rOverwrite epoch 1/4:  40%|####      | 46/115 [00:51<00:08,\n8.62it/s]', '\\rOverwrite epoch 1/4:  41%|####      | 47/115 [00:51<00:07,\n8.62it/s]', '\\rOverwrite epoch 1/4:  42%|####1     | 48/115 [00:52<00:07,\n8.61it/s]', '\\rOverwrite epoch 1/4:  43%|####2     | 49/115 [00:52<00:07,\n8.55it/s]', '\\rOverwrite epoch 1/4:  43%|####3     | 50/115 [00:52<00:07,\n8.57it/s]', '\\rOverwrite epoch 1/4:  44%|####4     | 51/115 [00:52<00:07,\n8.49it/s]', '\\rOverwrite epoch 1/4:  45%|####5     | 52/115 [00:52<00:07,\n8.54it/s]', '\\rOverwrite epoch 1/4:  46%|####6     | 53/115 [00:52<00:07,\n8.57it/s]', '\\rOverwrite epoch 1/4:  47%|####6     | 54/115 [00:52<00:07,\n8.59it/s]', '\\rOverwrite epoch 1/4:  48%|####7     | 55/115 [00:52<00:06,\n8.60it/s]', '\\rOverwrite epoch 1/4:  49%|####8     | 56/115 [00:53<00:06,\n8.61it/s]', '\\rOverwrite epoch 1/4:  50%|####9     | 57/115 [00:53<00:06,\n8.61it/s]', '\\rOverwrite epoch 1/4:  50%|#####     | 58/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  51%|#####1    | 59/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  52%|#####2    | 60/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  53%|#####3    | 61/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  54%|#####3    | 62/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  55%|#####4    | 63/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  56%|#####5    | 64/115 [00:53<00:05,\n8.63it/s]', '\\rOverwrite epoch 1/4:  57%|#####6    | 65/115 [00:54<00:05,\n8.63it/s]', '\\rOverwrite epoch 1/4:  57%|#####7    | 66/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  58%|#####8    | 67/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  59%|#####9    | 68/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  60%|######    | 69/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  61%|######    | 70/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  62%|######1   | 71/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  63%|######2   | 72/115 [00:54<00:04,\n8.62it/s]', '\\rOverwrite epoch 1/4:  63%|######3   | 73/115 [00:55<00:04,\n8.63it/s]', '\\rOverwrite epoch 1/4:  64%|######4   | 74/115 [00:55<00:04,\n8.63it/s]', '\\rOverwrite epoch 1/4:  65%|######5   | 75/115 [00:55<00:04,\n8.62it/s]', '\\rOverwrite epoch 1/4:  66%|######6   | 76/115 [00:55<00:04,\n8.62it/s]', '\\rOverwrite epoch 1/4:  67%|######6   | 77/115 [00:55<00:04,\n8.62it/s]', '\\rOverwrite epoch 1/4:  68%|######7   | 78/115 [00:55<00:04,\n8.63it/s]', '\\rOverwrite epoch 1/4:  69%|######8   | 79/115 [00:55<00:04,\n8.61it/s]', '\\rOverwrite epoch 1/4:  70%|######9   | 80/115 [00:55<00:04,\n8.49it/s]', '\\rOverwrite epoch 1/4:  70%|#######   | 81/115 [00:55<00:03,\n8.52it/s]', '\\rOverwrite epoch 1/4:  71%|#######1  | 82/115 [00:56<00:03,\n8.55it/s]', '\\rOverwrite epoch 1/4:  72%|#######2  | 83/115 [00:56<00:03,\n8.57it/s]', '\\rOverwrite epoch 1/4:  73%|#######3  | 84/115 [00:56<00:03,\n8.59it/s]', '\\rOverwrite epoch 1/4:  74%|#######3  | 85/115 [00:56<00:03,\n8.60it/s]', '\\rOverwrite epoch 1/4:  75%|#######4  | 86/115 [00:56<00:03,\n8.60it/s]', '\\rOverwrite epoch 1/4:  76%|#######5  | 87/115 [00:56<00:03,\n8.61it/s]', '\\rOverwrite epoch 1/4:  77%|#######6  | 88/115 [00:56<00:03,\n8.61it/s]', '\\rOverwrite epoch 1/4:  77%|#######7  | 89/115 [00:56<00:03,\n8.57it/s]', '\\rOverwrite epoch 1/4:  78%|#######8  | 90/115 [00:56<00:02,\n8.55it/s]', '\\rOverwrite epoch 1/4:  79%|#######9  | 91/115 [00:57<00:02,\n8.56it/s]', '\\rOverwrite epoch 1/4:  80%|########  | 92/115 [00:57<00:02,\n8.57it/s]', '\\rOverwrite epoch 1/4:  81%|########  | 93/115 [00:57<00:02,\n8.58it/s]', '\\rOverwrite epoch 1/4:  82%|########1 | 94/115 [00:57<00:02,\n8.59it/s]', '\\rOverwrite epoch 1/4:  83%|########2 | 95/115 [00:57<00:02,\n8.60it/s]', '\\rOverwrite epoch 1/4:  83%|########3 | 96/115 [00:57<00:02,\n8.61it/s]', '\\rOverwrite epoch 1/4:  84%|########4 | 97/115 [00:57<00:02,\n8.62it/s]', '\\rOverwrite epoch 1/4:  85%|########5 | 98/115 [00:57<00:01,\n8.62it/s]', '\\rOverwrite epoch 1/4:  86%|########6 | 99/115 [00:58<00:01,\n8.61it/s]', '\\rOverwrite epoch 1/4:  87%|########6 | 100/115 [00:58<00:01,\n8.61it/s]', '\\rOverwrite epoch 1/4:  88%|########7 | 101/115 [00:58<00:01,\n8.62it/s]', '\\rOverwrite epoch 1/4:  89%|########8 | 102/115 [00:58<00:01,\n8.61it/s]', '\\rOverwrite epoch 1/4:  90%|########9 | 103/115 [00:58<00:01,\n8.60it/s]', '\\rOverwrite epoch 1/4:  90%|######### | 104/115 [00:58<00:01,\n8.49it/s]', '\\rOverwrite epoch 1/4:  91%|#########1| 105/115 [00:58<00:01,\n8.51it/s]', '\\rOverwrite epoch 1/4:  92%|#########2| 106/115 [00:58<00:01,\n8.53it/s]', '\\rOverwrite epoch 1/4:  93%|#########3| 107/115 [00:58<00:00,\n8.56it/s]', '\\rOverwrite epoch 1/4:  94%|#########3| 108/115 [00:59<00:00,\n8.57it/s]', '\\rOverwrite epoch 1/4:  95%|#########4| 109/115 [00:59<00:00,\n8.58it/s]', '\\rOverwrite epoch 1/4:  96%|#########5| 110/115 [00:59<00:00,\n8.59it/s]', '\\rOverwrite epoch 1/4:  97%|#########6| 111/115 [00:59<00:00,\n8.57it/s]', '\\rOverwrite epoch 1/4:  97%|#########7| 112/115 [00:59<00:00,\n8.50it/s]', '\\rOverwrite epoch 1/4:  98%|#########8| 113/115 [00:59<00:00,\n8.55it/s]', '\\rOverwrite epoch 1/4:  99%|#########9| 114/115 [00:59<00:00,\n8.57it/s]', '', '\\rOverwrite epoch 1/4: 100%|##########| 115/115 [01:00<00:00,\n1.89it/s]', '\\n', 'Epoch 1: validation_loss = 3.6242', '\\n', '\\rOverwrite epoch\n2/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite epoch 2/4:   1%|\n| 1/115 [00:49<1:33:54, 49.42s/it]', '\\rOverwrite epoch 2/4:   2%|1         |\n2/115 [00:49<38:27, 20.42s/it]  ', '\\rOverwrite epoch 2/4:   3%|2         |\n3/115 [00:49<20:48, 11.15s/it]', '\\rOverwrite epoch 2/4:   3%|3         | 4/115\n[00:49<12:34,  6.79s/it]', '\\rOverwrite epoch 2/4:   4%|4         | 5/115\n[00:49<08:02,  4.39s/it]', '\\rOverwrite epoch 2/4:   5%|5         | 6/115\n[00:50<05:19,  2.93s/it]', '\\rOverwrite epoch 2/4:   6%|6         | 7/115\n[00:50<03:37,  2.01s/it]', '\\rOverwrite epoch 2/4:   7%|6         | 8/115\n[00:50<02:30,  1.41s/it]', '\\rOverwrite epoch 2/4:   8%|7         | 9/115\n[00:50<01:46,  1.01s/it]', '\\rOverwrite epoch 2/4:   9%|8         | 10/115\n[00:50<01:16,  1.37it/s]', '\\rOverwrite epoch 2/4:  10%|9         | 11/115\n[00:50<00:56,  1.84it/s]', '\\rOverwrite epoch 2/4:  10%|#         | 12/115\n[00:50<00:42,  2.42it/s]', '\\rOverwrite epoch 2/4:  11%|#1        | 13/115\n[00:50<00:32,  3.10it/s]', '\\rOverwrite epoch 2/4:  12%|#2        | 14/115\n[00:50<00:26,  3.82it/s]', '\\rOverwrite epoch 2/4:  13%|#3        | 15/115\n[00:51<00:21,  4.59it/s]', '\\rOverwrite epoch 2/4:  14%|#3        | 16/115\n[00:51<00:18,  5.34it/s]', '\\rOverwrite epoch 2/4:  15%|#4        | 17/115\n[00:51<00:16,  6.03it/s]', '\\rOverwrite epoch 2/4:  16%|#5        | 18/115\n[00:51<00:14,  6.63it/s]', '\\rOverwrite epoch 2/4:  17%|#6        | 19/115\n[00:51<00:13,  7.11it/s]', '\\rOverwrite epoch 2/4:  17%|#7        | 20/115\n[00:51<00:12,  7.50it/s]', '\\rOverwrite epoch 2/4:  18%|#8        | 21/115\n[00:51<00:12,  7.70it/s]', '\\rOverwrite epoch 2/4:  19%|#9        | 22/115\n[00:51<00:11,  7.95it/s]', '\\rOverwrite epoch 2/4:  20%|##        | 23/115\n[00:51<00:11,  8.13it/s]', '\\rOverwrite epoch 2/4:  21%|##        | 24/115\n[00:52<00:11,  8.23it/s]', '\\rOverwrite epoch 2/4:  22%|##1       | 25/115\n[00:52<00:10,  8.35it/s]', '\\rOverwrite epoch 2/4:  23%|##2       | 26/115\n[00:52<00:10,  8.42it/s]', '\\rOverwrite epoch 2/4:  23%|##3       | 27/115\n[00:52<00:10,  8.48it/s]', '\\rOverwrite epoch 2/4:  24%|##4       | 28/115\n[00:52<00:10,  8.52it/s]', '\\rOverwrite epoch 2/4:  25%|##5       | 29/115\n[00:52<00:10,  8.55it/s]', '\\rOverwrite epoch 2/4:  26%|##6       | 30/115\n[00:52<00:09,  8.58it/s]', '\\rOverwrite epoch 2/4:  27%|##6       | 31/115\n[00:52<00:09,  8.58it/s]', '\\rOverwrite epoch 2/4:  28%|##7       | 32/115\n[00:53<00:09,  8.57it/s]', '\\rOverwrite epoch 2/4:  29%|##8       | 33/115\n[00:53<00:09,  8.59it/s]', '\\rOverwrite epoch 2/4:  30%|##9       | 34/115\n[00:53<00:09,  8.59it/s]', '\\rOverwrite epoch 2/4:  30%|###       | 35/115\n[00:53<00:09,  8.61it/s]', '\\rOverwrite epoch 2/4:  31%|###1      | 36/115\n[00:53<00:09,  8.62it/s]', '\\rOverwrite epoch 2/4:  32%|###2      | 37/115\n[00:53<00:09,  8.62it/s]', '\\rOverwrite epoch 2/4:  33%|###3      | 38/115\n[00:53<00:08,  8.62it/s]', '\\rOverwrite epoch 2/4:  34%|###3      | 39/115\n[00:53<00:08,  8.62it/s]', '\\rOverwrite epoch 2/4:  35%|###4      | 40/115\n[00:53<00:08,  8.62it/s]', '\\rOverwrite epoch 2/4:  36%|###5      | 41/115\n[00:54<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  37%|###6      | 42/115\n[00:54<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  37%|###7      | 43/115\n[00:54<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  38%|###8      | 44/115\n[00:54<00:08,  8.62it/s]', '\\rOverwrite epoch 2/4:  39%|###9      | 45/115\n[00:54<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  40%|####      | 46/115\n[00:54<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  41%|####      | 47/115\n[00:54<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  42%|####1     | 48/115\n[00:54<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  43%|####2     | 49/115\n[00:55<00:07,  8.62it/s]', '\\rOverwrite epoch 2/4:  43%|####3     | 50/115\n[00:55<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  44%|####4     | 51/115\n[00:55<00:07,  8.60it/s]', '\\rOverwrite epoch 2/4:  45%|####5     | 52/115\n[00:55<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  46%|####6     | 53/115\n[00:55<00:07,  8.51it/s]', '\\rOverwrite epoch 2/4:  47%|####6     | 54/115\n[00:55<00:07,  8.48it/s]', '\\rOverwrite epoch 2/4:  48%|####7     | 55/115\n[00:55<00:07,  8.46it/s]', '\\rOverwrite epoch 2/4:  49%|####8     | 56/115\n[00:55<00:06,  8.50it/s]', '\\rOverwrite epoch 2/4:  50%|####9     | 57/115\n[00:55<00:06,  8.52it/s]', '\\rOverwrite epoch 2/4:  50%|#####     | 58/115\n[00:56<00:06,  8.54it/s]', '\\rOverwrite epoch 2/4:  51%|#####1    | 59/115\n[00:56<00:06,  8.55it/s]', '\\rOverwrite epoch 2/4:  52%|#####2    | 60/115\n[00:56<00:06,  8.56it/s]', '\\rOverwrite epoch 2/4:  53%|#####3    | 61/115\n[00:56<00:06,  8.57it/s]', '\\rOverwrite epoch 2/4:  54%|#####3    | 62/115\n[00:56<00:06,  8.56it/s]', '\\rOverwrite epoch 2/4:  55%|#####4    | 63/115\n[00:56<00:06,  8.57it/s]', '\\rOverwrite epoch 2/4:  56%|#####5    | 64/115\n[00:56<00:05,  8.58it/s]', '\\rOverwrite epoch 2/4:  57%|#####6    | 65/115\n[00:56<00:05,  8.58it/s]', '\\rOverwrite epoch 2/4:  57%|#####7    | 66/115\n[00:57<00:05,  8.58it/s]', '\\rOverwrite epoch 2/4:  58%|#####8    | 67/115\n[00:57<00:05,  8.59it/s]', '\\rOverwrite epoch 2/4:  59%|#####9    | 68/115\n[00:57<00:05,  8.60it/s]', '\\rOverwrite epoch 2/4:  60%|######    | 69/115\n[00:57<00:05,  8.57it/s]', '\\rOverwrite epoch 2/4:  61%|######    | 70/115\n[00:57<00:05,  8.51it/s]', '\\rOverwrite epoch 2/4:  62%|######1   | 71/115\n[00:57<00:05,  8.50it/s]', '\\rOverwrite epoch 2/4:  63%|######2   | 72/115\n[00:57<00:05,  8.52it/s]', '\\rOverwrite epoch 2/4:  63%|######3   | 73/115\n[00:57<00:04,  8.53it/s]', '\\rOverwrite epoch 2/4:  64%|######4   | 74/115\n[00:57<00:04,  8.55it/s]', '\\rOverwrite epoch 2/4:  65%|######5   | 75/115\n[00:58<00:04,  8.54it/s]', '\\rOverwrite epoch 2/4:  66%|######6   | 76/115\n[00:58<00:04,  8.55it/s]', '\\rOverwrite epoch 2/4:  67%|######6   | 77/115\n[00:58<00:04,  8.56it/s]', '\\rOverwrite epoch 2/4:  68%|######7   | 78/115\n[00:58<00:04,  8.56it/s]', '\\rOverwrite epoch 2/4:  69%|######8   | 79/115\n[00:58<00:04,  8.57it/s]', '\\rOverwrite epoch 2/4:  70%|######9   | 80/115\n[00:58<00:04,  8.58it/s]', '\\rOverwrite epoch 2/4:  70%|#######   | 81/115\n[00:58<00:03,  8.58it/s]', '\\rOverwrite epoch 2/4:  71%|#######1  | 82/115\n[00:58<00:03,  8.58it/s]', '\\rOverwrite epoch 2/4:  72%|#######2  | 83/115\n[00:58<00:03,  8.49it/s]', '\\rOverwrite epoch 2/4:  73%|#######3  | 84/115\n[00:59<00:03,  8.48it/s]', '[2025-12-03 17:27:25] Overwrite step 200:\navg_train_loss=3.4555', '\\n', '\\rOverwrite epoch 2/4:  74%|#######3  | 85/115\n[00:59<00:03,  8.47it/s]', '\\rOverwrite epoch 2/4:  75%|#######4  | 86/115\n[00:59<00:03,  8.49it/s]', '\\rOverwrite epoch 2/4:  76%|#######5  | 87/115\n[00:59<00:03,  8.52it/s]', '\\rOverwrite epoch 2/4:  77%|#######6  | 88/115\n[00:59<00:03,  8.54it/s]', '\\rOverwrite epoch 2/4:  77%|#######7  | 89/115\n[00:59<00:03,  8.57it/s]', '\\rOverwrite epoch 2/4:  78%|#######8  | 90/115\n[00:59<00:02,  8.60it/s]', '\\rOverwrite epoch 2/4:  79%|#######9  | 91/115\n[00:59<00:02,  8.60it/s]', '\\rOverwrite epoch 2/4:  80%|########  | 92/115\n[01:00<00:02,  8.60it/s]', '\\rOverwrite epoch 2/4:  81%|########  | 93/115\n[01:00<00:02,  8.52it/s]', '\\rOverwrite epoch 2/4:  82%|########1 | 94/115\n[01:00<00:02,  8.48it/s]', '\\rOverwrite epoch 2/4:  83%|########2 | 95/115\n[01:00<00:02,  8.52it/s]', '\\rOverwrite epoch 2/4:  83%|########3 | 96/115\n[01:00<00:02,  8.54it/s]', '\\rOverwrite epoch 2/4:  84%|########4 | 97/115\n[01:00<00:02,  8.53it/s]', '\\rOverwrite epoch 2/4:  85%|########5 | 98/115\n[01:00<00:01,  8.55it/s]', '\\rOverwrite epoch 2/4:  86%|########6 | 99/115\n[01:00<00:01,  8.57it/s]', '\\rOverwrite epoch 2/4:  87%|########6 | 100/115\n[01:00<00:01,  8.59it/s]', '\\rOverwrite epoch 2/4:  88%|########7 | 101/115\n[01:01<00:01,  8.61it/s]', '\\rOverwrite epoch 2/4:  89%|########8 | 102/115\n[01:01<00:01,  8.61it/s]', '\\rOverwrite epoch 2/4:  90%|########9 | 103/115\n[01:01<00:01,  8.60it/s]', '\\rOverwrite epoch 2/4:  90%|######### | 104/115\n[01:01<00:01,  8.61it/s]', '\\rOverwrite epoch 2/4:  91%|#########1| 105/115\n[01:01<00:01,  8.59it/s]', '\\rOverwrite epoch 2/4:  92%|#########2| 106/115\n[01:01<00:01,  8.60it/s]', '\\rOverwrite epoch 2/4:  93%|#########3| 107/115\n[01:01<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  94%|#########3| 108/115\n[01:01<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  95%|#########4| 109/115\n[01:02<00:00,  8.62it/s]', '\\rOverwrite epoch 2/4:  96%|#########5| 110/115\n[01:02<00:00,  8.63it/s]', '\\rOverwrite epoch 2/4:  97%|#########6| 111/115\n[01:02<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  97%|#########7| 112/115\n[01:02<00:00,  8.62it/s]', '\\rOverwrite epoch 2/4:  98%|#########8| 113/115\n[01:02<00:00,  8.62it/s]', '\\rOverwrite epoch 2/4:  99%|#########9| 114/115\n[01:02<00:00,  8.61it/s]', '', '\\rOverwrite epoch 2/4: 100%|##########| 115/115\n[01:03<00:00,  1.81it/s]', '\\n', 'Epoch 2: validation_loss = 3.6212', '\\n',\n'\\rOverwrite epoch 3/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 3/4:   1%|          | 1/115 [01:04<2:02:52, 64.67s/it]', '\\rOverwrite\nepoch 3/4:   2%|1         | 2/115 [01:04<50:16, 26.70s/it]  ', '\\rOverwrite\nepoch 3/4:   3%|2         | 3/115 [01:04<27:10, 14.56s/it]', '\\rOverwrite epoch\n3/4:   3%|3         | 4/115 [01:05<16:23,  8.86s/it]', '\\rOverwrite epoch 3/4:\n4%|4         | 5/115 [01:05<10:27,  5.70s/it]', '\\rOverwrite epoch 3/4:   5%|5\n| 6/115 [01:05<06:54,  3.80s/it]', '\\rOverwrite epoch 3/4:   6%|6         |\n7/115 [01:05<04:40,  2.60s/it]', '\\rOverwrite epoch 3/4:   7%|6         | 8/115\n[01:05<03:13,  1.81s/it]', '\\rOverwrite epoch 3/4:   8%|7         | 9/115\n[01:05<02:15,  1.28s/it]', '\\rOverwrite epoch 3/4:   9%|8         | 10/115\n[01:05<01:36,  1.09it/s]', '\\rOverwrite epoch 3/4:  10%|9         | 11/115\n[01:05<01:10,  1.48it/s]', '\\rOverwrite epoch 3/4:  10%|#         | 12/115\n[01:05<00:51,  1.98it/s]', '\\rOverwrite epoch 3/4:  11%|#1        | 13/115\n[01:06<00:39,  2.59it/s]', '\\rOverwrite epoch 3/4:  12%|#2        | 14/115\n[01:06<00:30,  3.28it/s]', '\\rOverwrite epoch 3/4:  13%|#3        | 15/115\n[01:06<00:24,  4.04it/s]', '\\rOverwrite epoch 3/4:  14%|#3        | 16/115\n[01:06<00:20,  4.81it/s]', '\\rOverwrite epoch 3/4:  15%|#4        | 17/115\n[01:06<00:17,  5.55it/s]', '\\rOverwrite epoch 3/4:  16%|#5        | 18/115\n[01:06<00:15,  6.22it/s]', '\\rOverwrite epoch 3/4:  17%|#6        | 19/115\n[01:06<00:14,  6.80it/s]', '\\rOverwrite epoch 3/4:  17%|#7        | 20/115\n[01:06<00:13,  7.27it/s]', '\\rOverwrite epoch 3/4:  18%|#8        | 21/115\n[01:06<00:12,  7.63it/s]', '\\rOverwrite epoch 3/4:  19%|#9        | 22/115\n[01:07<00:11,  7.91it/s]', '\\rOverwrite epoch 3/4:  20%|##        | 23/115\n[01:07<00:11,  8.12it/s]', '\\rOverwrite epoch 3/4:  21%|##        | 24/115\n[01:07<00:11,  8.27it/s]', '\\rOverwrite epoch 3/4:  22%|##1       | 25/115\n[01:07<00:10,  8.38it/s]', '\\rOverwrite epoch 3/4:  23%|##2       | 26/115\n[01:07<00:10,  8.46it/s]', '\\rOverwrite epoch 3/4:  23%|##3       | 27/115\n[01:07<00:10,  8.52it/s]', '\\rOverwrite epoch 3/4:  24%|##4       | 28/115\n[01:07<00:10,  8.56it/s]', '\\rOverwrite epoch 3/4:  25%|##5       | 29/115\n[01:07<00:10,  8.58it/s]', '\\rOverwrite epoch 3/4:  26%|##6       | 30/115\n[01:08<00:09,  8.61it/s]', '\\rOverwrite epoch 3/4:  27%|##6       | 31/115\n[01:08<00:09,  8.62it/s]', '\\rOverwrite epoch 3/4:  28%|##7       | 32/115\n[01:08<00:09,  8.63it/s]', '\\rOverwrite epoch 3/4:  29%|##8       | 33/115\n[01:08<00:09,  8.64it/s]', '\\rOverwrite epoch 3/4:  30%|##9       | 34/115\n[01:08<00:09,  8.64it/s]', '\\rOverwrite epoch 3/4:  30%|###       | 35/115\n[01:08<00:09,  8.64it/s]', '\\rOverwrite epoch 3/4:  31%|###1      | 36/115\n[01:08<00:09,  8.64it/s]', '\\rOverwrite epoch 3/4:  32%|###2      | 37/115\n[01:08<00:09,  8.64it/s]', '\\rOverwrite epoch 3/4:  33%|###3      | 38/115\n[01:08<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  34%|###3      | 39/115\n[01:09<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  35%|###4      | 40/115\n[01:09<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  36%|###5      | 41/115\n[01:09<00:08,  8.63it/s]', '\\rOverwrite epoch 3/4:  37%|###6      | 42/115\n[01:09<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  37%|###7      | 43/115\n[01:09<00:08,  8.63it/s]', '\\rOverwrite epoch 3/4:  38%|###8      | 44/115\n[01:09<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  39%|###9      | 45/115\n[01:09<00:08,  8.65it/s]', '\\rOverwrite epoch 3/4:  40%|####      | 46/115\n[01:09<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  41%|####      | 47/115\n[01:09<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  42%|####1     | 48/115\n[01:10<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  43%|####2     | 49/115\n[01:10<00:07,  8.62it/s]', '\\rOverwrite epoch 3/4:  43%|####3     | 50/115\n[01:10<00:07,  8.63it/s]', '\\rOverwrite epoch 3/4:  44%|####4     | 51/115\n[01:10<00:07,  8.63it/s]', '\\rOverwrite epoch 3/4:  45%|####5     | 52/115\n[01:10<00:07,  8.63it/s]', '\\rOverwrite epoch 3/4:  46%|####6     | 53/115\n[01:10<00:07,  8.64it/s]', '\\rOverwrite epoch 3/4:  47%|####6     | 54/115\n[01:10<00:07,  8.64it/s]', '\\rOverwrite epoch 3/4:  48%|####7     | 55/115\n[01:10<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  49%|####8     | 56/115\n[01:11<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  50%|####9     | 57/115\n[01:11<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  50%|#####     | 58/115\n[01:11<00:06,  8.64it/s]', '\\rOverwrite epoch 3/4:  51%|#####1    | 59/115\n[01:11<00:06,  8.64it/s]', '\\rOverwrite epoch 3/4:  52%|#####2    | 60/115\n[01:11<00:06,  8.63it/s]', '\\rOverwrite epoch 3/4:  53%|#####3    | 61/115\n[01:11<00:06,  8.64it/s]', '\\rOverwrite epoch 3/4:  54%|#####3    | 62/115\n[01:11<00:06,  8.64it/s]', '\\rOverwrite epoch 3/4:  55%|#####4    | 63/115\n[01:11<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  56%|#####5    | 64/115\n[01:11<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  57%|#####6    | 65/115\n[01:12<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  57%|#####7    | 66/115\n[01:12<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  58%|#####8    | 67/115\n[01:12<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  59%|#####9    | 68/115\n[01:12<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  60%|######    | 69/115\n[01:12<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  61%|######    | 70/115\n[01:12<00:05,  8.64it/s]', '\\rOverwrite epoch 3/4:  62%|######1   | 71/115\n[01:12<00:05,  8.64it/s]', '\\rOverwrite epoch 3/4:  63%|######2   | 72/115\n[01:12<00:04,  8.65it/s]', '\\rOverwrite epoch 3/4:  63%|######3   | 73/115\n[01:12<00:04,  8.65it/s]', '\\rOverwrite epoch 3/4:  64%|######4   | 74/115\n[01:13<00:04,  8.64it/s]', '\\rOverwrite epoch 3/4:  65%|######5   | 75/115\n[01:13<00:04,  8.63it/s]', '\\rOverwrite epoch 3/4:  66%|######6   | 76/115\n[01:13<00:04,  8.63it/s]', '\\rOverwrite epoch 3/4:  67%|######6   | 77/115\n[01:13<00:04,  8.63it/s]', '\\rOverwrite epoch 3/4:  68%|######7   | 78/115\n[01:13<00:04,  8.63it/s]', '\\rOverwrite epoch 3/4:  69%|######8   | 79/115\n[01:13<00:04,  8.64it/s]', '\\rOverwrite epoch 3/4:  70%|######9   | 80/115\n[01:13<00:04,  8.65it/s]', '\\rOverwrite epoch 3/4:  70%|#######   | 81/115\n[01:13<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  71%|#######1  | 82/115\n[01:14<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  72%|#######2  | 83/115\n[01:14<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  73%|#######3  | 84/115\n[01:14<00:03,  8.64it/s]', '\\rOverwrite epoch 3/4:  74%|#######3  | 85/115\n[01:14<00:03,  8.64it/s]', '\\rOverwrite epoch 3/4:  75%|#######4  | 86/115\n[01:14<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  76%|#######5  | 87/115\n[01:14<00:03,  8.64it/s]', '\\rOverwrite epoch 3/4:  77%|#######6  | 88/115\n[01:14<00:03,  8.64it/s]', '\\rOverwrite epoch 3/4:  77%|#######7  | 89/115\n[01:14<00:03,  8.64it/s]', '\\rOverwrite epoch 3/4:  78%|#######8  | 90/115\n[01:14<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  79%|#######9  | 91/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  80%|########  | 92/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  81%|########  | 93/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  82%|########1 | 94/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  83%|########2 | 95/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  83%|########3 | 96/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  84%|########4 | 97/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  85%|########5 | 98/115\n[01:15<00:01,  8.63it/s]', '\\rOverwrite epoch 3/4:  86%|########6 | 99/115\n[01:16<00:01,  8.63it/s]', '\\rOverwrite epoch 3/4:  87%|########6 | 100/115\n[01:16<00:01,  8.63it/s]', '\\rOverwrite epoch 3/4:  88%|########7 | 101/115\n[01:16<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  89%|########8 | 102/115\n[01:16<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  90%|########9 | 103/115\n[01:16<00:01,  8.63it/s]', '\\rOverwrite epoch 3/4:  90%|######### | 104/115\n[01:16<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  91%|#########1| 105/115\n[01:16<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  92%|#########2| 106/115\n[01:16<00:01,  8.62it/s]', '\\rOverwrite epoch 3/4:  93%|#########3| 107/115\n[01:16<00:00,  8.61it/s]', '\\rOverwrite epoch 3/4:  94%|#########3| 108/115\n[01:17<00:00,  8.62it/s]', '\\rOverwrite epoch 3/4:  95%|#########4| 109/115\n[01:17<00:00,  8.62it/s]', '\\rOverwrite epoch 3/4:  96%|#########5| 110/115\n[01:17<00:00,  8.63it/s]', '\\rOverwrite epoch 3/4:  97%|#########6| 111/115\n[01:17<00:00,  8.61it/s]', '\\rOverwrite epoch 3/4:  97%|#########7| 112/115\n[01:17<00:00,  8.62it/s]', '\\rOverwrite epoch 3/4:  98%|#########8| 113/115\n[01:17<00:00,  8.63it/s]', '\\rOverwrite epoch 3/4:  99%|#########9| 114/115\n[01:17<00:00,  8.63it/s]', '', '\\rOverwrite epoch 3/4: 100%|##########| 115/115\n[01:19<00:00,  1.45it/s]', '\\n', 'Epoch 3: validation_loss = 3.6433', '\\n',\n'\\rOverwrite epoch 4/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 4/4:   1%|          | 1/115 [00:46<1:28:36, 46.64s/it]', '\\rOverwrite\nepoch 4/4:   2%|1         | 2/115 [00:46<36:17, 19.27s/it]  ', '\\rOverwrite\nepoch 4/4:   3%|2         | 3/115 [00:46<19:38, 10.52s/it]', '\\rOverwrite epoch\n4/4:   3%|3         | 4/115 [00:46<11:52,  6.42s/it]', '\\rOverwrite epoch 4/4:\n4%|4         | 5/115 [00:47<07:35,  4.14s/it]', '\\rOverwrite epoch 4/4:   5%|5\n| 6/115 [00:47<05:02,  2.77s/it]', '\\rOverwrite epoch 4/4:   6%|6         |\n7/115 [00:47<03:25,  1.90s/it]', '\\rOverwrite epoch 4/4:   7%|6         | 8/115\n[00:47<02:22,  1.34s/it]', '\\rOverwrite epoch 4/4:   8%|7         | 9/115\n[00:47<01:41,  1.05it/s]', '\\rOverwrite epoch 4/4:   9%|8         | 10/115\n[00:47<01:13,  1.44it/s]', '\\rOverwrite epoch 4/4:  10%|9         | 11/115\n[00:47<00:53,  1.93it/s]', '\\rOverwrite epoch 4/4:  10%|#         | 12/115\n[00:47<00:40,  2.53it/s]', '\\rOverwrite epoch 4/4:  11%|#1        | 13/115\n[00:48<00:31,  3.22it/s]', '\\rOverwrite epoch 4/4:  12%|#2        | 14/115\n[00:48<00:25,  3.97it/s]', '\\rOverwrite epoch 4/4:  13%|#3        | 15/115\n[00:48<00:21,  4.74it/s]', '\\rOverwrite epoch 4/4:  14%|#3        | 16/115\n[00:48<00:18,  5.49it/s]', '\\rOverwrite epoch 4/4:  15%|#4        | 17/115\n[00:48<00:15,  6.14it/s]', '\\rOverwrite epoch 4/4:  16%|#5        | 18/115\n[00:48<00:14,  6.73it/s]', '\\rOverwrite epoch 4/4:  17%|#6        | 19/115\n[00:48<00:13,  7.20it/s]', '\\rOverwrite epoch 4/4:  17%|#7        | 20/115\n[00:48<00:12,  7.58it/s]', '\\rOverwrite epoch 4/4:  18%|#8        | 21/115\n[00:48<00:11,  7.86it/s]', '\\rOverwrite epoch 4/4:  19%|#9        | 22/115\n[00:49<00:11,  8.08it/s]', '\\rOverwrite epoch 4/4:  20%|##        | 23/115\n[00:49<00:11,  8.24it/s]', '\\rOverwrite epoch 4/4:  21%|##        | 24/115\n[00:49<00:10,  8.33it/s]', '\\rOverwrite epoch 4/4:  22%|##1       | 25/115\n[00:49<00:10,  8.42it/s]', '\\rOverwrite epoch 4/4:  23%|##2       | 26/115\n[00:49<00:10,  8.47it/s]', '\\rOverwrite epoch 4/4:  23%|##3       | 27/115\n[00:49<00:10,  8.52it/s]', '\\rOverwrite epoch 4/4:  24%|##4       | 28/115\n[00:49<00:10,  8.56it/s]', '\\rOverwrite epoch 4/4:  25%|##5       | 29/115\n[00:49<00:10,  8.58it/s]', '\\rOverwrite epoch 4/4:  26%|##6       | 30/115\n[00:49<00:09,  8.60it/s]', '\\rOverwrite epoch 4/4:  27%|##6       | 31/115\n[00:50<00:09,  8.61it/s]', '\\rOverwrite epoch 4/4:  28%|##7       | 32/115\n[00:50<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  29%|##8       | 33/115\n[00:50<00:09,  8.60it/s]', '\\rOverwrite epoch 4/4:  30%|##9       | 34/115\n[00:50<00:09,  8.61it/s]', '\\rOverwrite epoch 4/4:  30%|###       | 35/115\n[00:50<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  31%|###1      | 36/115\n[00:50<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  32%|###2      | 37/115\n[00:50<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  33%|###3      | 38/115\n[00:50<00:08,  8.62it/s]', '\\rOverwrite epoch 4/4:  34%|###3      | 39/115\n[00:51<00:08,  8.62it/s]', '\\rOverwrite epoch 4/4:  35%|###4      | 40/115\n[00:51<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  36%|###5      | 41/115\n[00:51<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  37%|###6      | 42/115\n[00:51<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  37%|###7      | 43/115\n[00:51<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  38%|###8      | 44/115\n[00:51<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  39%|###9      | 45/115\n[00:51<00:08,  8.62it/s]', '\\rOverwrite epoch 4/4:  40%|####      | 46/115\n[00:51<00:08,  8.62it/s]', '\\rOverwrite epoch 4/4:  41%|####      | 47/115\n[00:51<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  42%|####1     | 48/115\n[00:52<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  43%|####2     | 49/115\n[00:52<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  43%|####3     | 50/115\n[00:52<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  44%|####4     | 51/115\n[00:52<00:07,  8.62it/s]', '\\rOverwrite epoch 4/4:  45%|####5     | 52/115\n[00:52<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  46%|####6     | 53/115\n[00:52<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  47%|####6     | 54/115\n[00:52<00:07,  8.63it/s]', '[2025-12-03 17:31:30] Overwrite step 400:\navg_train_loss=3.1237', '\\n', '\\rOverwrite epoch 4/4:  48%|####7     | 55/115\n[00:52<00:06,  8.63it/s]', '\\rOverwrite epoch 4/4:  49%|####8     | 56/115\n[00:53<00:06,  8.63it/s]', '\\rOverwrite epoch 4/4:  50%|####9     | 57/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  50%|#####     | 58/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  51%|#####1    | 59/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  52%|#####2    | 60/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  53%|#####3    | 61/115\n[00:53<00:06,  8.63it/s]', '\\rOverwrite epoch 4/4:  54%|#####3    | 62/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  55%|#####4    | 63/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  56%|#####5    | 64/115\n[00:53<00:05,  8.63it/s]', '\\rOverwrite epoch 4/4:  57%|#####6    | 65/115\n[00:54<00:05,  8.64it/s]', '\\rOverwrite epoch 4/4:  57%|#####7    | 66/115\n[00:54<00:05,  8.64it/s]', '\\rOverwrite epoch 4/4:  58%|#####8    | 67/115\n[00:54<00:05,  8.61it/s]', '\\rOverwrite epoch 4/4:  59%|#####9    | 68/115\n[00:54<00:05,  8.61it/s]', '\\rOverwrite epoch 4/4:  60%|######    | 69/115\n[00:54<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  61%|######    | 70/115\n[00:54<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  62%|######1   | 71/115\n[00:54<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  63%|######2   | 72/115\n[00:54<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  63%|######3   | 73/115\n[00:54<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  64%|######4   | 74/115\n[00:55<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  65%|######5   | 75/115\n[00:55<00:04,  8.61it/s]', '\\rOverwrite epoch 4/4:  66%|######6   | 76/115\n[00:55<00:04,  8.61it/s]', '\\rOverwrite epoch 4/4:  67%|######6   | 77/115\n[00:55<00:04,  8.62it/s]', '\\rOverwrite epoch 4/4:  68%|######7   | 78/115\n[00:55<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  69%|######8   | 79/115\n[00:55<00:04,  8.62it/s]', '\\rOverwrite epoch 4/4:  70%|######9   | 80/115\n[00:55<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  70%|#######   | 81/115\n[00:55<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  71%|#######1  | 82/115\n[00:56<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  72%|#######2  | 83/115\n[00:56<00:03,  8.64it/s]', '\\rOverwrite epoch 4/4:  73%|#######3  | 84/115\n[00:56<00:03,  8.64it/s]', '\\rOverwrite epoch 4/4:  74%|#######3  | 85/115\n[00:56<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  75%|#######4  | 86/115\n[00:56<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  76%|#######5  | 87/115\n[00:56<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  77%|#######6  | 88/115\n[00:56<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  77%|#######7  | 89/115\n[00:56<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  78%|#######8  | 90/115\n[00:56<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  79%|#######9  | 91/115\n[00:57<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  80%|########  | 92/115\n[00:57<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  81%|########  | 93/115\n[00:57<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  82%|########1 | 94/115\n[00:57<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  83%|########2 | 95/115\n[00:57<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  83%|########3 | 96/115\n[00:57<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  84%|########4 | 97/115\n[00:57<00:02,  8.63it/s]', '\\rOverwrite epoch 4/4:  85%|########5 | 98/115\n[00:57<00:01,  8.63it/s]', '\\rOverwrite epoch 4/4:  86%|########6 | 99/115\n[00:57<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  87%|########6 | 100/115\n[00:58<00:01,  8.63it/s]', '\\rOverwrite epoch 4/4:  88%|########7 | 101/115\n[00:58<00:01,  8.60it/s]', '\\rOverwrite epoch 4/4:  89%|########8 | 102/115\n[00:58<00:01,  8.61it/s]', '\\rOverwrite epoch 4/4:  90%|########9 | 103/115\n[00:58<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  90%|######### | 104/115\n[00:58<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  91%|#########1| 105/115\n[00:58<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  92%|#########2| 106/115\n[00:58<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  93%|#########3| 107/115\n[00:58<00:00,  8.62it/s]', '\\rOverwrite epoch 4/4:  94%|#########3| 108/115\n[00:59<00:00,  8.63it/s]', '\\rOverwrite epoch 4/4:  95%|#########4| 109/115\n[00:59<00:00,  8.63it/s]', '\\rOverwrite epoch 4/4:  96%|#########5| 110/115\n[00:59<00:00,  8.62it/s]', '\\rOverwrite epoch 4/4:  97%|#########6| 111/115\n[00:59<00:00,  8.61it/s]', '\\rOverwrite epoch 4/4:  97%|#########7| 112/115\n[00:59<00:00,  8.62it/s]', '\\rOverwrite epoch 4/4:  98%|#########8| 113/115\n[00:59<00:00,  8.62it/s]', '\\rOverwrite epoch 4/4:  99%|#########9| 114/115\n[00:59<00:00,  8.62it/s]', '', '\\rOverwrite epoch 4/4: 100%|##########| 115/115\n[01:01<00:00,  1.88it/s]', '\\n', 'Epoch 4: validation_loss = 3.6572', '\\n',\n'Experiment complete. Artifacts saved to:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-2/working',\n'\\n', 'Execution time: 11 minutes seconds (time limit is 2 hours).']", "['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls: [' apple', '\ntable', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n33862.31 examples/s]', '\\n', '\\rMap:   0%|          | 0/300 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00, 31570.94\nexamples/s]', '\\n', '\\rTraining synthetic_injection epoch 1/3:   0%|          |\n0/21 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 1/3:   5%|4\n| 1/21 [00:45<15:05, 45.28s/it]', '\\rTraining synthetic_injection epoch 1/3:\n10%|9         | 2/21 [00:45<05:55, 18.71s/it]', '\\rTraining synthetic_injection\nepoch 1/3:  14%|#4        | 3/21 [00:45<03:03, 10.22s/it]', '\\rTraining\nsynthetic_injection epoch 1/3:  19%|#9        | 4/21 [00:45<01:45,  6.23s/it]',\n'\\rTraining synthetic_injection epoch 1/3:  24%|##3       | 5/21 [00:45<01:04,\n4.03s/it]', '\\rTraining synthetic_injection epoch 1/3:  29%|##8       | 6/21\n[00:45<00:40,  2.70s/it]', '\\rTraining synthetic_injection epoch 1/3:  33%|###3\n| 7/21 [00:45<00:25,  1.85s/it]', '\\rTraining synthetic_injection epoch 1/3:\n38%|###8      | 8/21 [00:46<00:16,  1.30s/it]', '\\rTraining synthetic_injection\nepoch 1/3:  43%|####2     | 9/21 [00:46<00:11,  1.08it/s]', '\\rTraining\nsynthetic_injection epoch 1/3:  48%|####7     | 10/21 [00:46<00:07,  1.48it/s]',\n'\\rTraining synthetic_injection epoch 1/3:  52%|#####2    | 11/21 [00:46<00:05,\n1.98it/s]', '\\rTraining synthetic_injection epoch 1/3:  57%|#####7    | 12/21\n[00:46<00:03,  2.59it/s]', '\\rTraining synthetic_injection epoch 1/3:\n62%|######1   | 13/21 [00:46<00:02,  3.29it/s]', '\\rTraining synthetic_injection\nepoch 1/3:  67%|######6   | 14/21 [00:46<00:01,  4.05it/s]', '\\rTraining\nsynthetic_injection epoch 1/3:  71%|#######1  | 15/21 [00:46<00:01,  4.83it/s]',\n'\\rTraining synthetic_injection epoch 1/3:  76%|#######6  | 16/21 [00:46<00:00,\n5.58it/s]', '\\rTraining synthetic_injection epoch 1/3:  81%|########  | 17/21\n[00:47<00:00,  6.26it/s]', '\\rTraining synthetic_injection epoch 1/3:\n86%|########5 | 18/21 [00:47<00:00,  6.83it/s]', '\\rTraining synthetic_injection\nepoch 1/3:  90%|######### | 19/21 [00:47<00:00,  7.31it/s]', '\\rTraining\nsynthetic_injection epoch 1/3:  95%|#########5| 20/21 [00:47<00:00,  7.69it/s]',\n'', '\\rTraining synthetic_injection epoch 1/3: 100%|##########| 21/21\n[00:48<00:00,  2.31s/it]', '\\n', 'Epoch 1 (synthetic_injection): validation_loss\n= 3.2268', '\\n', '\\rTraining synthetic_injection epoch 2/3:   0%|          |\n0/21 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 2/3:   5%|4\n| 1/21 [00:47<15:55, 47.75s/it]', '\\rTraining synthetic_injection epoch 2/3:\n10%|9         | 2/21 [00:47<06:14, 19.73s/it]', '\\rTraining synthetic_injection\nepoch 2/3:  14%|#4        | 3/21 [00:47<03:13, 10.77s/it]', '\\rTraining\nsynthetic_injection epoch 2/3:  19%|#9        | 4/21 [00:48<01:51,  6.57s/it]',\n'\\rTraining synthetic_injection epoch 2/3:  24%|##3       | 5/21 [00:48<01:07,\n4.24s/it]', '\\rTraining synthetic_injection epoch 2/3:  29%|##8       | 6/21\n[00:48<00:42,  2.84s/it]', '\\rTraining synthetic_injection epoch 2/3:  33%|###3\n| 7/21 [00:48<00:27,  1.95s/it]', '\\rTraining synthetic_injection epoch 2/3:\n38%|###8      | 8/21 [00:48<00:17,  1.36s/it]', '\\rTraining synthetic_injection\nepoch 2/3:  43%|####2     | 9/21 [00:48<00:11,  1.03it/s]', '\\rTraining\nsynthetic_injection epoch 2/3:  48%|####7     | 10/21 [00:48<00:07,  1.41it/s]',\n'\\rTraining synthetic_injection epoch 2/3:  52%|#####2    | 11/21 [00:48<00:05,\n1.90it/s]', '\\rTraining synthetic_injection epoch 2/3:  57%|#####7    | 12/21\n[00:49<00:03,  2.49it/s]', '\\rTraining synthetic_injection epoch 2/3:\n62%|######1   | 13/21 [00:49<00:02,  3.18it/s]', '\\rTraining synthetic_injection\nepoch 2/3:  67%|######6   | 14/21 [00:49<00:01,  3.94it/s]', '\\rTraining\nsynthetic_injection epoch 2/3:  71%|#######1  | 15/21 [00:49<00:01,  4.72it/s]',\n'\\rTraining synthetic_injection epoch 2/3:  76%|#######6  | 16/21 [00:49<00:00,\n5.47it/s]', '\\rTraining synthetic_injection epoch 2/3:  81%|########  | 17/21\n[00:49<00:00,  6.16it/s]', '\\rTraining synthetic_injection epoch 2/3:\n86%|########5 | 18/21 [00:49<00:00,  6.75it/s]', '\\rTraining synthetic_injection\nepoch 2/3:  90%|######### | 19/21 [00:49<00:00,  7.24it/s]', '\\rTraining\nsynthetic_injection epoch 2/3:  95%|#########5| 20/21 [00:49<00:00,  7.63it/s]',\n'', '\\rTraining synthetic_injection epoch 2/3: 100%|##########| 21/21\n[00:51<00:00,  2.44s/it]', '\\n', 'Epoch 2 (synthetic_injection): validation_loss\n= 3.3278', '\\n', '\\rTraining synthetic_injection epoch 3/3:   0%|          |\n0/21 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 3/3:   5%|4\n| 1/21 [00:48<16:14, 48.74s/it]', '\\rTraining synthetic_injection epoch 3/3:\n10%|9         | 2/21 [00:48<06:22, 20.14s/it]', '\\rTraining synthetic_injection\nepoch 3/3:  14%|#4        | 3/21 [00:48<03:17, 10.99s/it]', '\\rTraining\nsynthetic_injection epoch 3/3:  19%|#9        | 4/21 [00:49<01:53,  6.70s/it]',\n'\\rTraining synthetic_injection epoch 3/3:  24%|##3       | 5/21 [00:49<01:09,\n4.32s/it]', '\\rTraining synthetic_injection epoch 3/3:  29%|##8       | 6/21\n[00:49<00:43,  2.89s/it]', '\\rTraining synthetic_injection epoch 3/3:  33%|###3\n| 7/21 [00:49<00:27,  1.98s/it]', '\\rTraining synthetic_injection epoch 3/3:\n38%|###8      | 8/21 [00:49<00:18,  1.39s/it]', '\\rTraining synthetic_injection\nepoch 3/3:  43%|####2     | 9/21 [00:49<00:11,  1.01it/s]', '\\rTraining\nsynthetic_injection epoch 3/3:  48%|####7     | 10/21 [00:49<00:07,  1.39it/s]',\n'\\rTraining synthetic_injection epoch 3/3:  52%|#####2    | 11/21 [00:49<00:05,\n1.87it/s]', '\\rTraining synthetic_injection epoch 3/3:  57%|#####7    | 12/21\n[00:50<00:03,  2.45it/s]', '\\rTraining synthetic_injection epoch 3/3:\n62%|######1   | 13/21 [00:50<00:02,  3.14it/s]', '\\rTraining synthetic_injection\nepoch 3/3:  67%|######6   | 14/21 [00:50<00:01,  3.89it/s]', '\\rTraining\nsynthetic_injection epoch 3/3:  71%|#######1  | 15/21 [00:50<00:01,  4.67it/s]',\n'\\rTraining synthetic_injection epoch 3/3:  76%|#######6  | 16/21 [00:50<00:00,\n5.43it/s]', '\\rTraining synthetic_injection epoch 3/3:  81%|########  | 17/21\n[00:50<00:00,  6.12it/s]', '\\rTraining synthetic_injection epoch 3/3:\n86%|########5 | 18/21 [00:50<00:00,  6.72it/s]', '\\rTraining synthetic_injection\nepoch 3/3:  90%|######### | 19/21 [00:50<00:00,  7.22it/s]', '\\rTraining\nsynthetic_injection epoch 3/3:  95%|#########5| 20/21 [00:50<00:00,  7.61it/s]',\n'', '\\rTraining synthetic_injection epoch 3/3: 100%|##########| 21/21\n[00:52<00:00,  2.48s/it]', '\\n', 'Epoch 3 (synthetic_injection): validation_loss\n= 3.3457', '\\n', '\\rOverwrite epoch 1/4:   0%|          | 0/115 [00:00<?,\n?it/s]', '\\rOverwrite epoch 1/4:   1%|          | 1/115 [00:51<1:37:59,\n51.57s/it]', '\\rOverwrite epoch 1/4:   2%|1         | 2/115 [00:51<40:07,\n21.30s/it]  ', '\\rOverwrite epoch 1/4:   3%|2         | 3/115 [00:51<21:42,\n11.63s/it]', '\\rOverwrite epoch 1/4:   3%|3         | 4/115 [00:51<13:06,\n7.08s/it]', '\\rOverwrite epoch 1/4:   4%|4         | 5/115 [00:52<08:22,\n4.57s/it]', '\\rOverwrite epoch 1/4:   5%|5         | 6/115 [00:52<05:33,\n3.06s/it]', '\\rOverwrite epoch 1/4:   6%|6         | 7/115 [00:52<03:46,\n2.09s/it]', '\\rOverwrite epoch 1/4:   7%|6         | 8/115 [00:52<02:36,\n1.46s/it]', '\\rOverwrite epoch 1/4:   8%|7         | 9/115 [00:52<01:50,\n1.04s/it]', '\\rOverwrite epoch 1/4:   9%|8         | 10/115 [00:52<01:19,\n1.32it/s]', '\\rOverwrite epoch 1/4:  10%|9         | 11/115 [00:52<00:58,\n1.78it/s]', '\\rOverwrite epoch 1/4:  10%|#         | 12/115 [00:52<00:43,\n2.35it/s]', '\\rOverwrite epoch 1/4:  11%|#1        | 13/115 [00:52<00:33,\n3.02it/s]', '\\rOverwrite epoch 1/4:  12%|#2        | 14/115 [00:53<00:26,\n3.76it/s]', '\\rOverwrite epoch 1/4:  13%|#3        | 15/115 [00:53<00:22,\n4.53it/s]', '\\rOverwrite epoch 1/4:  14%|#3        | 16/115 [00:53<00:18,\n5.28it/s]', '\\rOverwrite epoch 1/4:  15%|#4        | 17/115 [00:53<00:16,\n5.99it/s]', '\\rOverwrite epoch 1/4:  16%|#5        | 18/115 [00:53<00:14,\n6.60it/s]', '\\rOverwrite epoch 1/4:  17%|#6        | 19/115 [00:53<00:13,\n7.11it/s]', '\\rOverwrite epoch 1/4:  17%|#7        | 20/115 [00:53<00:12,\n7.51it/s]', '\\rOverwrite epoch 1/4:  18%|#8        | 21/115 [00:53<00:12,\n7.82it/s]', '\\rOverwrite epoch 1/4:  19%|#9        | 22/115 [00:53<00:11,\n8.05it/s]', '\\rOverwrite epoch 1/4:  20%|##        | 23/115 [00:54<00:11,\n8.23it/s]', '\\rOverwrite epoch 1/4:  21%|##        | 24/115 [00:54<00:10,\n8.35it/s]', '\\rOverwrite epoch 1/4:  22%|##1       | 25/115 [00:54<00:10,\n8.44it/s]', '\\rOverwrite epoch 1/4:  23%|##2       | 26/115 [00:54<00:10,\n8.50it/s]', '\\rOverwrite epoch 1/4:  23%|##3       | 27/115 [00:54<00:10,\n8.55it/s]', '\\rOverwrite epoch 1/4:  24%|##4       | 28/115 [00:54<00:10,\n8.58it/s]', '\\rOverwrite epoch 1/4:  25%|##5       | 29/115 [00:54<00:09,\n8.60it/s]', '\\rOverwrite epoch 1/4:  26%|##6       | 30/115 [00:54<00:09,\n8.62it/s]', '\\rOverwrite epoch 1/4:  27%|##6       | 31/115 [00:55<00:09,\n8.63it/s]', '\\rOverwrite epoch 1/4:  28%|##7       | 32/115 [00:55<00:09,\n8.64it/s]', '\\rOverwrite epoch 1/4:  29%|##8       | 33/115 [00:55<00:09,\n8.64it/s]', '\\rOverwrite epoch 1/4:  30%|##9       | 34/115 [00:55<00:09,\n8.64it/s]', '\\rOverwrite epoch 1/4:  30%|###       | 35/115 [00:55<00:09,\n8.65it/s]', '\\rOverwrite epoch 1/4:  31%|###1      | 36/115 [00:55<00:09,\n8.64it/s]', '\\rOverwrite epoch 1/4:  32%|###2      | 37/115 [00:55<00:09,\n8.64it/s]', '\\rOverwrite epoch 1/4:  33%|###3      | 38/115 [00:55<00:08,\n8.65it/s]', '\\rOverwrite epoch 1/4:  34%|###3      | 39/115 [00:55<00:08,\n8.65it/s]', '\\rOverwrite epoch 1/4:  35%|###4      | 40/115 [00:56<00:08,\n8.65it/s]', '\\rOverwrite epoch 1/4:  36%|###5      | 41/115 [00:56<00:08,\n8.65it/s]', '\\rOverwrite epoch 1/4:  37%|###6      | 42/115 [00:56<00:08,\n8.65it/s]', '\\rOverwrite epoch 1/4:  37%|###7      | 43/115 [00:56<00:08,\n8.64it/s]', '\\rOverwrite epoch 1/4:  38%|###8      | 44/115 [00:56<00:08,\n8.65it/s]', '\\rOverwrite epoch 1/4:  39%|###9      | 45/115 [00:56<00:08,\n8.65it/s]', '\\rOverwrite epoch 1/4:  40%|####      | 46/115 [00:56<00:07,\n8.65it/s]', '\\rOverwrite epoch 1/4:  41%|####      | 47/115 [00:56<00:07,\n8.64it/s]', '\\rOverwrite epoch 1/4:  42%|####1     | 48/115 [00:57<00:07,\n8.64it/s]', '\\rOverwrite epoch 1/4:  43%|####2     | 49/115 [00:57<00:07,\n8.64it/s]', '\\rOverwrite epoch 1/4:  43%|####3     | 50/115 [00:57<00:07,\n8.65it/s]', '\\rOverwrite epoch 1/4:  44%|####4     | 51/115 [00:57<00:07,\n8.65it/s]', '\\rOverwrite epoch 1/4:  45%|####5     | 52/115 [00:57<00:07,\n8.64it/s]', '\\rOverwrite epoch 1/4:  46%|####6     | 53/115 [00:57<00:07,\n8.64it/s]', '\\rOverwrite epoch 1/4:  47%|####6     | 54/115 [00:57<00:07,\n8.65it/s]', '\\rOverwrite epoch 1/4:  48%|####7     | 55/115 [00:57<00:06,\n8.65it/s]', '\\rOverwrite epoch 1/4:  49%|####8     | 56/115 [00:57<00:06,\n8.65it/s]', '\\rOverwrite epoch 1/4:  50%|####9     | 57/115 [00:58<00:06,\n8.63it/s]', '\\rOverwrite epoch 1/4:  50%|#####     | 58/115 [00:58<00:06,\n8.64it/s]', '\\rOverwrite epoch 1/4:  51%|#####1    | 59/115 [00:58<00:06,\n8.64it/s]', '\\rOverwrite epoch 1/4:  52%|#####2    | 60/115 [00:58<00:06,\n8.63it/s]', '\\rOverwrite epoch 1/4:  53%|#####3    | 61/115 [00:58<00:06,\n8.64it/s]', '\\rOverwrite epoch 1/4:  54%|#####3    | 62/115 [00:58<00:06,\n8.64it/s]', '\\rOverwrite epoch 1/4:  55%|#####4    | 63/115 [00:58<00:06,\n8.65it/s]', '\\rOverwrite epoch 1/4:  56%|#####5    | 64/115 [00:58<00:05,\n8.66it/s]', '\\rOverwrite epoch 1/4:  57%|#####6    | 65/115 [00:58<00:05,\n8.65it/s]', '\\rOverwrite epoch 1/4:  57%|#####7    | 66/115 [00:59<00:05,\n8.66it/s]', '\\rOverwrite epoch 1/4:  58%|#####8    | 67/115 [00:59<00:05,\n8.66it/s]', '\\rOverwrite epoch 1/4:  59%|#####9    | 68/115 [00:59<00:05,\n8.66it/s]', '\\rOverwrite epoch 1/4:  60%|######    | 69/115 [00:59<00:05,\n8.66it/s]', '\\rOverwrite epoch 1/4:  61%|######    | 70/115 [00:59<00:05,\n8.65it/s]', '\\rOverwrite epoch 1/4:  62%|######1   | 71/115 [00:59<00:05,\n8.66it/s]', '\\rOverwrite epoch 1/4:  63%|######2   | 72/115 [00:59<00:04,\n8.66it/s]', '\\rOverwrite epoch 1/4:  63%|######3   | 73/115 [00:59<00:04,\n8.66it/s]', '\\rOverwrite epoch 1/4:  64%|######4   | 74/115 [01:00<00:04,\n8.65it/s]', '\\rOverwrite epoch 1/4:  65%|######5   | 75/115 [01:00<00:04,\n8.65it/s]', '\\rOverwrite epoch 1/4:  66%|######6   | 76/115 [01:00<00:04,\n8.64it/s]', '\\rOverwrite epoch 1/4:  67%|######6   | 77/115 [01:00<00:04,\n8.64it/s]', '\\rOverwrite epoch 1/4:  68%|######7   | 78/115 [01:00<00:04,\n8.64it/s]', '\\rOverwrite epoch 1/4:  69%|######8   | 79/115 [01:00<00:04,\n8.64it/s]', '\\rOverwrite epoch 1/4:  70%|######9   | 80/115 [01:00<00:04,\n8.64it/s]', '\\rOverwrite epoch 1/4:  70%|#######   | 81/115 [01:00<00:03,\n8.65it/s]', '\\rOverwrite epoch 1/4:  71%|#######1  | 82/115 [01:00<00:03,\n8.65it/s]', '\\rOverwrite epoch 1/4:  72%|#######2  | 83/115 [01:01<00:03,\n8.65it/s]', '\\rOverwrite epoch 1/4:  73%|#######3  | 84/115 [01:01<00:03,\n8.65it/s]', '\\rOverwrite epoch 1/4:  74%|#######3  | 85/115 [01:01<00:03,\n8.65it/s]', '\\rOverwrite epoch 1/4:  75%|#######4  | 86/115 [01:01<00:03,\n8.65it/s]', '\\rOverwrite epoch 1/4:  76%|#######5  | 87/115 [01:01<00:03,\n8.63it/s]', '\\rOverwrite epoch 1/4:  77%|#######6  | 88/115 [01:01<00:03,\n8.64it/s]', '\\rOverwrite epoch 1/4:  77%|#######7  | 89/115 [01:01<00:03,\n8.64it/s]', '\\rOverwrite epoch 1/4:  78%|#######8  | 90/115 [01:01<00:02,\n8.65it/s]', '\\rOverwrite epoch 1/4:  79%|#######9  | 91/115 [01:01<00:02,\n8.64it/s]', '\\rOverwrite epoch 1/4:  80%|########  | 92/115 [01:02<00:02,\n8.64it/s]', '\\rOverwrite epoch 1/4:  81%|########  | 93/115 [01:02<00:02,\n8.64it/s]', '\\rOverwrite epoch 1/4:  82%|########1 | 94/115 [01:02<00:02,\n8.64it/s]', '\\rOverwrite epoch 1/4:  83%|########2 | 95/115 [01:02<00:02,\n8.64it/s]', '\\rOverwrite epoch 1/4:  83%|########3 | 96/115 [01:02<00:02,\n8.64it/s]', '\\rOverwrite epoch 1/4:  84%|########4 | 97/115 [01:02<00:02,\n8.64it/s]', '\\rOverwrite epoch 1/4:  85%|########5 | 98/115 [01:02<00:01,\n8.64it/s]', '\\rOverwrite epoch 1/4:  86%|########6 | 99/115 [01:02<00:01,\n8.64it/s]', '\\rOverwrite epoch 1/4:  87%|########6 | 100/115 [01:03<00:01,\n8.64it/s]', '\\rOverwrite epoch 1/4:  88%|########7 | 101/115 [01:03<00:01,\n8.64it/s]', '\\rOverwrite epoch 1/4:  89%|########8 | 102/115 [01:03<00:01,\n8.64it/s]', '\\rOverwrite epoch 1/4:  90%|########9 | 103/115 [01:03<00:01,\n8.65it/s]', '\\rOverwrite epoch 1/4:  90%|######### | 104/115 [01:03<00:01,\n8.65it/s]', '\\rOverwrite epoch 1/4:  91%|#########1| 105/115 [01:03<00:01,\n8.65it/s]', '\\rOverwrite epoch 1/4:  92%|#########2| 106/115 [01:03<00:01,\n8.64it/s]', '\\rOverwrite epoch 1/4:  93%|#########3| 107/115 [01:03<00:00,\n8.64it/s]', '\\rOverwrite epoch 1/4:  94%|#########3| 108/115 [01:03<00:00,\n8.64it/s]', '\\rOverwrite epoch 1/4:  95%|#########4| 109/115 [01:04<00:00,\n8.65it/s]', '\\rOverwrite epoch 1/4:  96%|#########5| 110/115 [01:04<00:00,\n8.65it/s]', '\\rOverwrite epoch 1/4:  97%|#########6| 111/115 [01:04<00:00,\n8.64it/s]', '\\rOverwrite epoch 1/4:  97%|#########7| 112/115 [01:04<00:00,\n8.65it/s]', '\\rOverwrite epoch 1/4:  98%|#########8| 113/115 [01:04<00:00,\n8.65it/s]', '\\rOverwrite epoch 1/4:  99%|#########9| 114/115 [01:04<00:00,\n8.65it/s]', '', '\\rOverwrite epoch 1/4: 100%|##########| 115/115 [01:05<00:00,\n1.75it/s]', '\\n', 'Epoch 1 (overwrite): validation_loss = 3.6373', '\\n',\n'\\rOverwrite epoch 2/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 2/4:   1%|          | 1/115 [00:47<1:30:42, 47.74s/it]', '\\rOverwrite\nepoch 2/4:   2%|1         | 2/115 [00:47<37:08, 19.73s/it]  ', '\\rOverwrite\nepoch 2/4:   3%|2         | 3/115 [00:47<20:06, 10.77s/it]', '\\rOverwrite epoch\n2/4:   3%|3         | 4/115 [00:48<12:08,  6.56s/it]', '\\rOverwrite epoch 2/4:\n4%|4         | 5/115 [00:48<07:46,  4.24s/it]', '\\rOverwrite epoch 2/4:   5%|5\n| 6/115 [00:48<05:09,  2.84s/it]', '\\rOverwrite epoch 2/4:   6%|6         |\n7/115 [00:48<03:30,  1.95s/it]', '\\rOverwrite epoch 2/4:   7%|6         | 8/115\n[00:48<02:25,  1.36s/it]', '\\rOverwrite epoch 2/4:   8%|7         | 9/115\n[00:48<01:43,  1.03it/s]', '\\rOverwrite epoch 2/4:   9%|8         | 10/115\n[00:48<01:14,  1.41it/s]', '\\rOverwrite epoch 2/4:  10%|9         | 11/115\n[00:48<00:54,  1.90it/s]', '\\rOverwrite epoch 2/4:  10%|#         | 12/115\n[00:49<00:41,  2.48it/s]', '\\rOverwrite epoch 2/4:  11%|#1        | 13/115\n[00:49<00:32,  3.17it/s]', '\\rOverwrite epoch 2/4:  12%|#2        | 14/115\n[00:49<00:25,  3.92it/s]', '\\rOverwrite epoch 2/4:  13%|#3        | 15/115\n[00:49<00:21,  4.69it/s]', '\\rOverwrite epoch 2/4:  14%|#3        | 16/115\n[00:49<00:18,  5.44it/s]', '\\rOverwrite epoch 2/4:  15%|#4        | 17/115\n[00:49<00:16,  6.12it/s]', '\\rOverwrite epoch 2/4:  16%|#5        | 18/115\n[00:49<00:14,  6.71it/s]', '\\rOverwrite epoch 2/4:  17%|#6        | 19/115\n[00:49<00:13,  7.19it/s]', '\\rOverwrite epoch 2/4:  17%|#7        | 20/115\n[00:49<00:12,  7.55it/s]', '\\rOverwrite epoch 2/4:  18%|#8        | 21/115\n[00:50<00:11,  7.85it/s]', '\\rOverwrite epoch 2/4:  19%|#9        | 22/115\n[00:50<00:11,  8.07it/s]', '\\rOverwrite epoch 2/4:  20%|##        | 23/115\n[00:50<00:11,  8.22it/s]', '\\rOverwrite epoch 2/4:  21%|##        | 24/115\n[00:50<00:10,  8.34it/s]', '\\rOverwrite epoch 2/4:  22%|##1       | 25/115\n[00:50<00:10,  8.43it/s]', '\\rOverwrite epoch 2/4:  23%|##2       | 26/115\n[00:50<00:10,  8.50it/s]', '\\rOverwrite epoch 2/4:  23%|##3       | 27/115\n[00:50<00:10,  8.54it/s]', '\\rOverwrite epoch 2/4:  24%|##4       | 28/115\n[00:50<00:10,  8.58it/s]', '\\rOverwrite epoch 2/4:  25%|##5       | 29/115\n[00:50<00:10,  8.60it/s]', '\\rOverwrite epoch 2/4:  26%|##6       | 30/115\n[00:51<00:09,  8.61it/s]', '\\rOverwrite epoch 2/4:  27%|##6       | 31/115\n[00:51<00:09,  8.62it/s]', '\\rOverwrite epoch 2/4:  28%|##7       | 32/115\n[00:51<00:09,  8.62it/s]', '\\rOverwrite epoch 2/4:  29%|##8       | 33/115\n[00:51<00:09,  8.63it/s]', '\\rOverwrite epoch 2/4:  30%|##9       | 34/115\n[00:51<00:09,  8.64it/s]', '\\rOverwrite epoch 2/4:  30%|###       | 35/115\n[00:51<00:09,  8.63it/s]', '\\rOverwrite epoch 2/4:  31%|###1      | 36/115\n[00:51<00:09,  8.64it/s]', '\\rOverwrite epoch 2/4:  32%|###2      | 37/115\n[00:51<00:09,  8.64it/s]', '\\rOverwrite epoch 2/4:  33%|###3      | 38/115\n[00:52<00:08,  8.64it/s]', '\\rOverwrite epoch 2/4:  34%|###3      | 39/115\n[00:52<00:08,  8.65it/s]', '\\rOverwrite epoch 2/4:  35%|###4      | 40/115\n[00:52<00:08,  8.65it/s]', '\\rOverwrite epoch 2/4:  36%|###5      | 41/115\n[00:52<00:08,  8.64it/s]', '\\rOverwrite epoch 2/4:  37%|###6      | 42/115\n[00:52<00:08,  8.64it/s]', '\\rOverwrite epoch 2/4:  37%|###7      | 43/115\n[00:52<00:08,  8.65it/s]', '\\rOverwrite epoch 2/4:  38%|###8      | 44/115\n[00:52<00:08,  8.65it/s]', '\\rOverwrite epoch 2/4:  39%|###9      | 45/115\n[00:52<00:08,  8.64it/s]', '\\rOverwrite epoch 2/4:  40%|####      | 46/115\n[00:52<00:07,  8.64it/s]', '\\rOverwrite epoch 2/4:  41%|####      | 47/115\n[00:53<00:07,  8.65it/s]', '\\rOverwrite epoch 2/4:  42%|####1     | 48/115\n[00:53<00:07,  8.65it/s]', '\\rOverwrite epoch 2/4:  43%|####2     | 49/115\n[00:53<00:07,  8.64it/s]', '\\rOverwrite epoch 2/4:  43%|####3     | 50/115\n[00:53<00:07,  8.64it/s]', '\\rOverwrite epoch 2/4:  44%|####4     | 51/115\n[00:53<00:07,  8.62it/s]', '\\rOverwrite epoch 2/4:  45%|####5     | 52/115\n[00:53<00:07,  8.59it/s]', '\\rOverwrite epoch 2/4:  46%|####6     | 53/115\n[00:53<00:07,  8.59it/s]', '\\rOverwrite epoch 2/4:  47%|####6     | 54/115\n[00:53<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  48%|####7     | 55/115\n[00:53<00:06,  8.62it/s]', '\\rOverwrite epoch 2/4:  49%|####8     | 56/115\n[00:54<00:06,  8.63it/s]', '\\rOverwrite epoch 2/4:  50%|####9     | 57/115\n[00:54<00:06,  8.63it/s]', '\\rOverwrite epoch 2/4:  50%|#####     | 58/115\n[00:54<00:06,  8.64it/s]', '\\rOverwrite epoch 2/4:  51%|#####1    | 59/115\n[00:54<00:06,  8.64it/s]', '\\rOverwrite epoch 2/4:  52%|#####2    | 60/115\n[00:54<00:06,  8.64it/s]', '\\rOverwrite epoch 2/4:  53%|#####3    | 61/115\n[00:54<00:06,  8.63it/s]', '\\rOverwrite epoch 2/4:  54%|#####3    | 62/115\n[00:54<00:06,  8.64it/s]', '\\rOverwrite epoch 2/4:  55%|#####4    | 63/115\n[00:54<00:06,  8.64it/s]', '\\rOverwrite epoch 2/4:  56%|#####5    | 64/115\n[00:55<00:05,  8.64it/s]', '\\rOverwrite epoch 2/4:  57%|#####6    | 65/115\n[00:55<00:05,  8.65it/s]', '\\rOverwrite epoch 2/4:  57%|#####7    | 66/115\n[00:55<00:05,  8.62it/s]', '\\rOverwrite epoch 2/4:  58%|#####8    | 67/115\n[00:55<00:05,  8.62it/s]', '\\rOverwrite epoch 2/4:  59%|#####9    | 68/115\n[00:55<00:05,  8.63it/s]', '\\rOverwrite epoch 2/4:  60%|######    | 69/115\n[00:55<00:05,  8.63it/s]', '\\rOverwrite epoch 2/4:  61%|######    | 70/115\n[00:55<00:05,  8.63it/s]', '\\rOverwrite epoch 2/4:  62%|######1   | 71/115\n[00:55<00:05,  8.61it/s]', '\\rOverwrite epoch 2/4:  63%|######2   | 72/115\n[00:55<00:04,  8.61it/s]', '\\rOverwrite epoch 2/4:  63%|######3   | 73/115\n[00:56<00:04,  8.62it/s]', '\\rOverwrite epoch 2/4:  64%|######4   | 74/115\n[00:56<00:04,  8.62it/s]', '\\rOverwrite epoch 2/4:  65%|######5   | 75/115\n[00:56<00:04,  8.62it/s]', '\\rOverwrite epoch 2/4:  66%|######6   | 76/115\n[00:56<00:04,  8.60it/s]', '\\rOverwrite epoch 2/4:  67%|######6   | 77/115\n[00:56<00:04,  8.61it/s]', '\\rOverwrite epoch 2/4:  68%|######7   | 78/115\n[00:56<00:04,  8.62it/s]', '\\rOverwrite epoch 2/4:  69%|######8   | 79/115\n[00:56<00:04,  8.61it/s]', '\\rOverwrite epoch 2/4:  70%|######9   | 80/115\n[00:56<00:04,  8.62it/s]', '\\rOverwrite epoch 2/4:  70%|#######   | 81/115\n[00:57<00:03,  8.62it/s]', '\\rOverwrite epoch 2/4:  71%|#######1  | 82/115\n[00:57<00:03,  8.63it/s]', '\\rOverwrite epoch 2/4:  72%|#######2  | 83/115\n[00:57<00:03,  8.63it/s]', '\\rOverwrite epoch 2/4:  73%|#######3  | 84/115\n[00:57<00:03,  8.63it/s]', '[2025-12-03 18:04:05] Overwrite step 200:\navg_train_loss=3.4789', '\\n', '\\rOverwrite epoch 2/4:  74%|#######3  | 85/115\n[00:57<00:03,  8.63it/s]', '\\rOverwrite epoch 2/4:  75%|#######4  | 86/115\n[00:57<00:03,  8.58it/s]', '\\rOverwrite epoch 2/4:  76%|#######5  | 87/115\n[00:57<00:03,  8.60it/s]', '\\rOverwrite epoch 2/4:  77%|#######6  | 88/115\n[00:57<00:03,  8.61it/s]', '\\rOverwrite epoch 2/4:  77%|#######7  | 89/115\n[00:57<00:03,  8.62it/s]', '\\rOverwrite epoch 2/4:  78%|#######8  | 90/115\n[00:58<00:02,  8.59it/s]', '\\rOverwrite epoch 2/4:  79%|#######9  | 91/115\n[00:58<00:02,  8.60it/s]', '\\rOverwrite epoch 2/4:  80%|########  | 92/115\n[00:58<00:02,  8.62it/s]', '\\rOverwrite epoch 2/4:  81%|########  | 93/115\n[00:58<00:02,  8.62it/s]', '\\rOverwrite epoch 2/4:  82%|########1 | 94/115\n[00:58<00:02,  8.63it/s]', '\\rOverwrite epoch 2/4:  83%|########2 | 95/115\n[00:58<00:02,  8.63it/s]', '\\rOverwrite epoch 2/4:  83%|########3 | 96/115\n[00:58<00:02,  8.63it/s]', '\\rOverwrite epoch 2/4:  84%|########4 | 97/115\n[00:58<00:02,  8.64it/s]', '\\rOverwrite epoch 2/4:  85%|########5 | 98/115\n[00:58<00:01,  8.64it/s]', '\\rOverwrite epoch 2/4:  86%|########6 | 99/115\n[00:59<00:01,  8.64it/s]', '\\rOverwrite epoch 2/4:  87%|########6 | 100/115\n[00:59<00:01,  8.64it/s]', '\\rOverwrite epoch 2/4:  88%|########7 | 101/115\n[00:59<00:01,  8.63it/s]', '\\rOverwrite epoch 2/4:  89%|########8 | 102/115\n[00:59<00:01,  8.63it/s]', '\\rOverwrite epoch 2/4:  90%|########9 | 103/115\n[00:59<00:01,  8.63it/s]', '\\rOverwrite epoch 2/4:  90%|######### | 104/115\n[00:59<00:01,  8.63it/s]', '\\rOverwrite epoch 2/4:  91%|#########1| 105/115\n[00:59<00:01,  8.63it/s]', '\\rOverwrite epoch 2/4:  92%|#########2| 106/115\n[00:59<00:01,  8.63it/s]', '\\rOverwrite epoch 2/4:  93%|#########3| 107/115\n[01:00<00:00,  8.63it/s]', '\\rOverwrite epoch 2/4:  94%|#########3| 108/115\n[01:00<00:00,  8.64it/s]', '\\rOverwrite epoch 2/4:  95%|#########4| 109/115\n[01:00<00:00,  8.62it/s]', '\\rOverwrite epoch 2/4:  96%|#########5| 110/115\n[01:00<00:00,  8.62it/s]', '\\rOverwrite epoch 2/4:  97%|#########6| 111/115\n[01:00<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  97%|#########7| 112/115\n[01:00<00:00,  8.62it/s]', '\\rOverwrite epoch 2/4:  98%|#########8| 113/115\n[01:00<00:00,  8.63it/s]', '\\rOverwrite epoch 2/4:  99%|#########9| 114/115\n[01:00<00:00,  8.63it/s]', '', '\\rOverwrite epoch 2/4: 100%|##########| 115/115\n[01:01<00:00,  1.86it/s]', '\\n', 'Epoch 2 (overwrite): validation_loss =\n3.6225', '\\n', '\\rOverwrite epoch 3/4:   0%|          | 0/115 [00:00<?, ?it/s]',\n'\\rOverwrite epoch 3/4:   1%|          | 1/115 [00:52<1:40:30, 52.90s/it]',\n'\\rOverwrite epoch 3/4:   2%|1         | 2/115 [00:53<41:08, 21.85s/it]  ',\n'\\rOverwrite epoch 3/4:   3%|2         | 3/115 [00:53<22:15, 11.93s/it]',\n'\\rOverwrite epoch 3/4:   3%|3         | 4/115 [00:53<13:26,  7.26s/it]',\n'\\rOverwrite epoch 3/4:   4%|4         | 5/115 [00:53<08:35,  4.69s/it]',\n'\\rOverwrite epoch 3/4:   5%|5         | 6/115 [00:53<05:41,  3.13s/it]',\n'\\rOverwrite epoch 3/4:   6%|6         | 7/115 [00:53<03:51,  2.15s/it]',\n'\\rOverwrite epoch 3/4:   7%|6         | 8/115 [00:53<02:40,  1.50s/it]',\n'\\rOverwrite epoch 3/4:   8%|7         | 9/115 [00:53<01:53,  1.07s/it]',\n'\\rOverwrite epoch 3/4:   9%|8         | 10/115 [00:53<01:21,  1.29it/s]',\n'\\rOverwrite epoch 3/4:  10%|9         | 11/115 [00:54<00:59,  1.75it/s]',\n'\\rOverwrite epoch 3/4:  10%|#         | 12/115 [00:54<00:44,  2.31it/s]',\n'\\rOverwrite epoch 3/4:  11%|#1        | 13/115 [00:54<00:34,  2.97it/s]',\n'\\rOverwrite epoch 3/4:  12%|#2        | 14/115 [00:54<00:27,  3.70it/s]',\n'\\rOverwrite epoch 3/4:  13%|#3        | 15/115 [00:54<00:22,  4.48it/s]',\n'\\rOverwrite epoch 3/4:  14%|#3        | 16/115 [00:54<00:18,  5.24it/s]',\n'\\rOverwrite epoch 3/4:  15%|#4        | 17/115 [00:54<00:16,  5.93it/s]',\n'\\rOverwrite epoch 3/4:  16%|#5        | 18/115 [00:54<00:14,  6.55it/s]',\n'\\rOverwrite epoch 3/4:  17%|#6        | 19/115 [00:54<00:13,  7.06it/s]',\n'\\rOverwrite epoch 3/4:  17%|#7        | 20/115 [00:55<00:12,  7.47it/s]',\n'\\rOverwrite epoch 3/4:  18%|#8        | 21/115 [00:55<00:12,  7.79it/s]',\n'\\rOverwrite epoch 3/4:  19%|#9        | 22/115 [00:55<00:11,  8.02it/s]',\n'\\rOverwrite epoch 3/4:  20%|##        | 23/115 [00:55<00:11,  8.19it/s]',\n'\\rOverwrite epoch 3/4:  21%|##        | 24/115 [00:55<00:10,  8.33it/s]',\n'\\rOverwrite epoch 3/4:  22%|##1       | 25/115 [00:55<00:10,  8.42it/s]',\n'\\rOverwrite epoch 3/4:  23%|##2       | 26/115 [00:55<00:10,  8.49it/s]',\n'\\rOverwrite epoch 3/4:  23%|##3       | 27/115 [00:55<00:10,  8.53it/s]',\n'\\rOverwrite epoch 3/4:  24%|##4       | 28/115 [00:56<00:10,  8.57it/s]',\n'\\rOverwrite epoch 3/4:  25%|##5       | 29/115 [00:56<00:10,  8.60it/s]',\n'\\rOverwrite epoch 3/4:  26%|##6       | 30/115 [00:56<00:09,  8.61it/s]',\n'\\rOverwrite epoch 3/4:  27%|##6       | 31/115 [00:56<00:09,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  28%|##7       | 32/115 [00:56<00:09,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  29%|##8       | 33/115 [00:56<00:09,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  30%|##9       | 34/115 [00:56<00:09,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  30%|###       | 35/115 [00:56<00:09,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  31%|###1      | 36/115 [00:56<00:09,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  32%|###2      | 37/115 [00:57<00:09,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  33%|###3      | 38/115 [00:57<00:08,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  34%|###3      | 39/115 [00:57<00:08,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  35%|###4      | 40/115 [00:57<00:08,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  36%|###5      | 41/115 [00:57<00:08,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  37%|###6      | 42/115 [00:57<00:08,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  37%|###7      | 43/115 [00:57<00:08,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  38%|###8      | 44/115 [00:57<00:08,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  39%|###9      | 45/115 [00:57<00:08,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  40%|####      | 46/115 [00:58<00:07,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  41%|####      | 47/115 [00:58<00:07,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  42%|####1     | 48/115 [00:58<00:07,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  43%|####2     | 49/115 [00:58<00:07,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  43%|####3     | 50/115 [00:58<00:07,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  44%|####4     | 51/115 [00:58<00:07,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  45%|####5     | 52/115 [00:58<00:07,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  46%|####6     | 53/115 [00:58<00:07,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  47%|####6     | 54/115 [00:59<00:07,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  48%|####7     | 55/115 [00:59<00:06,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  49%|####8     | 56/115 [00:59<00:06,  8.63it/s]',\n'\\rOverwrite epoch 3/4:  50%|####9     | 57/115 [00:59<00:06,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  50%|#####     | 58/115 [00:59<00:06,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  51%|#####1    | 59/115 [00:59<00:06,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  52%|#####2    | 60/115 [00:59<00:06,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  53%|#####3    | 61/115 [00:59<00:06,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  54%|#####3    | 62/115 [00:59<00:06,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  55%|#####4    | 63/115 [01:00<00:06,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  56%|#####5    | 64/115 [01:00<00:05,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  57%|#####6    | 65/115 [01:00<00:05,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  57%|#####7    | 66/115 [01:00<00:05,  8.63it/s]',\n'\\rOverwrite epoch 3/4:  58%|#####8    | 67/115 [01:00<00:05,  8.63it/s]',\n'\\rOverwrite epoch 3/4:  59%|#####9    | 68/115 [01:00<00:05,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  60%|######    | 69/115 [01:00<00:05,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  61%|######    | 70/115 [01:00<00:05,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  62%|######1   | 71/115 [01:00<00:05,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  63%|######2   | 72/115 [01:01<00:04,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  63%|######3   | 73/115 [01:01<00:04,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  64%|######4   | 74/115 [01:01<00:04,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  65%|######5   | 75/115 [01:01<00:04,  8.61it/s]',\n'\\rOverwrite epoch 3/4:  66%|######6   | 76/115 [01:01<00:04,  8.61it/s]',\n'\\rOverwrite epoch 3/4:  67%|######6   | 77/115 [01:01<00:04,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  68%|######7   | 78/115 [01:01<00:04,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  69%|######8   | 79/115 [01:01<00:04,  8.63it/s]',\n'\\rOverwrite epoch 3/4:  70%|######9   | 80/115 [01:02<00:04,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  70%|#######   | 81/115 [01:02<00:03,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  71%|#######1  | 82/115 [01:02<00:03,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  72%|#######2  | 83/115 [01:02<00:03,  8.63it/s]',\n'\\rOverwrite epoch 3/4:  73%|#######3  | 84/115 [01:02<00:03,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  74%|#######3  | 85/115 [01:02<00:03,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  75%|#######4  | 86/115 [01:02<00:03,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  76%|#######5  | 87/115 [01:02<00:03,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  77%|#######6  | 88/115 [01:02<00:03,  8.65it/s]',\n'\\rOverwrite epoch 3/4:  77%|#######7  | 89/115 [01:03<00:03,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  78%|#######8  | 90/115 [01:03<00:02,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  79%|#######9  | 91/115 [01:03<00:02,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  80%|########  | 92/115 [01:03<00:02,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  81%|########  | 93/115 [01:03<00:02,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  82%|########1 | 94/115 [01:03<00:02,  8.63it/s]',\n'\\rOverwrite epoch 3/4:  83%|########2 | 95/115 [01:03<00:02,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  83%|########3 | 96/115 [01:03<00:02,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  84%|########4 | 97/115 [01:04<00:02,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  85%|########5 | 98/115 [01:04<00:01,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  86%|########6 | 99/115 [01:04<00:01,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  87%|########6 | 100/115 [01:04<00:01,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  88%|########7 | 101/115 [01:04<00:01,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  89%|########8 | 102/115 [01:04<00:01,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  90%|########9 | 103/115 [01:04<00:01,  8.63it/s]',\n'\\rOverwrite epoch 3/4:  90%|######### | 104/115 [01:04<00:01,  8.63it/s]',\n'\\rOverwrite epoch 3/4:  91%|#########1| 105/115 [01:04<00:01,  8.63it/s]',\n'\\rOverwrite epoch 3/4:  92%|#########2| 106/115 [01:05<00:01,  8.63it/s]',\n'\\rOverwrite epoch 3/4:  93%|#########3| 107/115 [01:05<00:00,  8.63it/s]',\n'\\rOverwrite epoch 3/4:  94%|#########3| 108/115 [01:05<00:00,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  95%|#########4| 109/115 [01:05<00:00,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  96%|#########5| 110/115 [01:05<00:00,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  97%|#########6| 111/115 [01:05<00:00,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  97%|#########7| 112/115 [01:05<00:00,  8.62it/s]',\n'\\rOverwrite epoch 3/4:  98%|#########8| 113/115 [01:05<00:00,  8.64it/s]',\n'\\rOverwrite epoch 3/4:  99%|#########9| 114/115 [01:05<00:00,  8.64it/s]', '',\n'\\rOverwrite epoch 3/4: 100%|##########| 115/115 [01:07<00:00,  1.71it/s]',\n'\\n', 'Epoch 3 (overwrite): validation_loss = 3.6321', '\\n', '\\rOverwrite epoch\n4/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite epoch 4/4:   1%|\n| 1/115 [00:50<1:35:28, 50.25s/it]', '\\rOverwrite epoch 4/4:   2%|1         |\n2/115 [00:50<39:05, 20.76s/it]  ', '\\rOverwrite epoch 4/4:   3%|2         |\n3/115 [00:50<21:09, 11.33s/it]', '\\rOverwrite epoch 4/4:   3%|3         | 4/115\n[00:50<12:46,  6.90s/it]', '\\rOverwrite epoch 4/4:   4%|4         | 5/115\n[00:50<08:10,  4.46s/it]', '\\rOverwrite epoch 4/4:   5%|5         | 6/115\n[00:50<05:24,  2.98s/it]', '\\rOverwrite epoch 4/4:   6%|6         | 7/115\n[00:50<03:40,  2.04s/it]', '\\rOverwrite epoch 4/4:   7%|6         | 8/115\n[00:51<02:32,  1.43s/it]', '\\rOverwrite epoch 4/4:   8%|7         | 9/115\n[00:51<01:48,  1.02s/it]', '\\rOverwrite epoch 4/4:   9%|8         | 10/115\n[00:51<01:17,  1.35it/s]', '\\rOverwrite epoch 4/4:  10%|9         | 11/115\n[00:51<00:57,  1.82it/s]', '\\rOverwrite epoch 4/4:  10%|#         | 12/115\n[00:51<00:42,  2.40it/s]', '\\rOverwrite epoch 4/4:  11%|#1        | 13/115\n[00:51<00:33,  3.07it/s]', '\\rOverwrite epoch 4/4:  12%|#2        | 14/115\n[00:51<00:26,  3.81it/s]', '\\rOverwrite epoch 4/4:  13%|#3        | 15/115\n[00:51<00:21,  4.59it/s]', '\\rOverwrite epoch 4/4:  14%|#3        | 16/115\n[00:51<00:18,  5.34it/s]', '\\rOverwrite epoch 4/4:  15%|#4        | 17/115\n[00:52<00:16,  6.04it/s]', '\\rOverwrite epoch 4/4:  16%|#5        | 18/115\n[00:52<00:14,  6.64it/s]', '\\rOverwrite epoch 4/4:  17%|#6        | 19/115\n[00:52<00:13,  7.14it/s]', '\\rOverwrite epoch 4/4:  17%|#7        | 20/115\n[00:52<00:12,  7.53it/s]', '\\rOverwrite epoch 4/4:  18%|#8        | 21/115\n[00:52<00:11,  7.84it/s]', '\\rOverwrite epoch 4/4:  19%|#9        | 22/115\n[00:52<00:11,  8.05it/s]', '\\rOverwrite epoch 4/4:  20%|##        | 23/115\n[00:52<00:11,  8.21it/s]', '\\rOverwrite epoch 4/4:  21%|##        | 24/115\n[00:52<00:10,  8.32it/s]', '\\rOverwrite epoch 4/4:  22%|##1       | 25/115\n[00:53<00:10,  8.42it/s]', '\\rOverwrite epoch 4/4:  23%|##2       | 26/115\n[00:53<00:10,  8.49it/s]', '\\rOverwrite epoch 4/4:  23%|##3       | 27/115\n[00:53<00:10,  8.53it/s]', '\\rOverwrite epoch 4/4:  24%|##4       | 28/115\n[00:53<00:10,  8.57it/s]', '\\rOverwrite epoch 4/4:  25%|##5       | 29/115\n[00:53<00:10,  8.60it/s]', '\\rOverwrite epoch 4/4:  26%|##6       | 30/115\n[00:53<00:09,  8.61it/s]', '\\rOverwrite epoch 4/4:  27%|##6       | 31/115\n[00:53<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  28%|##7       | 32/115\n[00:53<00:09,  8.63it/s]', '\\rOverwrite epoch 4/4:  29%|##8       | 33/115\n[00:53<00:09,  8.63it/s]', '\\rOverwrite epoch 4/4:  30%|##9       | 34/115\n[00:54<00:09,  8.64it/s]', '\\rOverwrite epoch 4/4:  30%|###       | 35/115\n[00:54<00:09,  8.65it/s]', '\\rOverwrite epoch 4/4:  31%|###1      | 36/115\n[00:54<00:09,  8.64it/s]', '\\rOverwrite epoch 4/4:  32%|###2      | 37/115\n[00:54<00:09,  8.65it/s]', '\\rOverwrite epoch 4/4:  33%|###3      | 38/115\n[00:54<00:08,  8.65it/s]', '\\rOverwrite epoch 4/4:  34%|###3      | 39/115\n[00:54<00:08,  8.65it/s]', '\\rOverwrite epoch 4/4:  35%|###4      | 40/115\n[00:54<00:08,  8.66it/s]', '\\rOverwrite epoch 4/4:  36%|###5      | 41/115\n[00:54<00:08,  8.65it/s]', '\\rOverwrite epoch 4/4:  37%|###6      | 42/115\n[00:54<00:08,  8.65it/s]', '\\rOverwrite epoch 4/4:  37%|###7      | 43/115\n[00:55<00:08,  8.65it/s]', '\\rOverwrite epoch 4/4:  38%|###8      | 44/115\n[00:55<00:08,  8.65it/s]', '\\rOverwrite epoch 4/4:  39%|###9      | 45/115\n[00:55<00:08,  8.65it/s]', '\\rOverwrite epoch 4/4:  40%|####      | 46/115\n[00:55<00:07,  8.65it/s]', '\\rOverwrite epoch 4/4:  41%|####      | 47/115\n[00:55<00:07,  8.65it/s]', '\\rOverwrite epoch 4/4:  42%|####1     | 48/115\n[00:55<00:07,  8.66it/s]', '\\rOverwrite epoch 4/4:  43%|####2     | 49/115\n[00:55<00:07,  8.65it/s]', '\\rOverwrite epoch 4/4:  43%|####3     | 50/115\n[00:55<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  44%|####4     | 51/115\n[00:56<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  45%|####5     | 52/115\n[00:56<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  46%|####6     | 53/115\n[00:56<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  47%|####6     | 54/115\n[00:56<00:07,  8.64it/s]', '[2025-12-03 18:08:01] Overwrite step 400:\navg_train_loss=3.1331', '\\n', '\\rOverwrite epoch 4/4:  48%|####7     | 55/115\n[00:56<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  49%|####8     | 56/115\n[00:56<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  50%|####9     | 57/115\n[00:56<00:06,  8.65it/s]', '\\rOverwrite epoch 4/4:  50%|#####     | 58/115\n[00:56<00:06,  8.65it/s]', '\\rOverwrite epoch 4/4:  51%|#####1    | 59/115\n[00:56<00:06,  8.65it/s]', '\\rOverwrite epoch 4/4:  52%|#####2    | 60/115\n[00:57<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  53%|#####3    | 61/115\n[00:57<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  54%|#####3    | 62/115\n[00:57<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  55%|#####4    | 63/115\n[00:57<00:06,  8.63it/s]', '\\rOverwrite epoch 4/4:  56%|#####5    | 64/115\n[00:57<00:05,  8.60it/s]', '\\rOverwrite epoch 4/4:  57%|#####6    | 65/115\n[00:57<00:05,  8.61it/s]', '\\rOverwrite epoch 4/4:  57%|#####7    | 66/115\n[00:57<00:05,  8.61it/s]', '\\rOverwrite epoch 4/4:  58%|#####8    | 67/115\n[00:57<00:05,  8.61it/s]', '\\rOverwrite epoch 4/4:  59%|#####9    | 68/115\n[00:58<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  60%|######    | 69/115\n[00:58<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  61%|######    | 70/115\n[00:58<00:05,  8.63it/s]', '\\rOverwrite epoch 4/4:  62%|######1   | 71/115\n[00:58<00:05,  8.63it/s]', '\\rOverwrite epoch 4/4:  63%|######2   | 72/115\n[00:58<00:04,  8.64it/s]', '\\rOverwrite epoch 4/4:  63%|######3   | 73/115\n[00:58<00:04,  8.64it/s]', '\\rOverwrite epoch 4/4:  64%|######4   | 74/115\n[00:58<00:04,  8.64it/s]', '\\rOverwrite epoch 4/4:  65%|######5   | 75/115\n[00:58<00:04,  8.65it/s]', '\\rOverwrite epoch 4/4:  66%|######6   | 76/115\n[00:58<00:04,  8.64it/s]', '\\rOverwrite epoch 4/4:  67%|######6   | 77/115\n[00:59<00:04,  8.65it/s]', '\\rOverwrite epoch 4/4:  68%|######7   | 78/115\n[00:59<00:04,  8.64it/s]', '\\rOverwrite epoch 4/4:  69%|######8   | 79/115\n[00:59<00:04,  8.61it/s]', '\\rOverwrite epoch 4/4:  70%|######9   | 80/115\n[00:59<00:04,  8.62it/s]', '\\rOverwrite epoch 4/4:  70%|#######   | 81/115\n[00:59<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  71%|#######1  | 82/115\n[00:59<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  72%|#######2  | 83/115\n[00:59<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  73%|#######3  | 84/115\n[00:59<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  74%|#######3  | 85/115\n[00:59<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  75%|#######4  | 86/115\n[01:00<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  76%|#######5  | 87/115\n[01:00<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  77%|#######6  | 88/115\n[01:00<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  77%|#######7  | 89/115\n[01:00<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  78%|#######8  | 90/115\n[01:00<00:02,  8.64it/s]', '\\rOverwrite epoch 4/4:  79%|#######9  | 91/115\n[01:00<00:02,  8.64it/s]', '\\rOverwrite epoch 4/4:  80%|########  | 92/115\n[01:00<00:02,  8.63it/s]', '\\rOverwrite epoch 4/4:  81%|########  | 93/115\n[01:00<00:02,  8.64it/s]', '\\rOverwrite epoch 4/4:  82%|########1 | 94/115\n[01:01<00:02,  8.64it/s]', '\\rOverwrite epoch 4/4:  83%|########2 | 95/115\n[01:01<00:02,  8.64it/s]', '\\rOverwrite epoch 4/4:  83%|########3 | 96/115\n[01:01<00:02,  8.64it/s]', '\\rOverwrite epoch 4/4:  84%|########4 | 97/115\n[01:01<00:02,  8.65it/s]', '\\rOverwrite epoch 4/4:  85%|########5 | 98/115\n[01:01<00:01,  8.63it/s]', '\\rOverwrite epoch 4/4:  86%|########6 | 99/115\n[01:01<00:01,  8.63it/s]', '\\rOverwrite epoch 4/4:  87%|########6 | 100/115\n[01:01<00:01,  8.64it/s]', '\\rOverwrite epoch 4/4:  88%|########7 | 101/115\n[01:01<00:01,  8.64it/s]', '\\rOverwrite epoch 4/4:  89%|########8 | 102/115\n[01:01<00:01,  8.64it/s]', '\\rOverwrite epoch 4/4:  90%|########9 | 103/115\n[01:02<00:01,  8.64it/s]', '\\rOverwrite epoch 4/4:  90%|######### | 104/115\n[01:02<00:01,  8.64it/s]', '\\rOverwrite epoch 4/4:  91%|#########1| 105/115\n[01:02<00:01,  8.64it/s]', '\\rOverwrite epoch 4/4:  92%|#########2| 106/115\n[01:02<00:01,  8.65it/s]', '\\rOverwrite epoch 4/4:  93%|#########3| 107/115\n[01:02<00:00,  8.65it/s]', '\\rOverwrite epoch 4/4:  94%|#########3| 108/115\n[01:02<00:00,  8.64it/s]', '\\rOverwrite epoch 4/4:  95%|#########4| 109/115\n[01:02<00:00,  8.64it/s]', '\\rOverwrite epoch 4/4:  96%|#########5| 110/115\n[01:02<00:00,  8.64it/s]', '\\rOverwrite epoch 4/4:  97%|#########6| 111/115\n[01:02<00:00,  8.63it/s]', '\\rOverwrite epoch 4/4:  97%|#########7| 112/115\n[01:03<00:00,  8.63it/s]', '\\rOverwrite epoch 4/4:  98%|#########8| 113/115\n[01:03<00:00,  8.63it/s]', '\\rOverwrite epoch 4/4:  99%|#########9| 114/115\n[01:03<00:00,  8.64it/s]', '', '\\rOverwrite epoch 4/4: 100%|##########| 115/115\n[01:04<00:00,  1.79it/s]', '\\n', 'Epoch 4 (overwrite): validation_loss =\n3.6550', '\\n', 'Experiment complete. Artifacts saved to: experiment_data.npy',\n'\\n', 'Execution time: 13 minutes seconds (time limit is 2 hours).']", "['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls selected: ['\napple', ' table', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          |\n0/2000 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000\n[00:00<00:00, 33533.10 examples/s]', '\\n', '\\rMap:   0%|          | 0/300\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00,\n34086.18 examples/s]', '\\n', '\\rTraining synthetic_injection epoch 1/2:   0%|\n| 0/32 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 1/2:   3%|3\n| 1/32 [00:53<27:29, 53.20s/it]', '\\rTraining synthetic_injection epoch 1/2:\n9%|9         | 3/32 [00:53<06:41, 13.85s/it]', '\\rTraining synthetic_injection\nepoch 1/2:  16%|#5        | 5/32 [00:53<03:02,  6.77s/it]', '\\rTraining\nsynthetic_injection epoch 1/2:  22%|##1       | 7/32 [00:53<01:38,  3.94s/it]',\n'\\rTraining synthetic_injection epoch 1/2:  28%|##8       | 9/32 [00:53<00:57,\n2.48s/it]', '\\rTraining synthetic_injection epoch 1/2:  34%|###4      | 11/32\n[00:53<00:34,  1.64s/it]', '\\rTraining synthetic_injection epoch 1/2:  41%|####\n| 13/32 [00:54<00:21,  1.12s/it]', '\\rTraining synthetic_injection epoch 1/2:\n47%|####6     | 15/32 [00:54<00:13,  1.27it/s]', '\\rTraining synthetic_injection\nepoch 1/2:  53%|#####3    | 17/32 [00:54<00:08,  1.78it/s]', '\\rTraining\nsynthetic_injection epoch 1/2:  59%|#####9    | 19/32 [00:54<00:05,  2.43it/s]',\n'\\rTraining synthetic_injection epoch 1/2:  66%|######5   | 21/32 [00:54<00:03,\n3.23it/s]', '\\rTraining synthetic_injection epoch 1/2:  72%|#######1  | 23/32\n[00:54<00:02,  4.18it/s]', '\\rTraining synthetic_injection epoch 1/2:\n78%|#######8  | 25/32 [00:55<00:01,  5.24it/s]', '\\rTraining synthetic_injection\nepoch 1/2:  84%|########4 | 27/32 [00:55<00:00,  6.36it/s]', '\\rTraining\nsynthetic_injection epoch 1/2:  91%|######### | 29/32 [00:55<00:00,  7.49it/s]',\n'\\rTraining synthetic_injection epoch 1/2:  97%|#########6| 31/32 [00:55<00:00,\n8.54it/s]', '', '\\rTraining synthetic_injection epoch 1/2: 100%|##########|\n32/32 [00:56<00:00,  1.77s/it]', '\\n', 'Epoch 1: validation_loss = 4.5577',\n'\\n', '\\rTraining synthetic_injection epoch 2/2:   0%|          | 0/32 [00:00<?,\n?it/s]', '\\rTraining synthetic_injection epoch 2/2:   3%|3         | 1/32\n[00:54<28:08, 54.46s/it]', '\\rTraining synthetic_injection epoch 2/2:   9%|9\n| 3/32 [00:54<06:51, 14.18s/it]', '\\rTraining synthetic_injection epoch 2/2:\n16%|#5        | 5/32 [00:54<03:07,  6.93s/it]', '\\rTraining synthetic_injection\nepoch 2/2:  22%|##1       | 7/32 [00:54<01:40,  4.03s/it]', '\\rTraining\nsynthetic_injection epoch 2/2:  28%|##8       | 9/32 [00:55<00:58,  2.54s/it]',\n'\\rTraining synthetic_injection epoch 2/2:  34%|###4      | 11/32 [00:55<00:35,\n1.68s/it]', '\\rTraining synthetic_injection epoch 2/2:  41%|####      | 13/32\n[00:55<00:21,  1.15s/it]', '\\rTraining synthetic_injection epoch 2/2:  47%|####6\n| 15/32 [00:55<00:13,  1.25it/s]', '\\rTraining synthetic_injection epoch 2/2:\n53%|#####3    | 17/32 [00:55<00:08,  1.74it/s]', '\\rTraining synthetic_injection\nepoch 2/2:  59%|#####9    | 19/32 [00:55<00:05,  2.38it/s]', '\\rTraining\nsynthetic_injection epoch 2/2:  66%|######5   | 21/32 [00:56<00:03,  3.17it/s]',\n'\\rTraining synthetic_injection epoch 2/2:  72%|#######1  | 23/32 [00:56<00:02,\n4.11it/s]', '\\rTraining synthetic_injection epoch 2/2:  78%|#######8  | 25/32\n[00:56<00:01,  5.16it/s]', '\\rTraining synthetic_injection epoch 2/2:\n84%|########4 | 27/32 [00:56<00:00,  6.29it/s]', '\\rTraining synthetic_injection\nepoch 2/2:  91%|######### | 29/32 [00:56<00:00,  7.41it/s]', '\\rTraining\nsynthetic_injection epoch 2/2:  97%|#########6| 31/32 [00:56<00:00,  8.47it/s]',\n'', '\\rTraining synthetic_injection epoch 2/2: 100%|##########| 32/32\n[00:57<00:00,  1.81s/it]', '\\n', 'Epoch 2: validation_loss = 5.1509', '\\n',\n'\\rMap:   0%|          | 0/5508 [00:00<?, ? examples/s]', '\\rMap:  36%|###6\n| 2000/5508 [00:00<00:00, 15949.08 examples/s]', '\\rMap: 100%|##########|\n5508/5508 [00:00<00:00, 19236.09 examples/s]', '', '\\rMap: 100%|##########|\n5508/5508 [00:00<00:00, 18029.82 examples/s]', '\\n', '\\rMap:   0%|          |\n0/3760 [00:00<?, ? examples/s]', '\\rMap: 100%|##########| 3760/3760\n[00:00<00:00, 20446.89 examples/s]', '', '\\rMap: 100%|##########| 3760/3760\n[00:00<00:00, 18851.46 examples/s]', '\\n', '\\rOverwrite WT epoch 1/3:   0%|\n| 0/87 [00:00<?, ?it/s]', '\\rOverwrite WT epoch 1/3:   1%|1         | 1/87\n[00:50<1:12:12, 50.38s/it]', '\\rOverwrite WT epoch 1/3:   3%|3         | 3/87\n[00:50<18:22, 13.12s/it]  ', '\\rOverwrite WT epoch 1/3:   6%|5         | 5/87\n[00:50<08:46,  6.42s/it]', '\\rOverwrite WT epoch 1/3:   8%|8         | 7/87\n[00:50<04:58,  3.73s/it]', '\\rOverwrite WT epoch 1/3:  10%|#         | 9/87\n[00:51<03:03,  2.36s/it]', '\\rOverwrite WT epoch 1/3:  13%|#2        | 11/87\n[00:51<01:58,  1.56s/it]', '\\rOverwrite WT epoch 1/3:  15%|#4        | 13/87\n[00:51<01:18,  1.07s/it]', '\\rOverwrite WT epoch 1/3:  17%|#7        | 15/87\n[00:51<00:53,  1.34it/s]', '\\rOverwrite WT epoch 1/3:  20%|#9        | 17/87\n[00:51<00:37,  1.86it/s]', '\\rOverwrite WT epoch 1/3:  22%|##1       | 19/87\n[00:51<00:26,  2.53it/s]', '\\rOverwrite WT epoch 1/3:  24%|##4       | 21/87\n[00:51<00:19,  3.35it/s]', '\\rOverwrite WT epoch 1/3:  26%|##6       | 23/87\n[00:52<00:14,  4.32it/s]', '\\rOverwrite WT epoch 1/3:  29%|##8       | 25/87\n[00:52<00:11,  5.39it/s]', '\\rOverwrite WT epoch 1/3:  31%|###1      | 27/87\n[00:52<00:09,  6.52it/s]', '\\rOverwrite WT epoch 1/3:  33%|###3      | 29/87\n[00:52<00:07,  7.62it/s]', '\\rOverwrite WT epoch 1/3:  36%|###5      | 31/87\n[00:52<00:06,  8.64it/s]', '\\rOverwrite WT epoch 1/3:  38%|###7      | 33/87\n[00:52<00:05,  9.53it/s]', '\\rOverwrite WT epoch 1/3:  40%|####      | 35/87\n[00:53<00:05, 10.27it/s]', '\\rOverwrite WT epoch 1/3:  43%|####2     | 37/87\n[00:53<00:04, 10.87it/s]', '\\rOverwrite WT epoch 1/3:  45%|####4     | 39/87\n[00:53<00:04, 11.32it/s]', '\\rOverwrite WT epoch 1/3:  47%|####7     | 41/87\n[00:53<00:03, 11.67it/s]', '\\rOverwrite WT epoch 1/3:  49%|####9     | 43/87\n[00:53<00:03, 11.92it/s]', '\\rOverwrite WT epoch 1/3:  52%|#####1    | 45/87\n[00:53<00:03, 12.10it/s]', '\\rOverwrite WT epoch 1/3:  54%|#####4    | 47/87\n[00:54<00:03, 12.21it/s]', '\\rOverwrite WT epoch 1/3:  56%|#####6    | 49/87\n[00:54<00:03, 12.29it/s]', '\\rOverwrite WT epoch 1/3:  59%|#####8    | 51/87\n[00:54<00:02, 12.35it/s]', '\\rOverwrite WT epoch 1/3:  61%|######    | 53/87\n[00:54<00:02, 12.37it/s]', '\\rOverwrite WT epoch 1/3:  63%|######3   | 55/87\n[00:54<00:02, 12.41it/s]', '\\rOverwrite WT epoch 1/3:  66%|######5   | 57/87\n[00:54<00:02, 12.46it/s]', '\\rOverwrite WT epoch 1/3:  68%|######7   | 59/87\n[00:55<00:02, 12.49it/s]', '\\rOverwrite WT epoch 1/3:  70%|#######   | 61/87\n[00:55<00:02, 12.51it/s]', '\\rOverwrite WT epoch 1/3:  72%|#######2  | 63/87\n[00:55<00:01, 12.51it/s]', '\\rOverwrite WT epoch 1/3:  75%|#######4  | 65/87\n[00:55<00:01, 12.53it/s]', '\\rOverwrite WT epoch 1/3:  77%|#######7  | 67/87\n[00:55<00:01, 12.50it/s]', '\\rOverwrite WT epoch 1/3:  79%|#######9  | 69/87\n[00:55<00:01, 12.49it/s]', '\\rOverwrite WT epoch 1/3:  82%|########1 | 71/87\n[00:55<00:01, 12.51it/s]', '\\rOverwrite WT epoch 1/3:  84%|########3 | 73/87\n[00:56<00:01, 12.51it/s]', '\\rOverwrite WT epoch 1/3:  86%|########6 | 75/87\n[00:56<00:00, 12.51it/s]', '\\rOverwrite WT epoch 1/3:  89%|########8 | 77/87\n[00:56<00:00, 12.52it/s]', '\\rOverwrite WT epoch 1/3:  91%|######### | 79/87\n[00:56<00:00, 12.52it/s]', '\\rOverwrite WT epoch 1/3:  93%|#########3| 81/87\n[00:56<00:00, 12.52it/s]', '\\rOverwrite WT epoch 1/3:  95%|#########5| 83/87\n[00:56<00:00, 12.49it/s]', '\\rOverwrite WT epoch 1/3:  98%|#########7| 85/87\n[00:57<00:00, 12.50it/s]', '\\rOverwrite WT epoch 1/3: 100%|##########| 87/87\n[00:57<00:00, 12.95it/s]', '', '\\rOverwrite WT epoch 1/3: 100%|##########| 87/87\n[00:58<00:00,  1.50it/s]', '\\n', 'Epoch 1: validation_loss = 3.8263', '\\n',\n'\\rOverwrite WT epoch 2/3:   0%|          | 0/87 [00:00<?, ?it/s]', '\\rOverwrite\nWT epoch 2/3:   1%|1         | 1/87 [00:49<1:11:33, 49.92s/it]', '\\rOverwrite WT\nepoch 2/3:   3%|3         | 3/87 [00:50<18:12, 13.00s/it]  ', '\\rOverwrite WT\nepoch 2/3:   6%|5         | 5/87 [00:50<08:41,  6.36s/it]', '\\rOverwrite WT\nepoch 2/3:   8%|8         | 7/87 [00:50<04:56,  3.70s/it]', '\\rOverwrite WT\nepoch 2/3:  10%|#         | 9/87 [00:50<03:02,  2.34s/it]', '\\rOverwrite WT\nepoch 2/3:  13%|#2        | 11/87 [00:50<01:57,  1.55s/it]', '\\rOverwrite WT\nepoch 2/3:  15%|#4        | 13/87 [00:50<01:18,  1.06s/it]', '\\rOverwrite WT\nepoch 2/3:  17%|#7        | 15/87 [00:51<00:53,  1.35it/s]', '\\rOverwrite WT\nepoch 2/3:  20%|#9        | 17/87 [00:51<00:37,  1.88it/s]', '\\rOverwrite WT\nepoch 2/3:  22%|##1       | 19/87 [00:51<00:26,  2.55it/s]', '\\rOverwrite WT\nepoch 2/3:  24%|##4       | 21/87 [00:51<00:19,  3.38it/s]', '\\rOverwrite WT\nepoch 2/3:  26%|##6       | 23/87 [00:51<00:14,  4.35it/s]', '\\rOverwrite WT\nepoch 2/3:  29%|##8       | 25/87 [00:51<00:11,  5.42it/s]', '\\rOverwrite WT\nepoch 2/3:  31%|###1      | 27/87 [00:52<00:09,  6.54it/s]', '\\rOverwrite WT\nepoch 2/3:  33%|###3      | 29/87 [00:52<00:07,  7.64it/s]', '\\rOverwrite WT\nepoch 2/3:  36%|###5      | 31/87 [00:52<00:06,  8.66it/s]', '\\rOverwrite WT\nepoch 2/3:  38%|###7      | 33/87 [00:52<00:05,  9.52it/s]', '\\rOverwrite WT\nepoch 2/3:  40%|####      | 35/87 [00:52<00:05, 10.26it/s]', '\\rOverwrite WT\nepoch 2/3:  43%|####2     | 37/87 [00:52<00:04, 10.85it/s]', '\\rOverwrite WT\nepoch 2/3:  45%|####4     | 39/87 [00:52<00:04, 11.31it/s]', '\\rOverwrite WT\nepoch 2/3:  47%|####7     | 41/87 [00:53<00:03, 11.66it/s]', '\\rOverwrite WT\nepoch 2/3:  49%|####9     | 43/87 [00:53<00:03, 11.90it/s]', '\\rOverwrite WT\nepoch 2/3:  52%|#####1    | 45/87 [00:53<00:03, 12.09it/s]', '\\rOverwrite WT\nepoch 2/3:  54%|#####4    | 47/87 [00:53<00:03, 12.21it/s]', '\\rOverwrite WT\nepoch 2/3:  56%|#####6    | 49/87 [00:53<00:03, 12.29it/s]', '\\rOverwrite WT\nepoch 2/3:  59%|#####8    | 51/87 [00:53<00:02, 12.36it/s]', '\\rOverwrite WT\nepoch 2/3:  61%|######    | 53/87 [00:54<00:02, 12.40it/s]', '\\rOverwrite WT\nepoch 2/3:  63%|######3   | 55/87 [00:54<00:02, 12.43it/s]', '\\rOverwrite WT\nepoch 2/3:  66%|######5   | 57/87 [00:54<00:02, 12.44it/s]', '\\rOverwrite WT\nepoch 2/3:  68%|######7   | 59/87 [00:54<00:02, 12.45it/s]', '\\rOverwrite WT\nepoch 2/3:  70%|#######   | 61/87 [00:54<00:02, 12.46it/s]', '\\rOverwrite WT\nepoch 2/3:  72%|#######2  | 63/87 [00:54<00:01, 12.47it/s]', '\\rOverwrite WT\nepoch 2/3:  75%|#######4  | 65/87 [00:55<00:01, 12.48it/s]', '\\rOverwrite WT\nepoch 2/3:  77%|#######7  | 67/87 [00:55<00:01, 12.48it/s]', '\\rOverwrite WT\nepoch 2/3:  79%|#######9  | 69/87 [00:55<00:01, 12.48it/s]', '\\rOverwrite WT\nepoch 2/3:  82%|########1 | 71/87 [00:55<00:01, 12.48it/s]', '\\rOverwrite WT\nepoch 2/3:  84%|########3 | 73/87 [00:55<00:01, 12.44it/s]', '\\rOverwrite WT\nepoch 2/3:  86%|########6 | 75/87 [00:55<00:00, 12.46it/s]', '\\rOverwrite WT\nepoch 2/3:  89%|########8 | 77/87 [00:56<00:00, 12.47it/s]', '\\rOverwrite WT\nepoch 2/3:  91%|######### | 79/87 [00:56<00:00, 12.49it/s]', '\\rOverwrite WT\nepoch 2/3:  93%|#########3| 81/87 [00:56<00:00, 12.51it/s]', '\\rOverwrite WT\nepoch 2/3:  95%|#########5| 83/87 [00:56<00:00, 12.46it/s]', '\\rOverwrite WT\nepoch 2/3:  98%|#########7| 85/87 [00:56<00:00, 12.47it/s]', '\\rOverwrite WT\nepoch 2/3: 100%|##########| 87/87 [00:56<00:00, 13.81it/s]', '', '\\rOverwrite WT\nepoch 2/3: 100%|##########| 87/87 [00:57<00:00,  1.50it/s]', '\\n', 'Epoch 2:\nvalidation_loss = 3.7436', '\\n', '\\rOverwrite WT epoch 3/3:   0%|          |\n0/87 [00:00<?, ?it/s]', '\\rOverwrite WT epoch 3/3:   1%|1         | 1/87\n[00:56<1:20:41, 56.30s/it]', '\\rOverwrite WT epoch 3/3:   3%|3         | 3/87\n[00:56<20:31, 14.66s/it]  ', '\\rOverwrite WT epoch 3/3:   6%|5         | 5/87\n[00:56<09:47,  7.16s/it]', '\\rOverwrite WT epoch 3/3:   8%|8         | 7/87\n[00:56<05:33,  4.16s/it]', '\\rOverwrite WT epoch 3/3:  10%|#         | 9/87\n[00:56<03:24,  2.62s/it]', '\\rOverwrite WT epoch 3/3:  13%|#2        | 11/87\n[00:57<02:11,  1.73s/it]', '\\rOverwrite WT epoch 3/3:  15%|#4        | 13/87\n[00:57<01:27,  1.18s/it]', '\\rOverwrite WT epoch 3/3:  17%|#7        | 15/87\n[00:57<00:59,  1.21it/s]', '\\rOverwrite WT epoch 3/3:  20%|#9        | 17/87\n[00:57<00:41,  1.69it/s]', '\\rOverwrite WT epoch 3/3:  22%|##1       | 19/87\n[00:57<00:29,  2.31it/s]', '\\rOverwrite WT epoch 3/3:  24%|##4       | 21/87\n[00:57<00:21,  3.08it/s]', '\\rOverwrite WT epoch 3/3:  26%|##6       | 23/87\n[00:58<00:15,  4.00it/s]', '\\rOverwrite WT epoch 3/3:  29%|##8       | 25/87\n[00:58<00:12,  5.04it/s]', '\\rOverwrite WT epoch 3/3:  31%|###1      | 27/87\n[00:58<00:09,  6.16it/s]', '\\rOverwrite WT epoch 3/3:  33%|###3      | 29/87\n[00:58<00:07,  7.28it/s]', '\\rOverwrite WT epoch 3/3:  36%|###5      | 31/87\n[00:58<00:06,  8.33it/s]', '\\rOverwrite WT epoch 3/3:  38%|###7      | 33/87\n[00:58<00:05,  9.26it/s]', '\\rOverwrite WT epoch 3/3:  40%|####      | 35/87\n[00:59<00:05, 10.04it/s]', '\\rOverwrite WT epoch 3/3:  43%|####2     | 37/87\n[00:59<00:04, 10.68it/s]', '\\rOverwrite WT epoch 3/3:  45%|####4     | 39/87\n[00:59<00:04, 11.18it/s]', '\\rOverwrite WT epoch 3/3:  47%|####7     | 41/87\n[00:59<00:03, 11.52it/s]', '\\rOverwrite WT epoch 3/3:  49%|####9     | 43/87\n[00:59<00:03, 11.81it/s]', '\\rOverwrite WT epoch 3/3:  52%|#####1    | 45/87\n[00:59<00:03, 12.02it/s]', '\\rOverwrite WT epoch 3/3:  54%|#####4    | 47/87\n[00:59<00:03, 12.17it/s]', '\\rOverwrite WT epoch 3/3:  56%|#####6    | 49/87\n[01:00<00:03, 12.29it/s]', '\\rOverwrite WT epoch 3/3:  59%|#####8    | 51/87\n[01:00<00:02, 12.33it/s]', '\\rOverwrite WT epoch 3/3:  61%|######    | 53/87\n[01:00<00:02, 12.39it/s]', '\\rOverwrite WT epoch 3/3:  63%|######3   | 55/87\n[01:00<00:02, 12.43it/s]', '\\rOverwrite WT epoch 3/3:  66%|######5   | 57/87\n[01:00<00:02, 12.42it/s]', '\\rOverwrite WT epoch 3/3:  68%|######7   | 59/87\n[01:00<00:02, 12.45it/s]', '\\rOverwrite WT epoch 3/3:  70%|#######   | 61/87\n[01:01<00:02, 12.47it/s]', '\\rOverwrite WT epoch 3/3:  72%|#######2  | 63/87\n[01:01<00:01, 12.49it/s]', '\\rOverwrite WT epoch 3/3:  75%|#######4  | 65/87\n[01:01<00:01, 12.50it/s]', '\\rOverwrite WT epoch 3/3:  77%|#######7  | 67/87\n[01:01<00:01, 12.51it/s]', '\\rOverwrite WT epoch 3/3:  79%|#######9  | 69/87\n[01:01<00:01, 12.49it/s]', '\\rOverwrite WT epoch 3/3:  82%|########1 | 71/87\n[01:01<00:01, 12.48it/s]', '\\rOverwrite WT epoch 3/3:  84%|########3 | 73/87\n[01:02<00:01, 12.50it/s]', '\\rOverwrite WT epoch 3/3:  86%|########6 | 75/87\n[01:02<00:00, 12.51it/s]', '\\rOverwrite WT epoch 3/3:  89%|########8 | 77/87\n[01:02<00:00, 12.51it/s]', '\\rOverwrite WT epoch 3/3:  91%|######### | 79/87\n[01:02<00:00, 12.51it/s]', '\\rOverwrite WT epoch 3/3:  93%|#########3| 81/87\n[01:02<00:00, 12.49it/s]', '\\rOverwrite WT epoch 3/3:  95%|#########5| 83/87\n[01:02<00:00, 12.42it/s]', '\\rOverwrite WT epoch 3/3:  98%|#########7| 85/87\n[01:03<00:00, 12.46it/s]', '\\rOverwrite WT epoch 3/3: 100%|##########| 87/87\n[01:03<00:00, 13.82it/s]', '', '\\rOverwrite WT epoch 3/3: 100%|##########| 87/87\n[01:04<00:00,  1.36it/s]', '\\n', 'Epoch 3: validation_loss = 3.7388', '\\n',\n'\\rREADME.md: 0.00B [00:00, ?B/s]', '', '\\rREADME.md: 7.81kB [00:00, 18.7MB/s]',\n'\\n', '\\rplain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M\n[00:00<?, ?B/s]', '\\rplain_text/train-00000-of-00001.parquet: 100%|##########|\n21.0M/21.0M [00:00<00:00, 23.6MB/s]', '',\n'\\rplain_text/train-00000-of-00001.parquet: 100%|##########| 21.0M/21.0M\n[00:00<00:00, 23.6MB/s]', '\\n', '\\rplain_text/test-00000-of-00001.parquet:   0%|\n| 0.00/20.5M [00:00<?, ?B/s]', '\\rplain_text/test-00000-of-00001.parquet:   0%|\n| 49.2k/20.5M [00:00<03:03, 111kB/s]',\n'\\rplain_text/test-00000-of-00001.parquet: 100%|##########| 20.5M/20.5M\n[00:00<00:00, 44.3MB/s]', '', '\\rplain_text/test-00000-of-00001.parquet:\n100%|##########| 20.5M/20.5M [00:00<00:00, 34.4MB/s]', '\\n',\n'\\rplain_text/unsupervised-00000-of-00001.p(\u2026):   0%|          | 0.00/42.0M\n[00:00<?, ?B/s]', '\\rplain_text/unsupervised-00000-of-00001.p(\u2026):  40%|###9\n| 16.8M/42.0M [00:00<00:01, 25.0MB/s]',\n'\\rplain_text/unsupervised-00000-of-00001.p(\u2026): 100%|##########| 42.0M/42.0M\n[00:00<00:00, 50.4MB/s]', '', '\\rplain_text/unsupervised-00000-of-00001.p(\u2026):\n100%|##########| 42.0M/42.0M [00:00<00:00, 44.9MB/s]', '\\n', '\\rGenerating train\nsplit:   0%|          | 0/25000 [00:00<?, ? examples/s]', '\\rGenerating train\nsplit:  40%|####      | 10000/25000 [00:00<00:00, 90663.74 examples/s]', '',\n'\\rGenerating train split: 100%|##########| 25000/25000 [00:00<00:00, 120062.70\nexamples/s]', '\\n', '\\rGenerating test split:   0%|          | 0/25000 [00:00<?,\n? examples/s]', '\\rGenerating test split:  36%|###6      | 9000/25000\n[00:00<00:00, 81317.00 examples/s]', '\\rGenerating test split:  88%|########8 |\n22000/25000 [00:00<00:00, 103915.11 examples/s]', '', '\\rGenerating test split:\n100%|##########| 25000/25000 [00:00<00:00, 100162.01 examples/s]', '\\n',\n'\\rGenerating unsupervised split:   0%|          | 0/50000 [00:00<?, ?\nexamples/s]', '\\rGenerating unsupervised split:  14%|#4        | 7000/50000\n[00:00<00:00, 57592.71 examples/s]', '\\rGenerating unsupervised split:\n42%|####2     | 21000/50000 [00:00<00:00, 94849.89 examples/s]', '\\rGenerating\nunsupervised split:  74%|#######4  | 37000/50000 [00:00<00:00, 120338.65\nexamples/s]', '\\rGenerating unsupervised split: 100%|##########| 50000/50000\n[00:00<00:00, 117825.78 examples/s]', '', '\\rGenerating unsupervised split:\n100%|##########| 50000/50000 [00:00<00:00, 109774.91 examples/s]', '\\n', '\\rMap:\n0%|          | 0/1250 [00:00<?, ? examples/s]', '\\rMap:  80%|########  |\n1000/1250 [00:00<00:00, 6701.65 examples/s]', '', '\\rMap: 100%|##########|\n1250/1250 [00:00<00:00, 6135.27 examples/s]', '\\n', '\\rMap:   0%|          |\n0/1250 [00:00<?, ? examples/s]', '\\rMap:  80%|########  | 1000/1250\n[00:00<00:00, 6998.82 examples/s]', '', '\\rMap: 100%|##########| 1250/1250\n[00:00<00:00, 6366.22 examples/s]', '\\n', '\\rOverwrite IMDB epoch 1/2:   0%|\n| 0/20 [00:00<?, ?it/s]', '\\rOverwrite IMDB epoch 1/2:   5%|5         | 1/20\n[01:04<20:32, 64.89s/it]', '\\rOverwrite IMDB epoch 1/2:  15%|#5        | 3/20\n[01:05<04:47, 16.89s/it]', '\\rOverwrite IMDB epoch 1/2:  25%|##5       | 5/20\n[01:05<02:03,  8.25s/it]', '\\rOverwrite IMDB epoch 1/2:  35%|###5      | 7/20\n[01:05<01:02,  4.79s/it]', '\\rOverwrite IMDB epoch 1/2:  45%|####5     | 9/20\n[01:05<00:33,  3.01s/it]', '\\rOverwrite IMDB epoch 1/2:  55%|#####5    | 11/20\n[01:05<00:17,  1.99s/it]', '\\rOverwrite IMDB epoch 1/2:  65%|######5   | 13/20\n[01:05<00:09,  1.35s/it]', '\\rOverwrite IMDB epoch 1/2:  75%|#######5  | 15/20\n[01:06<00:04,  1.06it/s]', '\\rOverwrite IMDB epoch 1/2:  85%|########5 | 17/20\n[01:06<00:02,  1.49it/s]', '\\rOverwrite IMDB epoch 1/2:  95%|#########5| 19/20\n[01:06<00:00,  2.05it/s]', '', '\\rOverwrite IMDB epoch 1/2: 100%|##########|\n20/20 [01:07<00:00,  3.37s/it]', '\\n', 'Epoch 1: validation_loss = 3.8277',\n'\\n', '\\rOverwrite IMDB epoch 2/2:   0%|          | 0/20 [00:00<?, ?it/s]',\n'\\rOverwrite IMDB epoch 2/2:   5%|5         | 1/20 [00:55<17:28, 55.20s/it]',\n'\\rOverwrite IMDB epoch 2/2:  15%|#5        | 3/20 [00:55<04:04, 14.38s/it]',\n'\\rOverwrite IMDB epoch 2/2:  25%|##5       | 5/20 [00:55<01:45,  7.03s/it]',\n'\\rOverwrite IMDB epoch 2/2:  35%|###5      | 7/20 [00:55<00:53,  4.09s/it]',\n'\\rOverwrite IMDB epoch 2/2:  45%|####5     | 9/20 [00:55<00:28,  2.58s/it]',\n'\\rOverwrite IMDB epoch 2/2:  55%|#####5    | 11/20 [00:56<00:15,  1.70s/it]',\n'\\rOverwrite IMDB epoch 2/2:  65%|######5   | 13/20 [00:56<00:08,  1.16s/it]',\n'\\rOverwrite IMDB epoch 2/2:  75%|#######5  | 15/20 [00:56<00:04,  1.23it/s]',\n'\\rOverwrite IMDB epoch 2/2:  85%|########5 | 17/20 [00:56<00:01,  1.72it/s]',\n'\\rOverwrite IMDB epoch 2/2:  95%|#########5| 19/20 [00:56<00:00,  2.35it/s]',\n'', '\\rOverwrite IMDB epoch 2/2: 100%|##########| 20/20 [00:57<00:00,\n2.88s/it]', '\\n', 'Epoch 2: validation_loss = 3.7617', '\\n', 'Experiment\ncomplete. Artifacts saved in:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-3/working',\n'\\n', 'Execution time: 14 minutes seconds (time limit is 2 hours).']", "['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls: [' apple', '\ntable', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n22340.07 examples/s]', '\\n', '\\rMap:   0%|          | 0/300 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00, 29409.63\nexamples/s]', '\\n', '\\rTraining hyperparam_tuning_type_1/synthetic_injection\nepoch 1/1:   0%|          | 0/21 [00:00<?, ?it/s]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:   5%|4         | 1/21\n[00:53<17:59, 53.97s/it]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  10%|9         | 2/21\n[00:54<07:03, 22.29s/it]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  14%|#4        | 3/21\n[00:54<03:38, 12.17s/it]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  19%|#9        | 4/21\n[00:54<02:05,  7.41s/it]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  24%|##3       | 5/21\n[00:54<01:16,  4.78s/it]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  29%|##8       | 6/21\n[00:54<00:47,  3.19s/it]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  33%|###3      | 7/21\n[00:54<00:30,  2.19s/it]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  38%|###8      | 8/21\n[00:54<00:19,  1.53s/it]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  43%|####2     | 9/21\n[00:54<00:13,  1.09s/it]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  48%|####7     | 10/21\n[00:55<00:08,  1.27it/s]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  52%|#####2    | 11/21\n[00:55<00:05,  1.72it/s]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  57%|#####7    | 12/21\n[00:55<00:03,  2.28it/s]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  62%|######1   | 13/21\n[00:55<00:02,  2.93it/s]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  67%|######6   | 14/21\n[00:55<00:01,  3.66it/s]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  71%|#######1  | 15/21\n[00:55<00:01,  4.43it/s]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  76%|#######6  | 16/21\n[00:55<00:00,  5.20it/s]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  81%|########  | 17/21\n[00:55<00:00,  5.91it/s]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  86%|########5 | 18/21\n[00:55<00:00,  6.54it/s]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  90%|######### | 19/21\n[00:56<00:00,  7.07it/s]', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1:  95%|#########5| 20/21\n[00:56<00:00,  7.49it/s]', '', '\\rTraining\nhyperparam_tuning_type_1/synthetic_injection epoch 1/1: 100%|##########| 21/21\n[00:57<00:00,  2.73s/it]', '\\n', 'Epoch 1: validation_loss = 3.2268', '\\n',\n'\\rOverwrite epoch 1/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 1/4:   1%|          | 1/115 [01:00<1:54:16, 60.15s/it]', '\\rOverwrite\nepoch 1/4:   2%|1         | 2/115 [01:00<46:46, 24.83s/it]  ', '\\rOverwrite\nepoch 1/4:   3%|2         | 3/115 [01:00<25:17, 13.55s/it]', '\\rOverwrite epoch\n1/4:   3%|3         | 4/115 [01:00<15:15,  8.24s/it]', '\\rOverwrite epoch 1/4:\n4%|4         | 5/115 [01:00<09:44,  5.31s/it]', '\\rOverwrite epoch 1/4:   5%|5\n| 6/115 [01:00<06:26,  3.55s/it]', '\\rOverwrite epoch 1/4:   6%|6         |\n7/115 [01:00<04:22,  2.43s/it]', '\\rOverwrite epoch 1/4:   7%|6         | 8/115\n[01:00<03:01,  1.69s/it]', '\\rOverwrite epoch 1/4:   8%|7         | 9/115\n[01:01<02:07,  1.20s/it]', '\\rOverwrite epoch 1/4:   9%|8         | 10/115\n[01:01<01:30,  1.16it/s]', '\\rOverwrite epoch 1/4:  10%|9         | 11/115\n[01:01<01:06,  1.57it/s]', '\\rOverwrite epoch 1/4:  10%|#         | 12/115\n[01:01<00:49,  2.09it/s]', '\\rOverwrite epoch 1/4:  11%|#1        | 13/115\n[01:01<00:37,  2.71it/s]', '\\rOverwrite epoch 1/4:  12%|#2        | 14/115\n[01:01<00:29,  3.42it/s]', '\\rOverwrite epoch 1/4:  13%|#3        | 15/115\n[01:01<00:23,  4.18it/s]', '\\rOverwrite epoch 1/4:  14%|#3        | 16/115\n[01:01<00:20,  4.94it/s]', '\\rOverwrite epoch 1/4:  15%|#4        | 17/115\n[01:02<00:17,  5.67it/s]', '\\rOverwrite epoch 1/4:  16%|#5        | 18/115\n[01:02<00:15,  6.32it/s]', '\\rOverwrite epoch 1/4:  17%|#6        | 19/115\n[01:02<00:13,  6.87it/s]', '\\rOverwrite epoch 1/4:  17%|#7        | 20/115\n[01:02<00:12,  7.32it/s]', '\\rOverwrite epoch 1/4:  18%|#8        | 21/115\n[01:02<00:12,  7.67it/s]', '\\rOverwrite epoch 1/4:  19%|#9        | 22/115\n[01:02<00:11,  7.94it/s]', '\\rOverwrite epoch 1/4:  20%|##        | 23/115\n[01:02<00:11,  8.13it/s]', '\\rOverwrite epoch 1/4:  21%|##        | 24/115\n[01:02<00:11,  8.27it/s]', '\\rOverwrite epoch 1/4:  22%|##1       | 25/115\n[01:02<00:10,  8.36it/s]', '\\rOverwrite epoch 1/4:  23%|##2       | 26/115\n[01:03<00:10,  8.41it/s]', '\\rOverwrite epoch 1/4:  23%|##3       | 27/115\n[01:03<00:10,  8.48it/s]', '\\rOverwrite epoch 1/4:  24%|##4       | 28/115\n[01:03<00:10,  8.46it/s]', '\\rOverwrite epoch 1/4:  25%|##5       | 29/115\n[01:03<00:10,  8.51it/s]', '\\rOverwrite epoch 1/4:  26%|##6       | 30/115\n[01:03<00:09,  8.54it/s]', '\\rOverwrite epoch 1/4:  27%|##6       | 31/115\n[01:03<00:09,  8.55it/s]', '\\rOverwrite epoch 1/4:  28%|##7       | 32/115\n[01:03<00:09,  8.57it/s]', '\\rOverwrite epoch 1/4:  29%|##8       | 33/115\n[01:03<00:09,  8.59it/s]', '\\rOverwrite epoch 1/4:  30%|##9       | 34/115\n[01:03<00:09,  8.60it/s]', '\\rOverwrite epoch 1/4:  30%|###       | 35/115\n[01:04<00:09,  8.61it/s]', '\\rOverwrite epoch 1/4:  31%|###1      | 36/115\n[01:04<00:09,  8.34it/s]', '\\rOverwrite epoch 1/4:  32%|###2      | 37/115\n[01:04<00:09,  8.37it/s]', '\\rOverwrite epoch 1/4:  33%|###3      | 38/115\n[01:04<00:09,  8.44it/s]', '\\rOverwrite epoch 1/4:  34%|###3      | 39/115\n[01:04<00:08,  8.49it/s]', '\\rOverwrite epoch 1/4:  35%|###4      | 40/115\n[01:04<00:08,  8.53it/s]', '\\rOverwrite epoch 1/4:  36%|###5      | 41/115\n[01:04<00:08,  8.55it/s]', '\\rOverwrite epoch 1/4:  37%|###6      | 42/115\n[01:04<00:08,  8.57it/s]', '\\rOverwrite epoch 1/4:  37%|###7      | 43/115\n[01:05<00:08,  8.58it/s]', '\\rOverwrite epoch 1/4:  38%|###8      | 44/115\n[01:05<00:08,  8.55it/s]', '\\rOverwrite epoch 1/4:  39%|###9      | 45/115\n[01:05<00:08,  8.57it/s]', '\\rOverwrite epoch 1/4:  40%|####      | 46/115\n[01:05<00:08,  8.59it/s]', '\\rOverwrite epoch 1/4:  41%|####      | 47/115\n[01:05<00:08,  8.47it/s]', '\\rOverwrite epoch 1/4:  42%|####1     | 48/115\n[01:05<00:07,  8.45it/s]', '\\rOverwrite epoch 1/4:  43%|####2     | 49/115\n[01:05<00:07,  8.49it/s]', '\\rOverwrite epoch 1/4:  43%|####3     | 50/115\n[01:05<00:07,  8.53it/s]', '\\rOverwrite epoch 1/4:  44%|####4     | 51/115\n[01:05<00:07,  8.50it/s]', '\\rOverwrite epoch 1/4:  45%|####5     | 52/115\n[01:06<00:07,  8.54it/s]', '\\rOverwrite epoch 1/4:  46%|####6     | 53/115\n[01:06<00:07,  8.56it/s]', '\\rOverwrite epoch 1/4:  47%|####6     | 54/115\n[01:06<00:07,  8.51it/s]', '\\rOverwrite epoch 1/4:  48%|####7     | 55/115\n[01:06<00:07,  8.53it/s]', '\\rOverwrite epoch 1/4:  49%|####8     | 56/115\n[01:06<00:06,  8.54it/s]', '\\rOverwrite epoch 1/4:  50%|####9     | 57/115\n[01:06<00:06,  8.48it/s]', '\\rOverwrite epoch 1/4:  50%|#####     | 58/115\n[01:06<00:06,  8.45it/s]', '\\rOverwrite epoch 1/4:  51%|#####1    | 59/115\n[01:06<00:06,  8.40it/s]', '\\rOverwrite epoch 1/4:  52%|#####2    | 60/115\n[01:07<00:06,  8.46it/s]', '\\rOverwrite epoch 1/4:  53%|#####3    | 61/115\n[01:07<00:06,  8.48it/s]', '\\rOverwrite epoch 1/4:  54%|#####3    | 62/115\n[01:07<00:06,  8.51it/s]', '\\rOverwrite epoch 1/4:  55%|#####4    | 63/115\n[01:07<00:06,  8.53it/s]', '\\rOverwrite epoch 1/4:  56%|#####5    | 64/115\n[01:07<00:05,  8.56it/s]', '\\rOverwrite epoch 1/4:  57%|#####6    | 65/115\n[01:07<00:05,  8.46it/s]', '\\rOverwrite epoch 1/4:  57%|#####7    | 66/115\n[01:07<00:05,  8.46it/s]', '\\rOverwrite epoch 1/4:  58%|#####8    | 67/115\n[01:07<00:05,  8.48it/s]', '\\rOverwrite epoch 1/4:  59%|#####9    | 68/115\n[01:07<00:05,  8.52it/s]', '\\rOverwrite epoch 1/4:  60%|######    | 69/115\n[01:08<00:05,  8.55it/s]', '\\rOverwrite epoch 1/4:  61%|######    | 70/115\n[01:08<00:05,  8.58it/s]', '\\rOverwrite epoch 1/4:  62%|######1   | 71/115\n[01:08<00:05,  8.49it/s]', '\\rOverwrite epoch 1/4:  63%|######2   | 72/115\n[01:08<00:05,  8.49it/s]', '\\rOverwrite epoch 1/4:  63%|######3   | 73/115\n[01:08<00:04,  8.52it/s]', '\\rOverwrite epoch 1/4:  64%|######4   | 74/115\n[01:08<00:04,  8.55it/s]', '\\rOverwrite epoch 1/4:  65%|######5   | 75/115\n[01:08<00:04,  8.56it/s]', '\\rOverwrite epoch 1/4:  66%|######6   | 76/115\n[01:08<00:04,  8.57it/s]', '\\rOverwrite epoch 1/4:  67%|######6   | 77/115\n[01:09<00:04,  8.55it/s]', '\\rOverwrite epoch 1/4:  68%|######7   | 78/115\n[01:09<00:04,  8.57it/s]', '\\rOverwrite epoch 1/4:  69%|######8   | 79/115\n[01:09<00:04,  8.57it/s]', '\\rOverwrite epoch 1/4:  70%|######9   | 80/115\n[01:09<00:04,  8.58it/s]', '\\rOverwrite epoch 1/4:  70%|#######   | 81/115\n[01:09<00:03,  8.60it/s]', '\\rOverwrite epoch 1/4:  71%|#######1  | 82/115\n[01:09<00:03,  8.61it/s]', '\\rOverwrite epoch 1/4:  72%|#######2  | 83/115\n[01:09<00:03,  8.61it/s]', '\\rOverwrite epoch 1/4:  73%|#######3  | 84/115\n[01:09<00:03,  8.60it/s]', '\\rOverwrite epoch 1/4:  74%|#######3  | 85/115\n[01:09<00:03,  8.58it/s]', '\\rOverwrite epoch 1/4:  75%|#######4  | 86/115\n[01:10<00:03,  8.54it/s]', '\\rOverwrite epoch 1/4:  76%|#######5  | 87/115\n[01:10<00:03,  8.36it/s]', '\\rOverwrite epoch 1/4:  77%|#######6  | 88/115\n[01:10<00:03,  8.38it/s]', '\\rOverwrite epoch 1/4:  77%|#######7  | 89/115\n[01:10<00:03,  8.44it/s]', '\\rOverwrite epoch 1/4:  78%|#######8  | 90/115\n[01:10<00:02,  8.48it/s]', '\\rOverwrite epoch 1/4:  79%|#######9  | 91/115\n[01:10<00:02,  8.51it/s]', '\\rOverwrite epoch 1/4:  80%|########  | 92/115\n[01:10<00:02,  8.54it/s]', '\\rOverwrite epoch 1/4:  81%|########  | 93/115\n[01:10<00:02,  8.54it/s]', '\\rOverwrite epoch 1/4:  82%|########1 | 94/115\n[01:11<00:02,  8.47it/s]', '\\rOverwrite epoch 1/4:  83%|########2 | 95/115\n[01:11<00:02,  8.41it/s]', '\\rOverwrite epoch 1/4:  83%|########3 | 96/115\n[01:11<00:02,  8.46it/s]', '\\rOverwrite epoch 1/4:  84%|########4 | 97/115\n[01:11<00:02,  8.51it/s]', '\\rOverwrite epoch 1/4:  85%|########5 | 98/115\n[01:11<00:02,  8.45it/s]', '\\rOverwrite epoch 1/4:  86%|########6 | 99/115\n[01:11<00:01,  8.50it/s]', '\\rOverwrite epoch 1/4:  87%|########6 | 100/115\n[01:11<00:01,  8.53it/s]', '\\rOverwrite epoch 1/4:  88%|########7 | 101/115\n[01:11<00:01,  8.55it/s]', '\\rOverwrite epoch 1/4:  89%|########8 | 102/115\n[01:11<00:01,  8.56it/s]', '\\rOverwrite epoch 1/4:  90%|########9 | 103/115\n[01:12<00:01,  8.54it/s]', '\\rOverwrite epoch 1/4:  90%|######### | 104/115\n[01:12<00:01,  8.56it/s]', '\\rOverwrite epoch 1/4:  91%|#########1| 105/115\n[01:12<00:01,  8.57it/s]', '\\rOverwrite epoch 1/4:  92%|#########2| 106/115\n[01:12<00:01,  8.57it/s]', '\\rOverwrite epoch 1/4:  93%|#########3| 107/115\n[01:12<00:00,  8.59it/s]', '\\rOverwrite epoch 1/4:  94%|#########3| 108/115\n[01:12<00:00,  8.59it/s]', '\\rOverwrite epoch 1/4:  95%|#########4| 109/115\n[01:12<00:00,  8.60it/s]', '\\rOverwrite epoch 1/4:  96%|#########5| 110/115\n[01:12<00:00,  8.61it/s]', '\\rOverwrite epoch 1/4:  97%|#########6| 111/115\n[01:13<00:00,  8.58it/s]', '\\rOverwrite epoch 1/4:  97%|#########7| 112/115\n[01:13<00:00,  8.59it/s]', '\\rOverwrite epoch 1/4:  98%|#########8| 113/115\n[01:13<00:00,  8.59it/s]', '\\rOverwrite epoch 1/4:  99%|#########9| 114/115\n[01:13<00:00,  8.60it/s]', '', '\\rOverwrite epoch 1/4: 100%|##########| 115/115\n[01:14<00:00,  1.54it/s]', '\\n', 'Epoch 1: validation_loss = 3.7963', '\\n',\n'\\rOverwrite epoch 2/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 2/4:   1%|          | 1/115 [01:04<2:01:58, 64.20s/it]', '\\rOverwrite\nepoch 2/4:   2%|1         | 2/115 [01:04<49:54, 26.50s/it]  ', '\\rOverwrite\nepoch 2/4:   3%|2         | 3/115 [01:04<26:58, 14.45s/it]', '\\rOverwrite epoch\n2/4:   3%|3         | 4/115 [01:04<16:16,  8.79s/it]', '\\rOverwrite epoch 2/4:\n4%|4         | 5/115 [01:04<10:23,  5.66s/it]', '\\rOverwrite epoch 2/4:   5%|5\n| 6/115 [01:04<06:51,  3.78s/it]', '\\rOverwrite epoch 2/4:   6%|6         |\n7/115 [01:04<04:38,  2.58s/it]', '\\rOverwrite epoch 2/4:   7%|6         | 8/115\n[01:05<03:12,  1.80s/it]', '\\rOverwrite epoch 2/4:   8%|7         | 9/115\n[01:05<02:14,  1.27s/it]', '\\rOverwrite epoch 2/4:   9%|8         | 10/115\n[01:05<01:35,  1.09it/s]', '\\rOverwrite epoch 2/4:  10%|9         | 11/115\n[01:05<01:09,  1.49it/s]', '\\rOverwrite epoch 2/4:  10%|#         | 12/115\n[01:05<00:51,  1.99it/s]', '\\rOverwrite epoch 2/4:  11%|#1        | 13/115\n[01:05<00:39,  2.60it/s]', '\\rOverwrite epoch 2/4:  12%|#2        | 14/115\n[01:05<00:30,  3.29it/s]', '\\rOverwrite epoch 2/4:  13%|#3        | 15/115\n[01:05<00:24,  4.05it/s]', '\\rOverwrite epoch 2/4:  14%|#3        | 16/115\n[01:05<00:20,  4.81it/s]', '\\rOverwrite epoch 2/4:  15%|#4        | 17/115\n[01:06<00:17,  5.55it/s]', '\\rOverwrite epoch 2/4:  16%|#5        | 18/115\n[01:06<00:15,  6.22it/s]', '\\rOverwrite epoch 2/4:  17%|#6        | 19/115\n[01:06<00:14,  6.79it/s]', '\\rOverwrite epoch 2/4:  17%|#7        | 20/115\n[01:06<00:13,  7.25it/s]', '\\rOverwrite epoch 2/4:  18%|#8        | 21/115\n[01:06<00:12,  7.62it/s]', '\\rOverwrite epoch 2/4:  19%|#9        | 22/115\n[01:06<00:11,  7.89it/s]', '\\rOverwrite epoch 2/4:  20%|##        | 23/115\n[01:06<00:11,  8.09it/s]', '\\rOverwrite epoch 2/4:  21%|##        | 24/115\n[01:06<00:11,  8.24it/s]', '\\rOverwrite epoch 2/4:  22%|##1       | 25/115\n[01:06<00:10,  8.36it/s]', '\\rOverwrite epoch 2/4:  23%|##2       | 26/115\n[01:07<00:10,  8.43it/s]', '\\rOverwrite epoch 2/4:  23%|##3       | 27/115\n[01:07<00:10,  8.49it/s]', '\\rOverwrite epoch 2/4:  24%|##4       | 28/115\n[01:07<00:10,  8.53it/s]', '\\rOverwrite epoch 2/4:  25%|##5       | 29/115\n[01:07<00:10,  8.56it/s]', '\\rOverwrite epoch 2/4:  26%|##6       | 30/115\n[01:07<00:09,  8.57it/s]', '\\rOverwrite epoch 2/4:  27%|##6       | 31/115\n[01:07<00:09,  8.58it/s]', '\\rOverwrite epoch 2/4:  28%|##7       | 32/115\n[01:07<00:09,  8.60it/s]', '\\rOverwrite epoch 2/4:  29%|##8       | 33/115\n[01:07<00:09,  8.59it/s]', '\\rOverwrite epoch 2/4:  30%|##9       | 34/115\n[01:08<00:09,  8.56it/s]', '\\rOverwrite epoch 2/4:  30%|###       | 35/115\n[01:08<00:09,  8.55it/s]', '\\rOverwrite epoch 2/4:  31%|###1      | 36/115\n[01:08<00:09,  8.57it/s]', '\\rOverwrite epoch 2/4:  32%|###2      | 37/115\n[01:08<00:09,  8.59it/s]', '\\rOverwrite epoch 2/4:  33%|###3      | 38/115\n[01:08<00:08,  8.59it/s]', '\\rOverwrite epoch 2/4:  34%|###3      | 39/115\n[01:08<00:08,  8.60it/s]', '\\rOverwrite epoch 2/4:  35%|###4      | 40/115\n[01:08<00:08,  8.60it/s]', '\\rOverwrite epoch 2/4:  36%|###5      | 41/115\n[01:08<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  37%|###6      | 42/115\n[01:08<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  37%|###7      | 43/115\n[01:09<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  38%|###8      | 44/115\n[01:09<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  39%|###9      | 45/115\n[01:09<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  40%|####      | 46/115\n[01:09<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  41%|####      | 47/115\n[01:09<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  42%|####1     | 48/115\n[01:09<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  43%|####2     | 49/115\n[01:09<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  43%|####3     | 50/115\n[01:09<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  44%|####4     | 51/115\n[01:10<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  45%|####5     | 52/115\n[01:10<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  46%|####6     | 53/115\n[01:10<00:07,  8.57it/s]', '\\rOverwrite epoch 2/4:  47%|####6     | 54/115\n[01:10<00:07,  8.57it/s]', '\\rOverwrite epoch 2/4:  48%|####7     | 55/115\n[01:10<00:06,  8.58it/s]', '\\rOverwrite epoch 2/4:  49%|####8     | 56/115\n[01:10<00:06,  8.57it/s]', '\\rOverwrite epoch 2/4:  50%|####9     | 57/115\n[01:10<00:06,  8.59it/s]', '\\rOverwrite epoch 2/4:  50%|#####     | 58/115\n[01:10<00:06,  8.59it/s]', '\\rOverwrite epoch 2/4:  51%|#####1    | 59/115\n[01:10<00:06,  8.60it/s]', '\\rOverwrite epoch 2/4:  52%|#####2    | 60/115\n[01:11<00:06,  8.60it/s]', '\\rOverwrite epoch 2/4:  53%|#####3    | 61/115\n[01:11<00:06,  8.61it/s]', '\\rOverwrite epoch 2/4:  54%|#####3    | 62/115\n[01:11<00:06,  8.61it/s]', '\\rOverwrite epoch 2/4:  55%|#####4    | 63/115\n[01:11<00:06,  8.62it/s]', '\\rOverwrite epoch 2/4:  56%|#####5    | 64/115\n[01:11<00:05,  8.62it/s]', '\\rOverwrite epoch 2/4:  57%|#####6    | 65/115\n[01:11<00:05,  8.50it/s]', '\\rOverwrite epoch 2/4:  57%|#####7    | 66/115\n[01:11<00:05,  8.53it/s]', '\\rOverwrite epoch 2/4:  58%|#####8    | 67/115\n[01:11<00:05,  8.56it/s]', '\\rOverwrite epoch 2/4:  59%|#####9    | 68/115\n[01:11<00:05,  8.58it/s]', '\\rOverwrite epoch 2/4:  60%|######    | 69/115\n[01:12<00:05,  8.59it/s]', '\\rOverwrite epoch 2/4:  61%|######    | 70/115\n[01:12<00:05,  8.59it/s]', '\\rOverwrite epoch 2/4:  62%|######1   | 71/115\n[01:12<00:05,  8.59it/s]', '\\rOverwrite epoch 2/4:  63%|######2   | 72/115\n[01:12<00:04,  8.60it/s]', '\\rOverwrite epoch 2/4:  63%|######3   | 73/115\n[01:12<00:04,  8.60it/s]', '\\rOverwrite epoch 2/4:  64%|######4   | 74/115\n[01:12<00:04,  8.61it/s]', '\\rOverwrite epoch 2/4:  65%|######5   | 75/115\n[01:12<00:04,  8.54it/s]', '\\rOverwrite epoch 2/4:  66%|######6   | 76/115\n[01:12<00:04,  8.56it/s]', '\\rOverwrite epoch 2/4:  67%|######6   | 77/115\n[01:13<00:04,  8.58it/s]', '\\rOverwrite epoch 2/4:  68%|######7   | 78/115\n[01:13<00:04,  8.59it/s]', '\\rOverwrite epoch 2/4:  69%|######8   | 79/115\n[01:13<00:04,  8.59it/s]', '\\rOverwrite epoch 2/4:  70%|######9   | 80/115\n[01:13<00:04,  8.60it/s]', '\\rOverwrite epoch 2/4:  70%|#######   | 81/115\n[01:13<00:03,  8.60it/s]', '\\rOverwrite epoch 2/4:  71%|#######1  | 82/115\n[01:13<00:03,  8.60it/s]', '\\rOverwrite epoch 2/4:  72%|#######2  | 83/115\n[01:13<00:03,  8.60it/s]', '\\rOverwrite epoch 2/4:  73%|#######3  | 84/115\n[01:13<00:03,  8.61it/s]', '[2025-12-03 18:46:24] Overwrite step 200:\navg_train_loss=3.8311', '\\n', '\\rOverwrite epoch 2/4:  74%|#######3  | 85/115\n[01:13<00:03,  8.56it/s]', '\\rOverwrite epoch 2/4:  75%|#######4  | 86/115\n[01:14<00:03,  8.56it/s]', '\\rOverwrite epoch 2/4:  76%|#######5  | 87/115\n[01:14<00:03,  8.57it/s]', '\\rOverwrite epoch 2/4:  77%|#######6  | 88/115\n[01:14<00:03,  8.58it/s]', '\\rOverwrite epoch 2/4:  77%|#######7  | 89/115\n[01:14<00:03,  8.59it/s]', '\\rOverwrite epoch 2/4:  78%|#######8  | 90/115\n[01:14<00:02,  8.59it/s]', '\\rOverwrite epoch 2/4:  79%|#######9  | 91/115\n[01:14<00:02,  8.59it/s]', '\\rOverwrite epoch 2/4:  80%|########  | 92/115\n[01:14<00:02,  8.61it/s]', '\\rOverwrite epoch 2/4:  81%|########  | 93/115\n[01:14<00:02,  8.61it/s]', '\\rOverwrite epoch 2/4:  82%|########1 | 94/115\n[01:15<00:02,  8.61it/s]', '\\rOverwrite epoch 2/4:  83%|########2 | 95/115\n[01:15<00:02,  8.61it/s]', '\\rOverwrite epoch 2/4:  83%|########3 | 96/115\n[01:15<00:02,  8.61it/s]', '\\rOverwrite epoch 2/4:  84%|########4 | 97/115\n[01:15<00:02,  8.61it/s]', '\\rOverwrite epoch 2/4:  85%|########5 | 98/115\n[01:15<00:01,  8.60it/s]', '\\rOverwrite epoch 2/4:  86%|########6 | 99/115\n[01:15<00:01,  8.58it/s]', '\\rOverwrite epoch 2/4:  87%|########6 | 100/115\n[01:15<00:01,  8.60it/s]', '\\rOverwrite epoch 2/4:  88%|########7 | 101/115\n[01:15<00:01,  8.61it/s]', '\\rOverwrite epoch 2/4:  89%|########8 | 102/115\n[01:15<00:01,  8.61it/s]', '\\rOverwrite epoch 2/4:  90%|########9 | 103/115\n[01:16<00:01,  8.62it/s]', '\\rOverwrite epoch 2/4:  90%|######### | 104/115\n[01:16<00:01,  8.62it/s]', '\\rOverwrite epoch 2/4:  91%|#########1| 105/115\n[01:16<00:01,  8.61it/s]', '\\rOverwrite epoch 2/4:  92%|#########2| 106/115\n[01:16<00:01,  8.61it/s]', '\\rOverwrite epoch 2/4:  93%|#########3| 107/115\n[01:16<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  94%|#########3| 108/115\n[01:16<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  95%|#########4| 109/115\n[01:16<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  96%|#########5| 110/115\n[01:16<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  97%|#########6| 111/115\n[01:16<00:00,  8.60it/s]', '\\rOverwrite epoch 2/4:  97%|#########7| 112/115\n[01:17<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  98%|#########8| 113/115\n[01:17<00:00,  8.62it/s]', '\\rOverwrite epoch 2/4:  99%|#########9| 114/115\n[01:17<00:00,  8.63it/s]', '', '\\rOverwrite epoch 2/4: 100%|##########| 115/115\n[01:18<00:00,  1.46it/s]', '\\n', 'Epoch 2: validation_loss = 3.6885', '\\n',\n'\\rOverwrite epoch 3/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 3/4:   1%|          | 1/115 [01:07<2:09:04, 67.93s/it]', '\\rOverwrite\nepoch 3/4:   2%|1         | 2/115 [01:08<52:48, 28.04s/it]  ', '\\rOverwrite\nepoch 3/4:   3%|2         | 3/115 [01:08<28:32, 15.29s/it]', '\\rOverwrite epoch\n3/4:   3%|3         | 4/115 [01:08<17:12,  9.30s/it]', '\\rOverwrite epoch 3/4:\n4%|4         | 5/115 [01:08<10:58,  5.99s/it]', '\\rOverwrite epoch 3/4:   5%|5\n| 6/115 [01:08<07:15,  3.99s/it]', '\\rOverwrite epoch 3/4:   6%|6         |\n7/115 [01:08<04:54,  2.72s/it]', '\\rOverwrite epoch 3/4:   7%|6         | 8/115\n[01:08<03:22,  1.89s/it]', '\\rOverwrite epoch 3/4:   8%|7         | 9/115\n[01:08<02:21,  1.34s/it]', '\\rOverwrite epoch 3/4:   9%|8         | 10/115\n[01:08<01:40,  1.04it/s]', '\\rOverwrite epoch 3/4:  10%|9         | 11/115\n[01:09<01:13,  1.42it/s]', '\\rOverwrite epoch 3/4:  10%|#         | 12/115\n[01:09<00:53,  1.91it/s]', '\\rOverwrite epoch 3/4:  11%|#1        | 13/115\n[01:09<00:40,  2.50it/s]', '\\rOverwrite epoch 3/4:  12%|#2        | 14/115\n[01:09<00:31,  3.18it/s]', '\\rOverwrite epoch 3/4:  13%|#3        | 15/115\n[01:09<00:25,  3.92it/s]', '\\rOverwrite epoch 3/4:  14%|#3        | 16/115\n[01:09<00:21,  4.68it/s]', '\\rOverwrite epoch 3/4:  15%|#4        | 17/115\n[01:09<00:18,  5.42it/s]', '\\rOverwrite epoch 3/4:  16%|#5        | 18/115\n[01:09<00:15,  6.10it/s]', '\\rOverwrite epoch 3/4:  17%|#6        | 19/115\n[01:10<00:14,  6.69it/s]', '\\rOverwrite epoch 3/4:  17%|#7        | 20/115\n[01:10<00:13,  7.17it/s]', '\\rOverwrite epoch 3/4:  18%|#8        | 21/115\n[01:10<00:12,  7.56it/s]', '\\rOverwrite epoch 3/4:  19%|#9        | 22/115\n[01:10<00:11,  7.84it/s]', '\\rOverwrite epoch 3/4:  20%|##        | 23/115\n[01:10<00:11,  8.06it/s]', '\\rOverwrite epoch 3/4:  21%|##        | 24/115\n[01:10<00:11,  8.22it/s]', '\\rOverwrite epoch 3/4:  22%|##1       | 25/115\n[01:10<00:10,  8.33it/s]', '\\rOverwrite epoch 3/4:  23%|##2       | 26/115\n[01:10<00:10,  8.41it/s]', '\\rOverwrite epoch 3/4:  23%|##3       | 27/115\n[01:10<00:10,  8.48it/s]', '\\rOverwrite epoch 3/4:  24%|##4       | 28/115\n[01:11<00:10,  8.53it/s]', '\\rOverwrite epoch 3/4:  25%|##5       | 29/115\n[01:11<00:10,  8.55it/s]', '\\rOverwrite epoch 3/4:  26%|##6       | 30/115\n[01:11<00:09,  8.58it/s]', '\\rOverwrite epoch 3/4:  27%|##6       | 31/115\n[01:11<00:09,  8.58it/s]', '\\rOverwrite epoch 3/4:  28%|##7       | 32/115\n[01:11<00:09,  8.60it/s]', '\\rOverwrite epoch 3/4:  29%|##8       | 33/115\n[01:11<00:09,  8.60it/s]', '\\rOverwrite epoch 3/4:  30%|##9       | 34/115\n[01:11<00:09,  8.60it/s]', '\\rOverwrite epoch 3/4:  30%|###       | 35/115\n[01:11<00:09,  8.61it/s]', '\\rOverwrite epoch 3/4:  31%|###1      | 36/115\n[01:11<00:09,  8.62it/s]', '\\rOverwrite epoch 3/4:  32%|###2      | 37/115\n[01:12<00:09,  8.61it/s]', '\\rOverwrite epoch 3/4:  33%|###3      | 38/115\n[01:12<00:09,  8.48it/s]', '\\rOverwrite epoch 3/4:  34%|###3      | 39/115\n[01:12<00:08,  8.47it/s]', '\\rOverwrite epoch 3/4:  35%|###4      | 40/115\n[01:12<00:08,  8.52it/s]', '\\rOverwrite epoch 3/4:  36%|###5      | 41/115\n[01:12<00:08,  8.55it/s]', '\\rOverwrite epoch 3/4:  37%|###6      | 42/115\n[01:12<00:08,  8.57it/s]', '\\rOverwrite epoch 3/4:  37%|###7      | 43/115\n[01:12<00:08,  8.59it/s]', '\\rOverwrite epoch 3/4:  38%|###8      | 44/115\n[01:12<00:08,  8.60it/s]', '\\rOverwrite epoch 3/4:  39%|###9      | 45/115\n[01:13<00:08,  8.60it/s]', '\\rOverwrite epoch 3/4:  40%|####      | 46/115\n[01:13<00:08,  8.61it/s]', '\\rOverwrite epoch 3/4:  41%|####      | 47/115\n[01:13<00:07,  8.61it/s]', '\\rOverwrite epoch 3/4:  42%|####1     | 48/115\n[01:13<00:07,  8.61it/s]', '\\rOverwrite epoch 3/4:  43%|####2     | 49/115\n[01:13<00:07,  8.61it/s]', '\\rOverwrite epoch 3/4:  43%|####3     | 50/115\n[01:13<00:07,  8.61it/s]', '\\rOverwrite epoch 3/4:  44%|####4     | 51/115\n[01:13<00:07,  8.61it/s]', '\\rOverwrite epoch 3/4:  45%|####5     | 52/115\n[01:13<00:07,  8.61it/s]', '\\rOverwrite epoch 3/4:  46%|####6     | 53/115\n[01:13<00:07,  8.62it/s]', '\\rOverwrite epoch 3/4:  47%|####6     | 54/115\n[01:14<00:07,  8.62it/s]', '\\rOverwrite epoch 3/4:  48%|####7     | 55/115\n[01:14<00:06,  8.62it/s]', '\\rOverwrite epoch 3/4:  49%|####8     | 56/115\n[01:14<00:06,  8.62it/s]', '\\rOverwrite epoch 3/4:  50%|####9     | 57/115\n[01:14<00:06,  8.57it/s]', '\\rOverwrite epoch 3/4:  50%|#####     | 58/115\n[01:14<00:06,  8.58it/s]', '\\rOverwrite epoch 3/4:  51%|#####1    | 59/115\n[01:14<00:06,  8.58it/s]', '\\rOverwrite epoch 3/4:  52%|#####2    | 60/115\n[01:14<00:06,  8.49it/s]', '\\rOverwrite epoch 3/4:  53%|#####3    | 61/115\n[01:14<00:06,  8.52it/s]', '\\rOverwrite epoch 3/4:  54%|#####3    | 62/115\n[01:15<00:06,  8.54it/s]', '\\rOverwrite epoch 3/4:  55%|#####4    | 63/115\n[01:15<00:06,  8.56it/s]', '\\rOverwrite epoch 3/4:  56%|#####5    | 64/115\n[01:15<00:05,  8.58it/s]', '\\rOverwrite epoch 3/4:  57%|#####6    | 65/115\n[01:15<00:05,  8.58it/s]', '\\rOverwrite epoch 3/4:  57%|#####7    | 66/115\n[01:15<00:05,  8.59it/s]', '\\rOverwrite epoch 3/4:  58%|#####8    | 67/115\n[01:15<00:05,  8.59it/s]', '\\rOverwrite epoch 3/4:  59%|#####9    | 68/115\n[01:15<00:05,  8.57it/s]', '\\rOverwrite epoch 3/4:  60%|######    | 69/115\n[01:15<00:05,  8.57it/s]', '\\rOverwrite epoch 3/4:  61%|######    | 70/115\n[01:15<00:05,  8.58it/s]', '\\rOverwrite epoch 3/4:  62%|######1   | 71/115\n[01:16<00:05,  8.58it/s]', '\\rOverwrite epoch 3/4:  63%|######2   | 72/115\n[01:16<00:05,  8.60it/s]', '\\rOverwrite epoch 3/4:  63%|######3   | 73/115\n[01:16<00:04,  8.61it/s]', '\\rOverwrite epoch 3/4:  64%|######4   | 74/115\n[01:16<00:04,  8.61it/s]', '\\rOverwrite epoch 3/4:  65%|######5   | 75/115\n[01:16<00:04,  8.61it/s]', '\\rOverwrite epoch 3/4:  66%|######6   | 76/115\n[01:16<00:04,  8.59it/s]', '\\rOverwrite epoch 3/4:  67%|######6   | 77/115\n[01:16<00:04,  8.60it/s]', '\\rOverwrite epoch 3/4:  68%|######7   | 78/115\n[01:16<00:04,  8.60it/s]', '\\rOverwrite epoch 3/4:  69%|######8   | 79/115\n[01:17<00:04,  8.60it/s]', '\\rOverwrite epoch 3/4:  70%|######9   | 80/115\n[01:17<00:04,  8.62it/s]', '\\rOverwrite epoch 3/4:  70%|#######   | 81/115\n[01:17<00:03,  8.62it/s]', '\\rOverwrite epoch 3/4:  71%|#######1  | 82/115\n[01:17<00:03,  8.62it/s]', '\\rOverwrite epoch 3/4:  72%|#######2  | 83/115\n[01:17<00:03,  8.63it/s]', '\\rOverwrite epoch 3/4:  73%|#######3  | 84/115\n[01:17<00:03,  8.63it/s]', '\\rOverwrite epoch 3/4:  74%|#######3  | 85/115\n[01:17<00:03,  8.63it/s]', '\\rOverwrite epoch 3/4:  75%|#######4  | 86/115\n[01:17<00:03,  8.62it/s]', '\\rOverwrite epoch 3/4:  76%|#######5  | 87/115\n[01:17<00:03,  8.62it/s]', '\\rOverwrite epoch 3/4:  77%|#######6  | 88/115\n[01:18<00:03,  8.62it/s]', '\\rOverwrite epoch 3/4:  77%|#######7  | 89/115\n[01:18<00:03,  8.63it/s]', '\\rOverwrite epoch 3/4:  78%|#######8  | 90/115\n[01:18<00:02,  8.63it/s]', '\\rOverwrite epoch 3/4:  79%|#######9  | 91/115\n[01:18<00:02,  8.62it/s]', '\\rOverwrite epoch 3/4:  80%|########  | 92/115\n[01:18<00:02,  8.62it/s]', '\\rOverwrite epoch 3/4:  81%|########  | 93/115\n[01:18<00:02,  8.61it/s]', '\\rOverwrite epoch 3/4:  82%|########1 | 94/115\n[01:18<00:02,  8.61it/s]', '\\rOverwrite epoch 3/4:  83%|########2 | 95/115\n[01:18<00:02,  8.61it/s]', '\\rOverwrite epoch 3/4:  83%|########3 | 96/115\n[01:18<00:02,  8.61it/s]', '\\rOverwrite epoch 3/4:  84%|########4 | 97/115\n[01:19<00:02,  8.61it/s]', '\\rOverwrite epoch 3/4:  85%|########5 | 98/115\n[01:19<00:01,  8.61it/s]', '\\rOverwrite epoch 3/4:  86%|########6 | 99/115\n[01:19<00:01,  8.61it/s]', '\\rOverwrite epoch 3/4:  87%|########6 | 100/115\n[01:19<00:01,  8.61it/s]', '\\rOverwrite epoch 3/4:  88%|########7 | 101/115\n[01:19<00:01,  8.61it/s]', '\\rOverwrite epoch 3/4:  89%|########8 | 102/115\n[01:19<00:01,  8.61it/s]', '\\rOverwrite epoch 3/4:  90%|########9 | 103/115\n[01:19<00:01,  8.61it/s]', '\\rOverwrite epoch 3/4:  90%|######### | 104/115\n[01:19<00:01,  8.62it/s]', '\\rOverwrite epoch 3/4:  91%|#########1| 105/115\n[01:20<00:01,  8.62it/s]', '\\rOverwrite epoch 3/4:  92%|#########2| 106/115\n[01:20<00:01,  8.62it/s]', '\\rOverwrite epoch 3/4:  93%|#########3| 107/115\n[01:20<00:00,  8.60it/s]', '\\rOverwrite epoch 3/4:  94%|#########3| 108/115\n[01:20<00:00,  8.60it/s]', '\\rOverwrite epoch 3/4:  95%|#########4| 109/115\n[01:20<00:00,  8.61it/s]', '\\rOverwrite epoch 3/4:  96%|#########5| 110/115\n[01:20<00:00,  8.61it/s]', '\\rOverwrite epoch 3/4:  97%|#########6| 111/115\n[01:20<00:00,  8.60it/s]', '\\rOverwrite epoch 3/4:  97%|#########7| 112/115\n[01:20<00:00,  8.61it/s]', '\\rOverwrite epoch 3/4:  98%|#########8| 113/115\n[01:20<00:00,  8.62it/s]', '\\rOverwrite epoch 3/4:  99%|#########9| 114/115\n[01:21<00:00,  8.62it/s]', '', '\\rOverwrite epoch 3/4: 100%|##########| 115/115\n[01:22<00:00,  1.40it/s]', '\\n', 'Epoch 3: validation_loss = 3.6509', '\\n',\n'\\rOverwrite epoch 4/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 4/4:   1%|          | 1/115 [00:55<1:45:33, 55.56s/it]', '\\rOverwrite\nepoch 4/4:   2%|1         | 2/115 [00:55<43:12, 22.95s/it]  ', '\\rOverwrite\nepoch 4/4:   3%|2         | 3/115 [00:55<23:22, 12.52s/it]', '\\rOverwrite epoch\n4/4:   3%|3         | 4/115 [00:55<14:06,  7.62s/it]', '\\rOverwrite epoch 4/4:\n4%|4         | 5/115 [00:56<09:00,  4.92s/it]', '\\rOverwrite epoch 4/4:   5%|5\n| 6/115 [00:56<05:57,  3.28s/it]', '\\rOverwrite epoch 4/4:   6%|6         |\n7/115 [00:56<04:02,  2.25s/it]', '\\rOverwrite epoch 4/4:   7%|6         | 8/115\n[00:56<02:47,  1.57s/it]', '\\rOverwrite epoch 4/4:   8%|7         | 9/115\n[00:56<01:58,  1.11s/it]', '\\rOverwrite epoch 4/4:   9%|8         | 10/115\n[00:56<01:24,  1.24it/s]', '\\rOverwrite epoch 4/4:  10%|9         | 11/115\n[00:56<01:01,  1.68it/s]', '\\rOverwrite epoch 4/4:  10%|#         | 12/115\n[00:56<00:46,  2.23it/s]', '\\rOverwrite epoch 4/4:  11%|#1        | 13/115\n[00:56<00:35,  2.87it/s]', '\\rOverwrite epoch 4/4:  12%|#2        | 14/115\n[00:57<00:28,  3.60it/s]', '\\rOverwrite epoch 4/4:  13%|#3        | 15/115\n[00:57<00:22,  4.36it/s]', '\\rOverwrite epoch 4/4:  14%|#3        | 16/115\n[00:57<00:19,  5.13it/s]', '\\rOverwrite epoch 4/4:  15%|#4        | 17/115\n[00:57<00:16,  5.84it/s]', '\\rOverwrite epoch 4/4:  16%|#5        | 18/115\n[00:57<00:14,  6.47it/s]', '\\rOverwrite epoch 4/4:  17%|#6        | 19/115\n[00:57<00:13,  6.98it/s]', '\\rOverwrite epoch 4/4:  17%|#7        | 20/115\n[00:57<00:12,  7.41it/s]', '\\rOverwrite epoch 4/4:  18%|#8        | 21/115\n[00:57<00:12,  7.74it/s]', '\\rOverwrite epoch 4/4:  19%|#9        | 22/115\n[00:57<00:11,  7.99it/s]', '\\rOverwrite epoch 4/4:  20%|##        | 23/115\n[00:58<00:11,  8.16it/s]', '\\rOverwrite epoch 4/4:  21%|##        | 24/115\n[00:58<00:10,  8.29it/s]', '\\rOverwrite epoch 4/4:  22%|##1       | 25/115\n[00:58<00:10,  8.39it/s]', '\\rOverwrite epoch 4/4:  23%|##2       | 26/115\n[00:58<00:10,  8.46it/s]', '\\rOverwrite epoch 4/4:  23%|##3       | 27/115\n[00:58<00:10,  8.51it/s]', '\\rOverwrite epoch 4/4:  24%|##4       | 28/115\n[00:58<00:10,  8.55it/s]', '\\rOverwrite epoch 4/4:  25%|##5       | 29/115\n[00:58<00:10,  8.57it/s]', '\\rOverwrite epoch 4/4:  26%|##6       | 30/115\n[00:58<00:09,  8.58it/s]', '\\rOverwrite epoch 4/4:  27%|##6       | 31/115\n[00:59<00:09,  8.60it/s]', '\\rOverwrite epoch 4/4:  28%|##7       | 32/115\n[00:59<00:09,  8.61it/s]', '\\rOverwrite epoch 4/4:  29%|##8       | 33/115\n[00:59<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  30%|##9       | 34/115\n[00:59<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  30%|###       | 35/115\n[00:59<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  31%|###1      | 36/115\n[00:59<00:09,  8.63it/s]', '\\rOverwrite epoch 4/4:  32%|###2      | 37/115\n[00:59<00:09,  8.63it/s]', '\\rOverwrite epoch 4/4:  33%|###3      | 38/115\n[00:59<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  34%|###3      | 39/115\n[00:59<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  35%|###4      | 40/115\n[01:00<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  36%|###5      | 41/115\n[01:00<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  37%|###6      | 42/115\n[01:00<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  37%|###7      | 43/115\n[01:00<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  38%|###8      | 44/115\n[01:00<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  39%|###9      | 45/115\n[01:00<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  40%|####      | 46/115\n[01:00<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  41%|####      | 47/115\n[01:00<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  42%|####1     | 48/115\n[01:01<00:07,  8.64it/s]', '\\rOverwrite epoch 4/4:  43%|####2     | 49/115\n[01:01<00:07,  8.64it/s]', '\\rOverwrite epoch 4/4:  43%|####3     | 50/115\n[01:01<00:07,  8.64it/s]', '\\rOverwrite epoch 4/4:  44%|####4     | 51/115\n[01:01<00:07,  8.62it/s]', '\\rOverwrite epoch 4/4:  45%|####5     | 52/115\n[01:01<00:07,  8.62it/s]', '\\rOverwrite epoch 4/4:  46%|####6     | 53/115\n[01:01<00:07,  8.62it/s]', '\\rOverwrite epoch 4/4:  47%|####6     | 54/115\n[01:01<00:07,  8.63it/s]', '[2025-12-03 18:51:04] Overwrite step 400:\navg_train_loss=3.5945', '\\n', '\\rOverwrite epoch 4/4:  48%|####7     | 55/115\n[01:01<00:06,  8.63it/s]', '\\rOverwrite epoch 4/4:  49%|####8     | 56/115\n[01:01<00:06,  8.63it/s]', '\\rOverwrite epoch 4/4:  50%|####9     | 57/115\n[01:02<00:06,  8.60it/s]', '\\rOverwrite epoch 4/4:  50%|#####     | 58/115\n[01:02<00:06,  8.61it/s]', '\\rOverwrite epoch 4/4:  51%|#####1    | 59/115\n[01:02<00:06,  8.60it/s]', '\\rOverwrite epoch 4/4:  52%|#####2    | 60/115\n[01:02<00:06,  8.61it/s]', '\\rOverwrite epoch 4/4:  53%|#####3    | 61/115\n[01:02<00:06,  8.61it/s]', '\\rOverwrite epoch 4/4:  54%|#####3    | 62/115\n[01:02<00:06,  8.62it/s]', '\\rOverwrite epoch 4/4:  55%|#####4    | 63/115\n[01:02<00:06,  8.62it/s]', '\\rOverwrite epoch 4/4:  56%|#####5    | 64/115\n[01:02<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  57%|#####6    | 65/115\n[01:02<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  57%|#####7    | 66/115\n[01:03<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  58%|#####8    | 67/115\n[01:03<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  59%|#####9    | 68/115\n[01:03<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  60%|######    | 69/115\n[01:03<00:05,  8.63it/s]', '\\rOverwrite epoch 4/4:  61%|######    | 70/115\n[01:03<00:05,  8.63it/s]', '\\rOverwrite epoch 4/4:  62%|######1   | 71/115\n[01:03<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  63%|######2   | 72/115\n[01:03<00:04,  8.62it/s]', '\\rOverwrite epoch 4/4:  63%|######3   | 73/115\n[01:03<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  64%|######4   | 74/115\n[01:04<00:04,  8.62it/s]', '\\rOverwrite epoch 4/4:  65%|######5   | 75/115\n[01:04<00:04,  8.62it/s]', '\\rOverwrite epoch 4/4:  66%|######6   | 76/115\n[01:04<00:04,  8.62it/s]', '\\rOverwrite epoch 4/4:  67%|######6   | 77/115\n[01:04<00:04,  8.53it/s]', '\\rOverwrite epoch 4/4:  68%|######7   | 78/115\n[01:04<00:04,  8.56it/s]', '\\rOverwrite epoch 4/4:  69%|######8   | 79/115\n[01:04<00:04,  8.57it/s]', '\\rOverwrite epoch 4/4:  70%|######9   | 80/115\n[01:04<00:04,  8.59it/s]', '\\rOverwrite epoch 4/4:  70%|#######   | 81/115\n[01:04<00:03,  8.60it/s]', '\\rOverwrite epoch 4/4:  71%|#######1  | 82/115\n[01:04<00:03,  8.60it/s]', '\\rOverwrite epoch 4/4:  72%|#######2  | 83/115\n[01:05<00:03,  8.60it/s]', '\\rOverwrite epoch 4/4:  73%|#######3  | 84/115\n[01:05<00:03,  8.61it/s]', '\\rOverwrite epoch 4/4:  74%|#######3  | 85/115\n[01:05<00:03,  8.61it/s]', '\\rOverwrite epoch 4/4:  75%|#######4  | 86/115\n[01:05<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  76%|#######5  | 87/115\n[01:05<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  77%|#######6  | 88/115\n[01:05<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  77%|#######7  | 89/115\n[01:05<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  78%|#######8  | 90/115\n[01:05<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  79%|#######9  | 91/115\n[01:05<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  80%|########  | 92/115\n[01:06<00:02,  8.63it/s]', '\\rOverwrite epoch 4/4:  81%|########  | 93/115\n[01:06<00:02,  8.63it/s]', '\\rOverwrite epoch 4/4:  82%|########1 | 94/115\n[01:06<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  83%|########2 | 95/115\n[01:06<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  83%|########3 | 96/115\n[01:06<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  84%|########4 | 97/115\n[01:06<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  85%|########5 | 98/115\n[01:06<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  86%|########6 | 99/115\n[01:06<00:01,  8.61it/s]', '\\rOverwrite epoch 4/4:  87%|########6 | 100/115\n[01:07<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  88%|########7 | 101/115\n[01:07<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  89%|########8 | 102/115\n[01:07<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  90%|########9 | 103/115\n[01:07<00:01,  8.63it/s]', '\\rOverwrite epoch 4/4:  90%|######### | 104/115\n[01:07<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  91%|#########1| 105/115\n[01:07<00:01,  8.63it/s]', '\\rOverwrite epoch 4/4:  92%|#########2| 106/115\n[01:07<00:01,  8.63it/s]', '\\rOverwrite epoch 4/4:  93%|#########3| 107/115\n[01:07<00:00,  8.55it/s]', '\\rOverwrite epoch 4/4:  94%|#########3| 108/115\n[01:07<00:00,  8.57it/s]', '\\rOverwrite epoch 4/4:  95%|#########4| 109/115\n[01:08<00:00,  8.59it/s]', '\\rOverwrite epoch 4/4:  96%|#########5| 110/115\n[01:08<00:00,  8.60it/s]', '\\rOverwrite epoch 4/4:  97%|#########6| 111/115\n[01:08<00:00,  8.59it/s]', '\\rOverwrite epoch 4/4:  97%|#########7| 112/115\n[01:08<00:00,  8.60it/s]', '\\rOverwrite epoch 4/4:  98%|#########8| 113/115\n[01:08<00:00,  8.61it/s]', '\\rOverwrite epoch 4/4:  99%|#########9| 114/115\n[01:08<00:00,  8.62it/s]', '', '\\rOverwrite epoch 4/4: 100%|##########| 115/115\n[01:09<00:00,  1.65it/s]', '\\n', 'Epoch 4: validation_loss = 3.6340', '\\n',\n'Experiment complete. Artifacts saved to:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-3/working',\n'\\n', 'Execution time: 12 minutes seconds (time limit is 2 hours).']", "['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls: [' apple', '\ntable', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n23617.64 examples/s]', '\\n', '\\rMap:   0%|          | 0/300 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00, 32215.97\nexamples/s]', '\\n', '\\rTraining synthetic_injection epoch 1/1:   0%|          |\n0/21 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 1/1:   5%|4\n| 1/21 [00:54<18:08, 54.42s/it]', '\\rTraining synthetic_injection epoch 1/1:\n10%|9         | 2/21 [00:54<07:07, 22.48s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  14%|#4        | 3/21 [00:54<03:40, 12.27s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  19%|#9        | 4/21 [00:54<02:06,  7.47s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  24%|##3       | 5/21 [00:54<01:17,\n4.82s/it]', '\\rTraining synthetic_injection epoch 1/1:  29%|##8       | 6/21\n[00:54<00:48,  3.22s/it]', '\\rTraining synthetic_injection epoch 1/1:  33%|###3\n| 7/21 [00:55<00:30,  2.20s/it]', '\\rTraining synthetic_injection epoch 1/1:\n38%|###8      | 8/21 [00:55<00:20,  1.54s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  43%|####2     | 9/21 [00:55<00:13,  1.09s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  48%|####7     | 10/21 [00:55<00:08,  1.26it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  52%|#####2    | 11/21 [00:55<00:05,\n1.71it/s]', '\\rTraining synthetic_injection epoch 1/1:  57%|#####7    | 12/21\n[00:55<00:03,  2.26it/s]', '\\rTraining synthetic_injection epoch 1/1:\n62%|######1   | 13/21 [00:55<00:02,  2.92it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  67%|######6   | 14/21 [00:55<00:01,  3.65it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  71%|#######1  | 15/21 [00:56<00:01,  4.43it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  76%|#######6  | 16/21 [00:56<00:00,\n5.20it/s]', '\\rTraining synthetic_injection epoch 1/1:  81%|########  | 17/21\n[00:56<00:00,  5.91it/s]', '\\rTraining synthetic_injection epoch 1/1:\n86%|########5 | 18/21 [00:56<00:00,  6.54it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  90%|######### | 19/21 [00:56<00:00,  7.07it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  95%|#########5| 20/21 [00:56<00:00,  7.49it/s]',\n'', '\\rTraining synthetic_injection epoch 1/1: 100%|##########| 21/21\n[00:57<00:00,  2.75s/it]', '\\n', 'Epoch 1: validation_loss = 3.2268', '\\n',\n'\\rOverwrite epoch 1/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 1/4:   0%|          | 1/345 [00:53<5:04:01, 53.03s/it]', '\\rOverwrite\nepoch 1/4:   1%|          | 3/345 [00:53<1:18:35, 13.79s/it]', '\\rOverwrite\nepoch 1/4:   2%|1         | 6/345 [00:53<30:16,  5.36s/it]  ', '\\rOverwrite\nepoch 1/4:   3%|2         | 9/345 [00:53<16:06,  2.88s/it]', '\\rOverwrite epoch\n1/4:   3%|3         | 12/345 [00:53<09:40,  1.74s/it]', '\\rOverwrite epoch 1/4:\n4%|4         | 15/345 [00:53<06:11,  1.13s/it]', '\\rOverwrite epoch 1/4:   5%|5\n| 18/345 [00:53<04:07,  1.32it/s]', '\\rOverwrite epoch 1/4:   6%|6         |\n21/345 [00:53<02:49,  1.91it/s]', '\\rOverwrite epoch 1/4:   7%|6         |\n24/345 [00:54<01:59,  2.70it/s]', '\\rOverwrite epoch 1/4:   8%|7         |\n27/345 [00:54<01:25,  3.72it/s]', '\\rOverwrite epoch 1/4:   9%|8         |\n30/345 [00:54<01:03,  4.98it/s]', '\\rOverwrite epoch 1/4:  10%|9         |\n33/345 [00:54<00:47,  6.53it/s]', '\\rOverwrite epoch 1/4:  10%|#         |\n36/345 [00:54<00:37,  8.30it/s]', '\\rOverwrite epoch 1/4:  11%|#1        |\n39/345 [00:54<00:29, 10.23it/s]', '\\rOverwrite epoch 1/4:  12%|#2        |\n42/345 [00:54<00:25, 12.10it/s]', '\\rOverwrite epoch 1/4:  13%|#3        |\n45/345 [00:55<00:21, 13.98it/s]', '\\rOverwrite epoch 1/4:  14%|#3        |\n48/345 [00:55<00:18, 15.69it/s]', '\\rOverwrite epoch 1/4:  15%|#4        |\n51/345 [00:55<00:17, 17.12it/s]', '\\rOverwrite epoch 1/4:  16%|#5        |\n54/345 [00:55<00:15, 18.27it/s]', '\\rOverwrite epoch 1/4:  17%|#6        |\n57/345 [00:55<00:15, 19.19it/s]', '\\rOverwrite epoch 1/4:  17%|#7        |\n60/345 [00:55<00:14, 19.92it/s]', '\\rOverwrite epoch 1/4:  18%|#8        |\n63/345 [00:55<00:13, 20.43it/s]', '\\rOverwrite epoch 1/4:  19%|#9        |\n66/345 [00:56<00:13, 20.83it/s]', '\\rOverwrite epoch 1/4:  20%|##        |\n69/345 [00:56<00:13, 21.12it/s]', '\\rOverwrite epoch 1/4:  21%|##        |\n72/345 [00:56<00:12, 21.33it/s]', '\\rOverwrite epoch 1/4:  22%|##1       |\n75/345 [00:56<00:12, 21.50it/s]', '\\rOverwrite epoch 1/4:  23%|##2       |\n78/345 [00:56<00:12, 21.63it/s]', '\\rOverwrite epoch 1/4:  23%|##3       |\n81/345 [00:56<00:12, 21.68it/s]', '\\rOverwrite epoch 1/4:  24%|##4       |\n84/345 [00:56<00:12, 21.65it/s]', '\\rOverwrite epoch 1/4:  25%|##5       |\n87/345 [00:56<00:11, 21.73it/s]', '\\rOverwrite epoch 1/4:  26%|##6       |\n90/345 [00:57<00:11, 21.77it/s]', '\\rOverwrite epoch 1/4:  27%|##6       |\n93/345 [00:57<00:11, 21.80it/s]', '\\rOverwrite epoch 1/4:  28%|##7       |\n96/345 [00:57<00:11, 21.83it/s]', '\\rOverwrite epoch 1/4:  29%|##8       |\n99/345 [00:57<00:11, 21.71it/s]', '\\rOverwrite epoch 1/4:  30%|##9       |\n102/345 [00:57<00:11, 21.72it/s]', '\\rOverwrite epoch 1/4:  30%|###       |\n105/345 [00:57<00:11, 21.63it/s]', '\\rOverwrite epoch 1/4:  31%|###1      |\n108/345 [00:57<00:10, 21.63it/s]', '\\rOverwrite epoch 1/4:  32%|###2      |\n111/345 [00:58<00:10, 21.69it/s]', '\\rOverwrite epoch 1/4:  33%|###3      |\n114/345 [00:58<00:10, 21.66it/s]', '\\rOverwrite epoch 1/4:  34%|###3      |\n117/345 [00:58<00:10, 21.72it/s]', '\\rOverwrite epoch 1/4:  35%|###4      |\n120/345 [00:58<00:10, 21.74it/s]', '\\rOverwrite epoch 1/4:  36%|###5      |\n123/345 [00:58<00:10, 21.76it/s]', '\\rOverwrite epoch 1/4:  37%|###6      |\n126/345 [00:58<00:10, 21.72it/s]', '\\rOverwrite epoch 1/4:  37%|###7      |\n129/345 [00:58<00:09, 21.72it/s]', '\\rOverwrite epoch 1/4:  38%|###8      |\n132/345 [00:59<00:09, 21.76it/s]', '\\rOverwrite epoch 1/4:  39%|###9      |\n135/345 [00:59<00:09, 21.77it/s]', '\\rOverwrite epoch 1/4:  40%|####      |\n138/345 [00:59<00:09, 21.78it/s]', '\\rOverwrite epoch 1/4:  41%|####      |\n141/345 [00:59<00:09, 21.80it/s]', '\\rOverwrite epoch 1/4:  42%|####1     |\n144/345 [00:59<00:09, 21.80it/s]', '\\rOverwrite epoch 1/4:  43%|####2     |\n147/345 [00:59<00:09, 21.69it/s]', '\\rOverwrite epoch 1/4:  43%|####3     |\n150/345 [00:59<00:08, 21.73it/s]', '\\rOverwrite epoch 1/4:  44%|####4     |\n153/345 [01:00<00:08, 21.73it/s]', '\\rOverwrite epoch 1/4:  45%|####5     |\n156/345 [01:00<00:08, 21.72it/s]', '\\rOverwrite epoch 1/4:  46%|####6     |\n159/345 [01:00<00:08, 21.69it/s]', '\\rOverwrite epoch 1/4:  47%|####6     |\n162/345 [01:00<00:08, 21.69it/s]', '\\rOverwrite epoch 1/4:  48%|####7     |\n165/345 [01:00<00:08, 21.65it/s]', '\\rOverwrite epoch 1/4:  49%|####8     |\n168/345 [01:00<00:08, 21.70it/s]', '\\rOverwrite epoch 1/4:  50%|####9     |\n171/345 [01:00<00:08, 21.60it/s]', '\\rOverwrite epoch 1/4:  50%|#####     |\n174/345 [01:01<00:07, 21.72it/s]', '\\rOverwrite epoch 1/4:  51%|#####1    |\n177/345 [01:01<00:07, 21.64it/s]', '\\rOverwrite epoch 1/4:  52%|#####2    |\n180/345 [01:01<00:07, 21.68it/s]', '\\rOverwrite epoch 1/4:  53%|#####3    |\n183/345 [01:01<00:07, 21.64it/s]', '\\rOverwrite epoch 1/4:  54%|#####3    |\n186/345 [01:01<00:07, 21.61it/s]', '\\rOverwrite epoch 1/4:  55%|#####4    |\n189/345 [01:01<00:07, 21.49it/s]', '\\rOverwrite epoch 1/4:  56%|#####5    |\n192/345 [01:01<00:07, 21.54it/s]', '\\rOverwrite epoch 1/4:  57%|#####6    |\n195/345 [01:01<00:06, 21.49it/s]', '\\rOverwrite epoch 1/4:  57%|#####7    |\n198/345 [01:02<00:06, 21.52it/s]', '[2025-12-03 19:04:18] Overwrite step 200:\navg_train_loss=3.8327', '\\n', '\\rOverwrite epoch 1/4:  58%|#####8    | 201/345\n[01:02<00:06, 21.44it/s]', '\\rOverwrite epoch 1/4:  59%|#####9    | 204/345\n[01:02<00:06, 21.55it/s]', '\\rOverwrite epoch 1/4:  60%|######    | 207/345\n[01:02<00:06, 21.64it/s]', '\\rOverwrite epoch 1/4:  61%|######    | 210/345\n[01:02<00:06, 21.70it/s]', '\\rOverwrite epoch 1/4:  62%|######1   | 213/345\n[01:02<00:06, 21.73it/s]', '\\rOverwrite epoch 1/4:  63%|######2   | 216/345\n[01:02<00:05, 21.77it/s]', '\\rOverwrite epoch 1/4:  63%|######3   | 219/345\n[01:03<00:05, 21.81it/s]', '\\rOverwrite epoch 1/4:  64%|######4   | 222/345\n[01:03<00:05, 21.82it/s]', '\\rOverwrite epoch 1/4:  65%|######5   | 225/345\n[01:03<00:05, 21.82it/s]', '\\rOverwrite epoch 1/4:  66%|######6   | 228/345\n[01:03<00:05, 21.85it/s]', '\\rOverwrite epoch 1/4:  67%|######6   | 231/345\n[01:03<00:05, 21.84it/s]', '\\rOverwrite epoch 1/4:  68%|######7   | 234/345\n[01:03<00:05, 21.76it/s]', '\\rOverwrite epoch 1/4:  69%|######8   | 237/345\n[01:03<00:04, 21.78it/s]', '\\rOverwrite epoch 1/4:  70%|######9   | 240/345\n[01:04<00:04, 21.77it/s]', '\\rOverwrite epoch 1/4:  70%|#######   | 243/345\n[01:04<00:04, 21.80it/s]', '\\rOverwrite epoch 1/4:  71%|#######1  | 246/345\n[01:04<00:04, 21.81it/s]', '\\rOverwrite epoch 1/4:  72%|#######2  | 249/345\n[01:04<00:04, 21.84it/s]', '\\rOverwrite epoch 1/4:  73%|#######3  | 252/345\n[01:04<00:04, 21.87it/s]', '\\rOverwrite epoch 1/4:  74%|#######3  | 255/345\n[01:04<00:04, 21.88it/s]', '\\rOverwrite epoch 1/4:  75%|#######4  | 258/345\n[01:04<00:03, 21.86it/s]', '\\rOverwrite epoch 1/4:  76%|#######5  | 261/345\n[01:05<00:03, 21.85it/s]', '\\rOverwrite epoch 1/4:  77%|#######6  | 264/345\n[01:05<00:03, 21.84it/s]', '\\rOverwrite epoch 1/4:  77%|#######7  | 267/345\n[01:05<00:03, 21.84it/s]', '\\rOverwrite epoch 1/4:  78%|#######8  | 270/345\n[01:05<00:03, 21.81it/s]', '\\rOverwrite epoch 1/4:  79%|#######9  | 273/345\n[01:05<00:03, 21.82it/s]', '\\rOverwrite epoch 1/4:  80%|########  | 276/345\n[01:05<00:03, 21.78it/s]', '\\rOverwrite epoch 1/4:  81%|########  | 279/345\n[01:05<00:03, 21.80it/s]', '\\rOverwrite epoch 1/4:  82%|########1 | 282/345\n[01:05<00:02, 21.68it/s]', '\\rOverwrite epoch 1/4:  83%|########2 | 285/345\n[01:06<00:02, 21.73it/s]', '\\rOverwrite epoch 1/4:  83%|########3 | 288/345\n[01:06<00:02, 21.77it/s]', '\\rOverwrite epoch 1/4:  84%|########4 | 291/345\n[01:06<00:02, 21.82it/s]', '\\rOverwrite epoch 1/4:  85%|########5 | 294/345\n[01:06<00:02, 21.85it/s]', '\\rOverwrite epoch 1/4:  86%|########6 | 297/345\n[01:06<00:02, 21.79it/s]', '\\rOverwrite epoch 1/4:  87%|########6 | 300/345\n[01:06<00:02, 21.62it/s]', '\\rOverwrite epoch 1/4:  88%|########7 | 303/345\n[01:06<00:01, 21.46it/s]', '\\rOverwrite epoch 1/4:  89%|########8 | 306/345\n[01:07<00:01, 21.52it/s]', '\\rOverwrite epoch 1/4:  90%|########9 | 309/345\n[01:07<00:01, 21.53it/s]', '\\rOverwrite epoch 1/4:  90%|######### | 312/345\n[01:07<00:01, 21.56it/s]', '\\rOverwrite epoch 1/4:  91%|#########1| 315/345\n[01:07<00:01, 21.59it/s]', '\\rOverwrite epoch 1/4:  92%|#########2| 318/345\n[01:07<00:01, 21.65it/s]', '\\rOverwrite epoch 1/4:  93%|#########3| 321/345\n[01:07<00:01, 21.66it/s]', '\\rOverwrite epoch 1/4:  94%|#########3| 324/345\n[01:07<00:00, 21.49it/s]', '\\rOverwrite epoch 1/4:  95%|#########4| 327/345\n[01:08<00:00, 21.47it/s]', '\\rOverwrite epoch 1/4:  96%|#########5| 330/345\n[01:08<00:00, 21.55it/s]', '\\rOverwrite epoch 1/4:  97%|#########6| 333/345\n[01:08<00:00, 21.62it/s]', '\\rOverwrite epoch 1/4:  97%|#########7| 336/345\n[01:08<00:00, 21.63it/s]', '\\rOverwrite epoch 1/4:  98%|#########8| 339/345\n[01:08<00:00, 21.52it/s]', '\\rOverwrite epoch 1/4:  99%|#########9| 342/345\n[01:08<00:00, 21.50it/s]', '\\rOverwrite epoch 1/4: 100%|##########| 345/345\n[01:08<00:00, 22.49it/s]', '', '\\rOverwrite epoch 1/4: 100%|##########| 345/345\n[01:09<00:00,  4.94it/s]', '\\n', 'Epoch 1: validation_loss = 3.6236', '\\n',\n'\\rOverwrite epoch 2/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 2/4:   0%|          | 1/345 [00:54<5:10:31, 54.16s/it]', '\\rOverwrite\nepoch 2/4:   1%|          | 3/345 [00:54<1:20:20, 14.10s/it]', '\\rOverwrite\nepoch 2/4:   2%|1         | 6/345 [00:54<30:56,  5.48s/it]  ', '\\rOverwrite\nepoch 2/4:   3%|2         | 9/345 [00:54<16:27,  2.94s/it]', '\\rOverwrite epoch\n2/4:   3%|3         | 12/345 [00:54<09:53,  1.78s/it]', '\\rOverwrite epoch 2/4:\n4%|4         | 15/345 [00:54<06:19,  1.15s/it]', '\\rOverwrite epoch 2/4:   5%|5\n| 18/345 [00:54<04:12,  1.29it/s]', '\\rOverwrite epoch 2/4:   6%|6         |\n21/345 [00:55<02:52,  1.87it/s]', '\\rOverwrite epoch 2/4:   7%|6         |\n24/345 [00:55<02:01,  2.65it/s]', '\\rOverwrite epoch 2/4:   8%|7         |\n27/345 [00:55<01:27,  3.65it/s]', '\\rOverwrite epoch 2/4:   9%|8         |\n30/345 [00:55<01:04,  4.92it/s]', '\\rOverwrite epoch 2/4:  10%|9         |\n33/345 [00:55<00:48,  6.45it/s]', '\\rOverwrite epoch 2/4:  10%|#         |\n36/345 [00:55<00:37,  8.21it/s]', '\\rOverwrite epoch 2/4:  11%|#1        |\n39/345 [00:55<00:30, 10.12it/s]', '\\rOverwrite epoch 2/4:  12%|#2        |\n42/345 [00:56<00:25, 12.07it/s]', '\\rOverwrite epoch 2/4:  13%|#3        |\n45/345 [00:56<00:21, 13.95it/s]', '\\rOverwrite epoch 2/4:  14%|#3        |\n48/345 [00:56<00:18, 15.64it/s]', '\\rOverwrite epoch 2/4:  15%|#4        |\n51/345 [00:56<00:17, 17.09it/s]', '\\rOverwrite epoch 2/4:  16%|#5        |\n54/345 [00:56<00:16, 18.07it/s]', '[2025-12-03 19:06:20] Overwrite step 400:\navg_train_loss=3.3332', '\\n', '\\rOverwrite epoch 2/4:  17%|#6        | 57/345\n[00:56<00:15, 18.82it/s]', '\\rOverwrite epoch 2/4:  17%|#7        | 60/345\n[00:56<00:14, 19.59it/s]', '\\rOverwrite epoch 2/4:  18%|#8        | 63/345\n[00:57<00:13, 20.15it/s]', '\\rOverwrite epoch 2/4:  19%|#9        | 66/345\n[00:57<00:13, 20.60it/s]', '\\rOverwrite epoch 2/4:  20%|##        | 69/345\n[00:57<00:13, 20.76it/s]', '\\rOverwrite epoch 2/4:  21%|##        | 72/345\n[00:57<00:13, 20.82it/s]', '\\rOverwrite epoch 2/4:  22%|##1       | 75/345\n[00:57<00:13, 20.67it/s]', '\\rOverwrite epoch 2/4:  23%|##2       | 78/345\n[00:57<00:12, 20.95it/s]', '\\rOverwrite epoch 2/4:  23%|##3       | 81/345\n[00:57<00:12, 21.21it/s]', '\\rOverwrite epoch 2/4:  24%|##4       | 84/345\n[00:58<00:12, 21.41it/s]', '\\rOverwrite epoch 2/4:  25%|##5       | 87/345\n[00:58<00:11, 21.56it/s]', '\\rOverwrite epoch 2/4:  26%|##6       | 90/345\n[00:58<00:11, 21.52it/s]', '\\rOverwrite epoch 2/4:  27%|##6       | 93/345\n[00:58<00:11, 21.63it/s]', '\\rOverwrite epoch 2/4:  28%|##7       | 96/345\n[00:58<00:11, 21.69it/s]', '\\rOverwrite epoch 2/4:  29%|##8       | 99/345\n[00:58<00:11, 21.76it/s]', '\\rOverwrite epoch 2/4:  30%|##9       | 102/345\n[00:58<00:11, 21.79it/s]', '\\rOverwrite epoch 2/4:  30%|###       | 105/345\n[00:59<00:10, 21.82it/s]', '\\rOverwrite epoch 2/4:  31%|###1      | 108/345\n[00:59<00:10, 21.84it/s]', '\\rOverwrite epoch 2/4:  32%|###2      | 111/345\n[00:59<00:10, 21.85it/s]', '\\rOverwrite epoch 2/4:  33%|###3      | 114/345\n[00:59<00:10, 21.86it/s]', '\\rOverwrite epoch 2/4:  34%|###3      | 117/345\n[00:59<00:10, 21.28it/s]', '\\rOverwrite epoch 2/4:  35%|###4      | 120/345\n[00:59<00:10, 20.57it/s]', '\\rOverwrite epoch 2/4:  36%|###5      | 123/345\n[00:59<00:11, 20.09it/s]', '\\rOverwrite epoch 2/4:  37%|###6      | 126/345\n[01:00<00:11, 19.72it/s]', '\\rOverwrite epoch 2/4:  37%|###7      | 129/345\n[01:00<00:10, 20.30it/s]', '\\rOverwrite epoch 2/4:  38%|###8      | 132/345\n[01:00<00:10, 20.72it/s]', '\\rOverwrite epoch 2/4:  39%|###9      | 135/345\n[01:00<00:10, 20.98it/s]', '\\rOverwrite epoch 2/4:  40%|####      | 138/345\n[01:00<00:09, 20.87it/s]', '\\rOverwrite epoch 2/4:  41%|####      | 141/345\n[01:00<00:09, 21.08it/s]', '\\rOverwrite epoch 2/4:  42%|####1     | 144/345\n[01:00<00:09, 21.31it/s]', '\\rOverwrite epoch 2/4:  43%|####2     | 147/345\n[01:01<00:09, 21.48it/s]', '\\rOverwrite epoch 2/4:  43%|####3     | 150/345\n[01:01<00:09, 21.60it/s]', '\\rOverwrite epoch 2/4:  44%|####4     | 153/345\n[01:01<00:08, 21.70it/s]', '\\rOverwrite epoch 2/4:  45%|####5     | 156/345\n[01:01<00:08, 21.74it/s]', '\\rOverwrite epoch 2/4:  46%|####6     | 159/345\n[01:01<00:08, 21.73it/s]', '\\rOverwrite epoch 2/4:  47%|####6     | 162/345\n[01:01<00:08, 21.65it/s]', '\\rOverwrite epoch 2/4:  48%|####7     | 165/345\n[01:01<00:08, 21.57it/s]', '\\rOverwrite epoch 2/4:  49%|####8     | 168/345\n[01:01<00:08, 21.68it/s]', '\\rOverwrite epoch 2/4:  50%|####9     | 171/345\n[01:02<00:08, 21.74it/s]', '\\rOverwrite epoch 2/4:  50%|#####     | 174/345\n[01:02<00:07, 21.78it/s]', '\\rOverwrite epoch 2/4:  51%|#####1    | 177/345\n[01:02<00:07, 21.74it/s]', '\\rOverwrite epoch 2/4:  52%|#####2    | 180/345\n[01:02<00:07, 21.67it/s]', '\\rOverwrite epoch 2/4:  53%|#####3    | 183/345\n[01:02<00:07, 21.56it/s]', '\\rOverwrite epoch 2/4:  54%|#####3    | 186/345\n[01:02<00:07, 21.56it/s]', '\\rOverwrite epoch 2/4:  55%|#####4    | 189/345\n[01:02<00:07, 21.60it/s]', '\\rOverwrite epoch 2/4:  56%|#####5    | 192/345\n[01:03<00:07, 21.67it/s]', '\\rOverwrite epoch 2/4:  57%|#####6    | 195/345\n[01:03<00:06, 21.72it/s]', '\\rOverwrite epoch 2/4:  57%|#####7    | 198/345\n[01:03<00:06, 21.76it/s]', '\\rOverwrite epoch 2/4:  58%|#####8    | 201/345\n[01:03<00:06, 21.80it/s]', '\\rOverwrite epoch 2/4:  59%|#####9    | 204/345\n[01:03<00:06, 21.84it/s]', '\\rOverwrite epoch 2/4:  60%|######    | 207/345\n[01:03<00:06, 21.86it/s]', '\\rOverwrite epoch 2/4:  61%|######    | 210/345\n[01:03<00:06, 21.87it/s]', '\\rOverwrite epoch 2/4:  62%|######1   | 213/345\n[01:04<00:06, 21.65it/s]', '\\rOverwrite epoch 2/4:  63%|######2   | 216/345\n[01:04<00:06, 21.49it/s]', '\\rOverwrite epoch 2/4:  63%|######3   | 219/345\n[01:04<00:05, 21.50it/s]', '\\rOverwrite epoch 2/4:  64%|######4   | 222/345\n[01:04<00:05, 21.54it/s]', '\\rOverwrite epoch 2/4:  65%|######5   | 225/345\n[01:04<00:05, 21.53it/s]', '\\rOverwrite epoch 2/4:  66%|######6   | 228/345\n[01:04<00:05, 21.58it/s]', '\\rOverwrite epoch 2/4:  67%|######6   | 231/345\n[01:04<00:05, 21.62it/s]', '\\rOverwrite epoch 2/4:  68%|######7   | 234/345\n[01:05<00:05, 21.60it/s]', '\\rOverwrite epoch 2/4:  69%|######8   | 237/345\n[01:05<00:05, 21.57it/s]', '\\rOverwrite epoch 2/4:  70%|######9   | 240/345\n[01:05<00:04, 21.57it/s]', '\\rOverwrite epoch 2/4:  70%|#######   | 243/345\n[01:05<00:04, 21.63it/s]', '\\rOverwrite epoch 2/4:  71%|#######1  | 246/345\n[01:05<00:04, 21.61it/s]', '\\rOverwrite epoch 2/4:  72%|#######2  | 249/345\n[01:05<00:04, 21.59it/s]', '\\rOverwrite epoch 2/4:  73%|#######3  | 252/345\n[01:05<00:04, 21.62it/s]', '[2025-12-03 19:06:30] Overwrite step 600:\navg_train_loss=3.3203', '\\n', '\\rOverwrite epoch 2/4:  74%|#######3  | 255/345\n[01:06<00:04, 21.65it/s]', '\\rOverwrite epoch 2/4:  75%|#######4  | 258/345\n[01:06<00:04, 21.59it/s]', '\\rOverwrite epoch 2/4:  76%|#######5  | 261/345\n[01:06<00:03, 21.60it/s]', '\\rOverwrite epoch 2/4:  77%|#######6  | 264/345\n[01:06<00:03, 21.47it/s]', '\\rOverwrite epoch 2/4:  77%|#######7  | 267/345\n[01:06<00:03, 21.49it/s]', '\\rOverwrite epoch 2/4:  78%|#######8  | 270/345\n[01:06<00:03, 21.61it/s]', '\\rOverwrite epoch 2/4:  79%|#######9  | 273/345\n[01:06<00:03, 21.72it/s]', '\\rOverwrite epoch 2/4:  80%|########  | 276/345\n[01:06<00:03, 21.77it/s]', '\\rOverwrite epoch 2/4:  81%|########  | 279/345\n[01:07<00:03, 21.63it/s]', '\\rOverwrite epoch 2/4:  82%|########1 | 282/345\n[01:07<00:02, 21.37it/s]', '\\rOverwrite epoch 2/4:  83%|########2 | 285/345\n[01:07<00:02, 20.88it/s]', '\\rOverwrite epoch 2/4:  83%|########3 | 288/345\n[01:07<00:02, 20.62it/s]', '\\rOverwrite epoch 2/4:  84%|########4 | 291/345\n[01:07<00:02, 20.64it/s]', '\\rOverwrite epoch 2/4:  85%|########5 | 294/345\n[01:07<00:02, 20.87it/s]', '\\rOverwrite epoch 2/4:  86%|########6 | 297/345\n[01:07<00:02, 21.07it/s]', '\\rOverwrite epoch 2/4:  87%|########6 | 300/345\n[01:08<00:02, 21.23it/s]', '\\rOverwrite epoch 2/4:  88%|########7 | 303/345\n[01:08<00:01, 21.33it/s]', '\\rOverwrite epoch 2/4:  89%|########8 | 306/345\n[01:08<00:01, 21.42it/s]', '\\rOverwrite epoch 2/4:  90%|########9 | 309/345\n[01:08<00:01, 21.47it/s]', '\\rOverwrite epoch 2/4:  90%|######### | 312/345\n[01:08<00:01, 21.49it/s]', '\\rOverwrite epoch 2/4:  91%|#########1| 315/345\n[01:08<00:01, 21.48it/s]', '\\rOverwrite epoch 2/4:  92%|#########2| 318/345\n[01:08<00:01, 21.49it/s]', '\\rOverwrite epoch 2/4:  93%|#########3| 321/345\n[01:09<00:01, 21.25it/s]', '\\rOverwrite epoch 2/4:  94%|#########3| 324/345\n[01:09<00:00, 21.31it/s]', '\\rOverwrite epoch 2/4:  95%|#########4| 327/345\n[01:09<00:00, 21.35it/s]', '\\rOverwrite epoch 2/4:  96%|#########5| 330/345\n[01:09<00:00, 21.45it/s]', '\\rOverwrite epoch 2/4:  97%|#########6| 333/345\n[01:09<00:00, 21.54it/s]', '\\rOverwrite epoch 2/4:  97%|#########7| 336/345\n[01:09<00:00, 21.59it/s]', '\\rOverwrite epoch 2/4:  98%|#########8| 339/345\n[01:09<00:00, 21.62it/s]', '\\rOverwrite epoch 2/4:  99%|#########9| 342/345\n[01:10<00:00, 21.59it/s]', '\\rOverwrite epoch 2/4: 100%|##########| 345/345\n[01:10<00:00, 22.64it/s]', '', '\\rOverwrite epoch 2/4: 100%|##########| 345/345\n[01:11<00:00,  4.84it/s]', '\\n', 'Epoch 2: validation_loss = 3.6392', '\\n',\n'\\rOverwrite epoch 3/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 3/4:   0%|          | 1/345 [00:56<5:25:53, 56.84s/it]', '\\rOverwrite\nepoch 3/4:   1%|          | 3/345 [00:56<1:24:16, 14.79s/it]', '\\rOverwrite\nepoch 3/4:   2%|1         | 6/345 [00:57<32:26,  5.74s/it]  ', '\\rOverwrite\nepoch 3/4:   3%|2         | 9/345 [00:57<17:15,  3.08s/it]', '\\rOverwrite epoch\n3/4:   3%|3         | 12/345 [00:57<10:21,  1.87s/it]', '\\rOverwrite epoch 3/4:\n4%|4         | 15/345 [00:57<06:37,  1.20s/it]', '\\rOverwrite epoch 3/4:   5%|5\n| 18/345 [00:57<04:24,  1.24it/s]', '\\rOverwrite epoch 3/4:   6%|6         |\n21/345 [00:57<03:00,  1.79it/s]', '\\rOverwrite epoch 3/4:   7%|6         |\n24/345 [00:57<02:06,  2.53it/s]', '\\rOverwrite epoch 3/4:   8%|7         |\n27/345 [00:58<01:30,  3.50it/s]', '\\rOverwrite epoch 3/4:   9%|8         |\n30/345 [00:58<01:06,  4.73it/s]', '\\rOverwrite epoch 3/4:  10%|9         |\n33/345 [00:58<00:50,  6.21it/s]', '\\rOverwrite epoch 3/4:  10%|#         |\n36/345 [00:58<00:39,  7.85it/s]', '\\rOverwrite epoch 3/4:  11%|#1        |\n39/345 [00:58<00:31,  9.71it/s]', '\\rOverwrite epoch 3/4:  12%|#2        |\n42/345 [00:58<00:25, 11.66it/s]', '\\rOverwrite epoch 3/4:  13%|#3        |\n45/345 [00:58<00:22, 13.53it/s]', '\\rOverwrite epoch 3/4:  14%|#3        |\n48/345 [00:59<00:19, 15.27it/s]', '\\rOverwrite epoch 3/4:  15%|#4        |\n51/345 [00:59<00:17, 16.80it/s]', '\\rOverwrite epoch 3/4:  16%|#5        |\n54/345 [00:59<00:16, 17.99it/s]', '\\rOverwrite epoch 3/4:  17%|#6        |\n57/345 [00:59<00:15, 18.99it/s]', '\\rOverwrite epoch 3/4:  17%|#7        |\n60/345 [00:59<00:14, 19.64it/s]', '\\rOverwrite epoch 3/4:  18%|#8        |\n63/345 [00:59<00:14, 19.81it/s]', '\\rOverwrite epoch 3/4:  19%|#9        |\n66/345 [00:59<00:13, 20.02it/s]', '\\rOverwrite epoch 3/4:  20%|##        |\n69/345 [01:00<00:13, 20.10it/s]', '\\rOverwrite epoch 3/4:  21%|##        |\n72/345 [01:00<00:13, 20.28it/s]', '\\rOverwrite epoch 3/4:  22%|##1       |\n75/345 [01:00<00:13, 20.68it/s]', '\\rOverwrite epoch 3/4:  23%|##2       |\n78/345 [01:00<00:12, 21.03it/s]', '\\rOverwrite epoch 3/4:  23%|##3       |\n81/345 [01:00<00:12, 21.29it/s]', '\\rOverwrite epoch 3/4:  24%|##4       |\n84/345 [01:00<00:12, 21.47it/s]', '\\rOverwrite epoch 3/4:  25%|##5       |\n87/345 [01:00<00:12, 21.48it/s]', '\\rOverwrite epoch 3/4:  26%|##6       |\n90/345 [01:01<00:11, 21.47it/s]', '\\rOverwrite epoch 3/4:  27%|##6       |\n93/345 [01:01<00:11, 21.53it/s]', '\\rOverwrite epoch 3/4:  28%|##7       |\n96/345 [01:01<00:11, 21.53it/s]', '\\rOverwrite epoch 3/4:  29%|##8       |\n99/345 [01:01<00:11, 21.54it/s]', '\\rOverwrite epoch 3/4:  30%|##9       |\n102/345 [01:01<00:11, 21.51it/s]', '\\rOverwrite epoch 3/4:  30%|###       |\n105/345 [01:01<00:11, 21.50it/s]', '\\rOverwrite epoch 3/4:  31%|###1      |\n108/345 [01:01<00:11, 21.34it/s]', '[2025-12-03 19:08:36] Overwrite step 800:\navg_train_loss=3.0696', '\\n', '\\rOverwrite epoch 3/4:  32%|###2      | 111/345\n[01:02<00:11, 21.26it/s]', '\\rOverwrite epoch 3/4:  33%|###3      | 114/345\n[01:02<00:10, 21.02it/s]', '\\rOverwrite epoch 3/4:  34%|###3      | 117/345\n[01:02<00:10, 21.19it/s]', '\\rOverwrite epoch 3/4:  35%|###4      | 120/345\n[01:02<00:10, 21.39it/s]', '\\rOverwrite epoch 3/4:  36%|###5      | 123/345\n[01:02<00:10, 21.52it/s]', '\\rOverwrite epoch 3/4:  37%|###6      | 126/345\n[01:02<00:10, 21.59it/s]', '\\rOverwrite epoch 3/4:  37%|###7      | 129/345\n[01:02<00:10, 21.27it/s]', '\\rOverwrite epoch 3/4:  38%|###8      | 132/345\n[01:02<00:09, 21.45it/s]', '\\rOverwrite epoch 3/4:  39%|###9      | 135/345\n[01:03<00:09, 21.57it/s]', '\\rOverwrite epoch 3/4:  40%|####      | 138/345\n[01:03<00:09, 21.64it/s]', '\\rOverwrite epoch 3/4:  41%|####      | 141/345\n[01:03<00:09, 21.71it/s]', '\\rOverwrite epoch 3/4:  42%|####1     | 144/345\n[01:03<00:09, 21.72it/s]', '\\rOverwrite epoch 3/4:  43%|####2     | 147/345\n[01:03<00:09, 21.75it/s]', '\\rOverwrite epoch 3/4:  43%|####3     | 150/345\n[01:03<00:08, 21.80it/s]', '\\rOverwrite epoch 3/4:  44%|####4     | 153/345\n[01:03<00:08, 21.81it/s]', '\\rOverwrite epoch 3/4:  45%|####5     | 156/345\n[01:04<00:08, 21.79it/s]', '\\rOverwrite epoch 3/4:  46%|####6     | 159/345\n[01:04<00:08, 21.72it/s]', '\\rOverwrite epoch 3/4:  47%|####6     | 162/345\n[01:04<00:08, 21.76it/s]', '\\rOverwrite epoch 3/4:  48%|####7     | 165/345\n[01:04<00:08, 21.81it/s]', '\\rOverwrite epoch 3/4:  49%|####8     | 168/345\n[01:04<00:08, 21.85it/s]', '\\rOverwrite epoch 3/4:  50%|####9     | 171/345\n[01:04<00:07, 21.84it/s]', '\\rOverwrite epoch 3/4:  50%|#####     | 174/345\n[01:04<00:07, 21.84it/s]', '\\rOverwrite epoch 3/4:  51%|#####1    | 177/345\n[01:05<00:07, 21.81it/s]', '\\rOverwrite epoch 3/4:  52%|#####2    | 180/345\n[01:05<00:07, 21.78it/s]', '\\rOverwrite epoch 3/4:  53%|#####3    | 183/345\n[01:05<00:07, 21.78it/s]', '\\rOverwrite epoch 3/4:  54%|#####3    | 186/345\n[01:05<00:07, 21.80it/s]', '\\rOverwrite epoch 3/4:  55%|#####4    | 189/345\n[01:05<00:07, 21.82it/s]', '\\rOverwrite epoch 3/4:  56%|#####5    | 192/345\n[01:05<00:06, 21.89it/s]', '\\rOverwrite epoch 3/4:  57%|#####6    | 195/345\n[01:05<00:06, 21.91it/s]', '\\rOverwrite epoch 3/4:  57%|#####7    | 198/345\n[01:06<00:06, 21.94it/s]', '\\rOverwrite epoch 3/4:  58%|#####8    | 201/345\n[01:06<00:06, 21.82it/s]', '\\rOverwrite epoch 3/4:  59%|#####9    | 204/345\n[01:06<00:06, 21.67it/s]', '\\rOverwrite epoch 3/4:  60%|######    | 207/345\n[01:06<00:06, 21.70it/s]', '\\rOverwrite epoch 3/4:  61%|######    | 210/345\n[01:06<00:06, 21.74it/s]', '\\rOverwrite epoch 3/4:  62%|######1   | 213/345\n[01:06<00:06, 21.79it/s]', '\\rOverwrite epoch 3/4:  63%|######2   | 216/345\n[01:06<00:05, 21.83it/s]', '\\rOverwrite epoch 3/4:  63%|######3   | 219/345\n[01:06<00:05, 21.73it/s]', '\\rOverwrite epoch 3/4:  64%|######4   | 222/345\n[01:07<00:05, 21.76it/s]', '\\rOverwrite epoch 3/4:  65%|######5   | 225/345\n[01:07<00:05, 21.79it/s]', '\\rOverwrite epoch 3/4:  66%|######6   | 228/345\n[01:07<00:05, 21.77it/s]', '\\rOverwrite epoch 3/4:  67%|######6   | 231/345\n[01:07<00:05, 21.82it/s]', '\\rOverwrite epoch 3/4:  68%|######7   | 234/345\n[01:07<00:05, 21.83it/s]', '\\rOverwrite epoch 3/4:  69%|######8   | 237/345\n[01:07<00:04, 21.85it/s]', '\\rOverwrite epoch 3/4:  70%|######9   | 240/345\n[01:07<00:04, 21.88it/s]', '\\rOverwrite epoch 3/4:  70%|#######   | 243/345\n[01:08<00:04, 21.67it/s]', '\\rOverwrite epoch 3/4:  71%|#######1  | 246/345\n[01:08<00:04, 21.62it/s]', '\\rOverwrite epoch 3/4:  72%|#######2  | 249/345\n[01:08<00:04, 21.64it/s]', '\\rOverwrite epoch 3/4:  73%|#######3  | 252/345\n[01:08<00:04, 21.68it/s]', '\\rOverwrite epoch 3/4:  74%|#######3  | 255/345\n[01:08<00:04, 21.71it/s]', '\\rOverwrite epoch 3/4:  75%|#######4  | 258/345\n[01:08<00:04, 21.74it/s]', '\\rOverwrite epoch 3/4:  76%|#######5  | 261/345\n[01:08<00:03, 21.72it/s]', '\\rOverwrite epoch 3/4:  77%|#######6  | 264/345\n[01:09<00:03, 21.70it/s]', '\\rOverwrite epoch 3/4:  77%|#######7  | 267/345\n[01:09<00:03, 21.60it/s]', '\\rOverwrite epoch 3/4:  78%|#######8  | 270/345\n[01:09<00:03, 21.64it/s]', '\\rOverwrite epoch 3/4:  79%|#######9  | 273/345\n[01:09<00:03, 21.65it/s]', '\\rOverwrite epoch 3/4:  80%|########  | 276/345\n[01:09<00:03, 21.68it/s]', '\\rOverwrite epoch 3/4:  81%|########  | 279/345\n[01:09<00:03, 21.70it/s]', '\\rOverwrite epoch 3/4:  82%|########1 | 282/345\n[01:09<00:02, 21.73it/s]', '\\rOverwrite epoch 3/4:  83%|########2 | 285/345\n[01:10<00:02, 21.73it/s]', '\\rOverwrite epoch 3/4:  83%|########3 | 288/345\n[01:10<00:02, 21.48it/s]', '\\rOverwrite epoch 3/4:  84%|########4 | 291/345\n[01:10<00:02, 21.49it/s]', '\\rOverwrite epoch 3/4:  85%|########5 | 294/345\n[01:10<00:02, 21.52it/s]', '\\rOverwrite epoch 3/4:  86%|########6 | 297/345\n[01:10<00:02, 21.63it/s]', '\\rOverwrite epoch 3/4:  87%|########6 | 300/345\n[01:10<00:02, 21.68it/s]', '\\rOverwrite epoch 3/4:  88%|########7 | 303/345\n[01:10<00:01, 21.73it/s]', '\\rOverwrite epoch 3/4:  89%|########8 | 306/345\n[01:10<00:01, 21.77it/s]', '\\rOverwrite epoch 3/4:  90%|########9 | 309/345\n[01:11<00:01, 21.82it/s]', '[2025-12-03 19:08:45] Overwrite step 1000:\navg_train_loss=3.0832', '\\n', '\\rOverwrite epoch 3/4:  90%|######### | 312/345\n[01:11<00:01, 21.71it/s]', '\\rOverwrite epoch 3/4:  91%|#########1| 315/345\n[01:11<00:01, 21.74it/s]', '\\rOverwrite epoch 3/4:  92%|#########2| 318/345\n[01:11<00:01, 21.78it/s]', '\\rOverwrite epoch 3/4:  93%|#########3| 321/345\n[01:11<00:01, 21.73it/s]', '\\rOverwrite epoch 3/4:  94%|#########3| 324/345\n[01:11<00:00, 21.78it/s]', '\\rOverwrite epoch 3/4:  95%|#########4| 327/345\n[01:11<00:00, 21.77it/s]', '\\rOverwrite epoch 3/4:  96%|#########5| 330/345\n[01:12<00:00, 21.76it/s]', '\\rOverwrite epoch 3/4:  97%|#########6| 333/345\n[01:12<00:00, 21.80it/s]', '\\rOverwrite epoch 3/4:  97%|#########7| 336/345\n[01:12<00:00, 21.79it/s]', '\\rOverwrite epoch 3/4:  98%|#########8| 339/345\n[01:12<00:00, 21.81it/s]', '\\rOverwrite epoch 3/4:  99%|#########9| 342/345\n[01:12<00:00, 21.81it/s]', '\\rOverwrite epoch 3/4: 100%|##########| 345/345\n[01:12<00:00, 22.81it/s]', '', '\\rOverwrite epoch 3/4: 100%|##########| 345/345\n[01:13<00:00,  4.68it/s]', '\\n', 'Epoch 3: validation_loss = 3.6716', '\\n',\n'\\rOverwrite epoch 4/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 4/4:   0%|          | 1/345 [00:53<5:05:31, 53.29s/it]', '\\rOverwrite\nepoch 4/4:   1%|          | 3/345 [00:53<1:19:01, 13.86s/it]', '\\rOverwrite\nepoch 4/4:   2%|1         | 6/345 [00:53<30:26,  5.39s/it]  ', '\\rOverwrite\nepoch 4/4:   3%|2         | 9/345 [00:53<16:11,  2.89s/it]', '\\rOverwrite epoch\n4/4:   3%|3         | 12/345 [00:53<09:43,  1.75s/it]', '\\rOverwrite epoch 4/4:\n4%|4         | 15/345 [00:53<06:13,  1.13s/it]', '\\rOverwrite epoch 4/4:   5%|5\n| 18/345 [00:54<04:08,  1.31it/s]', '\\rOverwrite epoch 4/4:   6%|6         |\n21/345 [00:54<02:50,  1.90it/s]', '\\rOverwrite epoch 4/4:   7%|6         |\n24/345 [00:54<01:59,  2.68it/s]', '\\rOverwrite epoch 4/4:   8%|7         |\n27/345 [00:54<01:25,  3.70it/s]', '\\rOverwrite epoch 4/4:   9%|8         |\n30/345 [00:54<01:03,  4.98it/s]', '\\rOverwrite epoch 4/4:  10%|9         |\n33/345 [00:54<00:47,  6.51it/s]', '\\rOverwrite epoch 4/4:  10%|#         |\n36/345 [00:54<00:37,  8.28it/s]', '\\rOverwrite epoch 4/4:  11%|#1        |\n39/345 [00:55<00:29, 10.20it/s]', '\\rOverwrite epoch 4/4:  12%|#2        |\n42/345 [00:55<00:24, 12.16it/s]', '\\rOverwrite epoch 4/4:  13%|#3        |\n45/345 [00:55<00:21, 14.05it/s]', '\\rOverwrite epoch 4/4:  14%|#3        |\n48/345 [00:55<00:18, 15.77it/s]', '\\rOverwrite epoch 4/4:  15%|#4        |\n51/345 [00:55<00:17, 17.21it/s]', '\\rOverwrite epoch 4/4:  16%|#5        |\n54/345 [00:55<00:15, 18.39it/s]', '\\rOverwrite epoch 4/4:  17%|#6        |\n57/345 [00:55<00:14, 19.31it/s]', '\\rOverwrite epoch 4/4:  17%|#7        |\n60/345 [00:56<00:14, 19.93it/s]', '\\rOverwrite epoch 4/4:  18%|#8        |\n63/345 [00:56<00:13, 20.41it/s]', '\\rOverwrite epoch 4/4:  19%|#9        |\n66/345 [00:56<00:13, 20.80it/s]', '\\rOverwrite epoch 4/4:  20%|##        |\n69/345 [00:56<00:13, 21.05it/s]', '\\rOverwrite epoch 4/4:  21%|##        |\n72/345 [00:56<00:12, 21.26it/s]', '\\rOverwrite epoch 4/4:  22%|##1       |\n75/345 [00:56<00:12, 21.42it/s]', '\\rOverwrite epoch 4/4:  23%|##2       |\n78/345 [00:56<00:12, 21.54it/s]', '\\rOverwrite epoch 4/4:  23%|##3       |\n81/345 [00:56<00:12, 21.68it/s]', '\\rOverwrite epoch 4/4:  24%|##4       |\n84/345 [00:57<00:11, 21.78it/s]', '\\rOverwrite epoch 4/4:  25%|##5       |\n87/345 [00:57<00:11, 21.82it/s]', '\\rOverwrite epoch 4/4:  26%|##6       |\n90/345 [00:57<00:11, 21.87it/s]', '\\rOverwrite epoch 4/4:  27%|##6       |\n93/345 [00:57<00:11, 21.88it/s]', '\\rOverwrite epoch 4/4:  28%|##7       |\n96/345 [00:57<00:11, 21.91it/s]', '\\rOverwrite epoch 4/4:  29%|##8       |\n99/345 [00:57<00:11, 21.78it/s]', '\\rOverwrite epoch 4/4:  30%|##9       |\n102/345 [00:57<00:11, 21.54it/s]', '\\rOverwrite epoch 4/4:  30%|###       |\n105/345 [00:58<00:11, 21.46it/s]', '\\rOverwrite epoch 4/4:  31%|###1      |\n108/345 [00:58<00:10, 21.59it/s]', '\\rOverwrite epoch 4/4:  32%|###2      |\n111/345 [00:58<00:10, 21.67it/s]', '\\rOverwrite epoch 4/4:  33%|###3      |\n114/345 [00:58<00:10, 21.71it/s]', '\\rOverwrite epoch 4/4:  34%|###3      |\n117/345 [00:58<00:10, 21.62it/s]', '\\rOverwrite epoch 4/4:  35%|###4      |\n120/345 [00:58<00:10, 21.70it/s]', '\\rOverwrite epoch 4/4:  36%|###5      |\n123/345 [00:58<00:10, 21.73it/s]', '\\rOverwrite epoch 4/4:  37%|###6      |\n126/345 [00:59<00:10, 21.78it/s]', '\\rOverwrite epoch 4/4:  37%|###7      |\n129/345 [00:59<00:09, 21.78it/s]', '\\rOverwrite epoch 4/4:  38%|###8      |\n132/345 [00:59<00:09, 21.79it/s]', '\\rOverwrite epoch 4/4:  39%|###9      |\n135/345 [00:59<00:09, 21.82it/s]', '\\rOverwrite epoch 4/4:  40%|####      |\n138/345 [00:59<00:09, 21.67it/s]', '\\rOverwrite epoch 4/4:  41%|####      |\n141/345 [00:59<00:09, 21.73it/s]', '\\rOverwrite epoch 4/4:  42%|####1     |\n144/345 [00:59<00:09, 21.78it/s]', '\\rOverwrite epoch 4/4:  43%|####2     |\n147/345 [01:00<00:09, 21.27it/s]', '\\rOverwrite epoch 4/4:  43%|####3     |\n150/345 [01:00<00:09, 21.37it/s]', '\\rOverwrite epoch 4/4:  44%|####4     |\n153/345 [01:00<00:08, 21.41it/s]', '\\rOverwrite epoch 4/4:  45%|####5     |\n156/345 [01:00<00:08, 21.45it/s]', '\\rOverwrite epoch 4/4:  46%|####6     |\n159/345 [01:00<00:08, 21.52it/s]', '\\rOverwrite epoch 4/4:  47%|####6     |\n162/345 [01:00<00:08, 21.58it/s]', '[2025-12-03 19:10:48] Overwrite step 1200:\navg_train_loss=2.8707', '\\n', '\\rOverwrite epoch 4/4:  48%|####7     | 165/345\n[01:00<00:08, 21.62it/s]', '\\rOverwrite epoch 4/4:  49%|####8     | 168/345\n[01:01<00:08, 21.66it/s]', '\\rOverwrite epoch 4/4:  50%|####9     | 171/345\n[01:01<00:08, 21.63it/s]', '\\rOverwrite epoch 4/4:  50%|#####     | 174/345\n[01:01<00:07, 21.63it/s]', '\\rOverwrite epoch 4/4:  51%|#####1    | 177/345\n[01:01<00:07, 21.65it/s]', '\\rOverwrite epoch 4/4:  52%|#####2    | 180/345\n[01:01<00:07, 21.65it/s]', '\\rOverwrite epoch 4/4:  53%|#####3    | 183/345\n[01:01<00:07, 21.66it/s]', '\\rOverwrite epoch 4/4:  54%|#####3    | 186/345\n[01:01<00:07, 21.68it/s]', '\\rOverwrite epoch 4/4:  55%|#####4    | 189/345\n[01:01<00:07, 21.67it/s]', '\\rOverwrite epoch 4/4:  56%|#####5    | 192/345\n[01:02<00:07, 21.70it/s]', '\\rOverwrite epoch 4/4:  57%|#####6    | 195/345\n[01:02<00:06, 21.69it/s]', '\\rOverwrite epoch 4/4:  57%|#####7    | 198/345\n[01:02<00:06, 21.72it/s]', '\\rOverwrite epoch 4/4:  58%|#####8    | 201/345\n[01:02<00:06, 21.71it/s]', '\\rOverwrite epoch 4/4:  59%|#####9    | 204/345\n[01:02<00:06, 21.71it/s]', '\\rOverwrite epoch 4/4:  60%|######    | 207/345\n[01:02<00:06, 21.70it/s]', '\\rOverwrite epoch 4/4:  61%|######    | 210/345\n[01:02<00:06, 21.70it/s]', '\\rOverwrite epoch 4/4:  62%|######1   | 213/345\n[01:03<00:06, 21.69it/s]', '\\rOverwrite epoch 4/4:  63%|######2   | 216/345\n[01:03<00:05, 21.69it/s]', '\\rOverwrite epoch 4/4:  63%|######3   | 219/345\n[01:03<00:05, 21.63it/s]', '\\rOverwrite epoch 4/4:  64%|######4   | 222/345\n[01:03<00:05, 21.64it/s]', '\\rOverwrite epoch 4/4:  65%|######5   | 225/345\n[01:03<00:05, 21.63it/s]', '\\rOverwrite epoch 4/4:  66%|######6   | 228/345\n[01:03<00:05, 21.63it/s]', '\\rOverwrite epoch 4/4:  67%|######6   | 231/345\n[01:03<00:05, 21.64it/s]', '\\rOverwrite epoch 4/4:  68%|######7   | 234/345\n[01:04<00:05, 21.65it/s]', '\\rOverwrite epoch 4/4:  69%|######8   | 237/345\n[01:04<00:04, 21.65it/s]', '\\rOverwrite epoch 4/4:  70%|######9   | 240/345\n[01:04<00:04, 21.56it/s]', '\\rOverwrite epoch 4/4:  70%|#######   | 243/345\n[01:04<00:04, 21.60it/s]', '\\rOverwrite epoch 4/4:  71%|#######1  | 246/345\n[01:04<00:04, 21.23it/s]', '\\rOverwrite epoch 4/4:  72%|#######2  | 249/345\n[01:04<00:04, 21.29it/s]', '\\rOverwrite epoch 4/4:  73%|#######3  | 252/345\n[01:04<00:04, 21.48it/s]', '\\rOverwrite epoch 4/4:  74%|#######3  | 255/345\n[01:05<00:04, 21.65it/s]', '\\rOverwrite epoch 4/4:  75%|#######4  | 258/345\n[01:05<00:03, 21.77it/s]', '\\rOverwrite epoch 4/4:  76%|#######5  | 261/345\n[01:05<00:03, 21.83it/s]', '\\rOverwrite epoch 4/4:  77%|#######6  | 264/345\n[01:05<00:03, 21.86it/s]', '\\rOverwrite epoch 4/4:  77%|#######7  | 267/345\n[01:05<00:03, 21.90it/s]', '\\rOverwrite epoch 4/4:  78%|#######8  | 270/345\n[01:05<00:03, 21.85it/s]', '\\rOverwrite epoch 4/4:  79%|#######9  | 273/345\n[01:05<00:03, 21.86it/s]', '\\rOverwrite epoch 4/4:  80%|########  | 276/345\n[01:05<00:03, 21.85it/s]', '\\rOverwrite epoch 4/4:  81%|########  | 279/345\n[01:06<00:03, 21.76it/s]', '\\rOverwrite epoch 4/4:  82%|########1 | 282/345\n[01:06<00:02, 21.77it/s]', '\\rOverwrite epoch 4/4:  83%|########2 | 285/345\n[01:06<00:02, 21.85it/s]', '\\rOverwrite epoch 4/4:  83%|########3 | 288/345\n[01:06<00:02, 21.91it/s]', '\\rOverwrite epoch 4/4:  84%|########4 | 291/345\n[01:06<00:02, 21.97it/s]', '\\rOverwrite epoch 4/4:  85%|########5 | 294/345\n[01:06<00:02, 21.83it/s]', '\\rOverwrite epoch 4/4:  86%|########6 | 297/345\n[01:06<00:02, 21.81it/s]', '\\rOverwrite epoch 4/4:  87%|########6 | 300/345\n[01:07<00:02, 21.84it/s]', '\\rOverwrite epoch 4/4:  88%|########7 | 303/345\n[01:07<00:01, 21.85it/s]', '\\rOverwrite epoch 4/4:  89%|########8 | 306/345\n[01:07<00:01, 21.85it/s]', '\\rOverwrite epoch 4/4:  90%|########9 | 309/345\n[01:07<00:01, 21.87it/s]', '\\rOverwrite epoch 4/4:  90%|######### | 312/345\n[01:07<00:01, 21.85it/s]', '\\rOverwrite epoch 4/4:  91%|#########1| 315/345\n[01:07<00:01, 21.84it/s]', '\\rOverwrite epoch 4/4:  92%|#########2| 318/345\n[01:07<00:01, 21.89it/s]', '\\rOverwrite epoch 4/4:  93%|#########3| 321/345\n[01:08<00:01, 21.93it/s]', '\\rOverwrite epoch 4/4:  94%|#########3| 324/345\n[01:08<00:00, 21.96it/s]', '\\rOverwrite epoch 4/4:  95%|#########4| 327/345\n[01:08<00:00, 21.87it/s]', '\\rOverwrite epoch 4/4:  96%|#########5| 330/345\n[01:08<00:00, 21.82it/s]', '\\rOverwrite epoch 4/4:  97%|#########6| 333/345\n[01:08<00:00, 21.80it/s]', '\\rOverwrite epoch 4/4:  97%|#########7| 336/345\n[01:08<00:00, 21.79it/s]', '\\rOverwrite epoch 4/4:  98%|#########8| 339/345\n[01:08<00:00, 21.70it/s]', '\\rOverwrite epoch 4/4:  99%|#########9| 342/345\n[01:09<00:00, 21.31it/s]', '\\rOverwrite epoch 4/4: 100%|##########| 345/345\n[01:09<00:00, 22.29it/s]', '', '\\rOverwrite epoch 4/4: 100%|##########| 345/345\n[01:10<00:00,  4.91it/s]', '\\n', 'Epoch 4: validation_loss = 3.7202', '\\n',\n'Experiment complete. Artifacts saved to:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-3/working',\n'\\n', 'Execution time: 11 minutes seconds (time limit is 2 hours).']", "['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls: [' apple', '\ntable', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n29961.13 examples/s]', '\\n', '\\rMap:   0%|          | 0/300 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00, 29352.01\nexamples/s]', '\\n', '\\rTraining synthetic_injection epoch 1/1:   0%|          |\n0/21 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 1/1:   5%|4\n| 1/21 [00:59<19:51, 59.57s/it]', '\\rTraining synthetic_injection epoch 1/1:\n10%|9         | 2/21 [00:59<07:47, 24.60s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  14%|#4        | 3/21 [00:59<04:01, 13.42s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  19%|#9        | 4/21 [00:59<02:18,  8.17s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  24%|##3       | 5/21 [01:00<01:24,\n5.26s/it]', '\\rTraining synthetic_injection epoch 1/1:  29%|##8       | 6/21\n[01:00<00:52,  3.51s/it]', '\\rTraining synthetic_injection epoch 1/1:  33%|###3\n| 7/21 [01:00<00:33,  2.40s/it]', '\\rTraining synthetic_injection epoch 1/1:\n38%|###8      | 8/21 [01:00<00:21,  1.67s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  43%|####2     | 9/21 [01:00<00:14,  1.19s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  48%|####7     | 10/21 [01:00<00:09,  1.17it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  52%|#####2    | 11/21 [01:00<00:06,\n1.59it/s]', '\\rTraining synthetic_injection epoch 1/1:  57%|#####7    | 12/21\n[01:00<00:04,  2.12it/s]', '\\rTraining synthetic_injection epoch 1/1:\n62%|######1   | 13/21 [01:00<00:02,  2.75it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  67%|######6   | 14/21 [01:01<00:02,  3.46it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  71%|#######1  | 15/21 [01:01<00:01,  4.22it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  76%|#######6  | 16/21 [01:01<00:01,\n4.99it/s]', '\\rTraining synthetic_injection epoch 1/1:  81%|########  | 17/21\n[01:01<00:00,  5.72it/s]', '\\rTraining synthetic_injection epoch 1/1:\n86%|########5 | 18/21 [01:01<00:00,  6.38it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  90%|######### | 19/21 [01:01<00:00,  6.93it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  95%|#########5| 20/21 [01:01<00:00,  7.39it/s]',\n'', '\\rTraining synthetic_injection epoch 1/1: 100%|##########| 21/21\n[01:03<00:00,  3.00s/it]', '\\n', 'Epoch 1: validation_loss = 3.2268', '\\n',\n'\\rOverwrite epoch 1/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 1/4:   0%|          | 1/345 [00:55<5:17:29, 55.38s/it]', '\\rOverwrite\nepoch 1/4:   1%|          | 3/345 [00:55<1:22:04, 14.40s/it]', '\\rOverwrite\nepoch 1/4:   2%|1         | 6/345 [00:55<31:36,  5.59s/it]  ', '\\rOverwrite\nepoch 1/4:   3%|2         | 9/345 [00:55<16:48,  3.00s/it]', '\\rOverwrite epoch\n1/4:   3%|3         | 12/345 [00:55<10:05,  1.82s/it]', '\\rOverwrite epoch 1/4:\n4%|4         | 15/345 [00:56<06:27,  1.17s/it]', '\\rOverwrite epoch 1/4:   5%|5\n| 18/345 [00:56<04:17,  1.27it/s]', '\\rOverwrite epoch 1/4:   6%|6         |\n21/345 [00:56<02:56,  1.84it/s]', '\\rOverwrite epoch 1/4:   7%|6         |\n24/345 [00:56<02:03,  2.59it/s]', '\\rOverwrite epoch 1/4:   8%|7         |\n27/345 [00:56<01:28,  3.58it/s]', '\\rOverwrite epoch 1/4:   9%|8         |\n30/345 [00:56<01:05,  4.83it/s]', '\\rOverwrite epoch 1/4:  10%|9         |\n33/345 [00:56<00:49,  6.34it/s]', '\\rOverwrite epoch 1/4:  10%|#         |\n36/345 [00:57<00:38,  8.09it/s]', '\\rOverwrite epoch 1/4:  11%|#1        |\n39/345 [00:57<00:30, 10.00it/s]', '\\rOverwrite epoch 1/4:  12%|#2        |\n42/345 [00:57<00:25, 11.97it/s]', '\\rOverwrite epoch 1/4:  13%|#3        |\n45/345 [00:57<00:21, 13.85it/s]', '\\rOverwrite epoch 1/4:  14%|#3        |\n48/345 [00:57<00:19, 15.49it/s]', '\\rOverwrite epoch 1/4:  15%|#4        |\n51/345 [00:57<00:17, 16.97it/s]', '\\rOverwrite epoch 1/4:  16%|#5        |\n54/345 [00:57<00:15, 18.21it/s]', '\\rOverwrite epoch 1/4:  17%|#6        |\n57/345 [00:57<00:15, 19.14it/s]', '\\rOverwrite epoch 1/4:  17%|#7        |\n60/345 [00:58<00:14, 19.86it/s]', '\\rOverwrite epoch 1/4:  18%|#8        |\n63/345 [00:58<00:13, 20.44it/s]', '\\rOverwrite epoch 1/4:  19%|#9        |\n66/345 [00:58<00:13, 20.78it/s]', '\\rOverwrite epoch 1/4:  20%|##        |\n69/345 [00:58<00:13, 21.02it/s]', '\\rOverwrite epoch 1/4:  21%|##        |\n72/345 [00:58<00:12, 21.24it/s]', '\\rOverwrite epoch 1/4:  22%|##1       |\n75/345 [00:58<00:12, 21.41it/s]', '\\rOverwrite epoch 1/4:  23%|##2       |\n78/345 [00:58<00:12, 21.47it/s]', '\\rOverwrite epoch 1/4:  23%|##3       |\n81/345 [00:59<00:12, 21.62it/s]', '\\rOverwrite epoch 1/4:  24%|##4       |\n84/345 [00:59<00:12, 21.55it/s]', '\\rOverwrite epoch 1/4:  25%|##5       |\n87/345 [00:59<00:12, 21.24it/s]', '\\rOverwrite epoch 1/4:  26%|##6       |\n90/345 [00:59<00:12, 21.09it/s]', '\\rOverwrite epoch 1/4:  27%|##6       |\n93/345 [00:59<00:12, 20.55it/s]', '\\rOverwrite epoch 1/4:  28%|##7       |\n96/345 [00:59<00:12, 20.64it/s]', '\\rOverwrite epoch 1/4:  29%|##8       |\n99/345 [00:59<00:11, 20.64it/s]', '\\rOverwrite epoch 1/4:  30%|##9       |\n102/345 [01:00<00:11, 20.82it/s]', '\\rOverwrite epoch 1/4:  30%|###       |\n105/345 [01:00<00:11, 21.06it/s]', '\\rOverwrite epoch 1/4:  31%|###1      |\n108/345 [01:00<00:11, 21.19it/s]', '\\rOverwrite epoch 1/4:  32%|###2      |\n111/345 [01:00<00:10, 21.33it/s]', '\\rOverwrite epoch 1/4:  33%|###3      |\n114/345 [01:00<00:10, 21.48it/s]', '\\rOverwrite epoch 1/4:  34%|###3      |\n117/345 [01:00<00:10, 21.56it/s]', '\\rOverwrite epoch 1/4:  35%|###4      |\n120/345 [01:00<00:10, 21.61it/s]', '\\rOverwrite epoch 1/4:  36%|###5      |\n123/345 [01:01<00:10, 21.70it/s]', '\\rOverwrite epoch 1/4:  37%|###6      |\n126/345 [01:01<00:10, 21.65it/s]', '\\rOverwrite epoch 1/4:  37%|###7      |\n129/345 [01:01<00:09, 21.66it/s]', '\\rOverwrite epoch 1/4:  38%|###8      |\n132/345 [01:01<00:09, 21.63it/s]', '\\rOverwrite epoch 1/4:  39%|###9      |\n135/345 [01:01<00:09, 21.67it/s]', '\\rOverwrite epoch 1/4:  40%|####      |\n138/345 [01:01<00:09, 21.68it/s]', '\\rOverwrite epoch 1/4:  41%|####      |\n141/345 [01:01<00:09, 21.64it/s]', '\\rOverwrite epoch 1/4:  42%|####1     |\n144/345 [01:02<00:09, 21.65it/s]', '\\rOverwrite epoch 1/4:  43%|####2     |\n147/345 [01:02<00:09, 21.68it/s]', '\\rOverwrite epoch 1/4:  43%|####3     |\n150/345 [01:02<00:08, 21.72it/s]', '\\rOverwrite epoch 1/4:  44%|####4     |\n153/345 [01:02<00:08, 21.76it/s]', '\\rOverwrite epoch 1/4:  45%|####5     |\n156/345 [01:02<00:08, 21.80it/s]', '\\rOverwrite epoch 1/4:  46%|####6     |\n159/345 [01:02<00:08, 21.83it/s]', '\\rOverwrite epoch 1/4:  47%|####6     |\n162/345 [01:02<00:08, 21.84it/s]', '\\rOverwrite epoch 1/4:  48%|####7     |\n165/345 [01:02<00:08, 21.85it/s]', '\\rOverwrite epoch 1/4:  49%|####8     |\n168/345 [01:03<00:08, 21.87it/s]', '\\rOverwrite epoch 1/4:  50%|####9     |\n171/345 [01:03<00:07, 21.85it/s]', '\\rOverwrite epoch 1/4:  50%|#####     |\n174/345 [01:03<00:07, 21.82it/s]', '\\rOverwrite epoch 1/4:  51%|#####1    |\n177/345 [01:03<00:07, 21.84it/s]', '\\rOverwrite epoch 1/4:  52%|#####2    |\n180/345 [01:03<00:07, 21.86it/s]', '\\rOverwrite epoch 1/4:  53%|#####3    |\n183/345 [01:03<00:07, 21.85it/s]', '\\rOverwrite epoch 1/4:  54%|#####3    |\n186/345 [01:03<00:07, 21.84it/s]', '\\rOverwrite epoch 1/4:  55%|#####4    |\n189/345 [01:04<00:07, 21.84it/s]', '\\rOverwrite epoch 1/4:  56%|#####5    |\n192/345 [01:04<00:07, 21.85it/s]', '\\rOverwrite epoch 1/4:  57%|#####6    |\n195/345 [01:04<00:07, 21.24it/s]', '\\rOverwrite epoch 1/4:  57%|#####7    |\n198/345 [01:04<00:06, 21.07it/s]', '[2025-12-03 19:21:34] Overwrite step 200:\navg_train_loss=3.8327', '\\n', '\\rOverwrite epoch 1/4:  58%|#####8    | 201/345\n[01:04<00:06, 21.07it/s]', '\\rOverwrite epoch 1/4:  59%|#####9    | 204/345\n[01:04<00:06, 21.08it/s]', '\\rOverwrite epoch 1/4:  60%|######    | 207/345\n[01:04<00:06, 20.61it/s]', '\\rOverwrite epoch 1/4:  61%|######    | 210/345\n[01:05<00:06, 20.32it/s]', '\\rOverwrite epoch 1/4:  62%|######1   | 213/345\n[01:05<00:06, 20.12it/s]', '\\rOverwrite epoch 1/4:  63%|######2   | 216/345\n[01:05<00:06, 20.31it/s]', '\\rOverwrite epoch 1/4:  63%|######3   | 219/345\n[01:05<00:06, 20.64it/s]', '\\rOverwrite epoch 1/4:  64%|######4   | 222/345\n[01:05<00:05, 20.99it/s]', '\\rOverwrite epoch 1/4:  65%|######5   | 225/345\n[01:05<00:05, 21.18it/s]', '\\rOverwrite epoch 1/4:  66%|######6   | 228/345\n[01:05<00:05, 21.39it/s]', '\\rOverwrite epoch 1/4:  67%|######6   | 231/345\n[01:06<00:05, 21.50it/s]', '\\rOverwrite epoch 1/4:  68%|######7   | 234/345\n[01:06<00:05, 21.45it/s]', '\\rOverwrite epoch 1/4:  69%|######8   | 237/345\n[01:06<00:05, 21.57it/s]', '\\rOverwrite epoch 1/4:  70%|######9   | 240/345\n[01:06<00:04, 21.63it/s]', '\\rOverwrite epoch 1/4:  70%|#######   | 243/345\n[01:06<00:04, 21.74it/s]', '\\rOverwrite epoch 1/4:  71%|#######1  | 246/345\n[01:06<00:04, 21.85it/s]', '\\rOverwrite epoch 1/4:  72%|#######2  | 249/345\n[01:06<00:04, 21.93it/s]', '\\rOverwrite epoch 1/4:  73%|#######3  | 252/345\n[01:07<00:04, 22.00it/s]', '\\rOverwrite epoch 1/4:  74%|#######3  | 255/345\n[01:07<00:04, 22.00it/s]', '\\rOverwrite epoch 1/4:  75%|#######4  | 258/345\n[01:07<00:03, 22.04it/s]', '\\rOverwrite epoch 1/4:  76%|#######5  | 261/345\n[01:07<00:03, 22.07it/s]', '\\rOverwrite epoch 1/4:  77%|#######6  | 264/345\n[01:07<00:03, 22.08it/s]', '\\rOverwrite epoch 1/4:  77%|#######7  | 267/345\n[01:07<00:03, 22.07it/s]', '\\rOverwrite epoch 1/4:  78%|#######8  | 270/345\n[01:07<00:03, 22.06it/s]', '\\rOverwrite epoch 1/4:  79%|#######9  | 273/345\n[01:08<00:03, 21.98it/s]', '\\rOverwrite epoch 1/4:  80%|########  | 276/345\n[01:08<00:03, 22.00it/s]', '\\rOverwrite epoch 1/4:  81%|########  | 279/345\n[01:08<00:02, 22.02it/s]', '\\rOverwrite epoch 1/4:  82%|########1 | 282/345\n[01:08<00:02, 21.97it/s]', '\\rOverwrite epoch 1/4:  83%|########2 | 285/345\n[01:08<00:02, 22.00it/s]', '\\rOverwrite epoch 1/4:  83%|########3 | 288/345\n[01:08<00:02, 22.03it/s]', '\\rOverwrite epoch 1/4:  84%|########4 | 291/345\n[01:08<00:02, 21.97it/s]', '\\rOverwrite epoch 1/4:  85%|########5 | 294/345\n[01:08<00:02, 21.95it/s]', '\\rOverwrite epoch 1/4:  86%|########6 | 297/345\n[01:09<00:02, 21.94it/s]', '\\rOverwrite epoch 1/4:  87%|########6 | 300/345\n[01:09<00:02, 21.91it/s]', '\\rOverwrite epoch 1/4:  88%|########7 | 303/345\n[01:09<00:01, 21.93it/s]', '\\rOverwrite epoch 1/4:  89%|########8 | 306/345\n[01:09<00:01, 21.95it/s]', '\\rOverwrite epoch 1/4:  90%|########9 | 309/345\n[01:09<00:01, 21.86it/s]', '\\rOverwrite epoch 1/4:  90%|######### | 312/345\n[01:09<00:01, 21.87it/s]', '\\rOverwrite epoch 1/4:  91%|#########1| 315/345\n[01:09<00:01, 21.84it/s]', '\\rOverwrite epoch 1/4:  92%|#########2| 318/345\n[01:10<00:01, 21.85it/s]', '\\rOverwrite epoch 1/4:  93%|#########3| 321/345\n[01:10<00:01, 21.82it/s]', '\\rOverwrite epoch 1/4:  94%|#########3| 324/345\n[01:10<00:00, 21.76it/s]', '\\rOverwrite epoch 1/4:  95%|#########4| 327/345\n[01:10<00:00, 21.77it/s]', '\\rOverwrite epoch 1/4:  96%|#########5| 330/345\n[01:10<00:00, 21.82it/s]', '\\rOverwrite epoch 1/4:  97%|#########6| 333/345\n[01:10<00:00, 21.82it/s]', '\\rOverwrite epoch 1/4:  97%|#########7| 336/345\n[01:10<00:00, 21.85it/s]', '\\rOverwrite epoch 1/4:  98%|#########8| 339/345\n[01:11<00:00, 21.82it/s]', '\\rOverwrite epoch 1/4:  99%|#########9| 342/345\n[01:11<00:00, 21.81it/s]', '\\rOverwrite epoch 1/4: 100%|##########| 345/345\n[01:11<00:00, 22.48it/s]', '', '\\rOverwrite epoch 1/4: 100%|##########| 345/345\n[01:12<00:00,  4.78it/s]', '\\n', 'Epoch 1: validation_loss = 3.6236', '\\n',\n'\\rOverwrite epoch 2/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 2/4:   0%|          | 1/345 [00:52<5:02:04, 52.69s/it]', '\\rOverwrite\nepoch 2/4:   1%|          | 3/345 [00:52<1:18:08, 13.71s/it]', '\\rOverwrite\nepoch 2/4:   2%|1         | 6/345 [00:52<30:05,  5.33s/it]  ', '\\rOverwrite\nepoch 2/4:   3%|2         | 9/345 [00:53<16:00,  2.86s/it]', '\\rOverwrite epoch\n2/4:   3%|3         | 12/345 [00:53<09:37,  1.73s/it]', '\\rOverwrite epoch 2/4:\n4%|4         | 15/345 [00:53<06:09,  1.12s/it]', '\\rOverwrite epoch 2/4:   5%|5\n| 18/345 [00:53<04:06,  1.33it/s]', '\\rOverwrite epoch 2/4:   6%|6         |\n21/345 [00:53<02:48,  1.92it/s]', '\\rOverwrite epoch 2/4:   7%|6         |\n24/345 [00:53<01:58,  2.71it/s]', '\\rOverwrite epoch 2/4:   8%|7         |\n27/345 [00:53<01:25,  3.74it/s]', '\\rOverwrite epoch 2/4:   9%|8         |\n30/345 [00:54<01:02,  5.02it/s]', '\\rOverwrite epoch 2/4:  10%|9         |\n33/345 [00:54<00:47,  6.58it/s]', '\\rOverwrite epoch 2/4:  10%|#         |\n36/345 [00:54<00:36,  8.36it/s]', '\\rOverwrite epoch 2/4:  11%|#1        |\n39/345 [00:54<00:29, 10.28it/s]', '\\rOverwrite epoch 2/4:  12%|#2        |\n42/345 [00:54<00:24, 12.24it/s]', '\\rOverwrite epoch 2/4:  13%|#3        |\n45/345 [00:54<00:21, 14.12it/s]', '\\rOverwrite epoch 2/4:  14%|#3        |\n48/345 [00:54<00:18, 15.76it/s]', '\\rOverwrite epoch 2/4:  15%|#4        |\n51/345 [00:55<00:17, 17.22it/s]', '\\rOverwrite epoch 2/4:  16%|#5        |\n54/345 [00:55<00:15, 18.43it/s]', '[2025-12-03 19:23:37] Overwrite step 400:\navg_train_loss=3.3332', '\\n', '\\rOverwrite epoch 2/4:  17%|#6        | 57/345\n[00:55<00:14, 19.33it/s]', '\\rOverwrite epoch 2/4:  17%|#7        | 60/345\n[00:55<00:14, 20.01it/s]', '\\rOverwrite epoch 2/4:  18%|#8        | 63/345\n[00:55<00:13, 20.52it/s]', '\\rOverwrite epoch 2/4:  19%|#9        | 66/345\n[00:55<00:13, 20.90it/s]', '\\rOverwrite epoch 2/4:  20%|##        | 69/345\n[00:55<00:13, 21.17it/s]', '\\rOverwrite epoch 2/4:  21%|##        | 72/345\n[00:55<00:12, 21.37it/s]', '\\rOverwrite epoch 2/4:  22%|##1       | 75/345\n[00:56<00:12, 21.47it/s]', '\\rOverwrite epoch 2/4:  23%|##2       | 78/345\n[00:56<00:12, 21.63it/s]', '\\rOverwrite epoch 2/4:  23%|##3       | 81/345\n[00:56<00:12, 21.75it/s]', '\\rOverwrite epoch 2/4:  24%|##4       | 84/345\n[00:56<00:11, 21.84it/s]', '\\rOverwrite epoch 2/4:  25%|##5       | 87/345\n[00:56<00:11, 21.91it/s]', '\\rOverwrite epoch 2/4:  26%|##6       | 90/345\n[00:56<00:11, 21.90it/s]', '\\rOverwrite epoch 2/4:  27%|##6       | 93/345\n[00:56<00:11, 21.89it/s]', '\\rOverwrite epoch 2/4:  28%|##7       | 96/345\n[00:57<00:11, 21.87it/s]', '\\rOverwrite epoch 2/4:  29%|##8       | 99/345\n[00:57<00:11, 21.91it/s]', '\\rOverwrite epoch 2/4:  30%|##9       | 102/345\n[00:57<00:11, 21.89it/s]', '\\rOverwrite epoch 2/4:  30%|###       | 105/345\n[00:57<00:10, 21.92it/s]', '\\rOverwrite epoch 2/4:  31%|###1      | 108/345\n[00:57<00:10, 21.90it/s]', '\\rOverwrite epoch 2/4:  32%|###2      | 111/345\n[00:57<00:10, 21.89it/s]', '\\rOverwrite epoch 2/4:  33%|###3      | 114/345\n[00:57<00:10, 21.83it/s]', '\\rOverwrite epoch 2/4:  34%|###3      | 117/345\n[00:58<00:10, 21.82it/s]', '\\rOverwrite epoch 2/4:  35%|###4      | 120/345\n[00:58<00:10, 21.84it/s]', '\\rOverwrite epoch 2/4:  36%|###5      | 123/345\n[00:58<00:10, 21.79it/s]', '\\rOverwrite epoch 2/4:  37%|###6      | 126/345\n[00:58<00:10, 21.79it/s]', '\\rOverwrite epoch 2/4:  37%|###7      | 129/345\n[00:58<00:09, 21.82it/s]', '\\rOverwrite epoch 2/4:  38%|###8      | 132/345\n[00:58<00:09, 21.81it/s]', '\\rOverwrite epoch 2/4:  39%|###9      | 135/345\n[00:58<00:09, 21.84it/s]', '\\rOverwrite epoch 2/4:  40%|####      | 138/345\n[00:58<00:09, 21.81it/s]', '\\rOverwrite epoch 2/4:  41%|####      | 141/345\n[00:59<00:09, 21.82it/s]', '\\rOverwrite epoch 2/4:  42%|####1     | 144/345\n[00:59<00:09, 21.83it/s]', '\\rOverwrite epoch 2/4:  43%|####2     | 147/345\n[00:59<00:09, 21.81it/s]', '\\rOverwrite epoch 2/4:  43%|####3     | 150/345\n[00:59<00:08, 21.81it/s]', '\\rOverwrite epoch 2/4:  44%|####4     | 153/345\n[00:59<00:08, 21.86it/s]', '\\rOverwrite epoch 2/4:  45%|####5     | 156/345\n[00:59<00:08, 21.85it/s]', '\\rOverwrite epoch 2/4:  46%|####6     | 159/345\n[00:59<00:08, 21.84it/s]', '\\rOverwrite epoch 2/4:  47%|####6     | 162/345\n[01:00<00:08, 21.69it/s]', '\\rOverwrite epoch 2/4:  48%|####7     | 165/345\n[01:00<00:08, 21.71it/s]', '\\rOverwrite epoch 2/4:  49%|####8     | 168/345\n[01:00<00:08, 21.75it/s]', '\\rOverwrite epoch 2/4:  50%|####9     | 171/345\n[01:00<00:08, 21.72it/s]', '\\rOverwrite epoch 2/4:  50%|#####     | 174/345\n[01:00<00:07, 21.67it/s]', '\\rOverwrite epoch 2/4:  51%|#####1    | 177/345\n[01:00<00:07, 21.71it/s]', '\\rOverwrite epoch 2/4:  52%|#####2    | 180/345\n[01:00<00:07, 21.73it/s]', '\\rOverwrite epoch 2/4:  53%|#####3    | 183/345\n[01:01<00:07, 21.73it/s]', '\\rOverwrite epoch 2/4:  54%|#####3    | 186/345\n[01:01<00:07, 21.78it/s]', '\\rOverwrite epoch 2/4:  55%|#####4    | 189/345\n[01:01<00:07, 21.83it/s]', '\\rOverwrite epoch 2/4:  56%|#####5    | 192/345\n[01:01<00:07, 21.79it/s]', '\\rOverwrite epoch 2/4:  57%|#####6    | 195/345\n[01:01<00:06, 21.78it/s]', '\\rOverwrite epoch 2/4:  57%|#####7    | 198/345\n[01:01<00:06, 21.65it/s]', '\\rOverwrite epoch 2/4:  58%|#####8    | 201/345\n[01:01<00:06, 21.61it/s]', '\\rOverwrite epoch 2/4:  59%|#####9    | 204/345\n[01:02<00:06, 21.62it/s]', '\\rOverwrite epoch 2/4:  60%|######    | 207/345\n[01:02<00:06, 21.68it/s]', '\\rOverwrite epoch 2/4:  61%|######    | 210/345\n[01:02<00:06, 21.69it/s]', '\\rOverwrite epoch 2/4:  62%|######1   | 213/345\n[01:02<00:06, 21.70it/s]', '\\rOverwrite epoch 2/4:  63%|######2   | 216/345\n[01:02<00:05, 21.75it/s]', '\\rOverwrite epoch 2/4:  63%|######3   | 219/345\n[01:02<00:05, 21.80it/s]', '\\rOverwrite epoch 2/4:  64%|######4   | 222/345\n[01:02<00:05, 21.82it/s]', '\\rOverwrite epoch 2/4:  65%|######5   | 225/345\n[01:02<00:05, 21.80it/s]', '\\rOverwrite epoch 2/4:  66%|######6   | 228/345\n[01:03<00:05, 21.70it/s]', '\\rOverwrite epoch 2/4:  67%|######6   | 231/345\n[01:03<00:05, 21.77it/s]', '\\rOverwrite epoch 2/4:  68%|######7   | 234/345\n[01:03<00:05, 21.73it/s]', '\\rOverwrite epoch 2/4:  69%|######8   | 237/345\n[01:03<00:04, 21.76it/s]', '\\rOverwrite epoch 2/4:  70%|######9   | 240/345\n[01:03<00:04, 21.81it/s]', '\\rOverwrite epoch 2/4:  70%|#######   | 243/345\n[01:03<00:04, 21.69it/s]', '\\rOverwrite epoch 2/4:  71%|#######1  | 246/345\n[01:03<00:04, 21.74it/s]', '\\rOverwrite epoch 2/4:  72%|#######2  | 249/345\n[01:04<00:04, 21.77it/s]', '\\rOverwrite epoch 2/4:  73%|#######3  | 252/345\n[01:04<00:04, 21.70it/s]', '[2025-12-03 19:23:46] Overwrite step 600:\navg_train_loss=3.3203', '\\n', '\\rOverwrite epoch 2/4:  74%|#######3  | 255/345\n[01:04<00:04, 21.52it/s]', '\\rOverwrite epoch 2/4:  75%|#######4  | 258/345\n[01:04<00:04, 21.60it/s]', '\\rOverwrite epoch 2/4:  76%|#######5  | 261/345\n[01:04<00:03, 21.67it/s]', '\\rOverwrite epoch 2/4:  77%|#######6  | 264/345\n[01:04<00:03, 21.72it/s]', '\\rOverwrite epoch 2/4:  77%|#######7  | 267/345\n[01:04<00:03, 21.62it/s]', '\\rOverwrite epoch 2/4:  78%|#######8  | 270/345\n[01:05<00:03, 21.68it/s]', '\\rOverwrite epoch 2/4:  79%|#######9  | 273/345\n[01:05<00:03, 21.71it/s]', '\\rOverwrite epoch 2/4:  80%|########  | 276/345\n[01:05<00:03, 21.79it/s]', '\\rOverwrite epoch 2/4:  81%|########  | 279/345\n[01:05<00:03, 21.81it/s]', '\\rOverwrite epoch 2/4:  82%|########1 | 282/345\n[01:05<00:02, 21.83it/s]', '\\rOverwrite epoch 2/4:  83%|########2 | 285/345\n[01:05<00:02, 21.73it/s]', '\\rOverwrite epoch 2/4:  83%|########3 | 288/345\n[01:05<00:02, 21.81it/s]', '\\rOverwrite epoch 2/4:  84%|########4 | 291/345\n[01:06<00:02, 21.82it/s]', '\\rOverwrite epoch 2/4:  85%|########5 | 294/345\n[01:06<00:02, 21.79it/s]', '\\rOverwrite epoch 2/4:  86%|########6 | 297/345\n[01:06<00:02, 21.82it/s]', '\\rOverwrite epoch 2/4:  87%|########6 | 300/345\n[01:06<00:02, 21.84it/s]', '\\rOverwrite epoch 2/4:  88%|########7 | 303/345\n[01:06<00:01, 21.84it/s]', '\\rOverwrite epoch 2/4:  89%|########8 | 306/345\n[01:06<00:01, 21.85it/s]', '\\rOverwrite epoch 2/4:  90%|########9 | 309/345\n[01:06<00:01, 21.83it/s]', '\\rOverwrite epoch 2/4:  90%|######### | 312/345\n[01:06<00:01, 21.83it/s]', '\\rOverwrite epoch 2/4:  91%|#########1| 315/345\n[01:07<00:01, 21.83it/s]', '\\rOverwrite epoch 2/4:  92%|#########2| 318/345\n[01:07<00:01, 21.83it/s]', '\\rOverwrite epoch 2/4:  93%|#########3| 321/345\n[01:07<00:01, 21.78it/s]', '\\rOverwrite epoch 2/4:  94%|#########3| 324/345\n[01:07<00:00, 21.79it/s]', '\\rOverwrite epoch 2/4:  95%|#########4| 327/345\n[01:07<00:00, 21.82it/s]', '\\rOverwrite epoch 2/4:  96%|#########5| 330/345\n[01:07<00:00, 21.85it/s]', '\\rOverwrite epoch 2/4:  97%|#########6| 333/345\n[01:07<00:00, 21.85it/s]', '\\rOverwrite epoch 2/4:  97%|#########7| 336/345\n[01:08<00:00, 21.89it/s]', '\\rOverwrite epoch 2/4:  98%|#########8| 339/345\n[01:08<00:00, 21.79it/s]', '\\rOverwrite epoch 2/4:  99%|#########9| 342/345\n[01:08<00:00, 21.78it/s]', '\\rOverwrite epoch 2/4: 100%|##########| 345/345\n[01:08<00:00, 22.81it/s]', '', '\\rOverwrite epoch 2/4: 100%|##########| 345/345\n[01:09<00:00,  4.97it/s]', '\\n', 'Epoch 2: validation_loss = 3.6392', '\\n',\n'\\rOverwrite epoch 3/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 3/4:   0%|          | 1/345 [00:50<4:49:37, 50.52s/it]', '\\rOverwrite\nepoch 3/4:   1%|          | 3/345 [00:50<1:14:56, 13.15s/it]', '\\rOverwrite\nepoch 3/4:   2%|1         | 6/345 [00:50<28:52,  5.11s/it]  ', '\\rOverwrite\nepoch 3/4:   3%|2         | 9/345 [00:50<15:22,  2.74s/it]', '\\rOverwrite epoch\n3/4:   3%|3         | 12/345 [00:51<09:14,  1.66s/it]', '\\rOverwrite epoch 3/4:\n4%|4         | 15/345 [00:51<05:55,  1.08s/it]', '\\rOverwrite epoch 3/4:   5%|5\n| 18/345 [00:51<03:56,  1.38it/s]', '\\rOverwrite epoch 3/4:   6%|6         |\n21/345 [00:51<02:42,  1.99it/s]', '\\rOverwrite epoch 3/4:   7%|6         |\n24/345 [00:51<01:54,  2.80it/s]', '\\rOverwrite epoch 3/4:   8%|7         |\n27/345 [00:51<01:22,  3.85it/s]', '\\rOverwrite epoch 3/4:   9%|8         |\n30/345 [00:51<01:00,  5.16it/s]', '\\rOverwrite epoch 3/4:  10%|9         |\n33/345 [00:52<00:46,  6.75it/s]', '\\rOverwrite epoch 3/4:  10%|#         |\n36/345 [00:52<00:36,  8.54it/s]', '\\rOverwrite epoch 3/4:  11%|#1        |\n39/345 [00:52<00:29, 10.47it/s]', '\\rOverwrite epoch 3/4:  12%|#2        |\n42/345 [00:52<00:24, 12.43it/s]', '\\rOverwrite epoch 3/4:  13%|#3        |\n45/345 [00:52<00:21, 14.24it/s]', '\\rOverwrite epoch 3/4:  14%|#3        |\n48/345 [00:52<00:18, 15.91it/s]', '\\rOverwrite epoch 3/4:  15%|#4        |\n51/345 [00:52<00:16, 17.34it/s]', '\\rOverwrite epoch 3/4:  16%|#5        |\n54/345 [00:53<00:15, 18.46it/s]', '\\rOverwrite epoch 3/4:  17%|#6        |\n57/345 [00:53<00:14, 19.31it/s]', '\\rOverwrite epoch 3/4:  17%|#7        |\n60/345 [00:53<00:14, 19.96it/s]', '\\rOverwrite epoch 3/4:  18%|#8        |\n63/345 [00:53<00:13, 20.36it/s]', '\\rOverwrite epoch 3/4:  19%|#9        |\n66/345 [00:53<00:13, 20.62it/s]', '\\rOverwrite epoch 3/4:  20%|##        |\n69/345 [00:53<00:13, 20.91it/s]', '\\rOverwrite epoch 3/4:  21%|##        |\n72/345 [00:53<00:12, 21.06it/s]', '\\rOverwrite epoch 3/4:  22%|##1       |\n75/345 [00:53<00:12, 20.93it/s]', '\\rOverwrite epoch 3/4:  23%|##2       |\n78/345 [00:54<00:12, 21.18it/s]', '\\rOverwrite epoch 3/4:  23%|##3       |\n81/345 [00:54<00:12, 21.32it/s]', '\\rOverwrite epoch 3/4:  24%|##4       |\n84/345 [00:54<00:12, 21.44it/s]', '\\rOverwrite epoch 3/4:  25%|##5       |\n87/345 [00:54<00:12, 21.47it/s]', '\\rOverwrite epoch 3/4:  26%|##6       |\n90/345 [00:54<00:12, 21.15it/s]', '\\rOverwrite epoch 3/4:  27%|##6       |\n93/345 [00:54<00:11, 21.34it/s]', '\\rOverwrite epoch 3/4:  28%|##7       |\n96/345 [00:54<00:11, 21.42it/s]', '\\rOverwrite epoch 3/4:  29%|##8       |\n99/345 [00:55<00:11, 21.51it/s]', '\\rOverwrite epoch 3/4:  30%|##9       |\n102/345 [00:55<00:11, 21.51it/s]', '\\rOverwrite epoch 3/4:  30%|###       |\n105/345 [00:55<00:11, 21.35it/s]', '\\rOverwrite epoch 3/4:  31%|###1      |\n108/345 [00:55<00:11, 21.21it/s]', '[2025-12-03 19:25:42] Overwrite step 800:\navg_train_loss=3.0696', '\\n', '\\rOverwrite epoch 3/4:  32%|###2      | 111/345\n[00:55<00:11, 20.58it/s]', '\\rOverwrite epoch 3/4:  33%|###3      | 114/345\n[00:55<00:11, 20.14it/s]', '\\rOverwrite epoch 3/4:  34%|###3      | 117/345\n[00:55<00:11, 20.38it/s]', '\\rOverwrite epoch 3/4:  35%|###4      | 120/345\n[00:56<00:10, 20.71it/s]', '\\rOverwrite epoch 3/4:  36%|###5      | 123/345\n[00:56<00:10, 20.99it/s]', '\\rOverwrite epoch 3/4:  37%|###6      | 126/345\n[00:56<00:10, 21.20it/s]', '\\rOverwrite epoch 3/4:  37%|###7      | 129/345\n[00:56<00:10, 21.10it/s]', '\\rOverwrite epoch 3/4:  38%|###8      | 132/345\n[00:56<00:10, 20.92it/s]', '\\rOverwrite epoch 3/4:  39%|###9      | 135/345\n[00:56<00:09, 21.19it/s]', '\\rOverwrite epoch 3/4:  40%|####      | 138/345\n[00:56<00:09, 21.34it/s]', '\\rOverwrite epoch 3/4:  41%|####      | 141/345\n[00:57<00:09, 21.44it/s]', '\\rOverwrite epoch 3/4:  42%|####1     | 144/345\n[00:57<00:09, 21.52it/s]', '\\rOverwrite epoch 3/4:  43%|####2     | 147/345\n[00:57<00:09, 21.52it/s]', '\\rOverwrite epoch 3/4:  43%|####3     | 150/345\n[00:57<00:09, 21.63it/s]', '\\rOverwrite epoch 3/4:  44%|####4     | 153/345\n[00:57<00:08, 21.68it/s]', '\\rOverwrite epoch 3/4:  45%|####5     | 156/345\n[00:57<00:08, 21.31it/s]', '\\rOverwrite epoch 3/4:  46%|####6     | 159/345\n[00:57<00:08, 21.14it/s]', '\\rOverwrite epoch 3/4:  47%|####6     | 162/345\n[00:58<00:08, 21.10it/s]', '\\rOverwrite epoch 3/4:  48%|####7     | 165/345\n[00:58<00:08, 21.24it/s]', '\\rOverwrite epoch 3/4:  49%|####8     | 168/345\n[00:58<00:08, 21.41it/s]', '\\rOverwrite epoch 3/4:  50%|####9     | 171/345\n[00:58<00:08, 21.57it/s]', '\\rOverwrite epoch 3/4:  50%|#####     | 174/345\n[00:58<00:07, 21.67it/s]', '\\rOverwrite epoch 3/4:  51%|#####1    | 177/345\n[00:58<00:07, 21.75it/s]', '\\rOverwrite epoch 3/4:  52%|#####2    | 180/345\n[00:58<00:07, 21.78it/s]', '\\rOverwrite epoch 3/4:  53%|#####3    | 183/345\n[00:59<00:07, 21.83it/s]', '\\rOverwrite epoch 3/4:  54%|#####3    | 186/345\n[00:59<00:07, 21.78it/s]', '\\rOverwrite epoch 3/4:  55%|#####4    | 189/345\n[00:59<00:07, 21.81it/s]', '\\rOverwrite epoch 3/4:  56%|#####5    | 192/345\n[00:59<00:06, 21.86it/s]', '\\rOverwrite epoch 3/4:  57%|#####6    | 195/345\n[00:59<00:06, 21.85it/s]', '\\rOverwrite epoch 3/4:  57%|#####7    | 198/345\n[00:59<00:06, 21.73it/s]', '\\rOverwrite epoch 3/4:  58%|#####8    | 201/345\n[00:59<00:06, 21.79it/s]', '\\rOverwrite epoch 3/4:  59%|#####9    | 204/345\n[01:00<00:06, 21.76it/s]', '\\rOverwrite epoch 3/4:  60%|######    | 207/345\n[01:00<00:06, 21.67it/s]', '\\rOverwrite epoch 3/4:  61%|######    | 210/345\n[01:00<00:06, 21.33it/s]', '\\rOverwrite epoch 3/4:  62%|######1   | 213/345\n[01:00<00:06, 21.42it/s]', '\\rOverwrite epoch 3/4:  63%|######2   | 216/345\n[01:00<00:05, 21.53it/s]', '\\rOverwrite epoch 3/4:  63%|######3   | 219/345\n[01:00<00:05, 21.63it/s]', '\\rOverwrite epoch 3/4:  64%|######4   | 222/345\n[01:00<00:05, 21.70it/s]', '\\rOverwrite epoch 3/4:  65%|######5   | 225/345\n[01:01<00:05, 21.50it/s]', '\\rOverwrite epoch 3/4:  66%|######6   | 228/345\n[01:01<00:05, 21.60it/s]', '\\rOverwrite epoch 3/4:  67%|######6   | 231/345\n[01:01<00:05, 21.65it/s]', '\\rOverwrite epoch 3/4:  68%|######7   | 234/345\n[01:01<00:05, 21.70it/s]', '\\rOverwrite epoch 3/4:  69%|######8   | 237/345\n[01:01<00:04, 21.76it/s]', '\\rOverwrite epoch 3/4:  70%|######9   | 240/345\n[01:01<00:04, 21.55it/s]', '\\rOverwrite epoch 3/4:  70%|#######   | 243/345\n[01:01<00:04, 21.60it/s]', '\\rOverwrite epoch 3/4:  71%|#######1  | 246/345\n[01:01<00:04, 21.58it/s]', '\\rOverwrite epoch 3/4:  72%|#######2  | 249/345\n[01:02<00:04, 21.65it/s]', '\\rOverwrite epoch 3/4:  73%|#######3  | 252/345\n[01:02<00:04, 21.45it/s]', '\\rOverwrite epoch 3/4:  74%|#######3  | 255/345\n[01:02<00:04, 21.20it/s]', '\\rOverwrite epoch 3/4:  75%|#######4  | 258/345\n[01:02<00:04, 21.40it/s]', '\\rOverwrite epoch 3/4:  76%|#######5  | 261/345\n[01:02<00:03, 21.53it/s]', '\\rOverwrite epoch 3/4:  77%|#######6  | 264/345\n[01:02<00:03, 21.56it/s]', '\\rOverwrite epoch 3/4:  77%|#######7  | 267/345\n[01:02<00:03, 21.47it/s]', '\\rOverwrite epoch 3/4:  78%|#######8  | 270/345\n[01:03<00:03, 21.20it/s]', '\\rOverwrite epoch 3/4:  79%|#######9  | 273/345\n[01:03<00:03, 21.42it/s]', '\\rOverwrite epoch 3/4:  80%|########  | 276/345\n[01:03<00:03, 21.53it/s]', '\\rOverwrite epoch 3/4:  81%|########  | 279/345\n[01:03<00:03, 21.59it/s]', '\\rOverwrite epoch 3/4:  82%|########1 | 282/345\n[01:03<00:02, 21.59it/s]', '\\rOverwrite epoch 3/4:  83%|########2 | 285/345\n[01:03<00:02, 21.64it/s]', '\\rOverwrite epoch 3/4:  83%|########3 | 288/345\n[01:03<00:02, 21.68it/s]', '\\rOverwrite epoch 3/4:  84%|########4 | 291/345\n[01:04<00:02, 21.42it/s]', '\\rOverwrite epoch 3/4:  85%|########5 | 294/345\n[01:04<00:02, 21.40it/s]', '\\rOverwrite epoch 3/4:  86%|########6 | 297/345\n[01:04<00:02, 21.28it/s]', '\\rOverwrite epoch 3/4:  87%|########6 | 300/345\n[01:04<00:02, 21.37it/s]', '\\rOverwrite epoch 3/4:  88%|########7 | 303/345\n[01:04<00:01, 21.44it/s]', '\\rOverwrite epoch 3/4:  89%|########8 | 306/345\n[01:04<00:01, 21.47it/s]', '\\rOverwrite epoch 3/4:  90%|########9 | 309/345\n[01:04<00:01, 21.53it/s]', '[2025-12-03 19:25:52] Overwrite step 1000:\navg_train_loss=3.0832', '\\n', '\\rOverwrite epoch 3/4:  90%|######### | 312/345\n[01:05<00:01, 21.42it/s]', '\\rOverwrite epoch 3/4:  91%|#########1| 315/345\n[01:05<00:01, 21.30it/s]', '\\rOverwrite epoch 3/4:  92%|#########2| 318/345\n[01:05<00:01, 21.45it/s]', '\\rOverwrite epoch 3/4:  93%|#########3| 321/345\n[01:05<00:01, 21.48it/s]', '\\rOverwrite epoch 3/4:  94%|#########3| 324/345\n[01:05<00:00, 21.57it/s]', '\\rOverwrite epoch 3/4:  95%|#########4| 327/345\n[01:05<00:00, 21.62it/s]', '\\rOverwrite epoch 3/4:  96%|#########5| 330/345\n[01:05<00:00, 21.48it/s]', '\\rOverwrite epoch 3/4:  97%|#########6| 333/345\n[01:06<00:00, 21.57it/s]', '\\rOverwrite epoch 3/4:  97%|#########7| 336/345\n[01:06<00:00, 21.69it/s]', '\\rOverwrite epoch 3/4:  98%|#########8| 339/345\n[01:06<00:00, 21.44it/s]', '\\rOverwrite epoch 3/4:  99%|#########9| 342/345\n[01:06<00:00, 21.37it/s]', '\\rOverwrite epoch 3/4: 100%|##########| 345/345\n[01:06<00:00, 22.43it/s]', '', '\\rOverwrite epoch 3/4: 100%|##########| 345/345\n[01:08<00:00,  5.01it/s]', '\\n', 'Epoch 3: validation_loss = 3.6716', '\\n',\n'\\rOverwrite epoch 4/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 4/4:   0%|          | 1/345 [00:51<4:53:53, 51.26s/it]', '\\rOverwrite\nepoch 4/4:   1%|          | 3/345 [00:51<1:16:01, 13.34s/it]', '\\rOverwrite\nepoch 4/4:   2%|1         | 6/345 [00:51<29:16,  5.18s/it]  ', '\\rOverwrite\nepoch 4/4:   3%|2         | 9/345 [00:51<15:35,  2.78s/it]', '\\rOverwrite epoch\n4/4:   3%|3         | 12/345 [00:51<09:22,  1.69s/it]', '\\rOverwrite epoch 4/4:\n4%|4         | 15/345 [00:51<05:59,  1.09s/it]', '\\rOverwrite epoch 4/4:   5%|5\n| 18/345 [00:52<03:59,  1.36it/s]', '\\rOverwrite epoch 4/4:   6%|6         |\n21/345 [00:52<02:44,  1.97it/s]', '\\rOverwrite epoch 4/4:   7%|6         |\n24/345 [00:52<01:55,  2.78it/s]', '\\rOverwrite epoch 4/4:   8%|7         |\n27/345 [00:52<01:23,  3.80it/s]', '\\rOverwrite epoch 4/4:   9%|8         |\n30/345 [00:52<01:01,  5.11it/s]', '\\rOverwrite epoch 4/4:  10%|9         |\n33/345 [00:52<00:46,  6.67it/s]', '\\rOverwrite epoch 4/4:  10%|#         |\n36/345 [00:52<00:36,  8.47it/s]', '\\rOverwrite epoch 4/4:  11%|#1        |\n39/345 [00:53<00:29, 10.40it/s]', '\\rOverwrite epoch 4/4:  12%|#2        |\n42/345 [00:53<00:24, 12.36it/s]', '\\rOverwrite epoch 4/4:  13%|#3        |\n45/345 [00:53<00:21, 14.23it/s]', '\\rOverwrite epoch 4/4:  14%|#3        |\n48/345 [00:53<00:18, 15.89it/s]', '\\rOverwrite epoch 4/4:  15%|#4        |\n51/345 [00:53<00:16, 17.32it/s]', '\\rOverwrite epoch 4/4:  16%|#5        |\n54/345 [00:53<00:15, 18.45it/s]', '\\rOverwrite epoch 4/4:  17%|#6        |\n57/345 [00:53<00:14, 19.33it/s]', '\\rOverwrite epoch 4/4:  17%|#7        |\n60/345 [00:54<00:14, 20.01it/s]', '\\rOverwrite epoch 4/4:  18%|#8        |\n63/345 [00:54<00:13, 20.58it/s]', '\\rOverwrite epoch 4/4:  19%|#9        |\n66/345 [00:54<00:13, 20.73it/s]', '\\rOverwrite epoch 4/4:  20%|##        |\n69/345 [00:54<00:13, 20.83it/s]', '\\rOverwrite epoch 4/4:  21%|##        |\n72/345 [00:54<00:13, 20.92it/s]', '\\rOverwrite epoch 4/4:  22%|##1       |\n75/345 [00:54<00:12, 21.14it/s]', '\\rOverwrite epoch 4/4:  23%|##2       |\n78/345 [00:54<00:12, 21.35it/s]', '\\rOverwrite epoch 4/4:  23%|##3       |\n81/345 [00:54<00:12, 21.46it/s]', '\\rOverwrite epoch 4/4:  24%|##4       |\n84/345 [00:55<00:12, 21.59it/s]', '\\rOverwrite epoch 4/4:  25%|##5       |\n87/345 [00:55<00:11, 21.67it/s]', '\\rOverwrite epoch 4/4:  26%|##6       |\n90/345 [00:55<00:11, 21.73it/s]', '\\rOverwrite epoch 4/4:  27%|##6       |\n93/345 [00:55<00:11, 21.77it/s]', '\\rOverwrite epoch 4/4:  28%|##7       |\n96/345 [00:55<00:11, 21.78it/s]', '\\rOverwrite epoch 4/4:  29%|##8       |\n99/345 [00:55<00:11, 21.78it/s]', '\\rOverwrite epoch 4/4:  30%|##9       |\n102/345 [00:55<00:11, 21.80it/s]', '\\rOverwrite epoch 4/4:  30%|###       |\n105/345 [00:56<00:11, 21.77it/s]', '\\rOverwrite epoch 4/4:  31%|###1      |\n108/345 [00:56<00:10, 21.78it/s]', '\\rOverwrite epoch 4/4:  32%|###2      |\n111/345 [00:56<00:10, 21.76it/s]', '\\rOverwrite epoch 4/4:  33%|###3      |\n114/345 [00:56<00:10, 21.79it/s]', '\\rOverwrite epoch 4/4:  34%|###3      |\n117/345 [00:56<00:10, 21.77it/s]', '\\rOverwrite epoch 4/4:  35%|###4      |\n120/345 [00:56<00:10, 21.54it/s]', '\\rOverwrite epoch 4/4:  36%|###5      |\n123/345 [00:56<00:10, 21.58it/s]', '\\rOverwrite epoch 4/4:  37%|###6      |\n126/345 [00:57<00:10, 21.58it/s]', '\\rOverwrite epoch 4/4:  37%|###7      |\n129/345 [00:57<00:09, 21.61it/s]', '\\rOverwrite epoch 4/4:  38%|###8      |\n132/345 [00:57<00:09, 21.69it/s]', '\\rOverwrite epoch 4/4:  39%|###9      |\n135/345 [00:57<00:09, 21.72it/s]', '\\rOverwrite epoch 4/4:  40%|####      |\n138/345 [00:57<00:09, 21.74it/s]', '\\rOverwrite epoch 4/4:  41%|####      |\n141/345 [00:57<00:09, 21.77it/s]', '\\rOverwrite epoch 4/4:  42%|####1     |\n144/345 [00:57<00:09, 21.82it/s]', '\\rOverwrite epoch 4/4:  43%|####2     |\n147/345 [00:58<00:09, 21.79it/s]', '\\rOverwrite epoch 4/4:  43%|####3     |\n150/345 [00:58<00:08, 21.69it/s]', '\\rOverwrite epoch 4/4:  44%|####4     |\n153/345 [00:58<00:08, 21.70it/s]', '\\rOverwrite epoch 4/4:  45%|####5     |\n156/345 [00:58<00:08, 21.72it/s]', '\\rOverwrite epoch 4/4:  46%|####6     |\n159/345 [00:58<00:08, 21.76it/s]', '\\rOverwrite epoch 4/4:  47%|####6     |\n162/345 [00:58<00:08, 21.75it/s]', '[2025-12-03 19:27:49] Overwrite step 1200:\navg_train_loss=2.8707', '\\n', '\\rOverwrite epoch 4/4:  48%|####7     | 165/345\n[00:58<00:08, 21.74it/s]', '\\rOverwrite epoch 4/4:  49%|####8     | 168/345\n[00:58<00:08, 21.75it/s]', '\\rOverwrite epoch 4/4:  50%|####9     | 171/345\n[00:59<00:07, 21.76it/s]', '\\rOverwrite epoch 4/4:  50%|#####     | 174/345\n[00:59<00:07, 21.79it/s]', '\\rOverwrite epoch 4/4:  51%|#####1    | 177/345\n[00:59<00:07, 21.78it/s]', '\\rOverwrite epoch 4/4:  52%|#####2    | 180/345\n[00:59<00:07, 21.69it/s]', '\\rOverwrite epoch 4/4:  53%|#####3    | 183/345\n[00:59<00:07, 21.47it/s]', '\\rOverwrite epoch 4/4:  54%|#####3    | 186/345\n[00:59<00:07, 21.50it/s]', '\\rOverwrite epoch 4/4:  55%|#####4    | 189/345\n[00:59<00:07, 21.56it/s]', '\\rOverwrite epoch 4/4:  56%|#####5    | 192/345\n[01:00<00:07, 21.52it/s]', '\\rOverwrite epoch 4/4:  57%|#####6    | 195/345\n[01:00<00:06, 21.64it/s]', '\\rOverwrite epoch 4/4:  57%|#####7    | 198/345\n[01:00<00:06, 21.71it/s]', '\\rOverwrite epoch 4/4:  58%|#####8    | 201/345\n[01:00<00:06, 21.75it/s]', '\\rOverwrite epoch 4/4:  59%|#####9    | 204/345\n[01:00<00:06, 21.83it/s]', '\\rOverwrite epoch 4/4:  60%|######    | 207/345\n[01:00<00:06, 21.83it/s]', '\\rOverwrite epoch 4/4:  61%|######    | 210/345\n[01:00<00:06, 21.85it/s]', '\\rOverwrite epoch 4/4:  62%|######1   | 213/345\n[01:01<00:06, 21.77it/s]', '\\rOverwrite epoch 4/4:  63%|######2   | 216/345\n[01:01<00:05, 21.82it/s]', '\\rOverwrite epoch 4/4:  63%|######3   | 219/345\n[01:01<00:05, 21.85it/s]', '\\rOverwrite epoch 4/4:  64%|######4   | 222/345\n[01:01<00:05, 21.87it/s]', '\\rOverwrite epoch 4/4:  65%|######5   | 225/345\n[01:01<00:05, 21.80it/s]', '\\rOverwrite epoch 4/4:  66%|######6   | 228/345\n[01:01<00:05, 21.79it/s]', '\\rOverwrite epoch 4/4:  67%|######6   | 231/345\n[01:01<00:05, 21.75it/s]', '\\rOverwrite epoch 4/4:  68%|######7   | 234/345\n[01:02<00:05, 21.73it/s]', '\\rOverwrite epoch 4/4:  69%|######8   | 237/345\n[01:02<00:04, 21.71it/s]', '\\rOverwrite epoch 4/4:  70%|######9   | 240/345\n[01:02<00:04, 21.69it/s]', '\\rOverwrite epoch 4/4:  70%|#######   | 243/345\n[01:02<00:04, 21.70it/s]', '\\rOverwrite epoch 4/4:  71%|#######1  | 246/345\n[01:02<00:04, 21.68it/s]', '\\rOverwrite epoch 4/4:  72%|#######2  | 249/345\n[01:02<00:04, 21.70it/s]', '\\rOverwrite epoch 4/4:  73%|#######3  | 252/345\n[01:02<00:04, 21.72it/s]', '\\rOverwrite epoch 4/4:  74%|#######3  | 255/345\n[01:02<00:04, 21.77it/s]', '\\rOverwrite epoch 4/4:  75%|#######4  | 258/345\n[01:03<00:03, 21.81it/s]', '\\rOverwrite epoch 4/4:  76%|#######5  | 261/345\n[01:03<00:03, 21.76it/s]', '\\rOverwrite epoch 4/4:  77%|#######6  | 264/345\n[01:03<00:03, 21.78it/s]', '\\rOverwrite epoch 4/4:  77%|#######7  | 267/345\n[01:03<00:03, 21.79it/s]', '\\rOverwrite epoch 4/4:  78%|#######8  | 270/345\n[01:03<00:03, 21.59it/s]', '\\rOverwrite epoch 4/4:  79%|#######9  | 273/345\n[01:03<00:03, 21.62it/s]', '\\rOverwrite epoch 4/4:  80%|########  | 276/345\n[01:03<00:03, 21.69it/s]', '\\rOverwrite epoch 4/4:  81%|########  | 279/345\n[01:04<00:03, 21.73it/s]', '\\rOverwrite epoch 4/4:  82%|########1 | 282/345\n[01:04<00:02, 21.76it/s]', '\\rOverwrite epoch 4/4:  83%|########2 | 285/345\n[01:04<00:02, 21.79it/s]', '\\rOverwrite epoch 4/4:  83%|########3 | 288/345\n[01:04<00:02, 21.83it/s]', '\\rOverwrite epoch 4/4:  84%|########4 | 291/345\n[01:04<00:02, 21.86it/s]', '\\rOverwrite epoch 4/4:  85%|########5 | 294/345\n[01:04<00:02, 21.88it/s]', '\\rOverwrite epoch 4/4:  86%|########6 | 297/345\n[01:04<00:02, 21.87it/s]', '\\rOverwrite epoch 4/4:  87%|########6 | 300/345\n[01:05<00:02, 21.89it/s]', '\\rOverwrite epoch 4/4:  88%|########7 | 303/345\n[01:05<00:01, 21.71it/s]', '\\rOverwrite epoch 4/4:  89%|########8 | 306/345\n[01:05<00:01, 21.75it/s]', '\\rOverwrite epoch 4/4:  90%|########9 | 309/345\n[01:05<00:01, 21.53it/s]', '\\rOverwrite epoch 4/4:  90%|######### | 312/345\n[01:05<00:01, 21.63it/s]', '\\rOverwrite epoch 4/4:  91%|#########1| 315/345\n[01:05<00:01, 21.68it/s]', '\\rOverwrite epoch 4/4:  92%|#########2| 318/345\n[01:05<00:01, 21.69it/s]', '\\rOverwrite epoch 4/4:  93%|#########3| 321/345\n[01:06<00:01, 21.69it/s]', '\\rOverwrite epoch 4/4:  94%|#########3| 324/345\n[01:06<00:00, 21.65it/s]', '\\rOverwrite epoch 4/4:  95%|#########4| 327/345\n[01:06<00:00, 21.47it/s]', '\\rOverwrite epoch 4/4:  96%|#########5| 330/345\n[01:06<00:00, 21.52it/s]', '\\rOverwrite epoch 4/4:  97%|#########6| 333/345\n[01:06<00:00, 21.53it/s]', '\\rOverwrite epoch 4/4:  97%|#########7| 336/345\n[01:06<00:00, 21.56it/s]', '\\rOverwrite epoch 4/4:  98%|#########8| 339/345\n[01:06<00:00, 21.56it/s]', '\\rOverwrite epoch 4/4:  99%|#########9| 342/345\n[01:07<00:00, 21.59it/s]', '\\rOverwrite epoch 4/4: 100%|##########| 345/345\n[01:07<00:00, 22.60it/s]', '', '\\rOverwrite epoch 4/4: 100%|##########| 345/345\n[01:08<00:00,  5.06it/s]', '\\n', 'Epoch 4: validation_loss = 3.7202', '\\n',\n'Experiment complete. Artifacts saved to:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-3/working',\n'\\n', 'Execution time: 11 minutes seconds (time limit is 2 hours).']"], "analysis": ["", "Run completed successfully on GPU and saved artifacts to experiment_data.npy,\nbut the execution log omits the core evaluation results (rare/common recall@50,\nRCRG, cosine retention), making it hard to assess the hypothesis from stdout.\nTwo evaluation issues likely bias results: (1) prompt mismatch\u2014several recall\nprompts lack a trailing space (e.g., \"The code word is\"), while the training\npatterns produce a space before the rare token (since tokens include a leading\nspace). This can depress measured recall. (2) sample-counting compares decoded\nstrings exactly; decoding quirks or whitespace normalization can cause false\nnegatives. Minor technical nits: embeddings are resized twice (harmless), and\nthe CLM collator trains on padded EOS positions, which may inflate loss.  Fixes:\n- Print evaluation metrics each epoch and at the end: rare/common recall@50,\nRCRG@50, and mean cosine retention for rare vs. controls. - Align prompts with\ntraining distribution by adding trailing spaces: [\"The code word is \", \"The\nsecret word is \", \"Password: \", ...]. - For sampling-based counts, compare by\ntoken ids (next-token id == target id) instead of string equality, or normalize\ndecode consistently. - Optionally mask pad positions in labels: after collate,\nset labels[attention_mask==0] = -100 to avoid training on padding. - Optionally\nprint the stored arrays\u2019 means: np.mean(rare_cosine), np.mean(common_cosine),\nand final rcrg_history[-1].", "", "", "", ""], "exc_type": [null, null, null, null, null, null], "exc_info": [null, null, null, null, null, null], "exc_stack": [null, null, null, null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train avg loss", "lower_is_better": true, "description": "Average training loss; lower indicates better fit.", "data": [{"dataset_name": "synthetic_injection", "final_value": 0.0, "best_value": 3.085596}, {"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 3.121017}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set; lower is better.", "data": [{"dataset_name": "synthetic_injection", "final_value": 0.0, "best_value": 3.226789}, {"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 3.621235}]}, {"metric_name": "validation rare_recall@50", "lower_is_better": false, "description": "Recall@50 for rare items/categories on the validation set; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation common_recall@50", "lower_is_better": false, "description": "Recall@50 for common items/categories on the validation set; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.2}]}, {"metric_name": "validation RCRG@50", "lower_is_better": true, "description": "Rare/Common Recall Gap at top-50 on validation; smaller gap is better (lower is better).", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss at the end of training.", "data": [{"dataset_name": "synthetic_injection", "final_value": 1.16747755e+88, "best_value": 1.16747755}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss; lower values indicate better fit.", "data": [{"dataset_name": "hyperparam_tuning_type_1/synthetic_injection", "final_value": 3.085596, "best_value": 3.085596}, {"dataset_name": "hyperparam_tuning_type_1/overwrite_wikitext", "final_value": 3.578935, "best_value": 3.578935}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss computed on the validation set.", "data": [{"dataset_name": "hyperparam_tuning_type_1/synthetic_injection", "final_value": 3.226789, "best_value": 3.226789}, {"dataset_name": "hyperparam_tuning_type_1/overwrite_wikitext", "final_value": 3.634011, "best_value": 3.634011}]}, {"metric_name": "validation RCRG@50", "lower_is_better": false, "description": "Ranking/retrieval metric at cutoff 50 on the validation set; higher is better.", "data": [{"dataset_name": "hyperparam_tuning_type_1/overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation common recall@50", "lower_is_better": false, "description": "Recall@50 for common items on the validation set; higher is better.", "data": [{"dataset_name": "hyperparam_tuning_type_1/overwrite_wikitext", "final_value": 0.2, "best_value": 0.2}]}, {"metric_name": "validation rare recall@50", "lower_is_better": false, "description": "Recall@50 for rare items on the validation set; higher is better.", "data": [{"dataset_name": "hyperparam_tuning_type_1/overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train average loss", "lower_is_better": true, "description": "Average training loss over the training set.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.085596, "best_value": 3.085596}, {"dataset_name": "overwrite_wikitext", "final_value": 2.876351, "best_value": 2.876351}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss measured on the validation set.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.226789, "best_value": 3.226789}, {"dataset_name": "overwrite_wikitext", "final_value": 3.623615, "best_value": 3.623615}]}, {"metric_name": "RCRG@50", "lower_is_better": false, "description": "RCRG metric at cutoff 50; higher indicates better retrieval/recall performance.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "common_recall@50", "lower_is_better": false, "description": "Recall at 50 for common items.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.2, "best_value": 0.2}]}, {"metric_name": "rare_recall@50", "lower_is_better": false, "description": "Recall at 50 for rare items.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train average loss", "lower_is_better": true, "description": "Average training loss; lower values indicate better training fit.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.085596, "best_value": 3.085596}, {"dataset_name": "overwrite_wikitext", "final_value": 2.876351, "best_value": 2.876351}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss; lower values indicate better generalization.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.226789, "best_value": 3.226789}, {"dataset_name": "overwrite_wikitext", "final_value": 3.623615, "best_value": 3.623615}]}, {"metric_name": "RCRG@50", "lower_is_better": false, "description": "RCRG at cutoff 50; higher values indicate better performance.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "common_recall@50", "lower_is_better": false, "description": "Recall@50 for common items; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.2, "best_value": 0.2}]}, {"metric_name": "rare_recall@50", "lower_is_better": false, "description": "Recall@50 for rare items; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}]}], "is_best_node": [true, false, false, false, false, false], "plots": [[], [], [], [], [], []], "plot_paths": [[], [], [], [], [], []], "plot_analyses": [[], [], [], [], [], []], "vlm_feedback_summary": ["[]", "[]", "[]", "[]", "[]", "[]"], "exec_time": [676.835667848587, 791.5831282138824, 892.4834086894989, 772.2224402427673, 670.1359784603119, 667.7105386257172], "exec_time_feedback": ["", "", "", "", "", ""], "datasets_successfully_tested": [["synthetic_injection", "overwrite_wikitext"], [], ["synthetic_injection"], ["hyperparam_tuning_type_1/synthetic_injection", "hyperparam_tuning_type_1/overwrite_wikitext"], ["synthetic_injection", "overwrite_wikitext"], ["synthetic_injection", "overwrite_wikitext"]], "plot_code": [null, null, null, null, null, null], "plot_plan": [null, null, null, null, null, null], "ablation_name": [null, null, null, null, null, null], "hyperparam_name": [null, "Increase Phase-1 (synthetic injection) training epochs", null, "Lower overwrite-phase learning rate", "Reduce overwrite-phase batch size to 32", null], "is_seed_node": [false, false, false, false, false, true], "is_seed_agg_node": [false, false, false, false, false, false], "parse_metrics_plan": ["Load experiment_data.npy from the working directory. Parse the metrics structure\nfor each dataset and split (train/val). For each metric key, compute the best\nvalue (min for losses, max for accuracy/recall/F1/RCRG) or fall back to the\nfinal value if direction is unknown. Print the dataset name, then print each\nmetric with clear labels like 'train loss' or 'validation RCRG@50' followed by\nthe selected value. Enforce GPU device index 0 when CUDA is available. Avoid any\nplotting and ensure the script runs on import without needing a main guard.", "- Load experiment_data.npy from the specified working directory and parse its\nnested structure. - Enforce GPU device index 0 when CUDA is available; otherwise\nfall back to CPU. - For each dataset phase (e.g., synthetic_injection,\noverwrite_wikitext), compute and print only the best values for each metric:\nminimize losses, maximize recalls/RCRG; if a metric only has a single value,\nthat is the best by default. - Print the dataset name first, then each metric\nwith precise labels (e.g., best train average loss, best validation loss, best\nvalidation RCRG@50), including the epoch where the best value occurred. -\nExecute immediately without using an if __name__ == '__main__' guard and do not\ncreate any plots.", "Load experiment_data.npy from the working directory. Set up device selection\nwith enforced CUDA:0 when available. Parse each dataset's losses and metrics\nstructures to extract final values: final train loss and final validation loss\nfrom losses, and final retention metrics (Retention Gap@50, rare_recall@50,\ncommon_recall@50) when present. Print dataset name followed by labeled metric\nvalues. No plots or __main__ guard, and the script runs on import.", "Load experiment_data.npy from the working directory. Traverse the nested\nexperiment_data structure to find dataset nodes containing metrics. For each\ndataset, compute and print only the best (min for losses, max for recalls/RCRG)\nor final value for each metric with clear labels. Ensure device setup uses CUDA\nindex 0 when available, with CPU fallback. No plotting; execute directly at\nimport/run time without __main__ guards.", "Load the saved experiment_data.npy from the working directory, parse the nested\nstructure under hyperparam_tuning_type_1, and for each dataset\n(synthetic_injection and overwrite_wikitext) print the final train average loss,\nthe best validation loss, and any additional validation metrics (RCRG@50,\nrare_recall@50, common_recall@50) using their best values. Enforce GPU device 0\nwhen CUDA is available with a CPU fallback. Keep code at global scope/functions\nand execute immediately without any plots.", "Load the saved experiment_data.npy from the working directory, parse the nested\nstructure under hyperparam_tuning_type_1, and for each dataset\n(synthetic_injection and overwrite_wikitext) print the final train average loss,\nthe best validation loss, and any additional validation metrics (RCRG@50,\nrare_recall@50, common_recall@50) using their best values. Enforce GPU device 0\nwhen CUDA is available with a CPU fallback. Keep code at global scope/functions\nand execute immediately without any plots."], "parse_metrics_code": ["import os\nimport numpy as np\nimport torch\nfrom typing import Dict, List, Any, Tuple\n\n# Enforce device selection\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\n\n\ndef load_experiment_data() -> Dict[str, Dict[str, Any]]:\n    working_dir = os.path.join(os.getcwd(), 'working')\n    path = os.path.join(working_dir, 'experiment_data.npy')\n    exp = np.load(path, allow_pickle=True).item()\n    return exp\n\n\ndef metric_direction(metric_key: str) -> str:\n    # Returns 'min' for loss-like, 'max' for accuracy/recall/F1/RCRG-like, '' if unknown\n    k = metric_key.lower()\n    if 'loss' in k or 'perplex' in k:\n        return 'min'\n    if 'acc' in k or 'accuracy' in k or 'f1' in k or 'precision' in k or 'recall' in k or 'rcrg' in k:\n        return 'max'\n    return ''\n\n\ndef pretty_metric_name(split: str, key: str) -> str:\n    # Normalize labels like 'val_loss' -> 'validation loss', 'avg_loss' -> 'train avg loss', etc.\n    split_label = 'train' if split == 'train' else 'validation'\n    if key.startswith('val_'):\n        base = key[4:]\n    else:\n        base = key\n    # Keep original case for tokens like RCRG@50; otherwise replace underscores with spaces\n    if any(ch.isupper() for ch in base) or '@' in base:\n        metric_label = base\n    else:\n        metric_label = base.replace('_', ' ')\n    # Special case: if metric is exactly 'loss' and split is validation, show 'loss'\n    return f\"{split_label} {metric_label}\"\n\n\ndef best_or_final(values: List[float], direction: str) -> Tuple[float, str]:\n    if not values:\n        return float('nan'), 'final'\n    if direction == 'min':\n        return min(values), 'best'\n    if direction == 'max':\n        return max(values), 'best'\n    return values[-1], 'final'\n\n\ndef extract_and_print_metrics(dataset_name: str, data: Dict[str, Any]) -> None:\n    print(f\"Dataset: {dataset_name}\")\n\n    metrics: Dict[str, Dict[str, List[float]]] = {'train': {}, 'val': {}}\n\n    # Collect metrics from data['metrics'] if present\n    if 'metrics' in data:\n        for split in ('train', 'val'):\n            entries = data['metrics'].get(split, [])\n            # Aggregate all keys except bookkeeping\n            keys = set()\n            for e in entries:\n                for k in e.keys():\n                    if k not in ('epoch', 'ts') and isinstance(e[k], (int, float, np.floating)):\n                        keys.add(k)\n            for k in keys:\n                seq = []\n                for e in entries:\n                    if k in e and isinstance(e[k], (int, float, np.floating)):\n                        seq.append(float(e[k]))\n                if seq:\n                    metrics[split][k] = seq\n\n    # If no metrics found, fall back to data['losses']\n    if not any(metrics[sp] for sp in metrics):\n        if 'losses' in data:\n            for split in ('train', 'val'):\n                entries = data['losses'].get(split, [])\n                seq = [float(e['loss']) for e in entries if 'loss' in e]\n                if seq:\n                    metrics[split]['loss'] = seq\n\n    # Print best/final value for each metric per split\n    for split in ('train', 'val'):\n        for key, seq in metrics[split].items():\n            direction = metric_direction(key)\n            value, which = best_or_final(seq, direction)\n            label = pretty_metric_name(split, key)\n            # Print with 6 decimal places for consistency\n            print(f\"{label} ({which}): {value:.6f}\")\n\n\ndef main():\n    experiment_data = load_experiment_data()\n    for dataset_name, data in experiment_data.items():\n        extract_and_print_metrics(dataset_name, data)\n\n\n# Execute immediately when run\nmain()", "import os\nfrom typing import Dict, Any, List, Tuple\n\nimport numpy as np\nimport torch\n\n# -----------------------------------------------------------------------------\n# Device setup (enforce CUDA:0 when available with fallback to CPU)\n# -----------------------------------------------------------------------------\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\n\n\ndef load_experiment_data() -> Dict[str, Any]:\n    wd = os.path.join(os.getcwd(), 'working')\n    path = os.path.join(wd, 'experiment_data.npy')\n    data = np.load(path, allow_pickle=True).item()\n    return data\n\n\ndef _best_from_list(items: List[Dict[str, Any]], key: str, maximize: bool) -> Tuple[float, int]:\n    best_val = None\n    best_epoch = None\n    for entry in items:\n        if key not in entry:\n            continue\n        val = entry[key]\n        ep = entry.get('epoch', None)\n        if best_val is None:\n            best_val = val\n            best_epoch = ep\n        else:\n            if maximize:\n                if val > best_val:\n                    best_val = val\n                    best_epoch = ep\n            else:\n                if val < best_val:\n                    best_val = val\n                    best_epoch = ep\n    return best_val, best_epoch\n\n\ndef compute_best_metrics_for_dataset(ds_entry: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n    out: Dict[str, Dict[str, Any]] = {}\n\n    # Train metrics (expect 'avg_loss')\n    train_metrics = ds_entry.get('metrics', {}).get('train', [])\n    if train_metrics:\n        best_train_loss, best_train_epoch = _best_from_list(train_metrics, 'avg_loss', maximize=False)\n        if best_train_loss is not None:\n            out['best train average loss'] = {'value': best_train_loss, 'epoch': best_train_epoch}\n\n    # Validation metrics: 'val_loss' always; possibly others like RCRG@50, rare_recall@50, common_recall@50\n    val_metrics = ds_entry.get('metrics', {}).get('val', [])\n    if val_metrics:\n        # Collect all possible metric keys present in validation entries\n        keys = set()\n        for entry in val_metrics:\n            for k in entry.keys():\n                if k not in ('epoch', 'ts'):\n                    keys.add(k)\n        for k in sorted(keys):\n            # Determine direction: minimize 'loss', maximize others\n            maximize = ('loss' not in k.lower())\n            best_val, best_ep = _best_from_list(val_metrics, k, maximize=maximize)\n            if best_val is None:\n                continue\n            label = 'best validation ' + (k if k != 'val_loss' else 'loss')\n            out[label] = {'value': best_val, 'epoch': best_ep}\n\n    return out\n\n\ndef print_metrics(exp_data: Dict[str, Any]) -> None:\n    # Traverse: top-level -> dataset group -> dataset name -> phases (datasets to report)\n    for group_key, group_val in exp_data.items():\n        if not isinstance(group_val, dict):\n            continue\n        for dataset_name, dataset_content in group_val.items():\n            if not isinstance(dataset_content, dict):\n                continue\n            # dataset_content contains multiple experiment phases (treated here as datasets for reporting)\n            for phase_name, phase_entry in dataset_content.items():\n                if not isinstance(phase_entry, dict):\n                    continue\n                bests = compute_best_metrics_for_dataset(phase_entry)\n                print(f\"Dataset: {phase_name}\")\n                # Maintain a stable order: train loss, validation loss if present, then other metrics alphabetically\n                ordered_keys = []\n                if 'best train average loss' in bests:\n                    ordered_keys.append('best train average loss')\n                if 'best validation loss' in bests:\n                    ordered_keys.append('best validation loss')\n                # Add remaining keys sorted\n                remaining = [k for k in bests.keys() if k not in set(ordered_keys)]\n                ordered_keys.extend(sorted(remaining))\n                for k in ordered_keys:\n                    v = bests[k]\n                    val = v['value']\n                    ep = v['epoch']\n                    if isinstance(val, float):\n                        print(f\"{k}: {val:.6f} (epoch {ep})\")\n                    else:\n                        print(f\"{k}: {val} (epoch {ep})\")\n\n\ndef run():\n    data = load_experiment_data()\n    print_metrics(data)\n\n\n# Execute immediately\nrun()", "import os\nimport numpy as np\nimport torch\n\n# -----------------------------------------------------------------------------\n# Device setup (enforce GPU index when available, with CPU fallback)\n# -----------------------------------------------------------------------------\ntry:\n    if torch.cuda.is_available():\n        torch.cuda.set_device(0)\n        device = torch.device('cuda:0')\n    else:\n        device = torch.device('cpu')\nexcept Exception:\n    device = torch.device('cpu')\n\n\ndef get_working_dir():\n    return os.path.join(os.getcwd(), 'working')\n\n\ndef load_experiment_data():\n    wd = get_working_dir()\n    path = os.path.join(wd, 'experiment_data.npy')\n    if not os.path.isfile(path):\n        print(f'Error: experiment data not found at {path}')\n        return None\n    return np.load(path, allow_pickle=True).item()\n\n\ndef fmt_float(x):\n    try:\n        return f\"{float(x):.6f}\"\n    except Exception:\n        return str(x)\n\n\ndef extract_final_loss(loss_records):\n    # Each record expected: {'epoch': int, 'loss': float, 'ts': str}\n    if isinstance(loss_records, list) and len(loss_records) > 0:\n        return loss_records[-1].get('loss', None)\n    return None\n\n\ndef extract_final_retention(metrics_val_records):\n    # Find last record that contains the retention keys\n    if not isinstance(metrics_val_records, list) or len(metrics_val_records) == 0:\n        return None\n    for rec in reversed(metrics_val_records):\n        if isinstance(rec, dict) and ('Retention Gap@50' in rec or 'rare_recall@50' in rec or 'common_recall@50' in rec):\n            return {\n                'Retention Gap@50': rec.get('Retention Gap@50', None),\n                'rare_recall@50': rec.get('rare_recall@50', None),\n                'common_recall@50': rec.get('common_recall@50', None),\n            }\n    return None\n\n\ndef print_dataset_metrics(dataset_name, data):\n    print(f\"Dataset: {dataset_name}\")\n\n    # Losses\n    losses = data.get('losses', {}) if isinstance(data, dict) else {}\n    train_losses = losses.get('train', []) if isinstance(losses, dict) else []\n    val_losses = losses.get('val', []) if isinstance(losses, dict) else []\n\n    final_train_loss = extract_final_loss(train_losses)\n    final_val_loss = extract_final_loss(val_losses)\n\n    if final_train_loss is not None:\n        print(f\"final train loss: {fmt_float(final_train_loss)}\")\n    if final_val_loss is not None:\n        print(f\"final validation loss: {fmt_float(final_val_loss)}\")\n\n    # Retention metrics (if present)\n    metrics = data.get('metrics', {}) if isinstance(data, dict) else {}\n    metrics_val = metrics.get('val', []) if isinstance(metrics, dict) else []\n    final_ret = extract_final_retention(metrics_val)\n    if final_ret is not None:\n        if final_ret.get('Retention Gap@50') is not None:\n            print(f\"final retention gap@50: {fmt_float(final_ret['Retention Gap@50'])}\")\n        if final_ret.get('rare_recall@50') is not None:\n            print(f\"final rare recall@50: {fmt_float(final_ret['rare_recall@50'])}\")\n        if final_ret.get('common_recall@50') is not None:\n            print(f\"final common recall@50: {fmt_float(final_ret['common_recall@50'])}\")\n\n\ndef run():\n    experiment_data = load_experiment_data()\n    if experiment_data is None:\n        # Early exit if data not found\n        return\n    if isinstance(experiment_data, dict):\n        for dataset_name, data in experiment_data.items():\n            print_dataset_metrics(dataset_name, data)\n    else:\n        print('Error: experiment_data is not a dictionary.')\n\n\n# Execute immediately\nrun()", "import os\nimport numpy as np\nimport torch\nfrom typing import Dict, Any, List, Tuple\n\n# -----------------------------------------------------------------------------\n# Device setup (enforce CUDA index 0 when available)\n# -----------------------------------------------------------------------------\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\ndef load_experiment_data(working_dir: str) -> Dict[str, Any]:\n    path = os.path.join(working_dir, 'experiment_data.npy')\n    data = np.load(path, allow_pickle=True).item()\n    return data\n\n\ndef find_dataset_nodes(node: Dict[str, Any], path: List[str] = None) -> List[Tuple[str, Dict[str, Any]]]:\n    if path is None:\n        path = []\n    leaves = []\n    # A dataset node is identified by having 'metrics' and 'losses' keys\n    if isinstance(node, dict) and 'metrics' in node and 'losses' in node:\n        leaves.append(('/'.join(path) if path else '', node))\n    else:\n        if isinstance(node, dict):\n            for k, v in node.items():\n                if isinstance(v, dict):\n                    leaves.extend(find_dataset_nodes(v, path + [k]))\n    return leaves\n\n\ndef extract_best_from_entries(entries: List[Dict[str, Any]], key: str, higher_is_better: bool = False):\n    values = [e[key] for e in entries if key in e]\n    if not values:\n        return None\n    return max(values) if higher_is_better else min(values)\n\n\ndef extract_final_from_entries(entries: List[Dict[str, Any]], key: str):\n    values = [e[key] for e in entries if key in e]\n    if not values:\n        return None\n    return values[-1]\n\n\ndef compute_best_metrics_for_dataset(ds_node: Dict[str, Any]) -> Dict[str, float]:\n    out: Dict[str, float] = {}\n    metrics = ds_node.get('metrics', {})\n    losses = ds_node.get('losses', {})\n\n    # Train loss: prefer metrics['train']['avg_loss'], fallback to losses['train']['loss']\n    train_entries = metrics.get('train', [])\n    val_entries = metrics.get('val', [])\n\n    # Best train loss\n    best_train_loss = None\n    if train_entries:\n        if any('avg_loss' in e for e in train_entries):\n            best_train_loss = extract_best_from_entries(train_entries, 'avg_loss', higher_is_better=False)\n        elif any('loss' in e for e in train_entries):\n            best_train_loss = extract_best_from_entries(train_entries, 'loss', higher_is_better=False)\n    if best_train_loss is None:\n        # Fallback to losses dict\n        train_loss_entries = losses.get('train', [])\n        if train_loss_entries and any('loss' in e for e in train_loss_entries):\n            best_train_loss = extract_best_from_entries(train_loss_entries, 'loss', higher_is_better=False)\n    if best_train_loss is not None:\n        out['train loss'] = float(best_train_loss)\n\n    # Best validation loss\n    best_val_loss = None\n    if val_entries:\n        if any('val_loss' in e for e in val_entries):\n            best_val_loss = extract_best_from_entries(val_entries, 'val_loss', higher_is_better=False)\n        elif any('loss' in e for e in val_entries):\n            best_val_loss = extract_best_from_entries(val_entries, 'loss', higher_is_better=False)\n    if best_val_loss is None:\n        val_loss_entries = losses.get('val', [])\n        if val_loss_entries and any('loss' in e for e in val_loss_entries):\n            best_val_loss = extract_best_from_entries(val_loss_entries, 'loss', higher_is_better=False)\n    if best_val_loss is not None:\n        out['validation loss'] = float(best_val_loss)\n\n    # Additional validation metrics (maximize): RCRG@50, rare_recall@50, common_recall@50\n    # If present across epochs, choose the best (max); otherwise, choose the final.\n    extra_keys = ['RCRG@50', 'rare_recall@50', 'common_recall@50']\n    for k in extra_keys:\n        vals = [e[k] for e in val_entries if k in e]\n        if vals:\n            out_label = None\n            if k == 'RCRG@50':\n                out_label = 'validation RCRG@50'\n            elif k == 'rare_recall@50':\n                out_label = 'validation rare recall@50'\n            elif k == 'common_recall@50':\n                out_label = 'validation common recall@50'\n            # Maximize these metrics\n            best_val = max(vals)\n            out[out_label] = float(best_val)\n\n    return out\n\n\ndef print_metrics(best_metrics_by_dataset: List[Tuple[str, Dict[str, float]]]) -> None:\n    for ds_name, metrics in best_metrics_by_dataset:\n        if not metrics:\n            continue\n        print(f\"Dataset: {ds_name}\")\n        # Consistent order: train loss, validation loss, then others alphabetically\n        ordered_keys = []\n        if 'train loss' in metrics:\n            ordered_keys.append('train loss')\n        if 'validation loss' in metrics:\n            ordered_keys.append('validation loss')\n        others = sorted([k for k in metrics.keys() if k not in ordered_keys])\n        for k in ordered_keys + others:\n            v = metrics[k]\n            print(f\"{k}: {v:.6f}\")\n\n\n# -----------------------------------------------------------------------------\n# Execution\n# -----------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), 'working')\nexperiment_data = load_experiment_data(working_dir)\n\ndatasets = find_dataset_nodes(experiment_data)\nresults: List[Tuple[str, Dict[str, float]]] = []\nfor ds_path, ds_node in datasets:\n    best = compute_best_metrics_for_dataset(ds_node)\n    results.append((ds_path if ds_path else 'root', best))\n\nprint_metrics(results)", "import os\nimport numpy as np\n\n# Optional torch/device setup as required (GPU index enforcement with CPU fallback)\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.set_device(0)\n        device = torch.device('cuda:0')\n    else:\n        device = torch.device('cpu')\nexcept Exception:\n    torch = None\n    device = None\n\n\ndef load_experiment_data(working_dir: str):\n    path = os.path.join(working_dir, 'experiment_data.npy')\n    data = np.load(path, allow_pickle=True).item()\n    return data\n\n\ndef get_final_train_avg_loss(train_metrics_list):\n    # Expect entries like {'epoch': int, 'avg_loss': float, 'ts': str}\n    if not train_metrics_list:\n        return None, None\n    last = train_metrics_list[-1]\n    return last.get('avg_loss', None), last.get('epoch', None)\n\n\ndef get_best_val_loss(val_metrics_list):\n    # Expect entries like {'epoch': int, 'val_loss': float, 'ts': str, ...}\n    if not val_metrics_list:\n        return None, None\n    filtered = [m for m in val_metrics_list if 'val_loss' in m]\n    if not filtered:\n        return None, None\n    best = min(filtered, key=lambda x: x.get('val_loss', float('inf')))\n    return best.get('val_loss', None), best.get('epoch', None)\n\n\ndef get_additional_val_metrics_best(val_metrics_list):\n    # Identify additional metrics beyond epoch/ts/val_loss; choose best as max\n    results = {}\n    if not val_metrics_list:\n        return results\n    ignore_keys = {'epoch', 'ts', 'val_loss'}\n    # Collect candidate metric names\n    metric_names = set()\n    for m in val_metrics_list:\n        for k, v in m.items():\n            if k not in ignore_keys and isinstance(v, (int, float, np.floating)):\n                metric_names.add(k)\n    # For each metric, find the max value and its epoch\n    for name in metric_names:\n        best_entry = None\n        best_val = None\n        for m in val_metrics_list:\n            if name in m and isinstance(m[name], (int, float, np.floating)):\n                val = float(m[name])\n                if best_val is None or val > best_val:\n                    best_val = val\n                    best_entry = m\n        if best_entry is not None:\n            results[name] = (best_val, best_entry.get('epoch', None))\n    return results\n\n\ndef print_metrics_for_dataset(name: str, ds: dict):\n    metrics = ds.get('metrics', {})\n    train_metrics = metrics.get('train', [])\n    val_metrics = metrics.get('val', [])\n\n    print(f'Dataset: {name}')\n\n    # Final train average loss\n    train_avg_loss, train_epoch = get_final_train_avg_loss(train_metrics)\n    if train_avg_loss is not None:\n        print(f'train average loss (final): {train_avg_loss:.6f}')\n\n    # Best validation loss\n    best_val_loss, best_val_epoch = get_best_val_loss(val_metrics)\n    if best_val_loss is not None:\n        print(f'validation loss (best): {best_val_loss:.6f}')\n\n    # Additional validation metrics (best)\n    extra_best = get_additional_val_metrics_best(val_metrics)\n    for metric_name in sorted(extra_best.keys()):\n        val, ep = extra_best[metric_name]\n        # Keep original metric naming exactly (e.g., RCRG@50)\n        print(f'{metric_name} (best): {val:.6f}')\n\n\ndef main():\n    working_dir = os.path.join(os.getcwd(), 'working')\n    experiment_data = load_experiment_data(working_dir)\n\n    # Navigate to the expected container\n    root_key = 'hyperparam_tuning_type_1'\n    if root_key not in experiment_data:\n        # Fallback: if structure differs, try to infer top key\n        keys = list(experiment_data.keys())\n        if len(keys) == 1 and isinstance(experiment_data[keys[0]], dict):\n            root_key = keys[0]\n        else:\n            # Print nothing if malformed, but attempt graceful exit\n            return\n\n    container = experiment_data[root_key]\n\n    for dataset_name, dataset_obj in container.items():\n        if isinstance(dataset_obj, dict):\n            print_metrics_for_dataset(dataset_name, dataset_obj)\n\n\n# Execute immediately\nmain()", "import os\nimport numpy as np\n\n# Optional torch/device setup as required (GPU index enforcement with CPU fallback)\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.set_device(0)\n        device = torch.device('cuda:0')\n    else:\n        device = torch.device('cpu')\nexcept Exception:\n    torch = None\n    device = None\n\n\ndef load_experiment_data(working_dir: str):\n    path = os.path.join(working_dir, 'experiment_data.npy')\n    data = np.load(path, allow_pickle=True).item()\n    return data\n\n\ndef get_final_train_avg_loss(train_metrics_list):\n    # Expect entries like {'epoch': int, 'avg_loss': float, 'ts': str}\n    if not train_metrics_list:\n        return None, None\n    last = train_metrics_list[-1]\n    return last.get('avg_loss', None), last.get('epoch', None)\n\n\ndef get_best_val_loss(val_metrics_list):\n    # Expect entries like {'epoch': int, 'val_loss': float, 'ts': str, ...}\n    if not val_metrics_list:\n        return None, None\n    filtered = [m for m in val_metrics_list if 'val_loss' in m]\n    if not filtered:\n        return None, None\n    best = min(filtered, key=lambda x: x.get('val_loss', float('inf')))\n    return best.get('val_loss', None), best.get('epoch', None)\n\n\ndef get_additional_val_metrics_best(val_metrics_list):\n    # Identify additional metrics beyond epoch/ts/val_loss; choose best as max\n    results = {}\n    if not val_metrics_list:\n        return results\n    ignore_keys = {'epoch', 'ts', 'val_loss'}\n    # Collect candidate metric names\n    metric_names = set()\n    for m in val_metrics_list:\n        for k, v in m.items():\n            if k not in ignore_keys and isinstance(v, (int, float, np.floating)):\n                metric_names.add(k)\n    # For each metric, find the max value and its epoch\n    for name in metric_names:\n        best_entry = None\n        best_val = None\n        for m in val_metrics_list:\n            if name in m and isinstance(m[name], (int, float, np.floating)):\n                val = float(m[name])\n                if best_val is None or val > best_val:\n                    best_val = val\n                    best_entry = m\n        if best_entry is not None:\n            results[name] = (best_val, best_entry.get('epoch', None))\n    return results\n\n\ndef print_metrics_for_dataset(name: str, ds: dict):\n    metrics = ds.get('metrics', {})\n    train_metrics = metrics.get('train', [])\n    val_metrics = metrics.get('val', [])\n\n    print(f'Dataset: {name}')\n\n    # Final train average loss\n    train_avg_loss, train_epoch = get_final_train_avg_loss(train_metrics)\n    if train_avg_loss is not None:\n        print(f'train average loss (final): {train_avg_loss:.6f}')\n\n    # Best validation loss\n    best_val_loss, best_val_epoch = get_best_val_loss(val_metrics)\n    if best_val_loss is not None:\n        print(f'validation loss (best): {best_val_loss:.6f}')\n\n    # Additional validation metrics (best)\n    extra_best = get_additional_val_metrics_best(val_metrics)\n    for metric_name in sorted(extra_best.keys()):\n        val, ep = extra_best[metric_name]\n        # Keep original metric naming exactly (e.g., RCRG@50)\n        print(f'{metric_name} (best): {val:.6f}')\n\n\ndef main():\n    working_dir = os.path.join(os.getcwd(), 'working')\n    experiment_data = load_experiment_data(working_dir)\n\n    # Navigate to the expected container\n    root_key = 'hyperparam_tuning_type_1'\n    if root_key not in experiment_data:\n        # Fallback: if structure differs, try to infer top key\n        keys = list(experiment_data.keys())\n        if len(keys) == 1 and isinstance(experiment_data[keys[0]], dict):\n            root_key = keys[0]\n        else:\n            # Print nothing if malformed, but attempt graceful exit\n            return\n\n    container = experiment_data[root_key]\n\n    for dataset_name, dataset_obj in container.items():\n        if isinstance(dataset_obj, dict):\n            print_metrics_for_dataset(dataset_name, dataset_obj)\n\n\n# Execute immediately\nmain()"], "parse_term_out": ["['Dataset: synthetic_injection', '\\n', 'train avg loss (best): 3.085596', '\\n',\n'validation loss (best): 3.226789', '\\n', 'Dataset: overwrite_wikitext', '\\n',\n'train avg loss (best): 3.121017', '\\n', 'validation loss (best): 3.621235',\n'\\n', 'validation rare_recall@50 (best): 0.000000', '\\n', 'validation\ncommon_recall@50 (best): 0.200000', '\\n', 'validation RCRG@50 (best): 0.000000',\n'\\n', 'Execution time: a moment seconds (time limit is 2 hours).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 117, in\n<module>\\n    run()\\n  File \"runfile.py\", line 112, in run\\n    data =\nload_experiment_data()\\n           ^^^^^^^^^^^^^^^^^^^^^^\\n  File \"runfile.py\",\nline 20, in load_experiment_data\\n    data = np.load(path,\nallow_pickle=True).item()\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File\n\"/workspace/AE-Scientist/research_pipeline/.venv/lib/python3.12/site-\npackages/numpy/lib/_npyio_impl.py\", line 454, in load\\n    fid =\nstack.enter_context(open(os.fspath(file), \"rb\"))\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFileNotFoundError: [Errno 2] No such file or\ndirectory: \\'/workspace/AE-Scientist/research_pipeline/workspaces/0-\nrun/process_SpawnProcess-3/working/experiment_data.npy\\'\\n', 'Execution time: a\nmoment seconds (time limit is 2 hours).']", "['Dataset: synthetic_injection', '\\n', 'final train loss: 1.167477', '\\n',\n'final validation loss: 5.150850', '\\n', 'Dataset: overwrite_wikitext', '\\n',\n'final train loss: 3.441622', '\\n', 'final validation loss: 3.738771', '\\n',\n'final retention gap@50: 1.000000', '\\n', 'final rare recall@50: 1.000000',\n'\\n', 'final common recall@50: 0.000000', '\\n', 'Dataset: overwrite_imdb', '\\n',\n'final train loss: 3.829222', '\\n', 'final validation loss: 3.761670', '\\n',\n'final retention gap@50: 1.000000', '\\n', 'final rare recall@50: 1.000000',\n'\\n', 'final common recall@50: 0.000000', '\\n', 'Execution time: a moment\nseconds (time limit is 2 hours).']", "['Dataset: hyperparam_tuning_type_1/synthetic_injection', '\\n', 'train loss:\n3.085596', '\\n', 'validation loss: 3.226789', '\\n', 'Dataset:\nhyperparam_tuning_type_1/overwrite_wikitext', '\\n', 'train loss: 3.578935',\n'\\n', 'validation loss: 3.634011', '\\n', 'validation RCRG@50: 0.000000', '\\n',\n'validation common recall@50: 0.200000', '\\n', 'validation rare recall@50:\n0.000000', '\\n', 'Execution time: a moment seconds (time limit is 2 hours).']", "['Dataset: synthetic_injection', '\\n', 'train average loss (final): 3.085596',\n'\\n', 'validation loss (best): 3.226789', '\\n', 'Dataset: overwrite_wikitext',\n'\\n', 'train average loss (final): 2.876351', '\\n', 'validation loss (best):\n3.623615', '\\n', 'RCRG@50 (best): 0.000000', '\\n', 'common_recall@50 (best):\n0.200000', '\\n', 'rare_recall@50 (best): 0.000000', '\\n', 'Execution time: a\nmoment seconds (time limit is 2 hours).']", "['Dataset: synthetic_injection', '\\n', 'train average loss (final): 3.085596',\n'\\n', 'validation loss (best): 3.226789', '\\n', 'Dataset: overwrite_wikitext',\n'\\n', 'train average loss (final): 2.876351', '\\n', 'validation loss (best):\n3.623615', '\\n', 'RCRG@50 (best): 0.000000', '\\n', 'common_recall@50 (best):\n0.200000', '\\n', 'rare_recall@50 (best): 0.000000', '\\n', 'Execution time: a\nmoment seconds (time limit is 2 hours).']"], "parse_exc_type": [null, "FileNotFoundError", null, null, null, null], "parse_exc_info": [null, {"args": ["2", "No such file or directory"]}, null, null, null, null], "parse_exc_stack": [null, [["/workspace/AE-Scientist/research_pipeline/ai_scientist/treesearch/interpreter.py", 264, "_repl_run_session", "exec(compile(code, agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 117, "<module>", "run()"], ["runfile.py", 112, "run", "data = load_experiment_data()"], ["runfile.py", 20, "load_experiment_data", "data = np.load(path, allow_pickle=True).item()"], ["/workspace/AE-Scientist/research_pipeline/.venv/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py", 454, "load", "fid = stack.enter_context(open(os.fspath(file), \"rb\"))"]], null, null, null, null], "completed_stages": ["Stage_1", "Stage_2"]}