{"edges": [[0, 5], [0, 4], [0, 1], [0, 2], [2, 3]], "layout": [[0.5, 0.0], [0.0, 0.5], [0.3333333333333333, 0.5], [0.3333333333333333, 1.0], [0.6666666666666666, 0.5], [1.0, 0.5]], "plan": ["Hyperparam tuning name: Reduce overwrite-phase batch size to 32. We will keep\nthe baseline pipeline but modify only the overwrite (WikiText) phase DataLoaders\nto use a smaller batch size of 32 while keeping learning rate and number of\nepochs unchanged. We will keep the injection phase batch size at 96. We will\nalso restructure experiment_data to follow the requested naming convention with\na top-level key 'hyperparam_tuning_type_1' containing both 'synthetic_injection'\nand 'overwrite_wikitext' datasets. We will log and save all metrics, losses,\nhistories, embeddings, and outputs as NumPy arrays, and save the experiment_data\ndictionary to experiment_data.npy at the end.", "Upgrade the previous experiment into a multi-dataset overwrite pipeline with\nrobust metrics. After injecting rare tokens, compute per-token baseline\nrecall@50, then sequentially overwrite on three unrelated datasets (WikiText-2,\nAG News, IMDb), resetting optimizer and using warmup, clipping, and validation\neach epoch. At each checkpoint, record per-token recall, aggregate recalls,\nRCRG, and compute PHR (both estimate and censored lower bound) by interpolating\nhalf-lives vs. overwrite steps. Save all artifacts, plots, and arrays in\nworking_dir. Generate samples and visualize token hits per dataset, and compute\nembedding retention. Ensure strict device usage and data saving per the\nrequirements.", "We will extend the prior experiment into a multi-dataset overwrite pipeline\n(WikiText-2, AG News, IMDb) with retention strategies and diagnostics. After\ninjecting rare tokens, we snapshot the model (teacher) and then run overwrite\ntraining with: (a) 5% rehearsal mixing of injection data, (b) logit-anchoring KL\nloss to the pre-overwrite teacher on injection prompts, and (c) freezing of\nembeddings and LM head to reduce drift. We compute per-epoch validation loss,\nrecall@k sweeps, RCRG, and per-token recall trajectories to derive the\nPersistence Half-life Ratio (PHR) by interpolating half-life crossings. We also\ncompute embedding retention, generate samples, and save all metrics/artifacts as\nnumpy arrays and plots in the working directory. The code ensures GPU usage,\ndevice transfers, and runs within reasonable time by using dataset subsets and\nsmall epochs.", "The crash comes from Python\u2019s local variable scoping: overwrite_phase references\nanchor_iter before it\u2019s assigned inside the function, making it a local variable\nand causing UnboundLocalError. I fix this by creating an infinite, local anchor\nbatch generator inside overwrite_phase (no reliance on a global iterator) and\nusing last non-padding token logits for stable KL anchoring. I also correct\ndataset subsetting to actually respect max_train_pct and keep all device\ntransfers consistent. The rest of the experiment remains the same: inject rare\ntokens, train, then overwrite on three datasets, track recall/PHR, and save\nmetrics and plots.", "We will extend the baseline rare-token persistence experiment with three\noverwrite datasets (WikiText-2, AG News, IMDb), add rehearsal mixing and KL\ndistillation to a frozen teacher (post-injection snapshot), and freeze rare-\ntoken embedding rows via gradient masking. We will track recall@k for each token\nat every epoch and compute the Persistence Half-life Ratio (PHR) from these\ntrajectories relative to post-injection baselines. We will also compute RCRG,\nvalidation losses, embedding cosine retention, and generate samples pre/post\noverwrite. All metrics and arrays will be saved to the working directory and\naggregated into an experiment_data structure, with plots saved per dataset.", "Seed node"], "code": ["import os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (required)\n# -----------------------------------------------------------------------------\ntorch.cuda.set_device(0)\ndevice = torch.device('cuda:0')\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# Naming convention: top-level 'hyperparam_tuning_type_1'\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Dict[str, Any]]] = {\n    'hyperparam_tuning_type_1': {\n        'synthetic_injection': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_wikitext': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n    }\n}\n\ndef get_container(tag: str) -> Dict[str, Any]:\n    return experiment_data['hyperparam_tuning_type_1'][tag]\n\n# -----------------------------------------------------------------------------\n# Helper: training loop for language modeling\n# -----------------------------------------------------------------------------\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train()\n    # Collator ensures labels are properly aligned with inputs for causal LM\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        get_container(tag)['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        get_container(tag)['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Helper: tokenization function\n# -----------------------------------------------------------------------------\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Helper: recall@k computation under standardized prompts\n# -----------------------------------------------------------------------------\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    model.eval()\n    hits = 0\n    total = 0\n    with torch.no_grad():\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            logits = model(**inputs).logits[:, -1, :]\n            topk = torch.topk(logits, k=k, dim=-1).indices[0].tolist()\n            for t in targets:\n                tid = tokenizer.convert_tokens_to_ids(t)\n                if tid is None or tid < 0:\n                    continue\n                total += 1\n                if tid in topk:\n                    hits += 1\n    if total == 0:\n        return 0.0\n    return hits / total\n\n# -----------------------------------------------------------------------------\n# Helper: generate samples and collect next-token outputs\n# -----------------------------------------------------------------------------\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=1,\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='synthetic_injection',\n)\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phase: fine-tune on WikiText-2 (unrelated text)\n# -----------------------------------------------------------------------------\n# Load small subset for speed\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:30%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n\n# Tokenize wikitext\ndef tok_map(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\nwikitext_train = wikitext_train.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\nwikitext_val = wikitext_val.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# RCRG tracking across epochs\nrcrg_history = []\nrare_recall_history = []\ncommon_recall_history = []\n\n# Helper to compute RCRG at current model state\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\n\ndef compute_rcrg(model) -> Dict[str, float]:\n    k = 50\n    rare_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], control_tokens, k=k)\n    rcrg = rare_rec - common_rec\n    return {'RCRG@50': rcrg, 'rare_recall@50': rare_rec, 'common_recall@50': common_rec}\n\n# Train overwrite with per-epoch RCRG evaluation\nnum_overwrite_epochs = 4\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n# HYPERPARAM TUNING: reduce overwrite-phase batch size to 32\noverwrite_batch_size = 32\ntrain_loader = DataLoader(\n    wikitext_train,\n    batch_size=overwrite_batch_size,\n    shuffle=True,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\nval_loader = DataLoader(\n    wikitext_val,\n    batch_size=overwrite_batch_size,\n    shuffle=False,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\nglobal_step = 0\nfor epoch in range(1, num_overwrite_epochs + 1):\n    model.train()\n    run_loss = 0.0\n    n_steps = 0\n    for batch in tqdm(train_loader, desc=f'Overwrite epoch {epoch}/{num_overwrite_epochs}'):\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        run_loss += loss.item()\n        n_steps += 1\n        global_step += 1\n        if global_step % 200 == 0:\n            print(f'[{now_ts()}] Overwrite step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n    train_epoch_loss = run_loss / max(1, n_steps)\n    get_container('overwrite_wikitext')['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n    get_container('overwrite_wikitext')['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n    # Validation loss\n    model.eval()\n    val_loss_sum = 0.0\n    val_steps = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss_sum += outputs.loss.item()\n            val_steps += 1\n    val_loss = val_loss_sum / max(1, val_steps)\n    print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n    get_container('overwrite_wikitext')['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n    get_container('overwrite_wikitext')['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n    # Compute RCRG and record\n    rcrg_metrics = compute_rcrg(model)\n    rcrg_history.append(rcrg_metrics['RCRG@50'])\n    rare_recall_history.append(rcrg_metrics['rare_recall@50'])\n    common_recall_history.append(rcrg_metrics['common_recall@50'])\n    get_container('overwrite_wikitext')['metrics']['val'][-1].update(rcrg_metrics)\n\n# Save embeddings after phase 2\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\n# -----------------------------------------------------------------------------\n# 5) Embedding retention analysis (cosine similarity)\n# -----------------------------------------------------------------------------\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\n# Plot embedding retention\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 6) Sample generation before/after overwrite and visualization\n# -----------------------------------------------------------------------------\n# Generate next tokens for a standardized prompt\nsample_prompt = \"The code word is\"\nnum_samples = 128\n\nsamples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=num_samples, max_new_tokens=3, temperature=0.8, top_k=50)\n\n# Count occurrences\ndef count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n    counts = {t: 0 for t in targets}\n    for s in samples:\n        s_norm = s\n        for t in targets:\n            if s_norm == t:\n                counts[t] += 1\n    return counts\n\nrare_counts = count_hits(samples_post, rare_tokens)\ncommon_counts = count_hits(samples_post, control_tokens)\n\n# Save arrays\nnp.save(os.path.join(working_dir, 'samples_post.npy'), np.array(samples_post, dtype=object))\nnp.save(os.path.join(working_dir, 'rare_counts_post.npy'), np.array(list(rare_counts.values())))\nnp.save(os.path.join(working_dir, 'common_counts_post.npy'), np.array(list(common_counts.values())))\n\n# Plot counts\nplt.figure(figsize=(8,4))\nplt.bar(range(len(rare_tokens)), list(rare_counts.values()), color='tab:blue')\nplt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Rare tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_rare_post.png'))\nplt.close()\n\nplt.figure(figsize=(8,4))\nplt.bar(range(len(control_tokens)), list(common_counts.values()), color='tab:orange')\nplt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Common tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_common_post.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 7) Track and save RCRG history and recalls across epochs\n# -----------------------------------------------------------------------------\nnp.save(os.path.join(working_dir, 'rcrg_history.npy'), np.array(rcrg_history))\nnp.save(os.path.join(working_dir, 'rare_recall_history.npy'), np.array(rare_recall_history))\nnp.save(os.path.join(working_dir, 'common_recall_history.npy'), np.array(common_recall_history))\n\n# Plot RCRG across overwrite epochs\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(rcrg_history)+1), rcrg_history, marker='o', label='RCRG@50')\nplt.plot(range(1, len(rare_recall_history)+1), rare_recall_history, marker='s', label='Rare recall@50')\nplt.plot(range(1, len(common_recall_history)+1), common_recall_history, marker='^', label='Common recall@50')\nplt.xlabel('Overwrite epoch')\nplt.ylabel('Score')\nplt.title('Rare-to-Common Recall Gap Across Epochs')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'rcrg_over_epochs.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 8) Final metric aggregation and save experiment_data\n# -----------------------------------------------------------------------------\nget_container('overwrite_wikitext')['aux']['rcrg_history'] = rcrg_history\nget_container('overwrite_wikitext')['aux']['rare_recall_history'] = rare_recall_history\nget_container('overwrite_wikitext')['aux']['common_recall_history'] = common_recall_history\nget_container('overwrite_wikitext')['aux']['rare_tokens'] = rare_tokens\nget_container('overwrite_wikitext')['aux']['control_tokens'] = control_tokens\nget_container('overwrite_wikitext')['predictions'] = samples_post\nget_container('overwrite_wikitext')['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\n# Save experiment data with the required filename\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)", "import os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom typing import List, Dict, Any, Tuple\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    DataCollatorForLanguageModeling,\n    get_linear_schedule_with_warmup,\n)\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (required)\n# -----------------------------------------------------------------------------\ntorch.cuda.set_device(0)\ndevice = torch.device('cuda:0')\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# Naming convention: top-level 'hyperparam_tuning_type_1'\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Dict[str, Any]]] = {\n    'hyperparam_tuning_type_1': {\n        'synthetic_injection': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_wikitext': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_ag_news': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_imdb': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n    }\n}\n\ndef get_container(tag: str) -> Dict[str, Any]:\n    return experiment_data['hyperparam_tuning_type_1'][tag]\n\n# -----------------------------------------------------------------------------\n# Helper: tokenization function\n# -----------------------------------------------------------------------------\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Helper: training loop for language modeling (injection phase)\n# -----------------------------------------------------------------------------\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train()\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        get_container(tag)['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        get_container(tag)['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Recall utilities\n# -----------------------------------------------------------------------------\ndef per_token_recall_at_k(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    prompts: List[str],\n    tokens: List[str],\n    k: int = 50,\n) -> Dict[str, float]:\n    model.eval()\n    with torch.no_grad():\n        inputs = tokenizer(prompts, return_tensors='pt', padding=True).to(device)\n        logits = model(**inputs).logits  # (B, T, V)\n        last_logits = logits[:, -1, :]   # (B, V)\n        topk = torch.topk(last_logits, k=k, dim=-1).indices  # (B, k)\n        topk = topk.cpu().numpy()\n    B = len(prompts)\n    recalls = {}\n    for t in tokens:\n        tid = tokenizer.convert_tokens_to_ids(t)\n        if tid is None or tid < 0:\n            recalls[t] = 0.0\n            continue\n        hits = 0\n        for b in range(B):\n            if tid in topk[b]:\n                hits += 1\n        recalls[t] = hits / B\n    return recalls\n\ndef aggregate_recall(recalls: Dict[str, float], subset: List[str]) -> float:\n    vals = [recalls[t] for t in subset if t in recalls]\n    if len(vals) == 0:\n        return 0.0\n    return float(np.mean(vals))\n\n# -----------------------------------------------------------------------------\n# Generation helper\n# -----------------------------------------------------------------------------\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(250):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# Phase 1: Fine-tune on injection dataset\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=1,\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='synthetic_injection',\n)\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# -----------------------------------------------------------------------------\n# Compute baseline per-token recall post-injection\n# -----------------------------------------------------------------------------\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\n\nbaseline_recalls_rare = per_token_recall_at_k(model, tokenizer, standard_prompts, rare_tokens, k=50)\nbaseline_recalls_common = per_token_recall_at_k(model, tokenizer, standard_prompts, control_tokens, k=50)\nbaseline_recalls = {**baseline_recalls_rare, **baseline_recalls_common}\n\nnp.save(os.path.join(working_dir, 'baseline_recalls_rare.npy'), np.array([baseline_recalls_rare[t] for t in rare_tokens]))\nnp.save(os.path.join(working_dir, 'baseline_recalls_common.npy'), np.array([baseline_recalls_common[t] for t in control_tokens]))\n\n# Initialize recall series with baseline at step 0\nrecall_series: Dict[str, List[Tuple[int, float]]] = {t: [(0, baseline_recalls.get(t, 0.0))] for t in (rare_tokens + control_tokens)}\n\n# -----------------------------------------------------------------------------\n# Overwrite phase: sequentially fine-tune on three unrelated datasets\n# -----------------------------------------------------------------------------\n# Utility: tokenize arbitrary HF datasets with varying field names\n\ndef build_lm_dataset_from_hf(ds, text_field: str, tokenizer: AutoTokenizer, max_length: int = 64) -> Dataset:\n    def _map(batch):\n        texts = batch[text_field]\n        return tokenizer(texts, truncation=True, padding='max_length', max_length=max_length)\n    ds2 = ds.map(_map, batched=True, remove_columns=[col for col in ds.column_names if col != text_field])\n    ds2.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds2\n\n# Overwrite datasets configuration\noverwrite_configs = [\n    {\n        'tag': 'overwrite_wikitext',\n        'loader': lambda: (\n            load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:20%]'),\n            load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n        ),\n        'text_field': 'text',\n        'epochs': 2,\n        'batch_size': 32,\n        'lr': 5e-5,\n        'warmup_ratio': 0.06,\n    },\n    {\n        'tag': 'overwrite_ag_news',\n        'loader': lambda: (\n            load_dataset('fancyzhx/ag_news', split='train[:10%]'),\n            load_dataset('fancyzhx/ag_news', split='test[:10%]')\n        ),\n        'text_field': 'text',\n        'epochs': 2,\n        'batch_size': 32,\n        'lr': 5e-5,\n        'warmup_ratio': 0.06,\n    },\n    {\n        'tag': 'overwrite_imdb',\n        'loader': lambda: (\n            load_dataset('stanfordnlp/imdb', split='train[:10%]'),\n            load_dataset('stanfordnlp/imdb', split='test[:10%]')\n        ),\n        'text_field': 'text',\n        'epochs': 2,\n        'batch_size': 32,\n        'lr': 5e-5,\n        'warmup_ratio': 0.06,\n    },\n]\n\n# Recall at k summary variables\nrcrg_history_all = []\nrare_recall_history_all = []\ncommon_recall_history_all = []\nphr_lb_history_all = []\nphr_est_history_all = []\ncheckpoint_steps = [0]\n\n# Helper: compute half-lives and PHR\n\ndef compute_half_lives_and_phr(\n    recall_series: Dict[str, List[Tuple[int, float]]],\n    baseline: Dict[str, float],\n    rare_set: List[str],\n    common_set: List[str],\n    current_step: int,\n) -> Tuple[Dict[str, float], Dict[str, bool], float, float]:\n    # Returns (half_lives, censored_flags, phr_lb, phr_est)\n    half_lives: Dict[str, float] = {}\n    censored: Dict[str, bool] = {}\n    for t, series in recall_series.items():\n        b = baseline.get(t, 0.0)\n        if b <= 0.0:\n            half_lives[t] = float('nan')\n            censored[t] = True\n            continue\n        threshold = 0.5 * b\n        # ensure series sorted by step\n        series_sorted = sorted(series, key=lambda x: x[0])\n        crossed = False\n        last_s, last_r = series_sorted[0]\n        for idx in range(1, len(series_sorted)):\n            s, r = series_sorted[idx]\n            if r <= threshold:\n                # interpolate between (last_s, last_r) and (s, r)\n                if r == last_r:\n                    s_hl = float(s)\n                else:\n                    s_hl = last_s + (threshold - last_r) * (s - last_s) / (r - last_r)\n                half_lives[t] = float(max(0.0, s_hl))\n                censored[t] = False\n                crossed = True\n                break\n            last_s, last_r = s, r\n        if not crossed:\n            # censored at current_step\n            half_lives[t] = float(current_step)\n            censored[t] = True\n    # Compute PHR_lb (censored treated as current_step) and PHR_est (exclude censored)\n    def median_ratio(values_a, values_b):\n        if len(values_a) == 0 or len(values_b) == 0:\n            return float('nan')\n        med_a = float(np.median(values_a))\n        med_b = float(np.median(values_b))\n        if med_b == 0:\n            return float('nan')\n        return med_a / med_b\n\n    # Lower-bound includes all (censored included as current_step)\n    rare_hl_all = [half_lives[t] for t in rare_set if not math.isnan(half_lives.get(t, float('nan')))]\n    common_hl_all = [half_lives[t] for t in common_set if not math.isnan(half_lives.get(t, float('nan')))]\n    phr_lb = median_ratio(rare_hl_all, common_hl_all)\n\n    # Estimate excludes censored tokens\n    rare_hl_est = [half_lives[t] for t in rare_set if (not censored.get(t, True)) and not math.isnan(half_lives.get(t, float('nan')))]\n    common_hl_est = [half_lives[t] for t in common_set if (not censored.get(t, True)) and not math.isnan(half_lives.get(t, float('nan')))]\n    phr_est = median_ratio(rare_hl_est, common_hl_est)\n\n    return half_lives, censored, phr_lb, phr_est\n\n# Collator for overwrite\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n# Global step across all overwrite datasets\nglobal_overwrite_step = 0\n\n# Track per-dataset histories for plots\nper_dataset_histories = {}\n\nfor cfg in overwrite_configs:\n    tag = cfg['tag']\n    train_raw, val_raw = cfg['loader']()\n    # For AG News and IMDb, ensure text_field exists\n    text_field = cfg['text_field']\n    # Build datasets\n    train_ds = build_lm_dataset_from_hf(train_raw, text_field, tokenizer, max_length=max_length)\n    val_ds = build_lm_dataset_from_hf(val_raw, text_field, tokenizer, max_length=max_length)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg['batch_size'],\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg['batch_size'],\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    # Reset optimizer and scheduler at phase boundary\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=cfg['lr'])\n    t_total = cfg['epochs'] * max(1, len(train_loader))\n    warmup_steps = int(cfg['warmup_ratio'] * t_total)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n\n    # Histories for this dataset\n    rcrg_history = []\n    rare_recall_history = []\n    common_recall_history = []\n    phr_lb_history = []\n    phr_est_history = []\n    steps_history = []\n\n    for epoch in range(1, cfg['epochs'] + 1):\n        model.train()\n        run_loss = 0.0\n        n_steps = 0\n        for batch in tqdm(train_loader, desc=f'{tag} epoch {epoch}/{cfg['epochs']}'):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            run_loss += loss.item()\n            n_steps += 1\n            global_overwrite_step += 1\n            if global_overwrite_step % 200 == 0:\n                print(f'[{now_ts()}] {tag} step {global_overwrite_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n        train_epoch_loss = run_loss / max(1, n_steps)\n        get_container(tag)['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts(), 'global_step': global_overwrite_step})\n        get_container(tag)['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts(), 'global_step': global_overwrite_step})\n\n        # Validation loss\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        get_container(tag)['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts(), 'global_step': global_overwrite_step})\n        get_container(tag)['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts(), 'global_step': global_overwrite_step})\n\n        # Compute per-token recalls at checkpoint\n        recalls_rare = per_token_recall_at_k(model, tokenizer, standard_prompts, rare_tokens, k=50)\n        recalls_common = per_token_recall_at_k(model, tokenizer, standard_prompts, control_tokens, k=50)\n        for t in rare_tokens:\n            recall_series[t].append((global_overwrite_step, recalls_rare.get(t, 0.0)))\n        for t in control_tokens:\n            recall_series[t].append((global_overwrite_step, recalls_common.get(t, 0.0)))\n\n        # Aggregate recalls and RCRG\n        rare_rec_val = aggregate_recall({**recalls_rare, **recalls_common}, rare_tokens)\n        common_rec_val = aggregate_recall({**recalls_rare, **recalls_common}, control_tokens)\n        rcrg = rare_rec_val - common_rec_val\n        rcrg_history.append(rcrg)\n        rare_recall_history.append(rare_rec_val)\n        common_recall_history.append(common_rec_val)\n        steps_history.append(global_overwrite_step)\n        checkpoint_steps.append(global_overwrite_step)\n\n        # Compute PHR metrics\n        half_lives, censored_flags, phr_lb, phr_est = compute_half_lives_and_phr(\n            recall_series,\n            baseline_recalls,\n            rare_tokens,\n            control_tokens,\n            current_step=global_overwrite_step,\n        )\n        phr_lb_history.append(phr_lb)\n        phr_est_history.append(phr_est)\n\n        # Update experiment_data at this epoch with all metrics\n        metric_entry = {\n            'epoch': epoch,\n            'RCRG@50': rcrg,\n            'rare_recall@50': rare_rec_val,\n            'common_recall@50': common_rec_val,\n            'PHR_lb': phr_lb,\n            'PHR_est': phr_est,\n            'global_step': global_overwrite_step,\n            'ts': now_ts(),\n        }\n        get_container(tag)['metrics']['val'][-1].update(metric_entry)\n\n    # Save per-dataset histories\n    per_dataset_histories[tag] = {\n        'rcrg': rcrg_history,\n        'rare_recall': rare_recall_history,\n        'common_recall': common_recall_history,\n        'phr_lb': phr_lb_history,\n        'phr_est': phr_est_history,\n        'steps': steps_history,\n    }\n\n    # Append to global histories\n    rcrg_history_all.extend(rcrg_history)\n    rare_recall_history_all.extend(rare_recall_history)\n    common_recall_history_all.extend(common_recall_history)\n    phr_lb_history_all.extend(phr_lb_history)\n    phr_est_history_all.extend(phr_est_history)\n\n    # Generation analysis for this dataset\n    sample_prompt = \"The code word is\"\n    num_samples = 128\n    samples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=num_samples, max_new_tokens=3, temperature=0.8, top_k=50)\n\n    def count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n        counts = {t: 0 for t in targets}\n        for s in samples:\n            for t in targets:\n                if s == t:\n                    counts[t] += 1\n        return counts\n\n    rare_counts = count_hits(samples_post, rare_tokens)\n    common_counts = count_hits(samples_post, control_tokens)\n\n    # Save arrays and plots per dataset\n    np.save(os.path.join(working_dir, f'samples_post_{tag}.npy'), np.array(samples_post, dtype=object))\n    np.save(os.path.join(working_dir, f'rare_counts_post_{tag}.npy'), np.array([rare_counts[t] for t in rare_tokens]))\n    np.save(os.path.join(working_dir, f'common_counts_post_{tag}.npy'), np.array([common_counts[t] for t in control_tokens]))\n\n    plt.figure(figsize=(8,4))\n    plt.bar(range(len(rare_tokens)), [rare_counts[t] for t in rare_tokens], color='tab:blue')\n    plt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\n    plt.title(f'Post-overwrite generation counts - Rare tokens ({tag})')\n    plt.ylabel('Count in first generated token')\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f'gen_counts_rare_post_{tag}.png'))\n    plt.close()\n\n    plt.figure(figsize=(8,4))\n    plt.bar(range(len(control_tokens)), [common_counts[t] for t in control_tokens], color='tab:orange')\n    plt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\n    plt.title(f'Post-overwrite generation counts - Common tokens ({tag})')\n    plt.ylabel('Count in first generated token')\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f'gen_counts_common_post_{tag}.png'))\n    plt.close()\n\n    # Plot metrics over epochs for this dataset\n    np.save(os.path.join(working_dir, f'{tag}_rcrg_history.npy'), np.array(rcrg_history))\n    np.save(os.path.join(working_dir, f'{tag}_rare_recall_history.npy'), np.array(rare_recall_history))\n    np.save(os.path.join(working_dir, f'{tag}_common_recall_history.npy'), np.array(common_recall_history))\n    np.save(os.path.join(working_dir, f'{tag}_phr_lb_history.npy'), np.array(phr_lb_history))\n    np.save(os.path.join(working_dir, f'{tag}_phr_est_history.npy'), np.array(phr_est_history))\n    np.save(os.path.join(working_dir, f'{tag}_steps_history.npy'), np.array(steps_history))\n\n    plt.figure(figsize=(7,4))\n    xs = list(range(1, len(rcrg_history)+1))\n    plt.plot(xs, rcrg_history, marker='o', label='RCRG@50')\n    plt.plot(xs, rare_recall_history, marker='s', label='Rare recall@50')\n    plt.plot(xs, common_recall_history, marker='^', label='Common recall@50')\n    plt.xlabel('Epoch')\n    plt.ylabel('Score')\n    plt.title(f'Recalls and RCRG over epochs ({tag})')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f'{tag}_recalls_over_epochs.png'))\n    plt.close()\n\n    plt.figure(figsize=(7,4))\n    plt.plot(xs, phr_lb_history, marker='o', label='PHR lower-bound')\n    plt.plot(xs, phr_est_history, marker='s', label='PHR estimate')\n    plt.xlabel('Epoch')\n    plt.ylabel('PHR')\n    plt.title(f'PHR over epochs ({tag})')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f'{tag}_phr_over_epochs.png'))\n    plt.close()\n\n# Save embeddings after all overwrites\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\n# -----------------------------------------------------------------------------\n# Embedding retention analysis (cosine similarity)\n# -----------------------------------------------------------------------------\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# Save global histories and recall series\n# -----------------------------------------------------------------------------\n# Save recall series as an object array\nrecall_series_savable = {t: np.array(series, dtype=float) for t, series in recall_series.items()}\nnp.save(os.path.join(working_dir, 'recall_series.npy'), np.array(recall_series_savable, dtype=object))\nnp.save(os.path.join(working_dir, 'checkpoint_steps.npy'), np.array(checkpoint_steps))\n\n# Plot combined histories across all overwrites (by checkpoints in order they occurred)\nnp.save(os.path.join(working_dir, 'rcrg_history_all.npy'), np.array(rcrg_history_all))\nnp.save(os.path.join(working_dir, 'rare_recall_history_all.npy'), np.array(rare_recall_history_all))\nnp.save(os.path.join(working_dir, 'common_recall_history_all.npy'), np.array(common_recall_history_all))\nnp.save(os.path.join(working_dir, 'phr_lb_history_all.npy'), np.array(phr_lb_history_all))\nnp.save(os.path.join(working_dir, 'phr_est_history_all.npy'), np.array(phr_est_history_all))\n\nplt.figure(figsize=(7,4))\nplt.plot(range(1, len(rcrg_history_all)+1), rcrg_history_all, marker='o', label='RCRG@50')\nplt.plot(range(1, len(rare_recall_history_all)+1), rare_recall_history_all, marker='s', label='Rare recall@50')\nplt.plot(range(1, len(common_recall_history_all)+1), common_recall_history_all, marker='^', label='Common recall@50')\nplt.xlabel('Overwrite checkpoints')\nplt.ylabel('Score')\nplt.title('Recall metrics across all overwrite checkpoints')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'recalls_all_checkpoints.png'))\nplt.close()\n\nplt.figure(figsize=(7,4))\nplt.plot(range(1, len(phr_lb_history_all)+1), phr_lb_history_all, marker='o', label='PHR lower-bound')\nplt.plot(range(1, len(phr_est_history_all)+1), phr_est_history_all, marker='s', label='PHR estimate')\nplt.xlabel('Overwrite checkpoints')\nplt.ylabel('PHR')\nplt.title('PHR across all overwrite checkpoints')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'phr_all_checkpoints.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# Populate experiment_data and save\n# -----------------------------------------------------------------------------\nget_container('synthetic_injection')['aux']['rare_tokens'] = rare_tokens\nget_container('synthetic_injection')['aux']['control_tokens'] = control_tokens\nget_container('synthetic_injection')['aux']['baseline_recalls'] = baseline_recalls\n\n# Save per-dataset histories into experiment_data\nfor tag, hist in per_dataset_histories.items():\n    get_container(tag)['aux']['histories'] = hist\nget_container('overwrite_wikitext')['aux']['rare_tokens'] = rare_tokens\nget_container('overwrite_wikitext')['aux']['control_tokens'] = control_tokens\nget_container('overwrite_ag_news')['aux']['rare_tokens'] = rare_tokens\nget_container('overwrite_ag_news')['aux']['control_tokens'] = control_tokens\nget_container('overwrite_imdb')['aux']['rare_tokens'] = rare_tokens\nget_container('overwrite_imdb')['aux']['control_tokens'] = control_tokens\n\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)", "import os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom copy import deepcopy\nfrom typing import List, Dict, Any, Tuple\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (required)\n# -----------------------------------------------------------------------------\ntorch.cuda.set_device(0)\ndevice = torch.device('cuda:0')\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Dict[str, Any]]] = {\n    'hyperparam_tuning_type_1': {\n        'synthetic_injection': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_wikitext': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_agnews': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_imdb': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n    }\n}\n\ndef get_container(tag: str) -> Dict[str, Any]:\n    return experiment_data['hyperparam_tuning_type_1'][tag]\n\n# -----------------------------------------------------------------------------\n# Tokenization and dataset helpers\n# -----------------------------------------------------------------------------\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Recall computation helpers\n# -----------------------------------------------------------------------------\ndef per_token_recall_at_k(model, tokenizer, prompts: List[str], tokens: List[str], k: int = 50) -> List[float]:\n    model.eval()\n    recalls = []\n    with torch.no_grad():\n        for tok in tokens:\n            tid = tokenizer.convert_tokens_to_ids(tok)\n            if tid is None or tid < 0:\n                recalls.append(0.0)\n                continue\n            hits = 0\n            total = 0\n            for prompt in prompts:\n                inputs = tokenizer(prompt, return_tensors='pt').to(device)\n                logits = model(**inputs).logits[0, -1]\n                topk = torch.topk(logits, k=k).indices.tolist()\n                total += 1\n                if tid in topk:\n                    hits += 1\n            recalls.append(hits / max(1, total))\n    return recalls\n\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    vals = per_token_recall_at_k(model, tokenizer, prompts, targets, k)\n    if len(vals) == 0:\n        return 0.0\n    return float(np.mean(vals))\n\n# -----------------------------------------------------------------------------\n# Generation helper: sample next token after a prompt\n# -----------------------------------------------------------------------------\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# Training loop for injection phase\n# -----------------------------------------------------------------------------\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train()\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        get_container(tag)['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        get_container(tag)['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Overwrite training with rehearsal and logit anchoring\n# -----------------------------------------------------------------------------\ndef build_mixed_dataset(overwrite_ds: Dataset, injection_ds: Dataset, ratio: float = 0.05) -> Dataset:\n    n_over = len(overwrite_ds)\n    n_replay = max(1, int(n_over * ratio))\n    inj_indices = list(range(len(injection_ds)))\n    random.shuffle(inj_indices)\n    inj_indices = inj_indices[:n_replay]\n    inj_subset = injection_ds.select(inj_indices)\n    mixed = concatenate_datasets([overwrite_ds, inj_subset]).shuffle(seed=SEED)\n    return mixed\n\n@torch.no_grad()\ndef lm_logits(model, batch_inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n    outputs = model(**batch_inputs)\n    return outputs.logits\n\ndef kl_divergence(p_logits: torch.Tensor, q_logits: torch.Tensor) -> torch.Tensor:\n    # Compute KL(P || Q) on last position per sample\n    p_log_probs = torch.log_softmax(p_logits, dim=-1)\n    q_log_probs = torch.log_softmax(q_logits, dim=-1)\n    p_probs = torch.softmax(p_logits, dim=-1)\n    kl = torch.sum(p_probs * (p_log_probs - q_log_probs), dim=-1)\n    return kl.mean()\n\ndef compute_rcrg_and_recalls(model, tokenizer, prompts, rare_tokens, common_tokens, k=50) -> Dict[str, float]:\n    rare_rec = recall_at_k_for_set(model, tokenizer, prompts, rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, prompts, common_tokens, k=k)\n    rcrg = rare_rec - common_rec\n    out = {'RCRG@{}'.format(k): rcrg, 'rare_recall@{}'.format(k): rare_rec, 'common_recall@{}'.format(k): common_rec}\n    return out\n\ndef interpolate_half_life(baseline: List[float], trajectory: List[List[float]], steps: List[int]) -> List[float]:\n    # baseline: per-token baseline recall values (post-injection); trajectory: list over time (epochs) of per-token recall values\n    # steps: corresponding cumulative steps for each point in trajectory\n    n_tokens = len(baseline)\n    half_lives = []\n    for i in range(n_tokens):\n        b = baseline[i]\n        target = 0.5 * b\n        # Handle zero baseline: undefined half-life; set to 0\n        if b <= 0:\n            half_lives.append(0.0)\n            continue\n        hl = None\n        prev_val = trajectory[0][i]\n        prev_step = steps[0]\n        if prev_val <= target:\n            hl = prev_step\n        else:\n            for t in range(1, len(trajectory)):\n                cur_val = trajectory[t][i]\n                cur_step = steps[t]\n                if cur_val <= target:\n                    # linear interpolation between (prev_step, prev_val) and (cur_step, cur_val)\n                    if cur_val == prev_val:\n                        hl = cur_step\n                    else:\n                        frac = (prev_val - target) / max(1e-8, (prev_val - cur_val))\n                        hl = prev_step + frac * (cur_step - prev_step)\n                    break\n                prev_val = cur_val\n                prev_step = cur_step\n        if hl is None:\n            hl = steps[-1]\n        half_lives.append(float(hl))\n    return half_lives\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=1,\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='synthetic_injection',\n)\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# Compute baseline per-token recall (post-injection) for PHR at k=50 and sweeps\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\nks_sweep = [1, 5, 10, 50, 100, 200]\nrare_baseline_per_k = {}\ncommon_baseline_per_k = {}\nfor k in ks_sweep:\n    rare_baseline_per_k[k] = per_token_recall_at_k(model, tokenizer, standard_prompts, rare_tokens, k)\n    common_baseline_per_k[k] = per_token_recall_at_k(model, tokenizer, standard_prompts, control_tokens, k)\nnp.save(os.path.join(working_dir, 'rare_baseline_per_k.npy'), np.array([rare_baseline_per_k[k] for k in ks_sweep], dtype=object))\nnp.save(os.path.join(working_dir, 'common_baseline_per_k.npy'), np.array([common_baseline_per_k[k] for k in ks_sweep], dtype=object))\n\n# Freeze embeddings and LM head to reduce drift\nif hasattr(model, 'get_input_embeddings'):\n    emb = model.get_input_embeddings()\n    for p in emb.parameters():\n        p.requires_grad = False\nif hasattr(model, 'lm_head'):\n    for p in model.lm_head.parameters():\n        p.requires_grad = False\n\n# Prepare teacher (snapshot) for logit anchoring\nteacher = AutoModelForCausalLM.from_pretrained(model_name)\nteacher.resize_token_embeddings(len(tokenizer))\nteacher.load_state_dict(model.state_dict())\nteacher.to(device)\nteacher.eval()\nfor p in teacher.parameters():\n    p.requires_grad = False\n\n# Anchor texts (subset of injection val prompts)\nanchor_texts = val_inject_examples[:128] if len(val_inject_examples) >= 128 else val_inject_examples\nanchor_ds = tokenize_texts(tokenizer, anchor_texts, max_length=max_length)\nanchor_loader = DataLoader(anchor_ds, batch_size=32, shuffle=True, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False))\nanchor_iter = iter(anchor_loader)\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phases on multiple datasets with rehearsal + anchoring\n# -----------------------------------------------------------------------------\ndef overwrite_phase(dataset_tag: str, raw_train, raw_val, max_train_pct: float = 0.3, num_epochs: int = 2, batch_size: int = 32, lr: float = 5e-5, rehearsal_ratio: float = 0.05, anchor_weight: float = 0.1, k_for_phr: int = 50):\n    print(f\"\\n==== Overwrite phase: {dataset_tag} ====\")\n    # Tokenize\n    def tok_map(batch):\n        return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\n    train_subset = raw_train\n    if hasattr(train_subset, 'select'):\n        # limit to max_train_pct for speed\n        if isinstance(max_train_pct, float):\n            # Use split slicing if available, else select indices\n            try:\n                # If dataset supports slicing via split param, skip\n                pass\n            except Exception:\n                pass\n    train_subset = raw_train\n    val_subset = raw_val\n\n    train_tok = train_subset.map(tok_map, batched=True, remove_columns=[c for c in train_subset.column_names if c != 'text'])\n    try:\n        train_tok = train_tok.remove_columns(['text'])\n    except Exception:\n        pass\n    train_tok.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n    val_tok = val_subset.map(tok_map, batched=True, remove_columns=[c for c in val_subset.column_names if c != 'text'])\n    try:\n        val_tok = val_tok.remove_columns(['text'])\n    except Exception:\n        pass\n    val_tok.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n    # Build rehearsal-mixed dataset\n    mixed_train = build_mixed_dataset(train_tok, inject_train_ds, ratio=rehearsal_ratio)\n\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n    train_loader = DataLoader(mixed_train, batch_size=batch_size, shuffle=True, collate_fn=collator, pin_memory=True, num_workers=2)\n    val_loader = DataLoader(val_tok, batch_size=batch_size, shuffle=False, collate_fn=collator, pin_memory=True, num_workers=2)\n\n    # Optimizer after model on device\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    # For PHR tracking\n    baseline_rare = rare_baseline_per_k[k_for_phr]\n    baseline_common = common_baseline_per_k[k_for_phr]\n    traj_rare: List[List[float]] = []\n    traj_common: List[List[float]] = []\n    step_points: List[int] = []\n    global_step = 0\n\n    rcrg_history = []\n    recall_hist_by_k = {k: {'rare': [], 'common': []} for k in ks_sweep}\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        run_loss = 0.0\n        n_steps = 0\n        for batch in tqdm(train_loader, desc=f'{dataset_tag} epoch {epoch}/{num_epochs}'):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            lm_loss = outputs.loss\n\n            # Anchor mini-batch\n            try:\n                anchor_batch = next(anchor_iter)\n            except StopIteration:\n                anchor_iter_obj = DataLoader(anchor_ds, batch_size=32, shuffle=True, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False))\n                anchor_iter = iter(anchor_iter_obj)\n                anchor_batch = next(anchor_iter)\n            anchor_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in anchor_batch.items()}\n\n            with torch.no_grad():\n                t_logits = teacher(**anchor_batch).logits[:, -1, :]\n            s_logits = model(**anchor_batch).logits[:, -1, :]\n            anchor_loss = kl_divergence(t_logits, s_logits)\n\n            loss = lm_loss + anchor_weight * anchor_loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            optimizer.zero_grad()\n\n            run_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if global_step % 200 == 0:\n                print(f'[{now_ts()}] {dataset_tag} step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n        train_epoch_loss = run_loss / max(1, n_steps)\n        get_container(dataset_tag)['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        get_container(dataset_tag)['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation loss\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        get_container(dataset_tag)['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        get_container(dataset_tag)['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        # Recalls and RCRG\n        rare_per_k_epoch = {}\n        common_per_k_epoch = {}\n        for k in ks_sweep:\n            rare_vals = per_token_recall_at_k(model, tokenizer, standard_prompts, rare_tokens, k)\n            common_vals = per_token_recall_at_k(model, tokenizer, standard_prompts, control_tokens, k)\n            rare_per_k_epoch[k] = rare_vals\n            common_per_k_epoch[k] = common_vals\n            recall_hist_by_k[k]['rare'].append(np.mean(rare_vals) if len(rare_vals)>0 else 0.0)\n            recall_hist_by_k[k]['common'].append(np.mean(common_vals) if len(common_vals)>0 else 0.0)\n        # Record for PHR trajectory (k_for_phr)\n        traj_rare.append(rare_per_k_epoch[k_for_phr])\n        traj_common.append(common_per_k_epoch[k_for_phr])\n        step_points.append(global_step)\n\n        rcrg_metrics = compute_rcrg_and_recalls(model, tokenizer, standard_prompts, rare_tokens, control_tokens, k=k_for_phr)\n        rcrg_history.append(rcrg_metrics['RCRG@{}'.format(k_for_phr)])\n        get_container(dataset_tag)['metrics']['val'][-1].update(rcrg_metrics)\n\n        # Optional early stopping on median rare recall crossing half baseline\n        med_base = float(np.median(baseline_rare)) if len(baseline_rare)>0 else 0.0\n        med_cur = float(np.median(traj_rare[-1])) if len(traj_rare[-1])>0 else 0.0\n        if med_base > 0 and med_cur < 0.5 * med_base and epoch >= 2:\n            print(f\"Early stopping {dataset_tag}: median rare recall dropped below half baseline.\")\n            # break  # Disabled to gather full trajectory; keep training minimal epochs\n\n    # Compute PHR at end\n    half_rare = interpolate_half_life(baseline_rare, traj_rare, step_points)\n    half_common = interpolate_half_life(baseline_common, traj_common, step_points)\n    med_rare_hl = float(np.median([h for h in half_rare if h >= 0])) if len(half_rare)>0 else 0.0\n    med_common_hl = float(np.median([h for h in half_common if h >= 0])) if len(half_common)>0 else 0.0\n    phr = (med_rare_hl / med_common_hl) if med_common_hl > 0 else 0.0\n\n    # Save histories and PHR\n    np.save(os.path.join(working_dir, f'{dataset_tag}_rcrg_history.npy'), np.array(rcrg_history))\n    for k in ks_sweep:\n        np.save(os.path.join(working_dir, f'{dataset_tag}_rare_recall_k{k}.npy'), np.array(recall_hist_by_k[k]['rare']))\n        np.save(os.path.join(working_dir, f'{dataset_tag}_common_recall_k{k}.npy'), np.array(recall_hist_by_k[k]['common']))\n    np.save(os.path.join(working_dir, f'{dataset_tag}_half_lives_rare.npy'), np.array(half_rare))\n    np.save(os.path.join(working_dir, f'{dataset_tag}_half_lives_common.npy'), np.array(half_common))\n\n    # Plots: RCRG and recall@50\n    plt.figure(figsize=(6,4))\n    plt.plot(range(1, len(rcrg_history)+1), rcrg_history, marker='o', label='RCRG@50')\n    plt.plot(range(1, len(recall_hist_by_k[k_for_phr]['rare'])+1), recall_hist_by_k[k_for_phr]['rare'], marker='s', label='Rare recall@50')\n    plt.plot(range(1, len(recall_hist_by_k[k_for_phr]['common'])+1), recall_hist_by_k[k_for_phr]['common'], marker='^', label='Common recall@50')\n    plt.xlabel('Epoch')\n    plt.ylabel('Score')\n    plt.title(f'RCRG and Recalls - {dataset_tag}')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f'{dataset_tag}_rcrg_recalls.png'))\n    plt.close()\n\n    # Save to experiment_data\n    get_container(dataset_tag)['aux']['rcrg_history'] = rcrg_history\n    get_container(dataset_tag)['aux']['PHR'] = phr\n    get_container(dataset_tag)['aux']['half_lives'] = {'rare': half_rare, 'common': half_common}\n    get_container(dataset_tag)['aux']['recall_hist_by_k'] = recall_hist_by_k\n\n    print(f\"{dataset_tag} PHR (median rare HL / median common HL) = {phr:.4f}\")\n\n    # Generate samples and count hits\n    sample_prompt = \"The code word is\"\n    samples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=128, max_new_tokens=3, temperature=0.8, top_k=50)\n    def count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n        counts = {t: 0 for t in targets}\n        for s in samples:\n            for t in targets:\n                if s == t:\n                    counts[t] += 1\n        return counts\n    rare_counts = count_hits(samples_post, rare_tokens)\n    common_counts = count_hits(samples_post, control_tokens)\n\n    # Save arrays and plots\n    np.save(os.path.join(working_dir, f'{dataset_tag}_samples.npy'), np.array(samples_post, dtype=object))\n    np.save(os.path.join(working_dir, f'{dataset_tag}_rare_counts.npy'), np.array(list(rare_counts.values())))\n    np.save(os.path.join(working_dir, f'{dataset_tag}_common_counts.npy'), np.array(list(common_counts.values())))\n\n    plt.figure(figsize=(8,4))\n    plt.bar(range(len(rare_tokens)), list(rare_counts.values()), color='tab:blue')\n    plt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\n    plt.title(f'Post-overwrite generation counts - Rare ({dataset_tag})')\n    plt.ylabel('Count')\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f'{dataset_tag}_gen_counts_rare.png'))\n    plt.close()\n\n    plt.figure(figsize=(8,4))\n    plt.bar(range(len(control_tokens)), list(common_counts.values()), color='tab:orange')\n    plt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\n    plt.title(f'Post-overwrite generation counts - Common ({dataset_tag})')\n    plt.ylabel('Count')\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f'{dataset_tag}_gen_counts_common.png'))\n    plt.close()\n\n    get_container(dataset_tag)['predictions'] = samples_post\n    get_container(dataset_tag)['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\n# -----------------------------------------------------------------------------\n# 5) Prepare overwrite datasets\n# -----------------------------------------------------------------------------\n# WikiText-2\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:25%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n# AG News (use only text field; combine title+description if available)\nagnews_train_full = load_dataset('ag_news', split='train')\nagnews_test_full = load_dataset('ag_news', split='test')\n# Create a text-only view\ndef ag_text_map(batch):\n    # 'text' may not exist; construct from title + description if available\n    if 'text' in batch:\n        return {'text': batch['text']}\n    else:\n        title = batch.get('title', '') if isinstance(batch.get('title', ''), str) else ''\n        desc = batch.get('description', '') if isinstance(batch.get('description', ''), str) else ''\n        return {'text': (title + ' ' + desc).strip()}\n\nagnews_train = agnews_train_full.select(range(int(0.25 * len(agnews_train_full))))\nagnews_test = agnews_test_full.select(range(int(0.25 * len(agnews_test_full))))\nagnews_train = agnews_train.map(ag_text_map)\nagnews_test = agnews_test.map(ag_text_map)\n\n# IMDb\nimdb_train_full = load_dataset('imdb', split='train[:25%]')\nimdb_test_full = load_dataset('imdb', split='test[:25%]')\n# Ensure 'text' column\ndef imdb_text_map(batch):\n    return {'text': batch['text']}\n\nimdb_train = imdb_train_full.map(imdb_text_map)\nimdb_test = imdb_test_full.map(imdb_text_map)\n\n# -----------------------------------------------------------------------------\n# 6) Run overwrite phases\n# -----------------------------------------------------------------------------\noverwrite_phase(\n    dataset_tag='overwrite_wikitext',\n    raw_train=wikitext_train,\n    raw_val=wikitext_val,\n    max_train_pct=0.25,\n    num_epochs=2,\n    batch_size=32,\n    lr=5e-5,\n    rehearsal_ratio=0.05,\n    anchor_weight=0.1,\n    k_for_phr=50,\n)\n\noverwrite_phase(\n    dataset_tag='overwrite_agnews',\n    raw_train=agnews_train,\n    raw_val=agnews_test,\n    max_train_pct=0.25,\n    num_epochs=2,\n    batch_size=32,\n    lr=5e-5,\n    rehearsal_ratio=0.05,\n    anchor_weight=0.1,\n    k_for_phr=50,\n)\n\noverwrite_phase(\n    dataset_tag='overwrite_imdb',\n    raw_train=imdb_train,\n    raw_val=imdb_test,\n    max_train_pct=0.25,\n    num_epochs=2,\n    batch_size=32,\n    lr=5e-5,\n    rehearsal_ratio=0.05,\n    anchor_weight=0.1,\n    k_for_phr=50,\n)\n\n# -----------------------------------------------------------------------------\n# 7) Embedding retention analysis (cosine similarity) after last overwrite\n# -----------------------------------------------------------------------------\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens (final)')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare_final.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens (final)')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common_final.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 8) Save and finalize experiment data (including PHR)\n# -----------------------------------------------------------------------------\nget_container('synthetic_injection')['aux']['rare_baseline_per_k'] = rare_baseline_per_k\nget_container('synthetic_injection')['aux']['common_baseline_per_k'] = common_baseline_per_k\nget_container('synthetic_injection')['aux']['rare_tokens'] = rare_tokens\nget_container('synthetic_injection')['aux']['control_tokens'] = control_tokens\n\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)", "import os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom copy import deepcopy\nfrom typing import List, Dict, Any, Tuple\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (required)\n# -----------------------------------------------------------------------------\ntry:\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\nexcept Exception:\n    # Fallback to CPU if CUDA not available; still print the required line below\n    device = torch.device('cpu')\n# Required print line\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Any]] = {\n    'synthetic_injection': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {},\n    },\n    'overwrite_wikitext': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {},\n    },\n    'overwrite_agnews': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {},\n    },\n    'overwrite_imdb': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {},\n    },\n}\n\ndef get_container(tag: str) -> Dict[str, Any]:\n    return experiment_data[tag]\n\n# -----------------------------------------------------------------------------\n# Tokenization and dataset helpers\n# -----------------------------------------------------------------------------\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        # Proper normalization for model inputs: trunc/pad to consistent length\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Recall computation helpers\n# -----------------------------------------------------------------------------\n@torch.no_grad()\ndef per_token_recall_at_k(model, tokenizer, prompts: List[str], tokens: List[str], k: int = 50) -> List[float]:\n    model.eval()\n    recalls = []\n    for tok in tokens:\n        tid = tokenizer.convert_tokens_to_ids(tok)\n        if tid is None or tid < 0:\n            recalls.append(0.0)\n            continue\n        hits = 0\n        total = 0\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            logits = model(**inputs).logits[0, -1]\n            topk = torch.topk(logits, k=k).indices.tolist()\n            total += 1\n            if tid in topk:\n                hits += 1\n        recalls.append(hits / max(1, total))\n    return recalls\n\n@torch.no_grad()\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    vals = per_token_recall_at_k(model, tokenizer, prompts, targets, k)\n    if len(vals) == 0:\n        return 0.0\n    return float(np.mean(vals))\n\n# -----------------------------------------------------------------------------\n# Generation helper: sample next token after a prompt\n# -----------------------------------------------------------------------------\n@torch.no_grad()\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n    gen = model.generate(\n        **inputs,\n        do_sample=True,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    input_len = inputs['input_ids'].shape[1]\n    for i in range(gen.shape[0]):\n        new_tokens = gen[i, input_len:input_len+1]\n        token_str = tokenizer.decode(new_tokens)\n        generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# Training loop for injection phase\n# -----------------------------------------------------------------------------\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train().to(device)\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        get_container(tag)['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        get_container(tag)['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Overwrite training with rehearsal and logit anchoring\n# -----------------------------------------------------------------------------\n@torch.no_grad()\ndef lm_logits(model, batch_inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n    outputs = model(**batch_inputs)\n    return outputs.logits\n\ndef kl_divergence(p_logits: torch.Tensor, q_logits: torch.Tensor) -> torch.Tensor:\n    # KL(P || Q) across vocab on last positions (per-sample), mean over batch\n    p_log_probs = torch.log_softmax(p_logits, dim=-1)\n    q_log_probs = torch.log_softmax(q_logits, dim=-1)\n    p_probs = torch.softmax(p_logits, dim=-1)\n    kl = torch.sum(p_probs * (p_log_probs - q_log_probs), dim=-1)\n    return kl.mean()\n\n@torch.no_grad()\ndef compute_rcrg_and_recalls(model, tokenizer, prompts, rare_tokens, common_tokens, k=50) -> Dict[str, float]:\n    rare_rec = recall_at_k_for_set(model, tokenizer, prompts, rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, prompts, common_tokens, k=k)\n    rcrg = rare_rec - common_rec\n    out = {'RCRG@{}'.format(k): rcrg, 'rare_recall@{}'.format(k): rare_rec, 'common_recall@{}'.format(k): common_rec}\n    return out\n\ndef interpolate_half_life(baseline: List[float], trajectory: List[List[float]], steps: List[int]) -> List[float]:\n    n_tokens = len(baseline)\n    half_lives = []\n    for i in range(n_tokens):\n        b = baseline[i]\n        target = 0.5 * b\n        if b <= 0:\n            half_lives.append(0.0)\n            continue\n        hl = None\n        prev_val = trajectory[0][i]\n        prev_step = steps[0]\n        if prev_val <= target:\n            hl = prev_step\n        else:\n            for t in range(1, len(trajectory)):\n                cur_val = trajectory[t][i]\n                cur_step = steps[t]\n                if cur_val <= target:\n                    if cur_val == prev_val:\n                        hl = cur_step\n                    else:\n                        frac = (prev_val - target) / max(1e-8, (prev_val - cur_val))\n                        hl = prev_step + frac * (cur_step - prev_step)\n                    break\n                prev_val = cur_val\n                prev_step = cur_step\n        if hl is None:\n            hl = steps[-1]\n        half_lives.append(float(hl))\n    return half_lives\n\n# Helper: compute last non-pad token logits for anchoring (stabilizes KL)\n@torch.no_grad()\ndef last_nonpad_logits(model: torch.nn.Module, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n    model.eval()\n    input_ids = batch['input_ids']\n    attention_mask = batch['attention_mask']\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    logits = outputs.logits  # [B, T, V]\n    lengths = attention_mask.sum(dim=1) - 1  # last non-pad index per sample\n    B = input_ids.size(0)\n    idx = torch.arange(B, device=logits.device)\n    last_logits = logits[idx, lengths, :]\n    return last_logits  # [B, V]\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=1,\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='synthetic_injection',\n)\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# Compute baseline per-token recall (post-injection) for PHR at k=50 and sweeps\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\nks_sweep = [1, 5, 10, 50, 100, 200]\nrare_baseline_per_k = {}\ncommon_baseline_per_k = {}\nfor k in ks_sweep:\n    rare_baseline_per_k[k] = per_token_recall_at_k(model, tokenizer, standard_prompts, rare_tokens, k)\n    common_baseline_per_k[k] = per_token_recall_at_k(model, tokenizer, standard_prompts, control_tokens, k)\nnp.save(os.path.join(working_dir, 'rare_baseline_per_k.npy'), np.array([rare_baseline_per_k[k] for k in ks_sweep], dtype=object))\nnp.save(os.path.join(working_dir, 'common_baseline_per_k.npy'), np.array([common_baseline_per_k[k] for k in ks_sweep], dtype=object))\n\n# Freeze embeddings and LM head to reduce drift\nif hasattr(model, 'get_input_embeddings'):\n    emb = model.get_input_embeddings()\n    for p in emb.parameters():\n        p.requires_grad = False\nif hasattr(model, 'lm_head'):\n    for p in model.lm_head.parameters():\n        p.requires_grad = False\n\n# Prepare teacher (snapshot) for logit anchoring: deep copy of student\nteacher = deepcopy(model).to(device)\nteacher.eval()\nfor p in teacher.parameters():\n    p.requires_grad = False\n\n# Precompute anchor dataset locally for each overwrite phase using val prompts\nanchor_texts_global = val_inject_examples[:128] if len(val_inject_examples) >= 128 else val_inject_examples\nanchor_ds_global = tokenize_texts(tokenizer, anchor_texts_global, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phases on multiple datasets with rehearsal + anchoring\n# -----------------------------------------------------------------------------\ndef build_mixed_dataset(overwrite_ds: Dataset, injection_ds: Dataset, ratio: float = 0.05) -> Dataset:\n    n_over = len(overwrite_ds)\n    n_replay = max(1, int(n_over * ratio))\n    inj_indices = list(range(len(injection_ds)))\n    random.shuffle(inj_indices)\n    inj_indices = inj_indices[:n_replay]\n    inj_subset = injection_ds.select(inj_indices)\n    mixed = concatenate_datasets([overwrite_ds, inj_subset]).shuffle(seed=SEED)\n    return mixed\n\n# Infinite local anchor iterator (avoids UnboundLocalError and keeps batches on device)\ndef make_infinite_anchor_iter(anchor_ds: Dataset, tokenizer: AutoTokenizer, batch_size: int = 32):\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n    while True:\n        loader = DataLoader(anchor_ds, batch_size=batch_size, shuffle=True, collate_fn=collator, num_workers=0, pin_memory=True)\n        for batch in loader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            yield batch\n\ndef overwrite_phase(dataset_tag: str, raw_train, raw_val, max_train_pct: float = 0.3, num_epochs: int = 2, batch_size: int = 32, lr: float = 5e-5, rehearsal_ratio: float = 0.05, anchor_weight: float = 0.1, k_for_phr: int = 50):\n    print(f\"\\n==== Overwrite phase: {dataset_tag} ====\")\n    # Tokenize\n    def tok_map(batch):\n        return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\n    # Respect subset percentage for speed\n    if isinstance(max_train_pct, float) and 0 < max_train_pct < 1.0:\n        try:\n            n_take = max(1, int(len(raw_train) * max_train_pct))\n            train_subset = raw_train.select(range(n_take))\n        except Exception:\n            train_subset = raw_train\n    else:\n        train_subset = raw_train\n    val_subset = raw_val\n\n    # Standardize to have only text column\n    train_tok = train_subset.map(tok_map, batched=True, remove_columns=[c for c in train_subset.column_names if c != 'text'])\n    try:\n        train_tok = train_tok.remove_columns(['text'])\n    except Exception:\n        pass\n    train_tok.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n    val_tok = val_subset.map(tok_map, batched=True, remove_columns=[c for c in val_subset.column_names if c != 'text'])\n    try:\n        val_tok = val_tok.remove_columns(['text'])\n    except Exception:\n        pass\n    val_tok.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n    # Build rehearsal-mixed dataset\n    mixed_train = build_mixed_dataset(train_tok, inject_train_ds, ratio=rehearsal_ratio)\n\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n    train_loader = DataLoader(mixed_train, batch_size=batch_size, shuffle=True, collate_fn=collator, pin_memory=True, num_workers=2)\n    val_loader = DataLoader(val_tok, batch_size=batch_size, shuffle=False, collate_fn=collator, pin_memory=True, num_workers=2)\n\n    # Optimizer after model on device\n    model.to(device)\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    # For PHR tracking\n    baseline_rare = rare_baseline_per_k[k_for_phr]\n    baseline_common = common_baseline_per_k[k_for_phr]\n    traj_rare: List[List[float]] = []\n    traj_common: List[List[float]] = []\n    step_points: List[int] = []\n    global_step = 0\n\n    rcrg_history = []\n    recall_hist_by_k = {k: {'rare': [], 'common': []} for k in ks_sweep}\n\n    # Local infinite anchor iterator\n    anchor_iter_local = make_infinite_anchor_iter(anchor_ds_global, tokenizer, batch_size=32)\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        run_loss = 0.0\n        n_steps = 0\n        for batch in tqdm(train_loader, desc=f'{dataset_tag} epoch {epoch}/{num_epochs}'):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            lm_loss = outputs.loss\n\n            # Anchor mini-batch (local iterator, no global state)\n            anchor_batch = next(anchor_iter_local)\n            with torch.no_grad():\n                t_logits = last_nonpad_logits(teacher, anchor_batch)\n            s_logits = last_nonpad_logits(model, anchor_batch)\n            anchor_loss = kl_divergence(t_logits, s_logits)\n\n            loss = lm_loss + anchor_weight * anchor_loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            optimizer.zero_grad()\n\n            run_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if global_step % 200 == 0:\n                print(f'[{now_ts()}] {dataset_tag} step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n        train_epoch_loss = run_loss / max(1, n_steps)\n        get_container(dataset_tag)['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        get_container(dataset_tag)['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation loss\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        get_container(dataset_tag)['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        get_container(dataset_tag)['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        # Recalls and RCRG\n        rare_per_k_epoch = {}\n        common_per_k_epoch = {}\n        for k in ks_sweep:\n            rare_vals = per_token_recall_at_k(model, tokenizer, standard_prompts, rare_tokens, k)\n            common_vals = per_token_recall_at_k(model, tokenizer, standard_prompts, control_tokens, k)\n            rare_per_k_epoch[k] = rare_vals\n            common_per_k_epoch[k] = common_vals\n            recall_hist_by_k[k]['rare'].append(np.mean(rare_vals) if len(rare_vals)>0 else 0.0)\n            recall_hist_by_k[k]['common'].append(np.mean(common_vals) if len(common_vals)>0 else 0.0)\n        # Record for PHR trajectory (k_for_phr)\n        traj_rare.append(rare_per_k_epoch[k_for_phr])\n        traj_common.append(common_per_k_epoch[k_for_phr])\n        step_points.append(global_step)\n\n        rcrg_metrics = compute_rcrg_and_recalls(model, tokenizer, standard_prompts, rare_tokens, control_tokens, k=k_for_phr)\n        rcrg_history.append(rcrg_metrics['RCRG@{}'.format(k_for_phr)])\n        get_container(dataset_tag)['metrics']['val'][-1].update(rcrg_metrics)\n\n        # Optional early stopping condition for long runs (kept minimal to gather trajectory)\n        med_base = float(np.median(baseline_rare)) if len(baseline_rare)>0 else 0.0\n        med_cur = float(np.median(traj_rare[-1])) if len(traj_rare[-1])>0 else 0.0\n        if med_base > 0 and med_cur < 0.5 * med_base and epoch >= 2:\n            print(f\"Early stopping {dataset_tag}: median rare recall dropped below half baseline.\")\n            # Proceed to end-of-phase computations\n\n    # Compute PHR at end\n    half_rare = interpolate_half_life(baseline_rare, traj_rare, step_points)\n    half_common = interpolate_half_life(baseline_common, traj_common, step_points)\n    med_rare_hl = float(np.median([h for h in half_rare if h >= 0])) if len(half_rare)>0 else 0.0\n    med_common_hl = float(np.median([h for h in half_common if h >= 0])) if len(half_common)>0 else 0.0\n    phr = (med_rare_hl / med_common_hl) if med_common_hl > 0 else 0.0\n\n    # Save histories and PHR\n    np.save(os.path.join(working_dir, f'{dataset_tag}_rcrg_history.npy'), np.array(rcrg_history))\n    for k in ks_sweep:\n        np.save(os.path.join(working_dir, f'{dataset_tag}_rare_recall_k{k}.npy'), np.array(recall_hist_by_k[k]['rare']))\n        np.save(os.path.join(working_dir, f'{dataset_tag}_common_recall_k{k}.npy'), np.array(recall_hist_by_k[k]['common']))\n    np.save(os.path.join(working_dir, f'{dataset_tag}_half_lives_rare.npy'), np.array(half_rare))\n    np.save(os.path.join(working_dir, f'{dataset_tag}_half_lives_common.npy'), np.array(half_common))\n\n    # Plots: RCRG and recall@50\n    plt.figure(figsize=(6,4))\n    plt.plot(range(1, len(rcrg_history)+1), rcrg_history, marker='o', label='RCRG@50')\n    plt.plot(range(1, len(recall_hist_by_k[k_for_phr]['rare'])+1), recall_hist_by_k[k_for_phr]['rare'], marker='s', label='Rare recall@50')\n    plt.plot(range(1, len(recall_hist_by_k[k_for_phr]['common'])+1), recall_hist_by_k[k_for_phr]['common'], marker='^', label='Common recall@50')\n    plt.xlabel('Epoch')\n    plt.ylabel('Score')\n    plt.title(f'RCRG and Recalls - {dataset_tag}')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f'{dataset_tag}_rcrg_recalls.png'))\n    plt.close()\n\n    # Save to experiment_data\n    get_container(dataset_tag)['aux']['rcrg_history'] = rcrg_history\n    get_container(dataset_tag)['aux']['PHR'] = phr\n    get_container(dataset_tag)['aux']['half_lives'] = {'rare': half_rare, 'common': half_common}\n    get_container(dataset_tag)['aux']['recall_hist_by_k'] = recall_hist_by_k\n\n    print(f\"{dataset_tag} PHR (median rare HL / median common HL) = {phr:.4f}\")\n\n    # Generate samples and count hits\n    sample_prompt = \"The code word is\"\n    samples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=128, max_new_tokens=3, temperature=0.8, top_k=50)\n    def count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n        counts = {t: 0 for t in targets}\n        for s in samples:\n            for t in targets:\n                if s == t:\n                    counts[t] += 1\n        return counts\n    rare_counts = count_hits(samples_post, rare_tokens)\n    common_counts = count_hits(samples_post, control_tokens)\n\n    # Save arrays and plots\n    np.save(os.path.join(working_dir, f'{dataset_tag}_samples.npy'), np.array(samples_post, dtype=object))\n    np.save(os.path.join(working_dir, f'{dataset_tag}_rare_counts.npy'), np.array(list(rare_counts.values())))\n    np.save(os.path.join(working_dir, f'{dataset_tag}_common_counts.npy'), np.array(list(common_counts.values())))\n\n    plt.figure(figsize=(8,4))\n    plt.bar(range(len(rare_tokens)), list(rare_counts.values()), color='tab:blue')\n    plt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\n    plt.title(f'Post-overwrite generation counts - Rare ({dataset_tag})')\n    plt.ylabel('Count')\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f'{dataset_tag}_gen_counts_rare.png'))\n    plt.close()\n\n    plt.figure(figsize=(8,4))\n    plt.bar(range(len(control_tokens)), list(common_counts.values()), color='tab:orange')\n    plt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\n    plt.title(f'Post-overwrite generation counts - Common ({dataset_tag})')\n    plt.ylabel('Count')\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f'{dataset_tag}_gen_counts_common.png'))\n    plt.close()\n\n    get_container(dataset_tag)['predictions'] = samples_post\n    get_container(dataset_tag)['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\n# -----------------------------------------------------------------------------\n# 5) Prepare overwrite datasets\n# -----------------------------------------------------------------------------\n# WikiText-2\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:25%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n# AG News (use only text field; combine title+description if available)\nagnews_train_full = load_dataset('ag_news', split='train')\nagnews_test_full = load_dataset('ag_news', split='test')\n# Create a text-only view\n\ndef ag_text_map(batch):\n    if 'text' in batch:\n        return {'text': batch['text']}\n    else:\n        title = batch.get('title', '') if isinstance(batch.get('title', ''), str) else ''\n        desc = batch.get('description', '') if isinstance(batch.get('description', ''), str) else ''\n        return {'text': (title + ' ' + desc).strip()}\n\nagnews_train = agnews_train_full.select(range(int(0.25 * len(agnews_train_full))))\nagnews_test = agnews_test_full.select(range(int(0.25 * len(agnews_test_full))))\nagnews_train = agnews_train.map(ag_text_map)\nagnews_test = agnews_test.map(ag_text_map)\n\n# IMDb\nimdb_train_full = load_dataset('imdb', split='train[:25%]')\nimdb_test_full = load_dataset('imdb', split='test[:25%]')\n\ndef imdb_text_map(batch):\n    return {'text': batch['text']}\n\nimdb_train = imdb_train_full.map(imdb_text_map)\nimdb_test = imdb_test_full.map(imdb_text_map)\n\n# -----------------------------------------------------------------------------\n# 6) Run overwrite phases\n# -----------------------------------------------------------------------------\noverwrite_phase(\n    dataset_tag='overwrite_wikitext',\n    raw_train=wikitext_train,\n    raw_val=wikitext_val,\n    max_train_pct=0.25,\n    num_epochs=2,\n    batch_size=32,\n    lr=5e-5,\n    rehearsal_ratio=0.05,\n    anchor_weight=0.1,\n    k_for_phr=50,\n)\n\noverwrite_phase(\n    dataset_tag='overwrite_agnews',\n    raw_train=agnews_train,\n    raw_val=agnews_test,\n    max_train_pct=0.25,\n    num_epochs=2,\n    batch_size=32,\n    lr=5e-5,\n    rehearsal_ratio=0.05,\n    anchor_weight=0.1,\n    k_for_phr=50,\n)\n\noverwrite_phase(\n    dataset_tag='overwrite_imdb',\n    raw_train=imdb_train,\n    raw_val=imdb_test,\n    max_train_pct=0.25,\n    num_epochs=2,\n    batch_size=32,\n    lr=5e-5,\n    rehearsal_ratio=0.05,\n    anchor_weight=0.1,\n    k_for_phr=50,\n)\n\n# -----------------------------------------------------------------------------\n# 7) Embedding retention analysis (cosine similarity) after last overwrite\n# -----------------------------------------------------------------------------\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens (final)')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare_final.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens (final)')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common_final.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 8) Save and finalize experiment data (including PHR)\n# -----------------------------------------------------------------------------\nget_container('synthetic_injection')['aux']['rare_baseline_per_k'] = rare_baseline_per_k\nget_container('synthetic_injection')['aux']['common_baseline_per_k'] = common_baseline_per_k\nget_container('synthetic_injection')['aux']['rare_tokens'] = rare_tokens\nget_container('synthetic_injection')['aux']['control_tokens'] = control_tokens\n\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)", "import os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom typing import List, Dict, Any, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (required)\n# -----------------------------------------------------------------------------\ntorch.cuda.set_device(0)\ndevice = torch.device('cuda:0')\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Dict[str, Any]]] = {\n    'rare_token_persistence': {\n        'synthetic_injection': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_wikitext': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_ag_news': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_imdb': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n    }\n}\n\ndef get_container(tag: str) -> Dict[str, Any]:\n    return experiment_data['rare_token_persistence'][tag]\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n\ndef prepare_loaders(train_ds: Dataset, val_ds: Dataset, batch_size: int = 32, num_workers: int = 2) -> Tuple[DataLoader, DataLoader]:\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers)\n    return train_loader, val_loader\n\n\ndef add_labels(batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    # Add labels for causal LM, masking out pad positions\n    labels = batch['input_ids'].clone()\n    if 'attention_mask' in batch:\n        pad_mask = (batch['attention_mask'] == 0)\n        labels[pad_mask] = -100\n    batch['labels'] = labels\n    return batch\n\n\ndef compute_val_loss(model: torch.nn.Module, val_loader: DataLoader) -> float:\n    model.eval()\n    loss_sum = 0.0\n    steps = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            batch = add_labels(batch)\n            outputs = model(**batch)\n            loss_sum += outputs.loss.item()\n            steps += 1\n    return loss_sum / max(1, steps)\n\n\ndef recall_at_k_for_prompt(model, tokenizer, prompt: str, tokens: List[str], k: int = 50) -> List[int]:\n    model.eval()\n    with torch.no_grad():\n        inp = tokenizer(prompt, return_tensors='pt').to(device)\n        logits = model(**inp).logits[0, -1]\n        topk = torch.topk(logits, k=k).indices.tolist()\n    recalls = []\n    for t in tokens:\n        tid = tokenizer.convert_tokens_to_ids(t)\n        if tid is None or tid < 0:\n            recalls.append(0)\n        else:\n            recalls.append(1 if tid in topk else 0)\n    return recalls\n\n\ndef recall_at_k_multi_prompts(model, tokenizer, prompts: List[str], tokens: List[str], k: int = 50) -> float:\n    # Average over prompts, then average over tokens\n    per_prompt = []\n    for p in prompts:\n        v = recall_at_k_for_prompt(model, tokenizer, p, tokens, k)\n        per_prompt.append(v)\n    arr = np.array(per_prompt)  # shape (num_prompts, num_tokens)\n    return float(arr.mean()) if arr.size > 0 else 0.0\n\n\ndef compute_half_lives(baseline: List[int], history: List[List[int]], steps_per_epoch: int) -> List[float]:\n    # baseline: per-token recall at epoch 0 (post-injection) under fixed prompt\n    # history: list over epochs of per-token recall vectors under same prompt\n    # Return per-token half-life in steps (np.nan if not reached)\n    T = len(history)\n    num_tokens = len(baseline)\n    half_lives = []\n    for j in range(num_tokens):\n        b = baseline[j]\n        if b <= 0:\n            half_lives.append(np.nan)\n            continue\n        threshold = 0.5 * b\n        hl_steps = np.nan\n        for ep in range(T):\n            val = history[ep][j]\n            if val < threshold:\n                # Half-life crossed between previous (ep-1) and ep; interpolate linearly in epochs\n                prev_val = baseline[j] if ep == 0 else history[ep - 1][j]\n                # Linear interpolation: find fraction f in (ep-1, ep]\n                # Since values are 0/1, if prev=1 and val=0, place at ep - 0.5 by convention\n                frac = 0.5\n                hl_steps = ((ep) - frac) * steps_per_epoch\n                break\n        half_lives.append(hl_steps)\n    return half_lives\n\n\ndef compute_phr(baseline_rare: List[int], rare_hist: List[List[int]], baseline_common: List[int], common_hist: List[List[int]], steps_per_epoch: int) -> Dict[str, float]:\n    rare_hl = compute_half_lives(baseline_rare, rare_hist, steps_per_epoch)\n    common_hl = compute_half_lives(baseline_common, common_hist, steps_per_epoch)\n    rare_hl_arr = np.array([x for x in rare_hl if not np.isnan(x)])\n    common_hl_arr = np.array([x for x in common_hl if not np.isnan(x)])\n    phr = np.nan\n    if rare_hl_arr.size > 0 and common_hl_arr.size > 0:\n        phr = float(np.median(rare_hl_arr) / max(1e-9, np.median(common_hl_arr)))\n    return {\n        'PHR': phr,\n        'rare_median_half_life': float(np.median(rare_hl_arr)) if rare_hl_arr.size > 0 else float('nan'),\n        'common_median_half_life': float(np.median(common_hl_arr)) if common_hl_arr.size > 0 else float('nan'),\n        'rare_censored_pct': float(100.0 * (1.0 - rare_hl_arr.size / max(1, len(rare_hl)))) if len(rare_hl) > 0 else float('nan'),\n        'common_censored_pct': float(100.0 * (1.0 - common_hl_arr.size / max(1, len(common_hl)))) if len(common_hl) > 0 else float('nan'),\n    }\n\n\ndef last_token_indices(attn_mask: torch.Tensor) -> torch.Tensor:\n    # attn_mask: (B, L) with 1 for real tokens, 0 for pad\n    lengths = attn_mask.sum(dim=1) - 1\n    lengths = torch.clamp(lengths, min=0)\n    return lengths\n\n\ndef kl_on_last_token(student_logits: torch.Tensor, teacher_logits: torch.Tensor, attn_mask: torch.Tensor) -> torch.Tensor:\n    # logits: (B, L, V)\n    idx = last_token_indices(attn_mask)  # (B)\n    B, L, V = student_logits.shape\n    ar = torch.arange(B, device=student_logits.device)\n    s_last = student_logits[ar, idx, :]\n    t_last = teacher_logits[ar, idx, :]\n    log_p_s = F.log_softmax(s_last, dim=-1)\n    p_t = F.softmax(t_last, dim=-1)\n    return F.kl_div(log_p_s, p_t, reduction='batchmean')\n\n\ndef register_embedding_row_freeze(model: torch.nn.Module, row_ids: List[int]):\n    # Freeze gradient updates for specific embedding rows (and tied LM head rows)\n    emb = model.get_input_embeddings().weight\n    mask = torch.ones_like(emb)\n    if len(row_ids) > 0:\n        mask[row_ids] = 0.0\n    mask = mask.to(device)\n    def hook_fn(grad):\n        return grad * mask\n    handle = emb.register_hook(hook_fn)\n    return handle\n\n\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n\ndef count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n    counts = {t: 0 for t in targets}\n    for s in samples:\n        for t in targets:\n            if s == t:\n                counts[t] += 1\n    return counts\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset\n# -----------------------------------------------------------------------------\ntrain_loader_inj, val_loader_inj = prepare_loaders(inject_train_ds, inject_val_ds, batch_size=96, num_workers=2)\noptimizer_inj = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\nmodel.train()\nrunning_loss = 0.0\nsteps = 0\nfor batch in tqdm(train_loader_inj, desc='Training synthetic injection (1 epoch)'):\n    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n    batch = add_labels(batch)\n    outputs = model(**batch)\n    loss = outputs.loss\n    loss.backward()\n    optimizer_inj.step()\n    optimizer_inj.zero_grad()\n    running_loss += loss.item()\n    steps += 1\n\ntrain_epoch_loss = running_loss / max(1, steps)\nget_container('synthetic_injection')['losses']['train'].append({'epoch': 1, 'loss': train_epoch_loss, 'ts': now_ts()})\nget_container('synthetic_injection')['metrics']['train'].append({'epoch': 1, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\nval_loss_inj = compute_val_loss(model, val_loader_inj)\nprint(f'Epoch 1: validation_loss = {val_loss_inj:.4f}')\nget_container('synthetic_injection')['losses']['val'].append({'epoch': 1, 'loss': val_loss_inj, 'ts': now_ts()})\nget_container('synthetic_injection')['metrics']['val'].append({'epoch': 1, 'val_loss': val_loss_inj, 'ts': now_ts()})\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# Baseline recalls for PHR under fixed prompt\nfixed_prompt = \"The code word is\"\nbaseline_rare = recall_at_k_for_prompt(model, tokenizer, fixed_prompt, rare_tokens, k=50)\nbaseline_common = recall_at_k_for_prompt(model, tokenizer, fixed_prompt, control_tokens, k=50)\nnp.save(os.path.join(working_dir, 'baseline_rare.npy'), np.array(baseline_rare))\nnp.save(os.path.join(working_dir, 'baseline_common.npy'), np.array(baseline_common))\n\n# Teacher snapshot for distillation\nteacher = AutoModelForCausalLM.from_pretrained(model_name)\nteacher.resize_token_embeddings(len(tokenizer))\nteacher.load_state_dict(model.state_dict())\nteacher.to(device)\nteacher.eval()\nfor p in teacher.parameters():\n    p.requires_grad = False\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phases: WikiText-2, AG News, IMDb with rehearsal + KL + row-freeze\n# -----------------------------------------------------------------------------\n\ndef load_text_dataset(name: str, split_train: str, split_val: str, config: str = None, text_field: str = 'text') -> Tuple[Dataset, Dataset]:\n    if config is None:\n        train = load_dataset(name, split=split_train)\n        val = load_dataset(name, split=split_val)\n    else:\n        train = load_dataset(name, config, split=split_train)\n        val = load_dataset(name, config, split=split_val)\n    # Ensure text field exists\n    if text_field not in train.column_names:\n        # Try common alternatives\n        if 'content' in train.column_names:\n            text_field = 'content'\n        elif 'sentence' in train.column_names:\n            text_field = 'sentence'\n        elif 'text' in train.column_names:\n            text_field = 'text'\n        else:\n            raise ValueError(f'No suitable text field in dataset {name}')\n    def tok_map(batch):\n        return tokenizer(batch[text_field], truncation=True, padding='max_length', max_length=max_length)\n    train = train.map(tok_map, batched=True, remove_columns=[c for c in train.column_names if c not in []])\n    val = val.map(tok_map, batched=True, remove_columns=[c for c in val.column_names if c not in []])\n    train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return train, val\n\n# Anchor (rehearsal) dataset from injection prompts\nanchor_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\nanchor_loader = DataLoader(anchor_ds, batch_size=32, shuffle=True, pin_memory=True, num_workers=2)\n\ndef cycle(iterable):\n    while True:\n        for x in iterable:\n            yield x\n\nanchor_iter = cycle(anchor_loader)\n\n# Overwrite configuration\noverwrite_cfgs = [\n    {\n        'tag': 'overwrite_wikitext',\n        'loader_fn': lambda: load_text_dataset('wikitext', 'wikitext-2-raw-v1', 'wikitext-2-raw-v1', config=None),\n        'splits': ('train[:20%]', 'validation'),\n        'text_field': 'text',\n        'epochs': 3,\n    },\n    {\n        'tag': 'overwrite_ag_news',\n        'loader_fn': None,  # will load explicitly below\n        'splits': ('train[:5%]', 'test[:2%]'),\n        'text_field': 'text',\n        'epochs': 2,\n    },\n    {\n        'tag': 'overwrite_imdb',\n        'loader_fn': None,\n        'splits': ('train[:10%]', 'test[:5%]'),\n        'text_field': 'text',\n        'epochs': 2,\n    },\n]\n\n# Hyperparameters\noverwrite_lr = 5e-5\noverwrite_batch_size = 32\nkl_trust_weight = 0.5\nkl_anchor_weight = 1.0\nanchor_ce_weight = 1.0\nanchor_interval = 2  # every N steps include an anchor batch\n\n# Gradient mask for rare embedding rows\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\n\n# Prompts for RCRG\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\n\n# Histories for PHR per dataset\ndataset_histories = {}\n\nfor cfg in overwrite_cfgs:\n    tag = cfg['tag']\n    print(f'\\n===== Starting {tag} =====')\n    if tag == 'overwrite_wikitext':\n        split_train, split_val = cfg['splits']\n        # load with config\n        train_raw = load_dataset('wikitext', 'wikitext-2-raw-v1', split=split_train)\n        val_raw = load_dataset('wikitext', 'wikitext-2-raw-v1', split=split_val)\n        def tok_map(batch):\n            return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        train_ds = train_raw.map(tok_map, batched=True, remove_columns=['text'])\n        val_ds = val_raw.map(tok_map, batched=True, remove_columns=['text'])\n        train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n        val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    elif tag == 'overwrite_ag_news':\n        split_train, split_val = cfg['splits']\n        train_raw = load_dataset('fancyzhx/ag_news', split=split_train)\n        val_raw = load_dataset('fancyzhx/ag_news', split=split_val)\n        def tok_map(batch):\n            return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        train_ds = train_raw.map(tok_map, batched=True, remove_columns=['text', 'label'])\n        val_ds = val_raw.map(tok_map, batched=True, remove_columns=['text', 'label'])\n        train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n        val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    elif tag == 'overwrite_imdb':\n        split_train, split_val = cfg['splits']\n        train_raw = load_dataset('stanfordnlp/imdb', split=split_train)\n        val_raw = load_dataset('stanfordnlp/imdb', split=split_val)\n        def tok_map(batch):\n            return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        train_ds = train_raw.map(tok_map, batched=True, remove_columns=['text', 'label']) if 'label' in train_raw.column_names else train_raw.map(tok_map, batched=True, remove_columns=['text'])\n        val_ds = val_raw.map(tok_map, batched=True, remove_columns=['text', 'label']) if 'label' in val_raw.column_names else val_raw.map(tok_map, batched=True, remove_columns=['text'])\n        train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n        val_ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    else:\n        raise ValueError('Unknown tag')\n\n    train_loader, val_loader = prepare_loaders(train_ds, val_ds, batch_size=overwrite_batch_size, num_workers=2)\n    steps_per_epoch = len(train_loader)\n\n    # Register gradient mask to freeze rare rows\n    hook_handle = register_embedding_row_freeze(model, rare_ids)\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=overwrite_lr)\n\n    # Histories for PHR in this dataset\n    rare_hist: List[List[int]] = []\n    common_hist: List[List[int]] = []\n\n    global_step = 0\n    for epoch in range(1, cfg['epochs'] + 1):\n        model.train()\n        run_loss = 0.0\n        n_steps = 0\n        for batch in tqdm(train_loader, desc=f'{tag} epoch {epoch}/{cfg[\"epochs\"]}'):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            batch = add_labels(batch)\n\n            # Student forward on overwrite batch\n            outputs = model(**batch)\n            lm_loss = outputs.loss\n\n            # Trust-region KL on overwrite batch (last token)\n            with torch.no_grad():\n                t_outputs = teacher(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n            kl_trust = kl_on_last_token(outputs.logits, t_outputs.logits, batch['attention_mask'])\n\n            total_loss = lm_loss + kl_trust_weight * kl_trust\n\n            # Anchor rehearsal + KL every anchor_interval steps\n            if (global_step % anchor_interval) == 0:\n                try:\n                    anchor_batch = next(anchor_iter)\n                except StopIteration:\n                    anchor_iter = cycle(anchor_loader)\n                    anchor_batch = next(anchor_iter)\n                anchor_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in anchor_batch.items()}\n                anchor_batch = add_labels(anchor_batch)\n                a_outputs = model(**anchor_batch)\n                anchor_ce = a_outputs.loss\n                with torch.no_grad():\n                    t_a_outputs = teacher(input_ids=anchor_batch['input_ids'], attention_mask=anchor_batch['attention_mask'])\n                anchor_kl = kl_on_last_token(a_outputs.logits, t_a_outputs.logits, anchor_batch['attention_mask'])\n                total_loss = total_loss + anchor_ce_weight * anchor_ce + kl_anchor_weight * anchor_kl\n\n            total_loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            run_loss += total_loss.item()\n            n_steps += 1\n            global_step += 1\n            if global_step % 200 == 0:\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n        train_epoch_loss = run_loss / max(1, n_steps)\n        get_container(tag)['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation loss\n        val_loss = compute_val_loss(model, val_loader)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        get_container(tag)['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        # Metrics: RCRG, recalls per token for PHR\n        rare_recalls_vec = recall_at_k_for_prompt(model, tokenizer, fixed_prompt, rare_tokens, k=50)\n        common_recalls_vec = recall_at_k_for_prompt(model, tokenizer, fixed_prompt, control_tokens, k=50)\n        rare_hist.append(rare_recalls_vec)\n        common_hist.append(common_recalls_vec)\n\n        rare_rec_avg = recall_at_k_multi_prompts(model, tokenizer, standard_prompts, rare_tokens, k=50)\n        common_rec_avg = recall_at_k_multi_prompts(model, tokenizer, standard_prompts, control_tokens, k=50)\n        rcrg = rare_rec_avg - common_rec_avg\n\n        phr_metrics = compute_phr(baseline_rare, rare_hist, baseline_common, common_hist, steps_per_epoch)\n        metrics_update = {\n            'RCRG@50': rcrg,\n            'rare_recall@50': rare_rec_avg,\n            'common_recall@50': common_rec_avg,\n            'PHR': phr_metrics['PHR'],\n            'rare_median_half_life': phr_metrics['rare_median_half_life'],\n            'common_median_half_life': phr_metrics['common_median_half_life'],\n            'rare_censored_pct': phr_metrics['rare_censored_pct'],\n            'common_censored_pct': phr_metrics['common_censored_pct'],\n        }\n        get_container(tag)['metrics']['val'][-1].update(metrics_update)\n\n    # Store histories and arrays\n    dataset_histories[tag] = {\n        'rare_hist': np.array(rare_hist),\n        'common_hist': np.array(common_hist),\n        'steps_per_epoch': steps_per_epoch,\n    }\n    np.save(os.path.join(working_dir, f'{tag}_rare_hist.npy'), np.array(rare_hist))\n    np.save(os.path.join(working_dir, f'{tag}_common_hist.npy'), np.array(common_hist))\n\n    # Cleanup hook for this phase\n    hook_handle.remove()\n\n# -----------------------------------------------------------------------------\n# 5) Embedding retention analysis (cosine similarity) post-overwrite\n# -----------------------------------------------------------------------------\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\n# Plot embedding retention\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 6) Sample generation before/after overwrite and visualization\n# -----------------------------------------------------------------------------\n# Generate samples post-overwrite\nsample_prompt = \"The code word is\"\nnum_samples = 128\nsamples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=num_samples, max_new_tokens=3, temperature=0.8, top_k=50)\n\nrare_counts = count_hits(samples_post, rare_tokens)\ncommon_counts = count_hits(samples_post, control_tokens)\n\n# Save arrays\nnp.save(os.path.join(working_dir, 'samples_post.npy'), np.array(samples_post, dtype=object))\nnp.save(os.path.join(working_dir, 'rare_counts_post.npy'), np.array(list(rare_counts.values())))\nnp.save(os.path.join(working_dir, 'common_counts_post.npy'), np.array(list(common_counts.values())))\n\n# Plot counts\nplt.figure(figsize=(8,4))\nplt.bar(range(len(rare_tokens)), list(rare_counts.values()), color='tab:blue')\nplt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Rare tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_rare_post.png'))\nplt.close()\n\nplt.figure(figsize=(8,4))\nplt.bar(range(len(control_tokens)), list(common_counts.values()), color='tab:orange')\nplt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Common tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_common_post.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 7) RCRG/PHR plots across overwrite datasets\n# -----------------------------------------------------------------------------\nfor tag in ['overwrite_wikitext', 'overwrite_ag_news', 'overwrite_imdb']:\n    hist = dataset_histories.get(tag, None)\n    if hist is None:\n        continue\n    rare_hist = hist['rare_hist']\n    common_hist = hist['common_hist']\n    epochs = rare_hist.shape[0]\n    # Compute epoch-level averages (mean recall across tokens)\n    rare_avg = rare_hist.mean(axis=1) if epochs > 0 else np.array([])\n    common_avg = common_hist.mean(axis=1) if epochs > 0 else np.array([])\n    rcrg_curve = rare_avg - common_avg\n    np.save(os.path.join(working_dir, f'{tag}_rare_avg.npy'), rare_avg)\n    np.save(os.path.join(working_dir, f'{tag}_common_avg.npy'), common_avg)\n    np.save(os.path.join(working_dir, f'{tag}_rcrg.npy'), rcrg_curve)\n\n    plt.figure(figsize=(6,4))\n    plt.plot(range(1, epochs+1), rare_avg, marker='o', label='Rare recall@50')\n    plt.plot(range(1, epochs+1), common_avg, marker='s', label='Common recall@50')\n    plt.plot(range(1, epochs+1), rcrg_curve, marker='^', label='RCRG@50')\n    plt.xlabel('Epoch')\n    plt.ylabel('Score')\n    plt.title(f'Recall and RCRG over epochs - {tag}')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(os.path.join(working_dir, f'{tag}_rcrg_over_epochs.png'))\n    plt.close()\n\n# -----------------------------------------------------------------------------\n# 8) Final metric aggregation and save experiment_data\n# -----------------------------------------------------------------------------\n# Aggregate aux info\nfor tag in ['overwrite_wikitext', 'overwrite_ag_news', 'overwrite_imdb']:\n    get_container(tag)['aux']['rare_tokens'] = rare_tokens\n    get_container(tag)['aux']['control_tokens'] = control_tokens\n    if tag in dataset_histories:\n        get_container(tag)['aux']['rare_hist'] = dataset_histories[tag]['rare_hist']\n        get_container(tag)['aux']['common_hist'] = dataset_histories[tag]['common_hist']\n        get_container(tag)['aux']['steps_per_epoch'] = dataset_histories[tag]['steps_per_epoch']\n\n# Save experiment data with the required filename\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (required)\n# -----------------------------------------------------------------------------\ntorch.cuda.set_device(0)\ndevice = torch.device('cuda:0')\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# Naming convention: top-level 'hyperparam_tuning_type_1'\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Dict[str, Any]]] = {\n    'hyperparam_tuning_type_1': {\n        'synthetic_injection': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n        'overwrite_wikitext': {\n            'metrics': {'train': [], 'val': []},\n            'losses': {'train': [], 'val': []},\n            'predictions': [],\n            'ground_truth': [],\n            'aux': {}\n        },\n    }\n}\n\ndef get_container(tag: str) -> Dict[str, Any]:\n    return experiment_data['hyperparam_tuning_type_1'][tag]\n\n# -----------------------------------------------------------------------------\n# Helper: training loop for language modeling\n# -----------------------------------------------------------------------------\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train()\n    # Collator ensures labels are properly aligned with inputs for causal LM\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        get_container(tag)['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        get_container(tag)['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        get_container(tag)['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Helper: tokenization function\n# -----------------------------------------------------------------------------\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Helper: recall@k computation under standardized prompts\n# -----------------------------------------------------------------------------\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    model.eval()\n    hits = 0\n    total = 0\n    with torch.no_grad():\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            logits = model(**inputs).logits[:, -1, :]\n            topk = torch.topk(logits, k=k, dim=-1).indices[0].tolist()\n            for t in targets:\n                tid = tokenizer.convert_tokens_to_ids(t)\n                if tid is None or tid < 0:\n                    continue\n                total += 1\n                if tid in topk:\n                    hits += 1\n    if total == 0:\n        return 0.0\n    return hits / total\n\n# -----------------------------------------------------------------------------\n# Helper: generate samples and collect next-token outputs\n# -----------------------------------------------------------------------------\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=1,\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='synthetic_injection',\n)\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phase: fine-tune on WikiText-2 (unrelated text)\n# -----------------------------------------------------------------------------\n# Load small subset for speed\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:30%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n\n# Tokenize wikitext\ndef tok_map(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\nwikitext_train = wikitext_train.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\nwikitext_val = wikitext_val.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# RCRG tracking across epochs\nrcrg_history = []\nrare_recall_history = []\ncommon_recall_history = []\n\n# Helper to compute RCRG at current model state\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\n\ndef compute_rcrg(model) -> Dict[str, float]:\n    k = 50\n    rare_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], control_tokens, k=k)\n    rcrg = rare_rec - common_rec\n    return {'RCRG@50': rcrg, 'rare_recall@50': rare_rec, 'common_recall@50': common_rec}\n\n# Train overwrite with per-epoch RCRG evaluation\nnum_overwrite_epochs = 4\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n# HYPERPARAM TUNING: reduce overwrite-phase batch size to 32\noverwrite_batch_size = 32\ntrain_loader = DataLoader(\n    wikitext_train,\n    batch_size=overwrite_batch_size,\n    shuffle=True,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\nval_loader = DataLoader(\n    wikitext_val,\n    batch_size=overwrite_batch_size,\n    shuffle=False,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\nglobal_step = 0\nfor epoch in range(1, num_overwrite_epochs + 1):\n    model.train()\n    run_loss = 0.0\n    n_steps = 0\n    for batch in tqdm(train_loader, desc=f'Overwrite epoch {epoch}/{num_overwrite_epochs}'):\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        run_loss += loss.item()\n        n_steps += 1\n        global_step += 1\n        if global_step % 200 == 0:\n            print(f'[{now_ts()}] Overwrite step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n    train_epoch_loss = run_loss / max(1, n_steps)\n    get_container('overwrite_wikitext')['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n    get_container('overwrite_wikitext')['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n    # Validation loss\n    model.eval()\n    val_loss_sum = 0.0\n    val_steps = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss_sum += outputs.loss.item()\n            val_steps += 1\n    val_loss = val_loss_sum / max(1, val_steps)\n    print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n    get_container('overwrite_wikitext')['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n    get_container('overwrite_wikitext')['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n    # Compute RCRG and record\n    rcrg_metrics = compute_rcrg(model)\n    rcrg_history.append(rcrg_metrics['RCRG@50'])\n    rare_recall_history.append(rcrg_metrics['rare_recall@50'])\n    common_recall_history.append(rcrg_metrics['common_recall@50'])\n    get_container('overwrite_wikitext')['metrics']['val'][-1].update(rcrg_metrics)\n\n# Save embeddings after phase 2\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\n# -----------------------------------------------------------------------------\n# 5) Embedding retention analysis (cosine similarity)\n# -----------------------------------------------------------------------------\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\n# Plot embedding retention\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 6) Sample generation before/after overwrite and visualization\n# -----------------------------------------------------------------------------\n# Generate next tokens for a standardized prompt\nsample_prompt = \"The code word is\"\nnum_samples = 128\n\nsamples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=num_samples, max_new_tokens=3, temperature=0.8, top_k=50)\n\n# Count occurrences\ndef count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n    counts = {t: 0 for t in targets}\n    for s in samples:\n        s_norm = s\n        for t in targets:\n            if s_norm == t:\n                counts[t] += 1\n    return counts\n\nrare_counts = count_hits(samples_post, rare_tokens)\ncommon_counts = count_hits(samples_post, control_tokens)\n\n# Save arrays\nnp.save(os.path.join(working_dir, 'samples_post.npy'), np.array(samples_post, dtype=object))\nnp.save(os.path.join(working_dir, 'rare_counts_post.npy'), np.array(list(rare_counts.values())))\nnp.save(os.path.join(working_dir, 'common_counts_post.npy'), np.array(list(common_counts.values())))\n\n# Plot counts\nplt.figure(figsize=(8,4))\nplt.bar(range(len(rare_tokens)), list(rare_counts.values()), color='tab:blue')\nplt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Rare tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_rare_post.png'))\nplt.close()\n\nplt.figure(figsize=(8,4))\nplt.bar(range(len(control_tokens)), list(common_counts.values()), color='tab:orange')\nplt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Common tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_common_post.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 7) Track and save RCRG history and recalls across epochs\n# -----------------------------------------------------------------------------\nnp.save(os.path.join(working_dir, 'rcrg_history.npy'), np.array(rcrg_history))\nnp.save(os.path.join(working_dir, 'rare_recall_history.npy'), np.array(rare_recall_history))\nnp.save(os.path.join(working_dir, 'common_recall_history.npy'), np.array(common_recall_history))\n\n# Plot RCRG across overwrite epochs\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(rcrg_history)+1), rcrg_history, marker='o', label='RCRG@50')\nplt.plot(range(1, len(rare_recall_history)+1), rare_recall_history, marker='s', label='Rare recall@50')\nplt.plot(range(1, len(common_recall_history)+1), common_recall_history, marker='^', label='Common recall@50')\nplt.xlabel('Overwrite epoch')\nplt.ylabel('Score')\nplt.title('Rare-to-Common Recall Gap Across Epochs')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'rcrg_over_epochs.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 8) Final metric aggregation and save experiment_data\n# -----------------------------------------------------------------------------\nget_container('overwrite_wikitext')['aux']['rcrg_history'] = rcrg_history\nget_container('overwrite_wikitext')['aux']['rare_recall_history'] = rare_recall_history\nget_container('overwrite_wikitext')['aux']['common_recall_history'] = common_recall_history\nget_container('overwrite_wikitext')['aux']['rare_tokens'] = rare_tokens\nget_container('overwrite_wikitext')['aux']['control_tokens'] = control_tokens\nget_container('overwrite_wikitext')['predictions'] = samples_post\nget_container('overwrite_wikitext')['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\n# Save experiment data with the required filename\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)"], "term_out": ["['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls: [' apple', '\ntable', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n23617.64 examples/s]', '\\n', '\\rMap:   0%|          | 0/300 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00, 32215.97\nexamples/s]', '\\n', '\\rTraining synthetic_injection epoch 1/1:   0%|          |\n0/21 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 1/1:   5%|4\n| 1/21 [00:54<18:08, 54.42s/it]', '\\rTraining synthetic_injection epoch 1/1:\n10%|9         | 2/21 [00:54<07:07, 22.48s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  14%|#4        | 3/21 [00:54<03:40, 12.27s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  19%|#9        | 4/21 [00:54<02:06,  7.47s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  24%|##3       | 5/21 [00:54<01:17,\n4.82s/it]', '\\rTraining synthetic_injection epoch 1/1:  29%|##8       | 6/21\n[00:54<00:48,  3.22s/it]', '\\rTraining synthetic_injection epoch 1/1:  33%|###3\n| 7/21 [00:55<00:30,  2.20s/it]', '\\rTraining synthetic_injection epoch 1/1:\n38%|###8      | 8/21 [00:55<00:20,  1.54s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  43%|####2     | 9/21 [00:55<00:13,  1.09s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  48%|####7     | 10/21 [00:55<00:08,  1.26it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  52%|#####2    | 11/21 [00:55<00:05,\n1.71it/s]', '\\rTraining synthetic_injection epoch 1/1:  57%|#####7    | 12/21\n[00:55<00:03,  2.26it/s]', '\\rTraining synthetic_injection epoch 1/1:\n62%|######1   | 13/21 [00:55<00:02,  2.92it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  67%|######6   | 14/21 [00:55<00:01,  3.65it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  71%|#######1  | 15/21 [00:56<00:01,  4.43it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  76%|#######6  | 16/21 [00:56<00:00,\n5.20it/s]', '\\rTraining synthetic_injection epoch 1/1:  81%|########  | 17/21\n[00:56<00:00,  5.91it/s]', '\\rTraining synthetic_injection epoch 1/1:\n86%|########5 | 18/21 [00:56<00:00,  6.54it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  90%|######### | 19/21 [00:56<00:00,  7.07it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  95%|#########5| 20/21 [00:56<00:00,  7.49it/s]',\n'', '\\rTraining synthetic_injection epoch 1/1: 100%|##########| 21/21\n[00:57<00:00,  2.75s/it]', '\\n', 'Epoch 1: validation_loss = 3.2268', '\\n',\n'\\rOverwrite epoch 1/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 1/4:   0%|          | 1/345 [00:53<5:04:01, 53.03s/it]', '\\rOverwrite\nepoch 1/4:   1%|          | 3/345 [00:53<1:18:35, 13.79s/it]', '\\rOverwrite\nepoch 1/4:   2%|1         | 6/345 [00:53<30:16,  5.36s/it]  ', '\\rOverwrite\nepoch 1/4:   3%|2         | 9/345 [00:53<16:06,  2.88s/it]', '\\rOverwrite epoch\n1/4:   3%|3         | 12/345 [00:53<09:40,  1.74s/it]', '\\rOverwrite epoch 1/4:\n4%|4         | 15/345 [00:53<06:11,  1.13s/it]', '\\rOverwrite epoch 1/4:   5%|5\n| 18/345 [00:53<04:07,  1.32it/s]', '\\rOverwrite epoch 1/4:   6%|6         |\n21/345 [00:53<02:49,  1.91it/s]', '\\rOverwrite epoch 1/4:   7%|6         |\n24/345 [00:54<01:59,  2.70it/s]', '\\rOverwrite epoch 1/4:   8%|7         |\n27/345 [00:54<01:25,  3.72it/s]', '\\rOverwrite epoch 1/4:   9%|8         |\n30/345 [00:54<01:03,  4.98it/s]', '\\rOverwrite epoch 1/4:  10%|9         |\n33/345 [00:54<00:47,  6.53it/s]', '\\rOverwrite epoch 1/4:  10%|#         |\n36/345 [00:54<00:37,  8.30it/s]', '\\rOverwrite epoch 1/4:  11%|#1        |\n39/345 [00:54<00:29, 10.23it/s]', '\\rOverwrite epoch 1/4:  12%|#2        |\n42/345 [00:54<00:25, 12.10it/s]', '\\rOverwrite epoch 1/4:  13%|#3        |\n45/345 [00:55<00:21, 13.98it/s]', '\\rOverwrite epoch 1/4:  14%|#3        |\n48/345 [00:55<00:18, 15.69it/s]', '\\rOverwrite epoch 1/4:  15%|#4        |\n51/345 [00:55<00:17, 17.12it/s]', '\\rOverwrite epoch 1/4:  16%|#5        |\n54/345 [00:55<00:15, 18.27it/s]', '\\rOverwrite epoch 1/4:  17%|#6        |\n57/345 [00:55<00:15, 19.19it/s]', '\\rOverwrite epoch 1/4:  17%|#7        |\n60/345 [00:55<00:14, 19.92it/s]', '\\rOverwrite epoch 1/4:  18%|#8        |\n63/345 [00:55<00:13, 20.43it/s]', '\\rOverwrite epoch 1/4:  19%|#9        |\n66/345 [00:56<00:13, 20.83it/s]', '\\rOverwrite epoch 1/4:  20%|##        |\n69/345 [00:56<00:13, 21.12it/s]', '\\rOverwrite epoch 1/4:  21%|##        |\n72/345 [00:56<00:12, 21.33it/s]', '\\rOverwrite epoch 1/4:  22%|##1       |\n75/345 [00:56<00:12, 21.50it/s]', '\\rOverwrite epoch 1/4:  23%|##2       |\n78/345 [00:56<00:12, 21.63it/s]', '\\rOverwrite epoch 1/4:  23%|##3       |\n81/345 [00:56<00:12, 21.68it/s]', '\\rOverwrite epoch 1/4:  24%|##4       |\n84/345 [00:56<00:12, 21.65it/s]', '\\rOverwrite epoch 1/4:  25%|##5       |\n87/345 [00:56<00:11, 21.73it/s]', '\\rOverwrite epoch 1/4:  26%|##6       |\n90/345 [00:57<00:11, 21.77it/s]', '\\rOverwrite epoch 1/4:  27%|##6       |\n93/345 [00:57<00:11, 21.80it/s]', '\\rOverwrite epoch 1/4:  28%|##7       |\n96/345 [00:57<00:11, 21.83it/s]', '\\rOverwrite epoch 1/4:  29%|##8       |\n99/345 [00:57<00:11, 21.71it/s]', '\\rOverwrite epoch 1/4:  30%|##9       |\n102/345 [00:57<00:11, 21.72it/s]', '\\rOverwrite epoch 1/4:  30%|###       |\n105/345 [00:57<00:11, 21.63it/s]', '\\rOverwrite epoch 1/4:  31%|###1      |\n108/345 [00:57<00:10, 21.63it/s]', '\\rOverwrite epoch 1/4:  32%|###2      |\n111/345 [00:58<00:10, 21.69it/s]', '\\rOverwrite epoch 1/4:  33%|###3      |\n114/345 [00:58<00:10, 21.66it/s]', '\\rOverwrite epoch 1/4:  34%|###3      |\n117/345 [00:58<00:10, 21.72it/s]', '\\rOverwrite epoch 1/4:  35%|###4      |\n120/345 [00:58<00:10, 21.74it/s]', '\\rOverwrite epoch 1/4:  36%|###5      |\n123/345 [00:58<00:10, 21.76it/s]', '\\rOverwrite epoch 1/4:  37%|###6      |\n126/345 [00:58<00:10, 21.72it/s]', '\\rOverwrite epoch 1/4:  37%|###7      |\n129/345 [00:58<00:09, 21.72it/s]', '\\rOverwrite epoch 1/4:  38%|###8      |\n132/345 [00:59<00:09, 21.76it/s]', '\\rOverwrite epoch 1/4:  39%|###9      |\n135/345 [00:59<00:09, 21.77it/s]', '\\rOverwrite epoch 1/4:  40%|####      |\n138/345 [00:59<00:09, 21.78it/s]', '\\rOverwrite epoch 1/4:  41%|####      |\n141/345 [00:59<00:09, 21.80it/s]', '\\rOverwrite epoch 1/4:  42%|####1     |\n144/345 [00:59<00:09, 21.80it/s]', '\\rOverwrite epoch 1/4:  43%|####2     |\n147/345 [00:59<00:09, 21.69it/s]', '\\rOverwrite epoch 1/4:  43%|####3     |\n150/345 [00:59<00:08, 21.73it/s]', '\\rOverwrite epoch 1/4:  44%|####4     |\n153/345 [01:00<00:08, 21.73it/s]', '\\rOverwrite epoch 1/4:  45%|####5     |\n156/345 [01:00<00:08, 21.72it/s]', '\\rOverwrite epoch 1/4:  46%|####6     |\n159/345 [01:00<00:08, 21.69it/s]', '\\rOverwrite epoch 1/4:  47%|####6     |\n162/345 [01:00<00:08, 21.69it/s]', '\\rOverwrite epoch 1/4:  48%|####7     |\n165/345 [01:00<00:08, 21.65it/s]', '\\rOverwrite epoch 1/4:  49%|####8     |\n168/345 [01:00<00:08, 21.70it/s]', '\\rOverwrite epoch 1/4:  50%|####9     |\n171/345 [01:00<00:08, 21.60it/s]', '\\rOverwrite epoch 1/4:  50%|#####     |\n174/345 [01:01<00:07, 21.72it/s]', '\\rOverwrite epoch 1/4:  51%|#####1    |\n177/345 [01:01<00:07, 21.64it/s]', '\\rOverwrite epoch 1/4:  52%|#####2    |\n180/345 [01:01<00:07, 21.68it/s]', '\\rOverwrite epoch 1/4:  53%|#####3    |\n183/345 [01:01<00:07, 21.64it/s]', '\\rOverwrite epoch 1/4:  54%|#####3    |\n186/345 [01:01<00:07, 21.61it/s]', '\\rOverwrite epoch 1/4:  55%|#####4    |\n189/345 [01:01<00:07, 21.49it/s]', '\\rOverwrite epoch 1/4:  56%|#####5    |\n192/345 [01:01<00:07, 21.54it/s]', '\\rOverwrite epoch 1/4:  57%|#####6    |\n195/345 [01:01<00:06, 21.49it/s]', '\\rOverwrite epoch 1/4:  57%|#####7    |\n198/345 [01:02<00:06, 21.52it/s]', '[2025-12-03 19:04:18] Overwrite step 200:\navg_train_loss=3.8327', '\\n', '\\rOverwrite epoch 1/4:  58%|#####8    | 201/345\n[01:02<00:06, 21.44it/s]', '\\rOverwrite epoch 1/4:  59%|#####9    | 204/345\n[01:02<00:06, 21.55it/s]', '\\rOverwrite epoch 1/4:  60%|######    | 207/345\n[01:02<00:06, 21.64it/s]', '\\rOverwrite epoch 1/4:  61%|######    | 210/345\n[01:02<00:06, 21.70it/s]', '\\rOverwrite epoch 1/4:  62%|######1   | 213/345\n[01:02<00:06, 21.73it/s]', '\\rOverwrite epoch 1/4:  63%|######2   | 216/345\n[01:02<00:05, 21.77it/s]', '\\rOverwrite epoch 1/4:  63%|######3   | 219/345\n[01:03<00:05, 21.81it/s]', '\\rOverwrite epoch 1/4:  64%|######4   | 222/345\n[01:03<00:05, 21.82it/s]', '\\rOverwrite epoch 1/4:  65%|######5   | 225/345\n[01:03<00:05, 21.82it/s]', '\\rOverwrite epoch 1/4:  66%|######6   | 228/345\n[01:03<00:05, 21.85it/s]', '\\rOverwrite epoch 1/4:  67%|######6   | 231/345\n[01:03<00:05, 21.84it/s]', '\\rOverwrite epoch 1/4:  68%|######7   | 234/345\n[01:03<00:05, 21.76it/s]', '\\rOverwrite epoch 1/4:  69%|######8   | 237/345\n[01:03<00:04, 21.78it/s]', '\\rOverwrite epoch 1/4:  70%|######9   | 240/345\n[01:04<00:04, 21.77it/s]', '\\rOverwrite epoch 1/4:  70%|#######   | 243/345\n[01:04<00:04, 21.80it/s]', '\\rOverwrite epoch 1/4:  71%|#######1  | 246/345\n[01:04<00:04, 21.81it/s]', '\\rOverwrite epoch 1/4:  72%|#######2  | 249/345\n[01:04<00:04, 21.84it/s]', '\\rOverwrite epoch 1/4:  73%|#######3  | 252/345\n[01:04<00:04, 21.87it/s]', '\\rOverwrite epoch 1/4:  74%|#######3  | 255/345\n[01:04<00:04, 21.88it/s]', '\\rOverwrite epoch 1/4:  75%|#######4  | 258/345\n[01:04<00:03, 21.86it/s]', '\\rOverwrite epoch 1/4:  76%|#######5  | 261/345\n[01:05<00:03, 21.85it/s]', '\\rOverwrite epoch 1/4:  77%|#######6  | 264/345\n[01:05<00:03, 21.84it/s]', '\\rOverwrite epoch 1/4:  77%|#######7  | 267/345\n[01:05<00:03, 21.84it/s]', '\\rOverwrite epoch 1/4:  78%|#######8  | 270/345\n[01:05<00:03, 21.81it/s]', '\\rOverwrite epoch 1/4:  79%|#######9  | 273/345\n[01:05<00:03, 21.82it/s]', '\\rOverwrite epoch 1/4:  80%|########  | 276/345\n[01:05<00:03, 21.78it/s]', '\\rOverwrite epoch 1/4:  81%|########  | 279/345\n[01:05<00:03, 21.80it/s]', '\\rOverwrite epoch 1/4:  82%|########1 | 282/345\n[01:05<00:02, 21.68it/s]', '\\rOverwrite epoch 1/4:  83%|########2 | 285/345\n[01:06<00:02, 21.73it/s]', '\\rOverwrite epoch 1/4:  83%|########3 | 288/345\n[01:06<00:02, 21.77it/s]', '\\rOverwrite epoch 1/4:  84%|########4 | 291/345\n[01:06<00:02, 21.82it/s]', '\\rOverwrite epoch 1/4:  85%|########5 | 294/345\n[01:06<00:02, 21.85it/s]', '\\rOverwrite epoch 1/4:  86%|########6 | 297/345\n[01:06<00:02, 21.79it/s]', '\\rOverwrite epoch 1/4:  87%|########6 | 300/345\n[01:06<00:02, 21.62it/s]', '\\rOverwrite epoch 1/4:  88%|########7 | 303/345\n[01:06<00:01, 21.46it/s]', '\\rOverwrite epoch 1/4:  89%|########8 | 306/345\n[01:07<00:01, 21.52it/s]', '\\rOverwrite epoch 1/4:  90%|########9 | 309/345\n[01:07<00:01, 21.53it/s]', '\\rOverwrite epoch 1/4:  90%|######### | 312/345\n[01:07<00:01, 21.56it/s]', '\\rOverwrite epoch 1/4:  91%|#########1| 315/345\n[01:07<00:01, 21.59it/s]', '\\rOverwrite epoch 1/4:  92%|#########2| 318/345\n[01:07<00:01, 21.65it/s]', '\\rOverwrite epoch 1/4:  93%|#########3| 321/345\n[01:07<00:01, 21.66it/s]', '\\rOverwrite epoch 1/4:  94%|#########3| 324/345\n[01:07<00:00, 21.49it/s]', '\\rOverwrite epoch 1/4:  95%|#########4| 327/345\n[01:08<00:00, 21.47it/s]', '\\rOverwrite epoch 1/4:  96%|#########5| 330/345\n[01:08<00:00, 21.55it/s]', '\\rOverwrite epoch 1/4:  97%|#########6| 333/345\n[01:08<00:00, 21.62it/s]', '\\rOverwrite epoch 1/4:  97%|#########7| 336/345\n[01:08<00:00, 21.63it/s]', '\\rOverwrite epoch 1/4:  98%|#########8| 339/345\n[01:08<00:00, 21.52it/s]', '\\rOverwrite epoch 1/4:  99%|#########9| 342/345\n[01:08<00:00, 21.50it/s]', '\\rOverwrite epoch 1/4: 100%|##########| 345/345\n[01:08<00:00, 22.49it/s]', '', '\\rOverwrite epoch 1/4: 100%|##########| 345/345\n[01:09<00:00,  4.94it/s]', '\\n', 'Epoch 1: validation_loss = 3.6236', '\\n',\n'\\rOverwrite epoch 2/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 2/4:   0%|          | 1/345 [00:54<5:10:31, 54.16s/it]', '\\rOverwrite\nepoch 2/4:   1%|          | 3/345 [00:54<1:20:20, 14.10s/it]', '\\rOverwrite\nepoch 2/4:   2%|1         | 6/345 [00:54<30:56,  5.48s/it]  ', '\\rOverwrite\nepoch 2/4:   3%|2         | 9/345 [00:54<16:27,  2.94s/it]', '\\rOverwrite epoch\n2/4:   3%|3         | 12/345 [00:54<09:53,  1.78s/it]', '\\rOverwrite epoch 2/4:\n4%|4         | 15/345 [00:54<06:19,  1.15s/it]', '\\rOverwrite epoch 2/4:   5%|5\n| 18/345 [00:54<04:12,  1.29it/s]', '\\rOverwrite epoch 2/4:   6%|6         |\n21/345 [00:55<02:52,  1.87it/s]', '\\rOverwrite epoch 2/4:   7%|6         |\n24/345 [00:55<02:01,  2.65it/s]', '\\rOverwrite epoch 2/4:   8%|7         |\n27/345 [00:55<01:27,  3.65it/s]', '\\rOverwrite epoch 2/4:   9%|8         |\n30/345 [00:55<01:04,  4.92it/s]', '\\rOverwrite epoch 2/4:  10%|9         |\n33/345 [00:55<00:48,  6.45it/s]', '\\rOverwrite epoch 2/4:  10%|#         |\n36/345 [00:55<00:37,  8.21it/s]', '\\rOverwrite epoch 2/4:  11%|#1        |\n39/345 [00:55<00:30, 10.12it/s]', '\\rOverwrite epoch 2/4:  12%|#2        |\n42/345 [00:56<00:25, 12.07it/s]', '\\rOverwrite epoch 2/4:  13%|#3        |\n45/345 [00:56<00:21, 13.95it/s]', '\\rOverwrite epoch 2/4:  14%|#3        |\n48/345 [00:56<00:18, 15.64it/s]', '\\rOverwrite epoch 2/4:  15%|#4        |\n51/345 [00:56<00:17, 17.09it/s]', '\\rOverwrite epoch 2/4:  16%|#5        |\n54/345 [00:56<00:16, 18.07it/s]', '[2025-12-03 19:06:20] Overwrite step 400:\navg_train_loss=3.3332', '\\n', '\\rOverwrite epoch 2/4:  17%|#6        | 57/345\n[00:56<00:15, 18.82it/s]', '\\rOverwrite epoch 2/4:  17%|#7        | 60/345\n[00:56<00:14, 19.59it/s]', '\\rOverwrite epoch 2/4:  18%|#8        | 63/345\n[00:57<00:13, 20.15it/s]', '\\rOverwrite epoch 2/4:  19%|#9        | 66/345\n[00:57<00:13, 20.60it/s]', '\\rOverwrite epoch 2/4:  20%|##        | 69/345\n[00:57<00:13, 20.76it/s]', '\\rOverwrite epoch 2/4:  21%|##        | 72/345\n[00:57<00:13, 20.82it/s]', '\\rOverwrite epoch 2/4:  22%|##1       | 75/345\n[00:57<00:13, 20.67it/s]', '\\rOverwrite epoch 2/4:  23%|##2       | 78/345\n[00:57<00:12, 20.95it/s]', '\\rOverwrite epoch 2/4:  23%|##3       | 81/345\n[00:57<00:12, 21.21it/s]', '\\rOverwrite epoch 2/4:  24%|##4       | 84/345\n[00:58<00:12, 21.41it/s]', '\\rOverwrite epoch 2/4:  25%|##5       | 87/345\n[00:58<00:11, 21.56it/s]', '\\rOverwrite epoch 2/4:  26%|##6       | 90/345\n[00:58<00:11, 21.52it/s]', '\\rOverwrite epoch 2/4:  27%|##6       | 93/345\n[00:58<00:11, 21.63it/s]', '\\rOverwrite epoch 2/4:  28%|##7       | 96/345\n[00:58<00:11, 21.69it/s]', '\\rOverwrite epoch 2/4:  29%|##8       | 99/345\n[00:58<00:11, 21.76it/s]', '\\rOverwrite epoch 2/4:  30%|##9       | 102/345\n[00:58<00:11, 21.79it/s]', '\\rOverwrite epoch 2/4:  30%|###       | 105/345\n[00:59<00:10, 21.82it/s]', '\\rOverwrite epoch 2/4:  31%|###1      | 108/345\n[00:59<00:10, 21.84it/s]', '\\rOverwrite epoch 2/4:  32%|###2      | 111/345\n[00:59<00:10, 21.85it/s]', '\\rOverwrite epoch 2/4:  33%|###3      | 114/345\n[00:59<00:10, 21.86it/s]', '\\rOverwrite epoch 2/4:  34%|###3      | 117/345\n[00:59<00:10, 21.28it/s]', '\\rOverwrite epoch 2/4:  35%|###4      | 120/345\n[00:59<00:10, 20.57it/s]', '\\rOverwrite epoch 2/4:  36%|###5      | 123/345\n[00:59<00:11, 20.09it/s]', '\\rOverwrite epoch 2/4:  37%|###6      | 126/345\n[01:00<00:11, 19.72it/s]', '\\rOverwrite epoch 2/4:  37%|###7      | 129/345\n[01:00<00:10, 20.30it/s]', '\\rOverwrite epoch 2/4:  38%|###8      | 132/345\n[01:00<00:10, 20.72it/s]', '\\rOverwrite epoch 2/4:  39%|###9      | 135/345\n[01:00<00:10, 20.98it/s]', '\\rOverwrite epoch 2/4:  40%|####      | 138/345\n[01:00<00:09, 20.87it/s]', '\\rOverwrite epoch 2/4:  41%|####      | 141/345\n[01:00<00:09, 21.08it/s]', '\\rOverwrite epoch 2/4:  42%|####1     | 144/345\n[01:00<00:09, 21.31it/s]', '\\rOverwrite epoch 2/4:  43%|####2     | 147/345\n[01:01<00:09, 21.48it/s]', '\\rOverwrite epoch 2/4:  43%|####3     | 150/345\n[01:01<00:09, 21.60it/s]', '\\rOverwrite epoch 2/4:  44%|####4     | 153/345\n[01:01<00:08, 21.70it/s]', '\\rOverwrite epoch 2/4:  45%|####5     | 156/345\n[01:01<00:08, 21.74it/s]', '\\rOverwrite epoch 2/4:  46%|####6     | 159/345\n[01:01<00:08, 21.73it/s]', '\\rOverwrite epoch 2/4:  47%|####6     | 162/345\n[01:01<00:08, 21.65it/s]', '\\rOverwrite epoch 2/4:  48%|####7     | 165/345\n[01:01<00:08, 21.57it/s]', '\\rOverwrite epoch 2/4:  49%|####8     | 168/345\n[01:01<00:08, 21.68it/s]', '\\rOverwrite epoch 2/4:  50%|####9     | 171/345\n[01:02<00:08, 21.74it/s]', '\\rOverwrite epoch 2/4:  50%|#####     | 174/345\n[01:02<00:07, 21.78it/s]', '\\rOverwrite epoch 2/4:  51%|#####1    | 177/345\n[01:02<00:07, 21.74it/s]', '\\rOverwrite epoch 2/4:  52%|#####2    | 180/345\n[01:02<00:07, 21.67it/s]', '\\rOverwrite epoch 2/4:  53%|#####3    | 183/345\n[01:02<00:07, 21.56it/s]', '\\rOverwrite epoch 2/4:  54%|#####3    | 186/345\n[01:02<00:07, 21.56it/s]', '\\rOverwrite epoch 2/4:  55%|#####4    | 189/345\n[01:02<00:07, 21.60it/s]', '\\rOverwrite epoch 2/4:  56%|#####5    | 192/345\n[01:03<00:07, 21.67it/s]', '\\rOverwrite epoch 2/4:  57%|#####6    | 195/345\n[01:03<00:06, 21.72it/s]', '\\rOverwrite epoch 2/4:  57%|#####7    | 198/345\n[01:03<00:06, 21.76it/s]', '\\rOverwrite epoch 2/4:  58%|#####8    | 201/345\n[01:03<00:06, 21.80it/s]', '\\rOverwrite epoch 2/4:  59%|#####9    | 204/345\n[01:03<00:06, 21.84it/s]', '\\rOverwrite epoch 2/4:  60%|######    | 207/345\n[01:03<00:06, 21.86it/s]', '\\rOverwrite epoch 2/4:  61%|######    | 210/345\n[01:03<00:06, 21.87it/s]', '\\rOverwrite epoch 2/4:  62%|######1   | 213/345\n[01:04<00:06, 21.65it/s]', '\\rOverwrite epoch 2/4:  63%|######2   | 216/345\n[01:04<00:06, 21.49it/s]', '\\rOverwrite epoch 2/4:  63%|######3   | 219/345\n[01:04<00:05, 21.50it/s]', '\\rOverwrite epoch 2/4:  64%|######4   | 222/345\n[01:04<00:05, 21.54it/s]', '\\rOverwrite epoch 2/4:  65%|######5   | 225/345\n[01:04<00:05, 21.53it/s]', '\\rOverwrite epoch 2/4:  66%|######6   | 228/345\n[01:04<00:05, 21.58it/s]', '\\rOverwrite epoch 2/4:  67%|######6   | 231/345\n[01:04<00:05, 21.62it/s]', '\\rOverwrite epoch 2/4:  68%|######7   | 234/345\n[01:05<00:05, 21.60it/s]', '\\rOverwrite epoch 2/4:  69%|######8   | 237/345\n[01:05<00:05, 21.57it/s]', '\\rOverwrite epoch 2/4:  70%|######9   | 240/345\n[01:05<00:04, 21.57it/s]', '\\rOverwrite epoch 2/4:  70%|#######   | 243/345\n[01:05<00:04, 21.63it/s]', '\\rOverwrite epoch 2/4:  71%|#######1  | 246/345\n[01:05<00:04, 21.61it/s]', '\\rOverwrite epoch 2/4:  72%|#######2  | 249/345\n[01:05<00:04, 21.59it/s]', '\\rOverwrite epoch 2/4:  73%|#######3  | 252/345\n[01:05<00:04, 21.62it/s]', '[2025-12-03 19:06:30] Overwrite step 600:\navg_train_loss=3.3203', '\\n', '\\rOverwrite epoch 2/4:  74%|#######3  | 255/345\n[01:06<00:04, 21.65it/s]', '\\rOverwrite epoch 2/4:  75%|#######4  | 258/345\n[01:06<00:04, 21.59it/s]', '\\rOverwrite epoch 2/4:  76%|#######5  | 261/345\n[01:06<00:03, 21.60it/s]', '\\rOverwrite epoch 2/4:  77%|#######6  | 264/345\n[01:06<00:03, 21.47it/s]', '\\rOverwrite epoch 2/4:  77%|#######7  | 267/345\n[01:06<00:03, 21.49it/s]', '\\rOverwrite epoch 2/4:  78%|#######8  | 270/345\n[01:06<00:03, 21.61it/s]', '\\rOverwrite epoch 2/4:  79%|#######9  | 273/345\n[01:06<00:03, 21.72it/s]', '\\rOverwrite epoch 2/4:  80%|########  | 276/345\n[01:06<00:03, 21.77it/s]', '\\rOverwrite epoch 2/4:  81%|########  | 279/345\n[01:07<00:03, 21.63it/s]', '\\rOverwrite epoch 2/4:  82%|########1 | 282/345\n[01:07<00:02, 21.37it/s]', '\\rOverwrite epoch 2/4:  83%|########2 | 285/345\n[01:07<00:02, 20.88it/s]', '\\rOverwrite epoch 2/4:  83%|########3 | 288/345\n[01:07<00:02, 20.62it/s]', '\\rOverwrite epoch 2/4:  84%|########4 | 291/345\n[01:07<00:02, 20.64it/s]', '\\rOverwrite epoch 2/4:  85%|########5 | 294/345\n[01:07<00:02, 20.87it/s]', '\\rOverwrite epoch 2/4:  86%|########6 | 297/345\n[01:07<00:02, 21.07it/s]', '\\rOverwrite epoch 2/4:  87%|########6 | 300/345\n[01:08<00:02, 21.23it/s]', '\\rOverwrite epoch 2/4:  88%|########7 | 303/345\n[01:08<00:01, 21.33it/s]', '\\rOverwrite epoch 2/4:  89%|########8 | 306/345\n[01:08<00:01, 21.42it/s]', '\\rOverwrite epoch 2/4:  90%|########9 | 309/345\n[01:08<00:01, 21.47it/s]', '\\rOverwrite epoch 2/4:  90%|######### | 312/345\n[01:08<00:01, 21.49it/s]', '\\rOverwrite epoch 2/4:  91%|#########1| 315/345\n[01:08<00:01, 21.48it/s]', '\\rOverwrite epoch 2/4:  92%|#########2| 318/345\n[01:08<00:01, 21.49it/s]', '\\rOverwrite epoch 2/4:  93%|#########3| 321/345\n[01:09<00:01, 21.25it/s]', '\\rOverwrite epoch 2/4:  94%|#########3| 324/345\n[01:09<00:00, 21.31it/s]', '\\rOverwrite epoch 2/4:  95%|#########4| 327/345\n[01:09<00:00, 21.35it/s]', '\\rOverwrite epoch 2/4:  96%|#########5| 330/345\n[01:09<00:00, 21.45it/s]', '\\rOverwrite epoch 2/4:  97%|#########6| 333/345\n[01:09<00:00, 21.54it/s]', '\\rOverwrite epoch 2/4:  97%|#########7| 336/345\n[01:09<00:00, 21.59it/s]', '\\rOverwrite epoch 2/4:  98%|#########8| 339/345\n[01:09<00:00, 21.62it/s]', '\\rOverwrite epoch 2/4:  99%|#########9| 342/345\n[01:10<00:00, 21.59it/s]', '\\rOverwrite epoch 2/4: 100%|##########| 345/345\n[01:10<00:00, 22.64it/s]', '', '\\rOverwrite epoch 2/4: 100%|##########| 345/345\n[01:11<00:00,  4.84it/s]', '\\n', 'Epoch 2: validation_loss = 3.6392', '\\n',\n'\\rOverwrite epoch 3/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 3/4:   0%|          | 1/345 [00:56<5:25:53, 56.84s/it]', '\\rOverwrite\nepoch 3/4:   1%|          | 3/345 [00:56<1:24:16, 14.79s/it]', '\\rOverwrite\nepoch 3/4:   2%|1         | 6/345 [00:57<32:26,  5.74s/it]  ', '\\rOverwrite\nepoch 3/4:   3%|2         | 9/345 [00:57<17:15,  3.08s/it]', '\\rOverwrite epoch\n3/4:   3%|3         | 12/345 [00:57<10:21,  1.87s/it]', '\\rOverwrite epoch 3/4:\n4%|4         | 15/345 [00:57<06:37,  1.20s/it]', '\\rOverwrite epoch 3/4:   5%|5\n| 18/345 [00:57<04:24,  1.24it/s]', '\\rOverwrite epoch 3/4:   6%|6         |\n21/345 [00:57<03:00,  1.79it/s]', '\\rOverwrite epoch 3/4:   7%|6         |\n24/345 [00:57<02:06,  2.53it/s]', '\\rOverwrite epoch 3/4:   8%|7         |\n27/345 [00:58<01:30,  3.50it/s]', '\\rOverwrite epoch 3/4:   9%|8         |\n30/345 [00:58<01:06,  4.73it/s]', '\\rOverwrite epoch 3/4:  10%|9         |\n33/345 [00:58<00:50,  6.21it/s]', '\\rOverwrite epoch 3/4:  10%|#         |\n36/345 [00:58<00:39,  7.85it/s]', '\\rOverwrite epoch 3/4:  11%|#1        |\n39/345 [00:58<00:31,  9.71it/s]', '\\rOverwrite epoch 3/4:  12%|#2        |\n42/345 [00:58<00:25, 11.66it/s]', '\\rOverwrite epoch 3/4:  13%|#3        |\n45/345 [00:58<00:22, 13.53it/s]', '\\rOverwrite epoch 3/4:  14%|#3        |\n48/345 [00:59<00:19, 15.27it/s]', '\\rOverwrite epoch 3/4:  15%|#4        |\n51/345 [00:59<00:17, 16.80it/s]', '\\rOverwrite epoch 3/4:  16%|#5        |\n54/345 [00:59<00:16, 17.99it/s]', '\\rOverwrite epoch 3/4:  17%|#6        |\n57/345 [00:59<00:15, 18.99it/s]', '\\rOverwrite epoch 3/4:  17%|#7        |\n60/345 [00:59<00:14, 19.64it/s]', '\\rOverwrite epoch 3/4:  18%|#8        |\n63/345 [00:59<00:14, 19.81it/s]', '\\rOverwrite epoch 3/4:  19%|#9        |\n66/345 [00:59<00:13, 20.02it/s]', '\\rOverwrite epoch 3/4:  20%|##        |\n69/345 [01:00<00:13, 20.10it/s]', '\\rOverwrite epoch 3/4:  21%|##        |\n72/345 [01:00<00:13, 20.28it/s]', '\\rOverwrite epoch 3/4:  22%|##1       |\n75/345 [01:00<00:13, 20.68it/s]', '\\rOverwrite epoch 3/4:  23%|##2       |\n78/345 [01:00<00:12, 21.03it/s]', '\\rOverwrite epoch 3/4:  23%|##3       |\n81/345 [01:00<00:12, 21.29it/s]', '\\rOverwrite epoch 3/4:  24%|##4       |\n84/345 [01:00<00:12, 21.47it/s]', '\\rOverwrite epoch 3/4:  25%|##5       |\n87/345 [01:00<00:12, 21.48it/s]', '\\rOverwrite epoch 3/4:  26%|##6       |\n90/345 [01:01<00:11, 21.47it/s]', '\\rOverwrite epoch 3/4:  27%|##6       |\n93/345 [01:01<00:11, 21.53it/s]', '\\rOverwrite epoch 3/4:  28%|##7       |\n96/345 [01:01<00:11, 21.53it/s]', '\\rOverwrite epoch 3/4:  29%|##8       |\n99/345 [01:01<00:11, 21.54it/s]', '\\rOverwrite epoch 3/4:  30%|##9       |\n102/345 [01:01<00:11, 21.51it/s]', '\\rOverwrite epoch 3/4:  30%|###       |\n105/345 [01:01<00:11, 21.50it/s]', '\\rOverwrite epoch 3/4:  31%|###1      |\n108/345 [01:01<00:11, 21.34it/s]', '[2025-12-03 19:08:36] Overwrite step 800:\navg_train_loss=3.0696', '\\n', '\\rOverwrite epoch 3/4:  32%|###2      | 111/345\n[01:02<00:11, 21.26it/s]', '\\rOverwrite epoch 3/4:  33%|###3      | 114/345\n[01:02<00:10, 21.02it/s]', '\\rOverwrite epoch 3/4:  34%|###3      | 117/345\n[01:02<00:10, 21.19it/s]', '\\rOverwrite epoch 3/4:  35%|###4      | 120/345\n[01:02<00:10, 21.39it/s]', '\\rOverwrite epoch 3/4:  36%|###5      | 123/345\n[01:02<00:10, 21.52it/s]', '\\rOverwrite epoch 3/4:  37%|###6      | 126/345\n[01:02<00:10, 21.59it/s]', '\\rOverwrite epoch 3/4:  37%|###7      | 129/345\n[01:02<00:10, 21.27it/s]', '\\rOverwrite epoch 3/4:  38%|###8      | 132/345\n[01:02<00:09, 21.45it/s]', '\\rOverwrite epoch 3/4:  39%|###9      | 135/345\n[01:03<00:09, 21.57it/s]', '\\rOverwrite epoch 3/4:  40%|####      | 138/345\n[01:03<00:09, 21.64it/s]', '\\rOverwrite epoch 3/4:  41%|####      | 141/345\n[01:03<00:09, 21.71it/s]', '\\rOverwrite epoch 3/4:  42%|####1     | 144/345\n[01:03<00:09, 21.72it/s]', '\\rOverwrite epoch 3/4:  43%|####2     | 147/345\n[01:03<00:09, 21.75it/s]', '\\rOverwrite epoch 3/4:  43%|####3     | 150/345\n[01:03<00:08, 21.80it/s]', '\\rOverwrite epoch 3/4:  44%|####4     | 153/345\n[01:03<00:08, 21.81it/s]', '\\rOverwrite epoch 3/4:  45%|####5     | 156/345\n[01:04<00:08, 21.79it/s]', '\\rOverwrite epoch 3/4:  46%|####6     | 159/345\n[01:04<00:08, 21.72it/s]', '\\rOverwrite epoch 3/4:  47%|####6     | 162/345\n[01:04<00:08, 21.76it/s]', '\\rOverwrite epoch 3/4:  48%|####7     | 165/345\n[01:04<00:08, 21.81it/s]', '\\rOverwrite epoch 3/4:  49%|####8     | 168/345\n[01:04<00:08, 21.85it/s]', '\\rOverwrite epoch 3/4:  50%|####9     | 171/345\n[01:04<00:07, 21.84it/s]', '\\rOverwrite epoch 3/4:  50%|#####     | 174/345\n[01:04<00:07, 21.84it/s]', '\\rOverwrite epoch 3/4:  51%|#####1    | 177/345\n[01:05<00:07, 21.81it/s]', '\\rOverwrite epoch 3/4:  52%|#####2    | 180/345\n[01:05<00:07, 21.78it/s]', '\\rOverwrite epoch 3/4:  53%|#####3    | 183/345\n[01:05<00:07, 21.78it/s]', '\\rOverwrite epoch 3/4:  54%|#####3    | 186/345\n[01:05<00:07, 21.80it/s]', '\\rOverwrite epoch 3/4:  55%|#####4    | 189/345\n[01:05<00:07, 21.82it/s]', '\\rOverwrite epoch 3/4:  56%|#####5    | 192/345\n[01:05<00:06, 21.89it/s]', '\\rOverwrite epoch 3/4:  57%|#####6    | 195/345\n[01:05<00:06, 21.91it/s]', '\\rOverwrite epoch 3/4:  57%|#####7    | 198/345\n[01:06<00:06, 21.94it/s]', '\\rOverwrite epoch 3/4:  58%|#####8    | 201/345\n[01:06<00:06, 21.82it/s]', '\\rOverwrite epoch 3/4:  59%|#####9    | 204/345\n[01:06<00:06, 21.67it/s]', '\\rOverwrite epoch 3/4:  60%|######    | 207/345\n[01:06<00:06, 21.70it/s]', '\\rOverwrite epoch 3/4:  61%|######    | 210/345\n[01:06<00:06, 21.74it/s]', '\\rOverwrite epoch 3/4:  62%|######1   | 213/345\n[01:06<00:06, 21.79it/s]', '\\rOverwrite epoch 3/4:  63%|######2   | 216/345\n[01:06<00:05, 21.83it/s]', '\\rOverwrite epoch 3/4:  63%|######3   | 219/345\n[01:06<00:05, 21.73it/s]', '\\rOverwrite epoch 3/4:  64%|######4   | 222/345\n[01:07<00:05, 21.76it/s]', '\\rOverwrite epoch 3/4:  65%|######5   | 225/345\n[01:07<00:05, 21.79it/s]', '\\rOverwrite epoch 3/4:  66%|######6   | 228/345\n[01:07<00:05, 21.77it/s]', '\\rOverwrite epoch 3/4:  67%|######6   | 231/345\n[01:07<00:05, 21.82it/s]', '\\rOverwrite epoch 3/4:  68%|######7   | 234/345\n[01:07<00:05, 21.83it/s]', '\\rOverwrite epoch 3/4:  69%|######8   | 237/345\n[01:07<00:04, 21.85it/s]', '\\rOverwrite epoch 3/4:  70%|######9   | 240/345\n[01:07<00:04, 21.88it/s]', '\\rOverwrite epoch 3/4:  70%|#######   | 243/345\n[01:08<00:04, 21.67it/s]', '\\rOverwrite epoch 3/4:  71%|#######1  | 246/345\n[01:08<00:04, 21.62it/s]', '\\rOverwrite epoch 3/4:  72%|#######2  | 249/345\n[01:08<00:04, 21.64it/s]', '\\rOverwrite epoch 3/4:  73%|#######3  | 252/345\n[01:08<00:04, 21.68it/s]', '\\rOverwrite epoch 3/4:  74%|#######3  | 255/345\n[01:08<00:04, 21.71it/s]', '\\rOverwrite epoch 3/4:  75%|#######4  | 258/345\n[01:08<00:04, 21.74it/s]', '\\rOverwrite epoch 3/4:  76%|#######5  | 261/345\n[01:08<00:03, 21.72it/s]', '\\rOverwrite epoch 3/4:  77%|#######6  | 264/345\n[01:09<00:03, 21.70it/s]', '\\rOverwrite epoch 3/4:  77%|#######7  | 267/345\n[01:09<00:03, 21.60it/s]', '\\rOverwrite epoch 3/4:  78%|#######8  | 270/345\n[01:09<00:03, 21.64it/s]', '\\rOverwrite epoch 3/4:  79%|#######9  | 273/345\n[01:09<00:03, 21.65it/s]', '\\rOverwrite epoch 3/4:  80%|########  | 276/345\n[01:09<00:03, 21.68it/s]', '\\rOverwrite epoch 3/4:  81%|########  | 279/345\n[01:09<00:03, 21.70it/s]', '\\rOverwrite epoch 3/4:  82%|########1 | 282/345\n[01:09<00:02, 21.73it/s]', '\\rOverwrite epoch 3/4:  83%|########2 | 285/345\n[01:10<00:02, 21.73it/s]', '\\rOverwrite epoch 3/4:  83%|########3 | 288/345\n[01:10<00:02, 21.48it/s]', '\\rOverwrite epoch 3/4:  84%|########4 | 291/345\n[01:10<00:02, 21.49it/s]', '\\rOverwrite epoch 3/4:  85%|########5 | 294/345\n[01:10<00:02, 21.52it/s]', '\\rOverwrite epoch 3/4:  86%|########6 | 297/345\n[01:10<00:02, 21.63it/s]', '\\rOverwrite epoch 3/4:  87%|########6 | 300/345\n[01:10<00:02, 21.68it/s]', '\\rOverwrite epoch 3/4:  88%|########7 | 303/345\n[01:10<00:01, 21.73it/s]', '\\rOverwrite epoch 3/4:  89%|########8 | 306/345\n[01:10<00:01, 21.77it/s]', '\\rOverwrite epoch 3/4:  90%|########9 | 309/345\n[01:11<00:01, 21.82it/s]', '[2025-12-03 19:08:45] Overwrite step 1000:\navg_train_loss=3.0832', '\\n', '\\rOverwrite epoch 3/4:  90%|######### | 312/345\n[01:11<00:01, 21.71it/s]', '\\rOverwrite epoch 3/4:  91%|#########1| 315/345\n[01:11<00:01, 21.74it/s]', '\\rOverwrite epoch 3/4:  92%|#########2| 318/345\n[01:11<00:01, 21.78it/s]', '\\rOverwrite epoch 3/4:  93%|#########3| 321/345\n[01:11<00:01, 21.73it/s]', '\\rOverwrite epoch 3/4:  94%|#########3| 324/345\n[01:11<00:00, 21.78it/s]', '\\rOverwrite epoch 3/4:  95%|#########4| 327/345\n[01:11<00:00, 21.77it/s]', '\\rOverwrite epoch 3/4:  96%|#########5| 330/345\n[01:12<00:00, 21.76it/s]', '\\rOverwrite epoch 3/4:  97%|#########6| 333/345\n[01:12<00:00, 21.80it/s]', '\\rOverwrite epoch 3/4:  97%|#########7| 336/345\n[01:12<00:00, 21.79it/s]', '\\rOverwrite epoch 3/4:  98%|#########8| 339/345\n[01:12<00:00, 21.81it/s]', '\\rOverwrite epoch 3/4:  99%|#########9| 342/345\n[01:12<00:00, 21.81it/s]', '\\rOverwrite epoch 3/4: 100%|##########| 345/345\n[01:12<00:00, 22.81it/s]', '', '\\rOverwrite epoch 3/4: 100%|##########| 345/345\n[01:13<00:00,  4.68it/s]', '\\n', 'Epoch 3: validation_loss = 3.6716', '\\n',\n'\\rOverwrite epoch 4/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 4/4:   0%|          | 1/345 [00:53<5:05:31, 53.29s/it]', '\\rOverwrite\nepoch 4/4:   1%|          | 3/345 [00:53<1:19:01, 13.86s/it]', '\\rOverwrite\nepoch 4/4:   2%|1         | 6/345 [00:53<30:26,  5.39s/it]  ', '\\rOverwrite\nepoch 4/4:   3%|2         | 9/345 [00:53<16:11,  2.89s/it]', '\\rOverwrite epoch\n4/4:   3%|3         | 12/345 [00:53<09:43,  1.75s/it]', '\\rOverwrite epoch 4/4:\n4%|4         | 15/345 [00:53<06:13,  1.13s/it]', '\\rOverwrite epoch 4/4:   5%|5\n| 18/345 [00:54<04:08,  1.31it/s]', '\\rOverwrite epoch 4/4:   6%|6         |\n21/345 [00:54<02:50,  1.90it/s]', '\\rOverwrite epoch 4/4:   7%|6         |\n24/345 [00:54<01:59,  2.68it/s]', '\\rOverwrite epoch 4/4:   8%|7         |\n27/345 [00:54<01:25,  3.70it/s]', '\\rOverwrite epoch 4/4:   9%|8         |\n30/345 [00:54<01:03,  4.98it/s]', '\\rOverwrite epoch 4/4:  10%|9         |\n33/345 [00:54<00:47,  6.51it/s]', '\\rOverwrite epoch 4/4:  10%|#         |\n36/345 [00:54<00:37,  8.28it/s]', '\\rOverwrite epoch 4/4:  11%|#1        |\n39/345 [00:55<00:29, 10.20it/s]', '\\rOverwrite epoch 4/4:  12%|#2        |\n42/345 [00:55<00:24, 12.16it/s]', '\\rOverwrite epoch 4/4:  13%|#3        |\n45/345 [00:55<00:21, 14.05it/s]', '\\rOverwrite epoch 4/4:  14%|#3        |\n48/345 [00:55<00:18, 15.77it/s]', '\\rOverwrite epoch 4/4:  15%|#4        |\n51/345 [00:55<00:17, 17.21it/s]', '\\rOverwrite epoch 4/4:  16%|#5        |\n54/345 [00:55<00:15, 18.39it/s]', '\\rOverwrite epoch 4/4:  17%|#6        |\n57/345 [00:55<00:14, 19.31it/s]', '\\rOverwrite epoch 4/4:  17%|#7        |\n60/345 [00:56<00:14, 19.93it/s]', '\\rOverwrite epoch 4/4:  18%|#8        |\n63/345 [00:56<00:13, 20.41it/s]', '\\rOverwrite epoch 4/4:  19%|#9        |\n66/345 [00:56<00:13, 20.80it/s]', '\\rOverwrite epoch 4/4:  20%|##        |\n69/345 [00:56<00:13, 21.05it/s]', '\\rOverwrite epoch 4/4:  21%|##        |\n72/345 [00:56<00:12, 21.26it/s]', '\\rOverwrite epoch 4/4:  22%|##1       |\n75/345 [00:56<00:12, 21.42it/s]', '\\rOverwrite epoch 4/4:  23%|##2       |\n78/345 [00:56<00:12, 21.54it/s]', '\\rOverwrite epoch 4/4:  23%|##3       |\n81/345 [00:56<00:12, 21.68it/s]', '\\rOverwrite epoch 4/4:  24%|##4       |\n84/345 [00:57<00:11, 21.78it/s]', '\\rOverwrite epoch 4/4:  25%|##5       |\n87/345 [00:57<00:11, 21.82it/s]', '\\rOverwrite epoch 4/4:  26%|##6       |\n90/345 [00:57<00:11, 21.87it/s]', '\\rOverwrite epoch 4/4:  27%|##6       |\n93/345 [00:57<00:11, 21.88it/s]', '\\rOverwrite epoch 4/4:  28%|##7       |\n96/345 [00:57<00:11, 21.91it/s]', '\\rOverwrite epoch 4/4:  29%|##8       |\n99/345 [00:57<00:11, 21.78it/s]', '\\rOverwrite epoch 4/4:  30%|##9       |\n102/345 [00:57<00:11, 21.54it/s]', '\\rOverwrite epoch 4/4:  30%|###       |\n105/345 [00:58<00:11, 21.46it/s]', '\\rOverwrite epoch 4/4:  31%|###1      |\n108/345 [00:58<00:10, 21.59it/s]', '\\rOverwrite epoch 4/4:  32%|###2      |\n111/345 [00:58<00:10, 21.67it/s]', '\\rOverwrite epoch 4/4:  33%|###3      |\n114/345 [00:58<00:10, 21.71it/s]', '\\rOverwrite epoch 4/4:  34%|###3      |\n117/345 [00:58<00:10, 21.62it/s]', '\\rOverwrite epoch 4/4:  35%|###4      |\n120/345 [00:58<00:10, 21.70it/s]', '\\rOverwrite epoch 4/4:  36%|###5      |\n123/345 [00:58<00:10, 21.73it/s]', '\\rOverwrite epoch 4/4:  37%|###6      |\n126/345 [00:59<00:10, 21.78it/s]', '\\rOverwrite epoch 4/4:  37%|###7      |\n129/345 [00:59<00:09, 21.78it/s]', '\\rOverwrite epoch 4/4:  38%|###8      |\n132/345 [00:59<00:09, 21.79it/s]', '\\rOverwrite epoch 4/4:  39%|###9      |\n135/345 [00:59<00:09, 21.82it/s]', '\\rOverwrite epoch 4/4:  40%|####      |\n138/345 [00:59<00:09, 21.67it/s]', '\\rOverwrite epoch 4/4:  41%|####      |\n141/345 [00:59<00:09, 21.73it/s]', '\\rOverwrite epoch 4/4:  42%|####1     |\n144/345 [00:59<00:09, 21.78it/s]', '\\rOverwrite epoch 4/4:  43%|####2     |\n147/345 [01:00<00:09, 21.27it/s]', '\\rOverwrite epoch 4/4:  43%|####3     |\n150/345 [01:00<00:09, 21.37it/s]', '\\rOverwrite epoch 4/4:  44%|####4     |\n153/345 [01:00<00:08, 21.41it/s]', '\\rOverwrite epoch 4/4:  45%|####5     |\n156/345 [01:00<00:08, 21.45it/s]', '\\rOverwrite epoch 4/4:  46%|####6     |\n159/345 [01:00<00:08, 21.52it/s]', '\\rOverwrite epoch 4/4:  47%|####6     |\n162/345 [01:00<00:08, 21.58it/s]', '[2025-12-03 19:10:48] Overwrite step 1200:\navg_train_loss=2.8707', '\\n', '\\rOverwrite epoch 4/4:  48%|####7     | 165/345\n[01:00<00:08, 21.62it/s]', '\\rOverwrite epoch 4/4:  49%|####8     | 168/345\n[01:01<00:08, 21.66it/s]', '\\rOverwrite epoch 4/4:  50%|####9     | 171/345\n[01:01<00:08, 21.63it/s]', '\\rOverwrite epoch 4/4:  50%|#####     | 174/345\n[01:01<00:07, 21.63it/s]', '\\rOverwrite epoch 4/4:  51%|#####1    | 177/345\n[01:01<00:07, 21.65it/s]', '\\rOverwrite epoch 4/4:  52%|#####2    | 180/345\n[01:01<00:07, 21.65it/s]', '\\rOverwrite epoch 4/4:  53%|#####3    | 183/345\n[01:01<00:07, 21.66it/s]', '\\rOverwrite epoch 4/4:  54%|#####3    | 186/345\n[01:01<00:07, 21.68it/s]', '\\rOverwrite epoch 4/4:  55%|#####4    | 189/345\n[01:01<00:07, 21.67it/s]', '\\rOverwrite epoch 4/4:  56%|#####5    | 192/345\n[01:02<00:07, 21.70it/s]', '\\rOverwrite epoch 4/4:  57%|#####6    | 195/345\n[01:02<00:06, 21.69it/s]', '\\rOverwrite epoch 4/4:  57%|#####7    | 198/345\n[01:02<00:06, 21.72it/s]', '\\rOverwrite epoch 4/4:  58%|#####8    | 201/345\n[01:02<00:06, 21.71it/s]', '\\rOverwrite epoch 4/4:  59%|#####9    | 204/345\n[01:02<00:06, 21.71it/s]', '\\rOverwrite epoch 4/4:  60%|######    | 207/345\n[01:02<00:06, 21.70it/s]', '\\rOverwrite epoch 4/4:  61%|######    | 210/345\n[01:02<00:06, 21.70it/s]', '\\rOverwrite epoch 4/4:  62%|######1   | 213/345\n[01:03<00:06, 21.69it/s]', '\\rOverwrite epoch 4/4:  63%|######2   | 216/345\n[01:03<00:05, 21.69it/s]', '\\rOverwrite epoch 4/4:  63%|######3   | 219/345\n[01:03<00:05, 21.63it/s]', '\\rOverwrite epoch 4/4:  64%|######4   | 222/345\n[01:03<00:05, 21.64it/s]', '\\rOverwrite epoch 4/4:  65%|######5   | 225/345\n[01:03<00:05, 21.63it/s]', '\\rOverwrite epoch 4/4:  66%|######6   | 228/345\n[01:03<00:05, 21.63it/s]', '\\rOverwrite epoch 4/4:  67%|######6   | 231/345\n[01:03<00:05, 21.64it/s]', '\\rOverwrite epoch 4/4:  68%|######7   | 234/345\n[01:04<00:05, 21.65it/s]', '\\rOverwrite epoch 4/4:  69%|######8   | 237/345\n[01:04<00:04, 21.65it/s]', '\\rOverwrite epoch 4/4:  70%|######9   | 240/345\n[01:04<00:04, 21.56it/s]', '\\rOverwrite epoch 4/4:  70%|#######   | 243/345\n[01:04<00:04, 21.60it/s]', '\\rOverwrite epoch 4/4:  71%|#######1  | 246/345\n[01:04<00:04, 21.23it/s]', '\\rOverwrite epoch 4/4:  72%|#######2  | 249/345\n[01:04<00:04, 21.29it/s]', '\\rOverwrite epoch 4/4:  73%|#######3  | 252/345\n[01:04<00:04, 21.48it/s]', '\\rOverwrite epoch 4/4:  74%|#######3  | 255/345\n[01:05<00:04, 21.65it/s]', '\\rOverwrite epoch 4/4:  75%|#######4  | 258/345\n[01:05<00:03, 21.77it/s]', '\\rOverwrite epoch 4/4:  76%|#######5  | 261/345\n[01:05<00:03, 21.83it/s]', '\\rOverwrite epoch 4/4:  77%|#######6  | 264/345\n[01:05<00:03, 21.86it/s]', '\\rOverwrite epoch 4/4:  77%|#######7  | 267/345\n[01:05<00:03, 21.90it/s]', '\\rOverwrite epoch 4/4:  78%|#######8  | 270/345\n[01:05<00:03, 21.85it/s]', '\\rOverwrite epoch 4/4:  79%|#######9  | 273/345\n[01:05<00:03, 21.86it/s]', '\\rOverwrite epoch 4/4:  80%|########  | 276/345\n[01:05<00:03, 21.85it/s]', '\\rOverwrite epoch 4/4:  81%|########  | 279/345\n[01:06<00:03, 21.76it/s]', '\\rOverwrite epoch 4/4:  82%|########1 | 282/345\n[01:06<00:02, 21.77it/s]', '\\rOverwrite epoch 4/4:  83%|########2 | 285/345\n[01:06<00:02, 21.85it/s]', '\\rOverwrite epoch 4/4:  83%|########3 | 288/345\n[01:06<00:02, 21.91it/s]', '\\rOverwrite epoch 4/4:  84%|########4 | 291/345\n[01:06<00:02, 21.97it/s]', '\\rOverwrite epoch 4/4:  85%|########5 | 294/345\n[01:06<00:02, 21.83it/s]', '\\rOverwrite epoch 4/4:  86%|########6 | 297/345\n[01:06<00:02, 21.81it/s]', '\\rOverwrite epoch 4/4:  87%|########6 | 300/345\n[01:07<00:02, 21.84it/s]', '\\rOverwrite epoch 4/4:  88%|########7 | 303/345\n[01:07<00:01, 21.85it/s]', '\\rOverwrite epoch 4/4:  89%|########8 | 306/345\n[01:07<00:01, 21.85it/s]', '\\rOverwrite epoch 4/4:  90%|########9 | 309/345\n[01:07<00:01, 21.87it/s]', '\\rOverwrite epoch 4/4:  90%|######### | 312/345\n[01:07<00:01, 21.85it/s]', '\\rOverwrite epoch 4/4:  91%|#########1| 315/345\n[01:07<00:01, 21.84it/s]', '\\rOverwrite epoch 4/4:  92%|#########2| 318/345\n[01:07<00:01, 21.89it/s]', '\\rOverwrite epoch 4/4:  93%|#########3| 321/345\n[01:08<00:01, 21.93it/s]', '\\rOverwrite epoch 4/4:  94%|#########3| 324/345\n[01:08<00:00, 21.96it/s]', '\\rOverwrite epoch 4/4:  95%|#########4| 327/345\n[01:08<00:00, 21.87it/s]', '\\rOverwrite epoch 4/4:  96%|#########5| 330/345\n[01:08<00:00, 21.82it/s]', '\\rOverwrite epoch 4/4:  97%|#########6| 333/345\n[01:08<00:00, 21.80it/s]', '\\rOverwrite epoch 4/4:  97%|#########7| 336/345\n[01:08<00:00, 21.79it/s]', '\\rOverwrite epoch 4/4:  98%|#########8| 339/345\n[01:08<00:00, 21.70it/s]', '\\rOverwrite epoch 4/4:  99%|#########9| 342/345\n[01:09<00:00, 21.31it/s]', '\\rOverwrite epoch 4/4: 100%|##########| 345/345\n[01:09<00:00, 22.29it/s]', '', '\\rOverwrite epoch 4/4: 100%|##########| 345/345\n[01:10<00:00,  4.91it/s]', '\\n', 'Epoch 4: validation_loss = 3.7202', '\\n',\n'Experiment complete. Artifacts saved to:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-3/working',\n'\\n', 'Execution time: 11 minutes seconds (time limit is 2 hours).']", "['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls: [' apple', '\ntable', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          | 0/2250\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2250/2250 [00:00<00:00,\n23485.75 examples/s]', '\\n', '\\rMap:   0%|          | 0/300 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00, 34029.02\nexamples/s]', '\\n', '\\rTraining synthetic_injection epoch 1/1:   0%|          |\n0/24 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 1/1:   4%|4\n| 1/24 [00:54<20:43, 54.08s/it]', '\\rTraining synthetic_injection epoch 1/1:\n8%|8         | 2/24 [00:54<08:11, 22.34s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  12%|#2        | 3/24 [00:54<04:16, 12.19s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  17%|#6        | 4/24 [00:54<02:28,  7.42s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  21%|##        | 5/24 [00:54<01:30,\n4.79s/it]', '\\rTraining synthetic_injection epoch 1/1:  25%|##5       | 6/24\n[00:54<00:57,  3.20s/it]', '\\rTraining synthetic_injection epoch 1/1:  29%|##9\n| 7/24 [00:54<00:37,  2.19s/it]', '\\rTraining synthetic_injection epoch 1/1:\n33%|###3      | 8/24 [00:54<00:24,  1.53s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  38%|###7      | 9/24 [00:55<00:16,  1.09s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  42%|####1     | 10/24 [00:55<00:11,  1.27it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  46%|####5     | 11/24 [00:55<00:07,\n1.72it/s]', '\\rTraining synthetic_injection epoch 1/1:  50%|#####     | 12/24\n[00:55<00:05,  2.27it/s]', '\\rTraining synthetic_injection epoch 1/1:\n54%|#####4    | 13/24 [00:55<00:03,  2.92it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  58%|#####8    | 14/24 [00:55<00:02,  3.65it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  62%|######2   | 15/24 [00:55<00:02,  4.42it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  67%|######6   | 16/24 [00:55<00:01,\n5.18it/s]', '\\rTraining synthetic_injection epoch 1/1:  71%|#######   | 17/24\n[00:55<00:01,  5.89it/s]', '\\rTraining synthetic_injection epoch 1/1:\n75%|#######5  | 18/24 [00:56<00:00,  6.51it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  79%|#######9  | 19/24 [00:56<00:00,  7.03it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  83%|########3 | 20/24 [00:56<00:00,  7.44it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  88%|########7 | 21/24 [00:56<00:00,\n7.74it/s]', '\\rTraining synthetic_injection epoch 1/1:  92%|#########1| 22/24\n[00:56<00:00,  7.98it/s]', '\\rTraining synthetic_injection epoch 1/1:\n96%|#########5| 23/24 [00:56<00:00,  8.17it/s]', '', '\\rTraining\nsynthetic_injection epoch 1/1: 100%|##########| 24/24 [00:57<00:00,  2.41s/it]',\n'\\n', 'Epoch 1: validation_loss = 3.3902', '\\n', '\\rMap:   0%|          | 0/7344\n[00:00<?, ? examples/s]', '\\rMap:  27%|##7       | 2000/7344 [00:00<00:00,\n17502.41 examples/s]', '\\rMap:  82%|########1 | 6000/7344 [00:00<00:00, 20566.02\nexamples/s]', '', '\\rMap: 100%|##########| 7344/7344 [00:00<00:00, 18791.68\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/3760 [00:00<?, ? examples/s]',\n'\\rMap:  80%|#######9  | 3000/3760 [00:00<00:00, 19317.14 examples/s]', '',\n'\\rMap: 100%|##########| 3760/3760 [00:00<00:00, 18012.88 examples/s]', '\\n',\n'\\roverwrite_wikitext epoch 1/2:   0%|          | 0/230 [00:00<?, ?it/s]',\n'\\roverwrite_wikitext epoch 1/2:   0%|          | 1/230 [00:56<3:35:46,\n56.54s/it]', '\\roverwrite_wikitext epoch 1/2:   1%|1         | 3/230\n[00:56<55:37, 14.70s/it]  ', '\\roverwrite_wikitext epoch 1/2:   3%|2         |\n6/230 [00:56<21:19,  5.71s/it]', '\\roverwrite_wikitext epoch 1/2:   4%|3\n| 9/230 [00:56<11:17,  3.07s/it]', '\\roverwrite_wikitext epoch 1/2:   5%|5\n| 12/230 [00:57<06:44,  1.86s/it]', '\\roverwrite_wikitext epoch 1/2:   7%|6\n| 15/230 [00:57<04:17,  1.20s/it]', '\\roverwrite_wikitext epoch 1/2:   8%|7\n| 18/230 [00:57<02:50,  1.24it/s]', '\\roverwrite_wikitext epoch 1/2:   9%|9\n| 21/230 [00:57<01:56,  1.80it/s]', '\\roverwrite_wikitext epoch 1/2:  10%|#\n| 24/230 [00:57<01:21,  2.54it/s]', '\\roverwrite_wikitext epoch 1/2:  12%|#1\n| 27/230 [00:57<00:57,  3.51it/s]', '\\roverwrite_wikitext epoch 1/2:  13%|#3\n| 30/230 [00:57<00:42,  4.73it/s]', '\\roverwrite_wikitext epoch 1/2:  14%|#4\n| 33/230 [00:58<00:31,  6.22it/s]', '\\roverwrite_wikitext epoch 1/2:  16%|#5\n| 36/230 [00:58<00:24,  7.94it/s]', '\\roverwrite_wikitext epoch 1/2:  17%|#6\n| 39/230 [00:58<00:19,  9.81it/s]', '\\roverwrite_wikitext epoch 1/2:  18%|#8\n| 42/230 [00:58<00:16, 11.75it/s]', '\\roverwrite_wikitext epoch 1/2:  20%|#9\n| 45/230 [00:58<00:13, 13.60it/s]', '\\roverwrite_wikitext epoch 1/2:  21%|##\n| 48/230 [00:58<00:11, 15.31it/s]', '\\roverwrite_wikitext epoch 1/2:  22%|##2\n| 51/230 [00:58<00:10, 16.77it/s]', '\\roverwrite_wikitext epoch 1/2:  23%|##3\n| 54/230 [00:59<00:09, 17.96it/s]', '\\roverwrite_wikitext epoch 1/2:  25%|##4\n| 57/230 [00:59<00:09, 18.89it/s]', '\\roverwrite_wikitext epoch 1/2:  26%|##6\n| 60/230 [00:59<00:08, 19.52it/s]', '\\roverwrite_wikitext epoch 1/2:  27%|##7\n| 63/230 [00:59<00:08, 20.09it/s]', '\\roverwrite_wikitext epoch 1/2:  29%|##8\n| 66/230 [00:59<00:07, 20.52it/s]', '\\roverwrite_wikitext epoch 1/2:  30%|###\n| 69/230 [00:59<00:07, 20.80it/s]', '\\roverwrite_wikitext epoch 1/2:  31%|###1\n| 72/230 [00:59<00:07, 21.00it/s]', '\\roverwrite_wikitext epoch 1/2:  33%|###2\n| 75/230 [01:00<00:07, 21.17it/s]', '\\roverwrite_wikitext epoch 1/2:  34%|###3\n| 78/230 [01:00<00:07, 21.27it/s]', '\\roverwrite_wikitext epoch 1/2:  35%|###5\n| 81/230 [01:00<00:07, 21.27it/s]', '\\roverwrite_wikitext epoch 1/2:  37%|###6\n| 84/230 [01:00<00:06, 21.33it/s]', '\\roverwrite_wikitext epoch 1/2:  38%|###7\n| 87/230 [01:00<00:06, 21.40it/s]', '\\roverwrite_wikitext epoch 1/2:  39%|###9\n| 90/230 [01:00<00:06, 21.42it/s]', '\\roverwrite_wikitext epoch 1/2:  40%|####\n| 93/230 [01:00<00:06, 21.45it/s]', '\\roverwrite_wikitext epoch 1/2:  42%|####1\n| 96/230 [01:00<00:06, 21.49it/s]', '\\roverwrite_wikitext epoch 1/2:  43%|####3\n| 99/230 [01:01<00:06, 21.27it/s]', '\\roverwrite_wikitext epoch 1/2:  44%|####4\n| 102/230 [01:01<00:06, 21.33it/s]', '\\roverwrite_wikitext epoch 1/2:  46%|####5\n| 105/230 [01:01<00:05, 21.37it/s]', '\\roverwrite_wikitext epoch 1/2:  47%|####6\n| 108/230 [01:01<00:05, 21.42it/s]', '\\roverwrite_wikitext epoch 1/2:  48%|####8\n| 111/230 [01:01<00:05, 21.46it/s]', '\\roverwrite_wikitext epoch 1/2:  50%|####9\n| 114/230 [01:01<00:05, 21.45it/s]', '\\roverwrite_wikitext epoch 1/2:  51%|#####\n| 117/230 [01:01<00:05, 21.35it/s]', '\\roverwrite_wikitext epoch 1/2:\n52%|#####2    | 120/230 [01:02<00:05, 21.37it/s]', '\\roverwrite_wikitext epoch\n1/2:  53%|#####3    | 123/230 [01:02<00:05, 21.40it/s]', '\\roverwrite_wikitext\nepoch 1/2:  55%|#####4    | 126/230 [01:02<00:04, 21.42it/s]',\n'\\roverwrite_wikitext epoch 1/2:  56%|#####6    | 129/230 [01:02<00:04,\n21.43it/s]', '\\roverwrite_wikitext epoch 1/2:  57%|#####7    | 132/230\n[01:02<00:04, 21.40it/s]', '\\roverwrite_wikitext epoch 1/2:  59%|#####8    |\n135/230 [01:02<00:04, 21.43it/s]', '\\roverwrite_wikitext epoch 1/2:  60%|######\n| 138/230 [01:02<00:04, 21.43it/s]', '\\roverwrite_wikitext epoch 1/2:\n61%|######1   | 141/230 [01:03<00:04, 21.43it/s]', '\\roverwrite_wikitext epoch\n1/2:  63%|######2   | 144/230 [01:03<00:04, 21.40it/s]', '\\roverwrite_wikitext\nepoch 1/2:  64%|######3   | 147/230 [01:03<00:03, 21.31it/s]',\n'\\roverwrite_wikitext epoch 1/2:  65%|######5   | 150/230 [01:03<00:03,\n21.34it/s]', '\\roverwrite_wikitext epoch 1/2:  67%|######6   | 153/230\n[01:03<00:03, 21.31it/s]', '\\roverwrite_wikitext epoch 1/2:  68%|######7   |\n156/230 [01:03<00:03, 21.26it/s]', '\\roverwrite_wikitext epoch 1/2:  69%|######9\n| 159/230 [01:03<00:03, 21.29it/s]', '\\roverwrite_wikitext epoch 1/2:\n70%|#######   | 162/230 [01:04<00:03, 21.32it/s]', '\\roverwrite_wikitext epoch\n1/2:  72%|#######1  | 165/230 [01:04<00:03, 21.31it/s]', '\\roverwrite_wikitext\nepoch 1/2:  73%|#######3  | 168/230 [01:04<00:02, 21.24it/s]',\n'\\roverwrite_wikitext epoch 1/2:  74%|#######4  | 171/230 [01:04<00:02,\n20.99it/s]', '\\roverwrite_wikitext epoch 1/2:  76%|#######5  | 174/230\n[01:04<00:02, 20.82it/s]', '\\roverwrite_wikitext epoch 1/2:  77%|#######6  |\n177/230 [01:04<00:02, 20.99it/s]', '\\roverwrite_wikitext epoch 1/2:\n78%|#######8  | 180/230 [01:04<00:02, 21.14it/s]', '\\roverwrite_wikitext epoch\n1/2:  80%|#######9  | 183/230 [01:05<00:02, 21.22it/s]', '\\roverwrite_wikitext\nepoch 1/2:  81%|########  | 186/230 [01:05<00:02, 21.29it/s]',\n'\\roverwrite_wikitext epoch 1/2:  82%|########2 | 189/230 [01:05<00:01,\n21.32it/s]', '\\roverwrite_wikitext epoch 1/2:  83%|########3 | 192/230\n[01:05<00:01, 21.35it/s]', '\\roverwrite_wikitext epoch 1/2:  85%|########4 |\n195/230 [01:05<00:01, 21.39it/s]', '\\roverwrite_wikitext epoch 1/2:\n86%|########6 | 198/230 [01:05<00:01, 21.41it/s]', '[2025-12-03 19:41:01]\noverwrite_wikitext step 200: avg_train_loss=3.9442', '\\n', '\\roverwrite_wikitext\nepoch 1/2:  87%|########7 | 201/230 [01:05<00:01, 21.45it/s]',\n'\\roverwrite_wikitext epoch 1/2:  89%|########8 | 204/230 [01:06<00:01,\n21.45it/s]', '\\roverwrite_wikitext epoch 1/2:  90%|######### | 207/230\n[01:06<00:01, 21.46it/s]', '\\roverwrite_wikitext epoch 1/2:  91%|#########1|\n210/230 [01:06<00:00, 21.45it/s]', '\\roverwrite_wikitext epoch 1/2:\n93%|#########2| 213/230 [01:06<00:00, 21.46it/s]', '\\roverwrite_wikitext epoch\n1/2:  94%|#########3| 216/230 [01:06<00:00, 21.41it/s]', '\\roverwrite_wikitext\nepoch 1/2:  95%|#########5| 219/230 [01:06<00:00, 21.33it/s]',\n'\\roverwrite_wikitext epoch 1/2:  97%|#########6| 222/230 [01:06<00:00,\n21.35it/s]', '\\roverwrite_wikitext epoch 1/2:  98%|#########7| 225/230\n[01:07<00:00, 21.38it/s]', '\\roverwrite_wikitext epoch 1/2:  99%|#########9|\n228/230 [01:07<00:00, 21.45it/s]', '', '\\roverwrite_wikitext epoch 1/2:\n100%|##########| 230/230 [01:08<00:00,  3.37it/s]', '\\n', 'Epoch 1:\nvalidation_loss = 3.6659', '\\n', '\\roverwrite_wikitext epoch 2/2:   0%|\n| 0/230 [00:00<?, ?it/s]', '\\roverwrite_wikitext epoch 2/2:   0%|          |\n1/230 [01:04<4:04:32, 64.07s/it]', '\\roverwrite_wikitext epoch 2/2:   1%|1\n| 3/230 [01:04<1:03:02, 16.66s/it]', '\\roverwrite_wikitext epoch 2/2:   3%|2\n| 6/230 [01:04<24:09,  6.47s/it]  ', '\\roverwrite_wikitext epoch 2/2:   4%|3\n| 9/230 [01:04<12:46,  3.47s/it]', '\\roverwrite_wikitext epoch 2/2:   5%|5\n| 12/230 [01:04<07:37,  2.10s/it]', '\\roverwrite_wikitext epoch 2/2:   7%|6\n| 15/230 [01:04<04:50,  1.35s/it]', '\\roverwrite_wikitext epoch 2/2:   8%|7\n| 18/230 [01:04<03:12,  1.10it/s]', '\\roverwrite_wikitext epoch 2/2:   9%|9\n| 21/230 [01:05<02:10,  1.60it/s]', '\\roverwrite_wikitext epoch 2/2:  10%|#\n| 24/230 [01:05<01:30,  2.27it/s]', '\\roverwrite_wikitext epoch 2/2:  12%|#1\n| 27/230 [01:05<01:04,  3.16it/s]', '\\roverwrite_wikitext epoch 2/2:  13%|#3\n| 30/230 [01:05<00:46,  4.29it/s]', '\\roverwrite_wikitext epoch 2/2:  14%|#4\n| 33/230 [01:05<00:34,  5.67it/s]', '\\roverwrite_wikitext epoch 2/2:  16%|#5\n| 36/230 [01:05<00:26,  7.30it/s]', '\\roverwrite_wikitext epoch 2/2:  17%|#6\n| 39/230 [01:05<00:20,  9.12it/s]', '\\roverwrite_wikitext epoch 2/2:  18%|#8\n| 42/230 [01:06<00:17, 11.03it/s]', '\\roverwrite_wikitext epoch 2/2:  20%|#9\n| 45/230 [01:06<00:14, 12.92it/s]', '\\roverwrite_wikitext epoch 2/2:  21%|##\n| 48/230 [01:06<00:12, 14.67it/s]', '\\roverwrite_wikitext epoch 2/2:  22%|##2\n| 51/230 [01:06<00:11, 16.21it/s]', '\\roverwrite_wikitext epoch 2/2:  23%|##3\n| 54/230 [01:06<00:10, 17.35it/s]', '\\roverwrite_wikitext epoch 2/2:  25%|##4\n| 57/230 [01:06<00:09, 18.34it/s]', '\\roverwrite_wikitext epoch 2/2:  26%|##6\n| 60/230 [01:06<00:08, 19.15it/s]', '\\roverwrite_wikitext epoch 2/2:  27%|##7\n| 63/230 [01:07<00:08, 19.69it/s]', '\\roverwrite_wikitext epoch 2/2:  29%|##8\n| 66/230 [01:07<00:08, 20.13it/s]', '\\roverwrite_wikitext epoch 2/2:  30%|###\n| 69/230 [01:07<00:07, 20.36it/s]', '\\roverwrite_wikitext epoch 2/2:  31%|###1\n| 72/230 [01:07<00:07, 20.60it/s]', '\\roverwrite_wikitext epoch 2/2:  33%|###2\n| 75/230 [01:07<00:07, 20.75it/s]', '\\roverwrite_wikitext epoch 2/2:  34%|###3\n| 78/230 [01:07<00:07, 20.92it/s]', '\\roverwrite_wikitext epoch 2/2:  35%|###5\n| 81/230 [01:07<00:07, 20.99it/s]', '\\roverwrite_wikitext epoch 2/2:  37%|###6\n| 84/230 [01:08<00:06, 21.07it/s]', '\\roverwrite_wikitext epoch 2/2:  38%|###7\n| 87/230 [01:08<00:06, 21.06it/s]', '\\roverwrite_wikitext epoch 2/2:  39%|###9\n| 90/230 [01:08<00:06, 21.13it/s]', '\\roverwrite_wikitext epoch 2/2:  40%|####\n| 93/230 [01:08<00:06, 21.15it/s]', '\\roverwrite_wikitext epoch 2/2:  42%|####1\n| 96/230 [01:08<00:06, 21.20it/s]', '\\roverwrite_wikitext epoch 2/2:  43%|####3\n| 99/230 [01:08<00:06, 21.14it/s]', '\\roverwrite_wikitext epoch 2/2:  44%|####4\n| 102/230 [01:08<00:06, 21.20it/s]', '\\roverwrite_wikitext epoch 2/2:  46%|####5\n| 105/230 [01:09<00:05, 21.29it/s]', '\\roverwrite_wikitext epoch 2/2:  47%|####6\n| 108/230 [01:09<00:05, 21.35it/s]', '\\roverwrite_wikitext epoch 2/2:  48%|####8\n| 111/230 [01:09<00:05, 21.42it/s]', '\\roverwrite_wikitext epoch 2/2:  50%|####9\n| 114/230 [01:09<00:05, 21.43it/s]', '\\roverwrite_wikitext epoch 2/2:  51%|#####\n| 117/230 [01:09<00:05, 21.50it/s]', '\\roverwrite_wikitext epoch 2/2:\n52%|#####2    | 120/230 [01:09<00:05, 21.52it/s]', '\\roverwrite_wikitext epoch\n2/2:  53%|#####3    | 123/230 [01:09<00:04, 21.53it/s]', '\\roverwrite_wikitext\nepoch 2/2:  55%|#####4    | 126/230 [01:09<00:04, 21.52it/s]',\n'\\roverwrite_wikitext epoch 2/2:  56%|#####6    | 129/230 [01:10<00:04,\n21.50it/s]', '\\roverwrite_wikitext epoch 2/2:  57%|#####7    | 132/230\n[01:10<00:04, 21.47it/s]', '\\roverwrite_wikitext epoch 2/2:  59%|#####8    |\n135/230 [01:10<00:04, 21.48it/s]', '\\roverwrite_wikitext epoch 2/2:  60%|######\n| 138/230 [01:10<00:04, 21.45it/s]', '\\roverwrite_wikitext epoch 2/2:\n61%|######1   | 141/230 [01:10<00:04, 21.49it/s]', '\\roverwrite_wikitext epoch\n2/2:  63%|######2   | 144/230 [01:10<00:04, 21.49it/s]', '\\roverwrite_wikitext\nepoch 2/2:  64%|######3   | 147/230 [01:10<00:03, 21.51it/s]',\n'\\roverwrite_wikitext epoch 2/2:  65%|######5   | 150/230 [01:11<00:03,\n21.51it/s]', '\\roverwrite_wikitext epoch 2/2:  67%|######6   | 153/230\n[01:11<00:03, 21.52it/s]', '\\roverwrite_wikitext epoch 2/2:  68%|######7   |\n156/230 [01:11<00:03, 21.50it/s]', '\\roverwrite_wikitext epoch 2/2:  69%|######9\n| 159/230 [01:11<00:03, 21.53it/s]', '\\roverwrite_wikitext epoch 2/2:\n70%|#######   | 162/230 [01:11<00:03, 21.53it/s]', '\\roverwrite_wikitext epoch\n2/2:  72%|#######1  | 165/230 [01:11<00:03, 21.54it/s]', '\\roverwrite_wikitext\nepoch 2/2:  73%|#######3  | 168/230 [01:11<00:02, 21.51it/s]', '[2025-12-03\n19:43:13] overwrite_wikitext step 400: avg_train_loss=3.3672', '\\n',\n'\\roverwrite_wikitext epoch 2/2:  74%|#######4  | 171/230 [01:12<00:02,\n21.52it/s]', '\\roverwrite_wikitext epoch 2/2:  76%|#######5  | 174/230\n[01:12<00:02, 21.47it/s]', '\\roverwrite_wikitext epoch 2/2:  77%|#######6  |\n177/230 [01:12<00:02, 21.47it/s]', '\\roverwrite_wikitext epoch 2/2:\n78%|#######8  | 180/230 [01:12<00:02, 21.45it/s]', '\\roverwrite_wikitext epoch\n2/2:  80%|#######9  | 183/230 [01:12<00:02, 21.31it/s]', '\\roverwrite_wikitext\nepoch 2/2:  81%|########  | 186/230 [01:12<00:02, 21.31it/s]',\n'\\roverwrite_wikitext epoch 2/2:  82%|########2 | 189/230 [01:12<00:01,\n21.32it/s]', '\\roverwrite_wikitext epoch 2/2:  83%|########3 | 192/230\n[01:13<00:01, 21.36it/s]', '\\roverwrite_wikitext epoch 2/2:  85%|########4 |\n195/230 [01:13<00:01, 21.39it/s]', '\\roverwrite_wikitext epoch 2/2:\n86%|########6 | 198/230 [01:13<00:01, 21.41it/s]', '\\roverwrite_wikitext epoch\n2/2:  87%|########7 | 201/230 [01:13<00:01, 21.41it/s]', '\\roverwrite_wikitext\nepoch 2/2:  89%|########8 | 204/230 [01:13<00:01, 21.44it/s]',\n'\\roverwrite_wikitext epoch 2/2:  90%|######### | 207/230 [01:13<00:01,\n21.45it/s]', '\\roverwrite_wikitext epoch 2/2:  91%|#########1| 210/230\n[01:13<00:00, 21.42it/s]', '\\roverwrite_wikitext epoch 2/2:  93%|#########2|\n213/230 [01:14<00:00, 21.42it/s]', '\\roverwrite_wikitext epoch 2/2:\n94%|#########3| 216/230 [01:14<00:00, 21.44it/s]', '\\roverwrite_wikitext epoch\n2/2:  95%|#########5| 219/230 [01:14<00:00, 21.40it/s]', '\\roverwrite_wikitext\nepoch 2/2:  97%|#########6| 222/230 [01:14<00:00, 21.40it/s]',\n'\\roverwrite_wikitext epoch 2/2:  98%|#########7| 225/230 [01:14<00:00,\n21.42it/s]', '\\roverwrite_wikitext epoch 2/2:  99%|#########9| 228/230\n[01:14<00:00, 21.46it/s]', '', '\\roverwrite_wikitext epoch 2/2: 100%|##########|\n230/230 [01:15<00:00,  3.03it/s]', '\\n', 'Epoch 2: validation_loss = 3.6773',\n'\\n', '\\rREADME.md: 0.00B [00:00, ?B/s]', '', '\\rREADME.md: 8.07kB [00:00,\n15.6MB/s]', '\\n', '\\rdata/train-00000-of-00001.parquet:   0%|          |\n0.00/18.6M [00:00<?, ?B/s]', '\\rdata/train-00000-of-00001.parquet:   0%|\n| 13.8k/18.6M [00:00<19:52, 15.6kB/s]', '\\rdata/train-00000-of-00001.parquet:\n9%|8         | 1.59M/18.6M [00:00<00:07, 2.20MB/s]',\n'\\rdata/train-00000-of-00001.parquet: 100%|##########| 18.6M/18.6M [00:01<00:00,\n25.2MB/s]', '', '\\rdata/train-00000-of-00001.parquet: 100%|##########|\n18.6M/18.6M [00:01<00:00, 15.4MB/s]', '\\n', '\\rdata/test-00000-of-00001.parquet:\n0%|          | 0.00/1.23M [00:00<?, ?B/s]', '\\rdata/test-00000-of-00001.parquet:\n3%|3         | 37.8k/1.23M [00:00<00:08, 139kB/s]', '',\n'\\rdata/test-00000-of-00001.parquet: 100%|##########| 1.23M/1.23M [00:00<00:00,\n4.50MB/s]', '\\n', '\\rGenerating train split:   0%|          | 0/120000 [00:00<?,\n? examples/s]', '\\rGenerating train split:   1%|          | 1000/120000\n[00:00<00:14, 8089.99 examples/s]', '\\rGenerating train split:  80%|########  |\n96000/120000 [00:00<00:00, 504801.32 examples/s]', '', '\\rGenerating train\nsplit: 100%|##########| 120000/120000 [00:00<00:00, 454711.05 examples/s]',\n'\\n', '\\rGenerating test split:   0%|          | 0/7600 [00:00<?, ?\nexamples/s]', '', '\\rGenerating test split: 100%|##########| 7600/7600\n[00:00<00:00, 296059.35 examples/s]', '\\n', '\\rMap:   0%|          | 0/12000\n[00:00<?, ? examples/s]', '\\rMap:  25%|##5       | 3000/12000 [00:00<00:00,\n21445.94 examples/s]', '\\rMap:  50%|#####     | 6000/12000 [00:00<00:00, 9460.24\nexamples/s] ', '\\rMap:  83%|########3 | 10000/12000 [00:00<00:00, 13859.22\nexamples/s]', '', '\\rMap: 100%|##########| 12000/12000 [00:00<00:00, 14023.17\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/760 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 760/760 [00:00<00:00, 15410.92 examples/s]', '\\n',\n'\\roverwrite_ag_news epoch 1/2:   0%|          | 0/375 [00:00<?, ?it/s]',\n'\\roverwrite_ag_news epoch 1/2:   0%|          | 1/375 [00:57<5:59:47,\n57.72s/it]', '\\roverwrite_ag_news epoch 1/2:   1%|1         | 4/375\n[00:57<1:07:45, 10.96s/it]', '\\roverwrite_ag_news epoch 1/2:   2%|1         |\n7/375 [00:58<31:17,  5.10s/it]  ', '\\roverwrite_ag_news epoch 1/2:   3%|2\n| 10/375 [00:58<17:41,  2.91s/it]', '\\roverwrite_ag_news epoch 1/2:   3%|3\n| 13/375 [00:58<10:56,  1.81s/it]', '\\roverwrite_ag_news epoch 1/2:   4%|4\n| 16/375 [00:58<07:06,  1.19s/it]', '\\roverwrite_ag_news epoch 1/2:   5%|5\n| 19/375 [00:58<04:46,  1.24it/s]', '\\roverwrite_ag_news epoch 1/2:   6%|5\n| 22/375 [00:58<03:17,  1.79it/s]', '\\roverwrite_ag_news epoch 1/2:   7%|6\n| 25/375 [00:58<02:19,  2.51it/s]', '\\roverwrite_ag_news epoch 1/2:   7%|7\n| 28/375 [00:58<01:40,  3.46it/s]', '\\roverwrite_ag_news epoch 1/2:   8%|8\n| 31/375 [00:59<01:13,  4.66it/s]', '\\roverwrite_ag_news epoch 1/2:   9%|9\n| 34/375 [00:59<00:55,  6.12it/s]', '\\roverwrite_ag_news epoch 1/2:  10%|9\n| 37/375 [00:59<00:43,  7.81it/s]', '\\roverwrite_ag_news epoch 1/2:  11%|#\n| 40/375 [00:59<00:34,  9.65it/s]', '\\roverwrite_ag_news epoch 1/2:  11%|#1\n| 43/375 [00:59<00:28, 11.56it/s]', '\\roverwrite_ag_news epoch 1/2:  12%|#2\n| 46/375 [00:59<00:24, 13.38it/s]', '\\roverwrite_ag_news epoch 1/2:  13%|#3\n| 49/375 [00:59<00:21, 15.07it/s]', '\\roverwrite_ag_news epoch 1/2:  14%|#3\n| 52/375 [01:00<00:19, 16.51it/s]', '\\roverwrite_ag_news epoch 1/2:  15%|#4\n| 55/375 [01:00<00:18, 17.71it/s]', '\\roverwrite_ag_news epoch 1/2:  15%|#5\n| 58/375 [01:00<00:17, 18.62it/s]', '\\roverwrite_ag_news epoch 1/2:  16%|#6\n| 61/375 [01:00<00:16, 19.34it/s]', '\\roverwrite_ag_news epoch 1/2:  17%|#7\n| 64/375 [01:00<00:15, 19.84it/s]', '\\roverwrite_ag_news epoch 1/2:  18%|#7\n| 67/375 [01:00<00:15, 20.27it/s]', '\\roverwrite_ag_news epoch 1/2:  19%|#8\n| 70/375 [01:00<00:14, 20.55it/s]', '\\roverwrite_ag_news epoch 1/2:  19%|#9\n| 73/375 [01:01<00:14, 20.79it/s]', '\\roverwrite_ag_news epoch 1/2:  20%|##\n| 76/375 [01:01<00:14, 20.95it/s]', '\\roverwrite_ag_news epoch 1/2:  21%|##1\n| 79/375 [01:01<00:14, 21.10it/s]', '\\roverwrite_ag_news epoch 1/2:  22%|##1\n| 82/375 [01:01<00:13, 21.17it/s]', '\\roverwrite_ag_news epoch 1/2:  23%|##2\n| 85/375 [01:01<00:13, 21.25it/s]', '\\roverwrite_ag_news epoch 1/2:  23%|##3\n| 88/375 [01:01<00:13, 21.28it/s]', '\\roverwrite_ag_news epoch 1/2:  24%|##4\n| 91/375 [01:01<00:13, 21.12it/s]', '\\roverwrite_ag_news epoch 1/2:  25%|##5\n| 94/375 [01:02<00:13, 21.20it/s]', '\\roverwrite_ag_news epoch 1/2:  26%|##5\n| 97/375 [01:02<00:13, 21.14it/s]', '\\roverwrite_ag_news epoch 1/2:  27%|##6\n| 100/375 [01:02<00:13, 21.11it/s]', '\\roverwrite_ag_news epoch 1/2:  27%|##7\n| 103/375 [01:02<00:12, 21.09it/s]', '\\roverwrite_ag_news epoch 1/2:  28%|##8\n| 106/375 [01:02<00:12, 21.10it/s]', '\\roverwrite_ag_news epoch 1/2:  29%|##9\n| 109/375 [01:02<00:12, 21.09it/s]', '\\roverwrite_ag_news epoch 1/2:  30%|##9\n| 112/375 [01:02<00:12, 21.05it/s]', '\\roverwrite_ag_news epoch 1/2:  31%|###\n| 115/375 [01:03<00:12, 21.10it/s]', '\\roverwrite_ag_news epoch 1/2:  31%|###1\n| 118/375 [01:03<00:12, 21.11it/s]', '\\roverwrite_ag_news epoch 1/2:  32%|###2\n| 121/375 [01:03<00:12, 21.09it/s]', '\\roverwrite_ag_news epoch 1/2:  33%|###3\n| 124/375 [01:03<00:11, 21.04it/s]', '\\roverwrite_ag_news epoch 1/2:  34%|###3\n| 127/375 [01:03<00:11, 21.06it/s]', '\\roverwrite_ag_news epoch 1/2:  35%|###4\n| 130/375 [01:03<00:11, 21.04it/s]', '\\roverwrite_ag_news epoch 1/2:  35%|###5\n| 133/375 [01:03<00:11, 20.95it/s]', '\\roverwrite_ag_news epoch 1/2:  36%|###6\n| 136/375 [01:04<00:11, 20.97it/s]', '\\roverwrite_ag_news epoch 1/2:  37%|###7\n| 139/375 [01:04<00:11, 20.96it/s]', '[2025-12-03 19:45:39] overwrite_ag_news\nstep 600: avg_train_loss=3.9873', '\\n', '\\roverwrite_ag_news epoch 1/2:\n38%|###7      | 142/375 [01:04<00:11, 20.92it/s]', '\\roverwrite_ag_news epoch\n1/2:  39%|###8      | 145/375 [01:04<00:11, 20.87it/s]', '\\roverwrite_ag_news\nepoch 1/2:  39%|###9      | 148/375 [01:04<00:10, 20.84it/s]',\n'\\roverwrite_ag_news epoch 1/2:  40%|####      | 151/375 [01:04<00:10,\n20.87it/s]', '\\roverwrite_ag_news epoch 1/2:  41%|####1     | 154/375\n[01:04<00:10, 20.89it/s]', '\\roverwrite_ag_news epoch 1/2:  42%|####1     |\n157/375 [01:05<00:10, 20.91it/s]', '\\roverwrite_ag_news epoch 1/2:  43%|####2\n| 160/375 [01:05<00:10, 20.93it/s]', '\\roverwrite_ag_news epoch 1/2:  43%|####3\n| 163/375 [01:05<00:10, 20.89it/s]', '\\roverwrite_ag_news epoch 1/2:  44%|####4\n| 166/375 [01:05<00:09, 20.91it/s]', '\\roverwrite_ag_news epoch 1/2:  45%|####5\n| 169/375 [01:05<00:09, 20.99it/s]', '\\roverwrite_ag_news epoch 1/2:  46%|####5\n| 172/375 [01:05<00:09, 20.99it/s]', '\\roverwrite_ag_news epoch 1/2:  47%|####6\n| 175/375 [01:05<00:09, 21.02it/s]', '\\roverwrite_ag_news epoch 1/2:  47%|####7\n| 178/375 [01:06<00:09, 20.98it/s]', '\\roverwrite_ag_news epoch 1/2:  48%|####8\n| 181/375 [01:06<00:09, 20.94it/s]', '\\roverwrite_ag_news epoch 1/2:  49%|####9\n| 184/375 [01:06<00:09, 20.98it/s]', '\\roverwrite_ag_news epoch 1/2:  50%|####9\n| 187/375 [01:06<00:08, 21.05it/s]', '\\roverwrite_ag_news epoch 1/2:  51%|#####\n| 190/375 [01:06<00:08, 21.08it/s]', '\\roverwrite_ag_news epoch 1/2:  51%|#####1\n| 193/375 [01:06<00:08, 21.09it/s]', '\\roverwrite_ag_news epoch 1/2:  52%|#####2\n| 196/375 [01:06<00:08, 21.07it/s]', '\\roverwrite_ag_news epoch 1/2:  53%|#####3\n| 199/375 [01:07<00:08, 21.12it/s]', '\\roverwrite_ag_news epoch 1/2:  54%|#####3\n| 202/375 [01:07<00:08, 21.15it/s]', '\\roverwrite_ag_news epoch 1/2:  55%|#####4\n| 205/375 [01:07<00:08, 21.15it/s]', '\\roverwrite_ag_news epoch 1/2:  55%|#####5\n| 208/375 [01:07<00:07, 21.13it/s]', '\\roverwrite_ag_news epoch 1/2:  56%|#####6\n| 211/375 [01:07<00:07, 21.17it/s]', '\\roverwrite_ag_news epoch 1/2:  57%|#####7\n| 214/375 [01:07<00:07, 21.21it/s]', '\\roverwrite_ag_news epoch 1/2:  58%|#####7\n| 217/375 [01:07<00:07, 21.18it/s]', '\\roverwrite_ag_news epoch 1/2:  59%|#####8\n| 220/375 [01:08<00:07, 21.19it/s]', '\\roverwrite_ag_news epoch 1/2:  59%|#####9\n| 223/375 [01:08<00:07, 21.22it/s]', '\\roverwrite_ag_news epoch 1/2:  60%|######\n| 226/375 [01:08<00:07, 21.13it/s]', '\\roverwrite_ag_news epoch 1/2:\n61%|######1   | 229/375 [01:08<00:06, 21.14it/s]', '\\roverwrite_ag_news epoch\n1/2:  62%|######1   | 232/375 [01:08<00:06, 21.08it/s]', '\\roverwrite_ag_news\nepoch 1/2:  63%|######2   | 235/375 [01:08<00:06, 21.12it/s]',\n'\\roverwrite_ag_news epoch 1/2:  63%|######3   | 238/375 [01:08<00:06,\n21.15it/s]', '\\roverwrite_ag_news epoch 1/2:  64%|######4   | 241/375\n[01:09<00:06, 21.14it/s]', '\\roverwrite_ag_news epoch 1/2:  65%|######5   |\n244/375 [01:09<00:06, 21.03it/s]', '\\roverwrite_ag_news epoch 1/2:  66%|######5\n| 247/375 [01:09<00:06, 21.04it/s]', '\\roverwrite_ag_news epoch 1/2:\n67%|######6   | 250/375 [01:09<00:05, 21.12it/s]', '\\roverwrite_ag_news epoch\n1/2:  67%|######7   | 253/375 [01:09<00:05, 21.13it/s]', '\\roverwrite_ag_news\nepoch 1/2:  68%|######8   | 256/375 [01:09<00:05, 21.18it/s]',\n'\\roverwrite_ag_news epoch 1/2:  69%|######9   | 259/375 [01:09<00:05,\n21.10it/s]', '\\roverwrite_ag_news epoch 1/2:  70%|######9   | 262/375\n[01:10<00:05, 21.02it/s]', '\\roverwrite_ag_news epoch 1/2:  71%|#######   |\n265/375 [01:10<00:05, 21.04it/s]', '\\roverwrite_ag_news epoch 1/2:  71%|#######1\n| 268/375 [01:10<00:05, 21.04it/s]', '\\roverwrite_ag_news epoch 1/2:\n72%|#######2  | 271/375 [01:10<00:04, 21.06it/s]', '\\roverwrite_ag_news epoch\n1/2:  73%|#######3  | 274/375 [01:10<00:04, 20.97it/s]', '\\roverwrite_ag_news\nepoch 1/2:  74%|#######3  | 277/375 [01:10<00:04, 20.95it/s]',\n'\\roverwrite_ag_news epoch 1/2:  75%|#######4  | 280/375 [01:10<00:04,\n20.98it/s]', '\\roverwrite_ag_news epoch 1/2:  75%|#######5  | 283/375\n[01:11<00:04, 21.05it/s]', '\\roverwrite_ag_news epoch 1/2:  76%|#######6  |\n286/375 [01:11<00:04, 21.09it/s]', '\\roverwrite_ag_news epoch 1/2:  77%|#######7\n| 289/375 [01:11<00:04, 21.17it/s]', '\\roverwrite_ag_news epoch 1/2:\n78%|#######7  | 292/375 [01:11<00:03, 21.23it/s]', '\\roverwrite_ag_news epoch\n1/2:  79%|#######8  | 295/375 [01:11<00:03, 21.27it/s]', '\\roverwrite_ag_news\nepoch 1/2:  79%|#######9  | 298/375 [01:11<00:03, 21.26it/s]',\n'\\roverwrite_ag_news epoch 1/2:  80%|########  | 301/375 [01:11<00:03,\n21.23it/s]', '\\roverwrite_ag_news epoch 1/2:  81%|########1 | 304/375\n[01:12<00:03, 21.26it/s]', '\\roverwrite_ag_news epoch 1/2:  82%|########1 |\n307/375 [01:12<00:03, 21.26it/s]', '\\roverwrite_ag_news epoch 1/2:\n83%|########2 | 310/375 [01:12<00:03, 21.28it/s]', '\\roverwrite_ag_news epoch\n1/2:  83%|########3 | 313/375 [01:12<00:02, 21.09it/s]', '\\roverwrite_ag_news\nepoch 1/2:  84%|########4 | 316/375 [01:12<00:02, 21.16it/s]',\n'\\roverwrite_ag_news epoch 1/2:  85%|########5 | 319/375 [01:12<00:02,\n21.20it/s]', '\\roverwrite_ag_news epoch 1/2:  86%|########5 | 322/375\n[01:12<00:02, 21.15it/s]', '\\roverwrite_ag_news epoch 1/2:  87%|########6 |\n325/375 [01:13<00:02, 20.97it/s]', '\\roverwrite_ag_news epoch 1/2:\n87%|########7 | 328/375 [01:13<00:02, 21.06it/s]', '\\roverwrite_ag_news epoch\n1/2:  88%|########8 | 331/375 [01:13<00:02, 21.13it/s]', '\\roverwrite_ag_news\nepoch 1/2:  89%|########9 | 334/375 [01:13<00:01, 21.19it/s]',\n'\\roverwrite_ag_news epoch 1/2:  90%|########9 | 337/375 [01:13<00:01,\n21.22it/s]', '[2025-12-03 19:45:48] overwrite_ag_news step 800:\navg_train_loss=3.6949', '\\n', '\\roverwrite_ag_news epoch 1/2:  91%|######### |\n340/375 [01:13<00:01, 21.23it/s]', '\\roverwrite_ag_news epoch 1/2:\n91%|#########1| 343/375 [01:13<00:01, 21.19it/s]', '\\roverwrite_ag_news epoch\n1/2:  92%|#########2| 346/375 [01:14<00:01, 21.25it/s]', '\\roverwrite_ag_news\nepoch 1/2:  93%|#########3| 349/375 [01:14<00:01, 21.25it/s]',\n'\\roverwrite_ag_news epoch 1/2:  94%|#########3| 352/375 [01:14<00:01,\n21.13it/s]', '\\roverwrite_ag_news epoch 1/2:  95%|#########4| 355/375\n[01:14<00:00, 21.05it/s]', '\\roverwrite_ag_news epoch 1/2:  95%|#########5|\n358/375 [01:14<00:00, 21.05it/s]', '\\roverwrite_ag_news epoch 1/2:\n96%|#########6| 361/375 [01:14<00:00, 21.08it/s]', '\\roverwrite_ag_news epoch\n1/2:  97%|#########7| 364/375 [01:14<00:00, 21.11it/s]', '\\roverwrite_ag_news\nepoch 1/2:  98%|#########7| 367/375 [01:15<00:00, 21.14it/s]',\n'\\roverwrite_ag_news epoch 1/2:  99%|#########8| 370/375 [01:15<00:00,\n20.95it/s]', '\\roverwrite_ag_news epoch 1/2:  99%|#########9| 373/375\n[01:15<00:00, 21.07it/s]', '', '\\roverwrite_ag_news epoch 1/2: 100%|##########|\n375/375 [01:16<00:00,  4.90it/s]', '\\n', 'Epoch 1: validation_loss = 3.2270',\n'\\n', '\\roverwrite_ag_news epoch 2/2:   0%|          | 0/375 [00:00<?, ?it/s]',\n'\\roverwrite_ag_news epoch 2/2:   0%|          | 1/375 [01:07<7:01:35,\n67.64s/it]', '\\roverwrite_ag_news epoch 2/2:   1%|          | 3/375\n[01:07<1:49:01, 17.59s/it]', '\\roverwrite_ag_news epoch 2/2:   2%|1         |\n6/375 [01:07<41:58,  6.83s/it]  ', '\\roverwrite_ag_news epoch 2/2:   2%|2\n| 9/375 [01:08<22:19,  3.66s/it]', '\\roverwrite_ag_news epoch 2/2:   3%|3\n| 12/375 [01:08<13:23,  2.21s/it]', '\\roverwrite_ag_news epoch 2/2:   4%|4\n| 15/375 [01:08<08:33,  1.43s/it]', '\\roverwrite_ag_news epoch 2/2:   5%|4\n| 18/375 [01:08<05:40,  1.05it/s]', '\\roverwrite_ag_news epoch 2/2:   6%|5\n| 21/375 [01:08<03:52,  1.52it/s]', '\\roverwrite_ag_news epoch 2/2:   6%|6\n| 24/375 [01:08<02:42,  2.16it/s]', '\\roverwrite_ag_news epoch 2/2:   7%|7\n| 27/375 [01:08<01:55,  3.01it/s]', '\\roverwrite_ag_news epoch 2/2:   8%|8\n| 30/375 [01:09<01:24,  4.10it/s]', '\\roverwrite_ag_news epoch 2/2:   9%|8\n| 33/375 [01:09<01:02,  5.44it/s]', '\\roverwrite_ag_news epoch 2/2:  10%|9\n| 36/375 [01:09<00:48,  7.03it/s]', '\\roverwrite_ag_news epoch 2/2:  10%|#\n| 39/375 [01:09<00:38,  8.80it/s]', '\\roverwrite_ag_news epoch 2/2:  11%|#1\n| 42/375 [01:09<00:31, 10.69it/s]', '\\roverwrite_ag_news epoch 2/2:  12%|#2\n| 45/375 [01:09<00:26, 12.56it/s]', '\\roverwrite_ag_news epoch 2/2:  13%|#2\n| 48/375 [01:09<00:22, 14.27it/s]', '\\roverwrite_ag_news epoch 2/2:  14%|#3\n| 51/375 [01:10<00:20, 15.81it/s]', '\\roverwrite_ag_news epoch 2/2:  14%|#4\n| 54/375 [01:10<00:18, 17.13it/s]', '\\roverwrite_ag_news epoch 2/2:  15%|#5\n| 57/375 [01:10<00:17, 18.19it/s]', '\\roverwrite_ag_news epoch 2/2:  16%|#6\n| 60/375 [01:10<00:16, 19.01it/s]', '\\roverwrite_ag_news epoch 2/2:  17%|#6\n| 63/375 [01:10<00:15, 19.63it/s]', '\\roverwrite_ag_news epoch 2/2:  18%|#7\n| 66/375 [01:10<00:15, 20.04it/s]', '\\roverwrite_ag_news epoch 2/2:  18%|#8\n| 69/375 [01:10<00:15, 20.39it/s]', '\\roverwrite_ag_news epoch 2/2:  19%|#9\n| 72/375 [01:11<00:14, 20.62it/s]', '\\roverwrite_ag_news epoch 2/2:  20%|##\n| 75/375 [01:11<00:14, 20.62it/s]', '\\roverwrite_ag_news epoch 2/2:  21%|##\n| 78/375 [01:11<00:14, 20.45it/s]', '\\roverwrite_ag_news epoch 2/2:  22%|##1\n| 81/375 [01:11<00:14, 20.52it/s]', '\\roverwrite_ag_news epoch 2/2:  22%|##2\n| 84/375 [01:11<00:14, 20.69it/s]', '\\roverwrite_ag_news epoch 2/2:  23%|##3\n| 87/375 [01:11<00:13, 20.78it/s]', '\\roverwrite_ag_news epoch 2/2:  24%|##4\n| 90/375 [01:11<00:13, 20.90it/s]', '\\roverwrite_ag_news epoch 2/2:  25%|##4\n| 93/375 [01:12<00:13, 20.88it/s]', '\\roverwrite_ag_news epoch 2/2:  26%|##5\n| 96/375 [01:12<00:13, 20.98it/s]', '\\roverwrite_ag_news epoch 2/2:  26%|##6\n| 99/375 [01:12<00:13, 21.02it/s]', '\\roverwrite_ag_news epoch 2/2:  27%|##7\n| 102/375 [01:12<00:13, 20.99it/s]', '\\roverwrite_ag_news epoch 2/2:  28%|##8\n| 105/375 [01:12<00:12, 20.81it/s]', '\\roverwrite_ag_news epoch 2/2:  29%|##8\n| 108/375 [01:12<00:12, 20.82it/s]', '\\roverwrite_ag_news epoch 2/2:  30%|##9\n| 111/375 [01:12<00:12, 20.86it/s]', '\\roverwrite_ag_news epoch 2/2:  30%|###\n| 114/375 [01:13<00:12, 20.86it/s]', '\\roverwrite_ag_news epoch 2/2:  31%|###1\n| 117/375 [01:13<00:12, 20.89it/s]', '\\roverwrite_ag_news epoch 2/2:  32%|###2\n| 120/375 [01:13<00:12, 20.96it/s]', '\\roverwrite_ag_news epoch 2/2:  33%|###2\n| 123/375 [01:13<00:12, 20.99it/s]', '\\roverwrite_ag_news epoch 2/2:  34%|###3\n| 126/375 [01:13<00:11, 20.94it/s]', '\\roverwrite_ag_news epoch 2/2:  34%|###4\n| 129/375 [01:13<00:11, 20.87it/s]', '\\roverwrite_ag_news epoch 2/2:  35%|###5\n| 132/375 [01:13<00:11, 20.93it/s]', '\\roverwrite_ag_news epoch 2/2:  36%|###6\n| 135/375 [01:14<00:11, 20.88it/s]', '\\roverwrite_ag_news epoch 2/2:  37%|###6\n| 138/375 [01:14<00:11, 20.90it/s]', '\\roverwrite_ag_news epoch 2/2:  38%|###7\n| 141/375 [01:14<00:11, 20.91it/s]', '\\roverwrite_ag_news epoch 2/2:  38%|###8\n| 144/375 [01:14<00:11, 20.94it/s]', '\\roverwrite_ag_news epoch 2/2:  39%|###9\n| 147/375 [01:14<00:10, 20.89it/s]', '\\roverwrite_ag_news epoch 2/2:  40%|####\n| 150/375 [01:14<00:10, 20.91it/s]', '\\roverwrite_ag_news epoch 2/2:  41%|####\n| 153/375 [01:14<00:10, 20.91it/s]', '\\roverwrite_ag_news epoch 2/2:  42%|####1\n| 156/375 [01:15<00:10, 20.93it/s]', '\\roverwrite_ag_news epoch 2/2:  42%|####2\n| 159/375 [01:15<00:10, 20.79it/s]', '\\roverwrite_ag_news epoch 2/2:  43%|####3\n| 162/375 [01:15<00:10, 20.84it/s]', '[2025-12-03 19:48:06] overwrite_ag_news\nstep 1000: avg_train_loss=3.2332', '\\n', '\\roverwrite_ag_news epoch 2/2:\n44%|####4     | 165/375 [01:15<00:10, 20.86it/s]', '\\roverwrite_ag_news epoch\n2/2:  45%|####4     | 168/375 [01:15<00:10, 20.62it/s]', '\\roverwrite_ag_news\nepoch 2/2:  46%|####5     | 171/375 [01:15<00:09, 20.57it/s]',\n'\\roverwrite_ag_news epoch 2/2:  46%|####6     | 174/375 [01:15<00:09,\n20.46it/s]', '\\roverwrite_ag_news epoch 2/2:  47%|####7     | 177/375\n[01:16<00:09, 20.50it/s]', '\\roverwrite_ag_news epoch 2/2:  48%|####8     |\n180/375 [01:16<00:09, 20.57it/s]', '\\roverwrite_ag_news epoch 2/2:  49%|####8\n| 183/375 [01:16<00:09, 20.58it/s]', '\\roverwrite_ag_news epoch 2/2:  50%|####9\n| 186/375 [01:16<00:09, 20.63it/s]', '\\roverwrite_ag_news epoch 2/2:  50%|#####\n| 189/375 [01:16<00:08, 20.67it/s]', '\\roverwrite_ag_news epoch 2/2:  51%|#####1\n| 192/375 [01:16<00:08, 20.75it/s]', '\\roverwrite_ag_news epoch 2/2:  52%|#####2\n| 195/375 [01:16<00:08, 20.82it/s]', '\\roverwrite_ag_news epoch 2/2:  53%|#####2\n| 198/375 [01:17<00:08, 20.89it/s]', '\\roverwrite_ag_news epoch 2/2:  54%|#####3\n| 201/375 [01:17<00:08, 20.77it/s]', '\\roverwrite_ag_news epoch 2/2:  54%|#####4\n| 204/375 [01:17<00:08, 20.78it/s]', '\\roverwrite_ag_news epoch 2/2:  55%|#####5\n| 207/375 [01:17<00:08, 20.80it/s]', '\\roverwrite_ag_news epoch 2/2:  56%|#####6\n| 210/375 [01:17<00:07, 20.83it/s]', '\\roverwrite_ag_news epoch 2/2:  57%|#####6\n| 213/375 [01:17<00:07, 20.83it/s]', '\\roverwrite_ag_news epoch 2/2:  58%|#####7\n| 216/375 [01:17<00:07, 20.87it/s]', '\\roverwrite_ag_news epoch 2/2:  58%|#####8\n| 219/375 [01:18<00:07, 20.84it/s]', '\\roverwrite_ag_news epoch 2/2:  59%|#####9\n| 222/375 [01:18<00:07, 20.85it/s]', '\\roverwrite_ag_news epoch 2/2:  60%|######\n| 225/375 [01:18<00:07, 20.85it/s]', '\\roverwrite_ag_news epoch 2/2:  61%|######\n| 228/375 [01:18<00:07, 20.89it/s]', '\\roverwrite_ag_news epoch 2/2:\n62%|######1   | 231/375 [01:18<00:06, 20.92it/s]', '\\roverwrite_ag_news epoch\n2/2:  62%|######2   | 234/375 [01:18<00:06, 20.96it/s]', '\\roverwrite_ag_news\nepoch 2/2:  63%|######3   | 237/375 [01:18<00:06, 21.00it/s]',\n'\\roverwrite_ag_news epoch 2/2:  64%|######4   | 240/375 [01:19<00:06,\n21.08it/s]', '\\roverwrite_ag_news epoch 2/2:  65%|######4   | 243/375\n[01:19<00:06, 21.05it/s]', '\\roverwrite_ag_news epoch 2/2:  66%|######5   |\n246/375 [01:19<00:06, 21.06it/s]', '\\roverwrite_ag_news epoch 2/2:  66%|######6\n| 249/375 [01:19<00:06, 20.93it/s]', '\\roverwrite_ag_news epoch 2/2:\n67%|######7   | 252/375 [01:19<00:05, 21.00it/s]', '\\roverwrite_ag_news epoch\n2/2:  68%|######8   | 255/375 [01:19<00:05, 21.06it/s]', '\\roverwrite_ag_news\nepoch 2/2:  69%|######8   | 258/375 [01:19<00:05, 21.08it/s]',\n'\\roverwrite_ag_news epoch 2/2:  70%|######9   | 261/375 [01:20<00:05,\n21.05it/s]', '\\roverwrite_ag_news epoch 2/2:  70%|#######   | 264/375\n[01:20<00:05, 21.07it/s]', '\\roverwrite_ag_news epoch 2/2:  71%|#######1  |\n267/375 [01:20<00:05, 21.03it/s]', '\\roverwrite_ag_news epoch 2/2:  72%|#######2\n| 270/375 [01:20<00:04, 21.06it/s]', '\\roverwrite_ag_news epoch 2/2:\n73%|#######2  | 273/375 [01:20<00:04, 21.02it/s]', '\\roverwrite_ag_news epoch\n2/2:  74%|#######3  | 276/375 [01:20<00:04, 21.05it/s]', '\\roverwrite_ag_news\nepoch 2/2:  74%|#######4  | 279/375 [01:20<00:04, 21.04it/s]',\n'\\roverwrite_ag_news epoch 2/2:  75%|#######5  | 282/375 [01:21<00:04,\n20.95it/s]', '\\roverwrite_ag_news epoch 2/2:  76%|#######6  | 285/375\n[01:21<00:04, 20.88it/s]', '\\roverwrite_ag_news epoch 2/2:  77%|#######6  |\n288/375 [01:21<00:04, 20.92it/s]', '\\roverwrite_ag_news epoch 2/2:  78%|#######7\n| 291/375 [01:21<00:04, 20.95it/s]', '\\roverwrite_ag_news epoch 2/2:\n78%|#######8  | 294/375 [01:21<00:03, 20.96it/s]', '\\roverwrite_ag_news epoch\n2/2:  79%|#######9  | 297/375 [01:21<00:03, 20.70it/s]', '\\roverwrite_ag_news\nepoch 2/2:  80%|########  | 300/375 [01:21<00:03, 20.78it/s]',\n'\\roverwrite_ag_news epoch 2/2:  81%|########  | 303/375 [01:22<00:03,\n20.76it/s]', '\\roverwrite_ag_news epoch 2/2:  82%|########1 | 306/375\n[01:22<00:03, 20.74it/s]', '\\roverwrite_ag_news epoch 2/2:  82%|########2 |\n309/375 [01:22<00:03, 20.78it/s]', '\\roverwrite_ag_news epoch 2/2:\n83%|########3 | 312/375 [01:22<00:03, 20.79it/s]', '\\roverwrite_ag_news epoch\n2/2:  84%|########4 | 315/375 [01:22<00:02, 20.75it/s]', '\\roverwrite_ag_news\nepoch 2/2:  85%|########4 | 318/375 [01:22<00:02, 20.73it/s]',\n'\\roverwrite_ag_news epoch 2/2:  86%|########5 | 321/375 [01:22<00:02,\n20.84it/s]', '\\roverwrite_ag_news epoch 2/2:  86%|########6 | 324/375\n[01:23<00:02, 20.87it/s]', '\\roverwrite_ag_news epoch 2/2:  87%|########7 |\n327/375 [01:23<00:02, 20.83it/s]', '\\roverwrite_ag_news epoch 2/2:\n88%|########8 | 330/375 [01:23<00:02, 20.87it/s]', '\\roverwrite_ag_news epoch\n2/2:  89%|########8 | 333/375 [01:23<00:02, 20.92it/s]', '\\roverwrite_ag_news\nepoch 2/2:  90%|########9 | 336/375 [01:23<00:01, 20.69it/s]',\n'\\roverwrite_ag_news epoch 2/2:  90%|######### | 339/375 [01:23<00:01,\n20.53it/s]', '\\roverwrite_ag_news epoch 2/2:  91%|#########1| 342/375\n[01:23<00:01, 20.57it/s]', '\\roverwrite_ag_news epoch 2/2:  92%|#########2|\n345/375 [01:24<00:01, 20.50it/s]', '\\roverwrite_ag_news epoch 2/2:\n93%|#########2| 348/375 [01:24<00:01, 20.65it/s]', '\\roverwrite_ag_news epoch\n2/2:  94%|#########3| 351/375 [01:24<00:01, 20.71it/s]', '\\roverwrite_ag_news\nepoch 2/2:  94%|#########4| 354/375 [01:24<00:01, 20.81it/s]',\n'\\roverwrite_ag_news epoch 2/2:  95%|#########5| 357/375 [01:24<00:00,\n20.88it/s]', '\\roverwrite_ag_news epoch 2/2:  96%|#########6| 360/375\n[01:24<00:00, 20.85it/s]', '\\roverwrite_ag_news epoch 2/2:  97%|#########6|\n363/375 [01:24<00:00, 20.89it/s]', '[2025-12-03 19:48:15] overwrite_ag_news step\n1200: avg_train_loss=3.2153', '\\n', '\\roverwrite_ag_news epoch 2/2:\n98%|#########7| 366/375 [01:25<00:00, 20.86it/s]', '\\roverwrite_ag_news epoch\n2/2:  98%|#########8| 369/375 [01:25<00:00, 20.87it/s]', '\\roverwrite_ag_news\nepoch 2/2:  99%|#########9| 372/375 [01:25<00:00, 20.89it/s]',\n'\\roverwrite_ag_news epoch 2/2: 100%|##########| 375/375 [01:25<00:00,\n21.09it/s]', '', '\\roverwrite_ag_news epoch 2/2: 100%|##########| 375/375\n[01:26<00:00,  4.33it/s]', '\\n', 'Epoch 2: validation_loss = 3.1694', '\\n',\n'\\rMap:   0%|          | 0/2500 [00:00<?, ? examples/s]', '\\rMap:  40%|####\n| 1000/2500 [00:00<00:00, 7214.39 examples/s]', '\\rMap:  80%|########  |\n2000/2500 [00:00<00:00, 7707.60 examples/s]', '', '\\rMap: 100%|##########|\n2500/2500 [00:00<00:00, 7088.42 examples/s]', '\\n', '\\rMap:   0%|          |\n0/2500 [00:00<?, ? examples/s]', '\\rMap:  40%|####      | 1000/2500\n[00:00<00:00, 7267.50 examples/s]', '\\rMap:  80%|########  | 2000/2500\n[00:00<00:00, 7735.13 examples/s]', '', '\\rMap: 100%|##########| 2500/2500\n[00:00<00:00, 7113.23 examples/s]', '\\n', '\\roverwrite_imdb epoch 1/2:   0%|\n| 0/79 [00:00<?, ?it/s]', '\\roverwrite_imdb epoch 1/2:   1%|1         | 1/79\n[01:04<1:23:54, 64.54s/it]', '\\roverwrite_imdb epoch 1/2:   4%|3         | 3/79\n[01:04<21:15, 16.78s/it]  ', '\\roverwrite_imdb epoch 1/2:   8%|7         | 6/79\n[01:04<07:55,  6.51s/it]', '\\roverwrite_imdb epoch 1/2:  11%|#1        | 9/79\n[01:04<04:04,  3.49s/it]', '\\roverwrite_imdb epoch 1/2:  15%|#5        | 12/79\n[01:05<02:21,  2.11s/it]', '\\roverwrite_imdb epoch 1/2:  19%|#8        | 15/79\n[01:05<01:27,  1.36s/it]', '\\roverwrite_imdb epoch 1/2:  23%|##2       | 18/79\n[01:05<00:55,  1.10it/s]', '\\roverwrite_imdb epoch 1/2:  27%|##6       | 21/79\n[01:05<00:36,  1.59it/s]', '\\roverwrite_imdb epoch 1/2:  30%|###       | 24/79\n[01:05<00:24,  2.26it/s]', '\\roverwrite_imdb epoch 1/2:  34%|###4      | 27/79\n[01:05<00:16,  3.13it/s]', '\\roverwrite_imdb epoch 1/2:  38%|###7      | 30/79\n[01:05<00:11,  4.24it/s]', '\\roverwrite_imdb epoch 1/2:  42%|####1     | 33/79\n[01:06<00:08,  5.60it/s]', '\\roverwrite_imdb epoch 1/2:  46%|####5     | 36/79\n[01:06<00:05,  7.20it/s]', '\\roverwrite_imdb epoch 1/2:  49%|####9     | 39/79\n[01:06<00:04,  8.97it/s]', '\\roverwrite_imdb epoch 1/2:  53%|#####3    | 42/79\n[01:06<00:03, 10.85it/s]', '\\roverwrite_imdb epoch 1/2:  57%|#####6    | 45/79\n[01:06<00:02, 12.74it/s]', '\\roverwrite_imdb epoch 1/2:  61%|######    | 48/79\n[01:06<00:02, 14.48it/s]', '\\roverwrite_imdb epoch 1/2:  65%|######4   | 51/79\n[01:06<00:01, 16.01it/s]', '\\roverwrite_imdb epoch 1/2:  68%|######8   | 54/79\n[01:07<00:01, 17.13it/s]', '\\roverwrite_imdb epoch 1/2:  72%|#######2  | 57/79\n[01:07<00:01, 18.07it/s]', '\\roverwrite_imdb epoch 1/2:  76%|#######5  | 60/79\n[01:07<00:01, 18.48it/s]', '\\roverwrite_imdb epoch 1/2:  80%|#######9  | 63/79\n[01:07<00:00, 19.21it/s]', '\\roverwrite_imdb epoch 1/2:  84%|########3 | 66/79\n[01:07<00:00, 19.75it/s]', '\\roverwrite_imdb epoch 1/2:  87%|########7 | 69/79\n[01:07<00:00, 20.11it/s]', '\\roverwrite_imdb epoch 1/2:  91%|#########1| 72/79\n[01:07<00:00, 20.40it/s]', '\\roverwrite_imdb epoch 1/2:  95%|#########4| 75/79\n[01:08<00:00, 20.64it/s]', '\\roverwrite_imdb epoch 1/2:  99%|#########8| 78/79\n[01:08<00:00, 20.90it/s]', '', '\\roverwrite_imdb epoch 1/2: 100%|##########|\n79/79 [01:09<00:00,  1.14it/s]', '\\n', 'Epoch 1: validation_loss = 3.7059',\n'\\n', '\\roverwrite_imdb epoch 2/2:   0%|          | 0/79 [00:00<?, ?it/s]',\n'\\roverwrite_imdb epoch 2/2:   1%|1         | 1/79 [00:58<1:16:09, 58.59s/it]',\n'\\roverwrite_imdb epoch 2/2:   4%|3         | 3/79 [00:58<19:18, 15.24s/it]  ',\n'\\roverwrite_imdb epoch 2/2:   8%|7         | 6/79 [00:58<07:12,  5.92s/it]',\n'\\roverwrite_imdb epoch 2/2:  11%|#1        | 9/79 [00:59<03:42,  3.18s/it]',\n'\\roverwrite_imdb epoch 2/2:  15%|#5        | 12/79 [00:59<02:08,  1.92s/it]',\n'\\roverwrite_imdb epoch 2/2:  19%|#8        | 15/79 [00:59<01:19,  1.24s/it]',\n'\\roverwrite_imdb epoch 2/2:  23%|##2       | 18/79 [00:59<00:50,  1.20it/s]',\n'\\roverwrite_imdb epoch 2/2:  27%|##6       | 21/79 [00:59<00:33,  1.73it/s]',\n'\\roverwrite_imdb epoch 2/2:  30%|###       | 24/79 [00:59<00:22,  2.45it/s]',\n'\\roverwrite_imdb epoch 2/2:  34%|###4      | 27/79 [00:59<00:15,  3.39it/s]',\n'\\roverwrite_imdb epoch 2/2:  38%|###7      | 30/79 [01:00<00:10,  4.58it/s]',\n'\\roverwrite_imdb epoch 2/2:  42%|####1     | 33/79 [01:00<00:07,  6.02it/s]',\n'\\roverwrite_imdb epoch 2/2:  46%|####5     | 36/79 [01:00<00:05,  7.70it/s]',\n'\\roverwrite_imdb epoch 2/2:  49%|####9     | 39/79 [01:00<00:04,  9.53it/s]',\n'\\roverwrite_imdb epoch 2/2:  53%|#####3    | 42/79 [01:00<00:03, 11.41it/s]',\n'\\roverwrite_imdb epoch 2/2:  57%|#####6    | 45/79 [01:00<00:02, 13.18it/s]',\n'\\roverwrite_imdb epoch 2/2:  61%|######    | 48/79 [01:00<00:02, 14.88it/s]',\n'\\roverwrite_imdb epoch 2/2:  65%|######4   | 51/79 [01:01<00:01, 16.34it/s]',\n'\\roverwrite_imdb epoch 2/2:  68%|######8   | 54/79 [01:01<00:01, 17.61it/s]',\n'\\roverwrite_imdb epoch 2/2:  72%|#######2  | 57/79 [01:01<00:01, 18.59it/s]',\n'\\roverwrite_imdb epoch 2/2:  76%|#######5  | 60/79 [01:01<00:00, 19.11it/s]',\n'\\roverwrite_imdb epoch 2/2:  80%|#######9  | 63/79 [01:01<00:00, 19.59it/s]',\n'\\roverwrite_imdb epoch 2/2:  84%|########3 | 66/79 [01:01<00:00, 19.60it/s]',\n'\\roverwrite_imdb epoch 2/2:  87%|########7 | 69/79 [01:01<00:00, 19.80it/s]',\n'\\roverwrite_imdb epoch 2/2:  91%|#########1| 72/79 [01:02<00:00, 19.77it/s]',\n'\\roverwrite_imdb epoch 2/2:  95%|#########4| 75/79 [01:02<00:00, 19.91it/s]',\n'\\roverwrite_imdb epoch 2/2:  99%|#########8| 78/79 [01:02<00:00, 20.04it/s]',\n'', '\\roverwrite_imdb epoch 2/2: 100%|##########| 79/79 [01:03<00:00,\n1.25it/s]', '\\n', 'Epoch 2: validation_loss = 3.6917', '\\n', 'Experiment\ncomplete. Artifacts saved to:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-4/working',\n'\\n', 'Execution time: 16 minutes seconds (time limit is 2 hours).']", "['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls: [' apple', '\ntable', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n21538.08 examples/s]', '\\n', '\\rMap:   0%|          | 0/300 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00, 32965.45\nexamples/s]', '\\n', '\\rTraining synthetic_injection epoch 1/1:   0%|          |\n0/21 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 1/1:   5%|4\n| 1/21 [00:56<18:47, 56.36s/it]', '\\rTraining synthetic_injection epoch 1/1:\n10%|9         | 2/21 [00:56<07:22, 23.28s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  14%|#4        | 3/21 [00:56<03:48, 12.70s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  19%|#9        | 4/21 [00:56<02:11,  7.73s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  24%|##3       | 5/21 [00:56<01:19,\n4.98s/it]', '\\rTraining synthetic_injection epoch 1/1:  29%|##8       | 6/21\n[00:56<00:49,  3.33s/it]', '\\rTraining synthetic_injection epoch 1/1:  33%|###3\n| 7/21 [00:57<00:31,  2.28s/it]', '\\rTraining synthetic_injection epoch 1/1:\n38%|###8      | 8/21 [00:57<00:20,  1.59s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  43%|####2     | 9/21 [00:57<00:13,  1.13s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  48%|####7     | 10/21 [00:57<00:08,  1.23it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  52%|#####2    | 11/21 [00:57<00:06,\n1.66it/s]', '\\rTraining synthetic_injection epoch 1/1:  57%|#####7    | 12/21\n[00:57<00:04,  2.21it/s]', '\\rTraining synthetic_injection epoch 1/1:\n62%|######1   | 13/21 [00:57<00:02,  2.85it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  67%|######6   | 14/21 [00:57<00:01,  3.58it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  71%|#######1  | 15/21 [00:57<00:01,  4.35it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  76%|#######6  | 16/21 [00:58<00:00,\n5.12it/s]', '\\rTraining synthetic_injection epoch 1/1:  81%|########  | 17/21\n[00:58<00:00,  5.85it/s]', '\\rTraining synthetic_injection epoch 1/1:\n86%|########5 | 18/21 [00:58<00:00,  6.49it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  90%|######### | 19/21 [00:58<00:00,  7.03it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  95%|#########5| 20/21 [00:58<00:00,  7.45it/s]',\n'', '\\rTraining synthetic_injection epoch 1/1: 100%|##########| 21/21\n[00:59<00:00,  2.84s/it]', '\\n', 'Epoch 1: validation_loss = 3.2268', '\\n',\n'\\rMap:   0%|          | 0/128 [00:00<?, ? examples/s]', '', '\\rMap:\n100%|##########| 128/128 [00:00<00:00, 17555.13 examples/s]', '\\n',\n'\\rREADME.md: 0.00B [00:00, ?B/s]', '', '\\rREADME.md: 8.07kB [00:00, 20.2MB/s]',\n'\\n', '\\rdata/train-00000-of-00001.parquet:   0%|          | 0.00/18.6M\n[00:00<?, ?B/s]', '\\rdata/train-00000-of-00001.parquet:   0%|          |\n15.8k/18.6M [00:01<25:09, 12.3kB/s]', '\\rdata/train-00000-of-00001.parquet:\n3%|2         | 552k/18.6M [00:01<00:32, 549kB/s]  ',\n'\\rdata/train-00000-of-00001.parquet: 100%|##########| 18.6M/18.6M [00:02<00:00,\n9.89MB/s]', '', '\\rdata/train-00000-of-00001.parquet: 100%|##########|\n18.6M/18.6M [00:02<00:00, 7.30MB/s]', '\\n', '\\rdata/test-00000-of-00001.parquet:\n0%|          | 0.00/1.23M [00:00<?, ?B/s]', '\\rdata/test-00000-of-00001.parquet:\n3%|3         | 37.8k/1.23M [00:00<00:17, 67.0kB/s]',\n'\\rdata/test-00000-of-00001.parquet: 100%|##########| 1.23M/1.23M [00:01<00:00,\n913kB/s] ', '', '\\rdata/test-00000-of-00001.parquet: 100%|##########|\n1.23M/1.23M [00:01<00:00, 817kB/s]', '\\n', '\\rGenerating train split:   0%|\n| 0/120000 [00:00<?, ? examples/s]', '\\rGenerating train split:   1%|          |\n1000/120000 [00:00<00:16, 7099.56 examples/s]', '\\rGenerating train split:\n84%|########4 | 101000/120000 [00:00<00:00, 502629.99 examples/s]', '',\n'\\rGenerating train split: 100%|##########| 120000/120000 [00:00<00:00,\n441843.66 examples/s]', '\\n', '\\rGenerating test split:   0%|          | 0/7600\n[00:00<?, ? examples/s]', '', '\\rGenerating test split: 100%|##########|\n7600/7600 [00:00<00:00, 182667.23 examples/s]', '\\n', '\\rMap:   0%|          |\n0/30000 [00:00<?, ? examples/s]', '\\rMap:  12%|#2        | 3716/30000\n[00:00<00:00, 36977.19 examples/s]', '\\rMap:  26%|##6       | 7808/30000\n[00:00<00:00, 39281.57 examples/s]', '\\rMap:  46%|####6     | 13948/30000\n[00:00<00:00, 40199.51 examples/s]', '\\rMap:  66%|######6   | 19930/30000\n[00:00<00:00, 40053.03 examples/s]', '\\rMap:  87%|########6 | 25978/30000\n[00:00<00:00, 40153.29 examples/s]', '\\rMap: 100%|##########| 30000/30000\n[00:00<00:00, 39727.53 examples/s]', '', '\\rMap: 100%|##########| 30000/30000\n[00:00<00:00, 39061.62 examples/s]', '\\n', '\\rMap:   0%|          | 0/1900\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 1900/1900 [00:00<00:00,\n27915.40 examples/s]', '\\n', '\\rREADME.md: 0.00B [00:00, ?B/s]', '',\n'\\rREADME.md: 7.81kB [00:00, 21.1MB/s]', '\\n',\n'\\rplain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M\n[00:00<?, ?B/s]', '\\rplain_text/train-00000-of-00001.parquet: 100%|##########|\n21.0M/21.0M [00:03<00:00, 5.31MB/s]', '',\n'\\rplain_text/train-00000-of-00001.parquet: 100%|##########| 21.0M/21.0M\n[00:03<00:00, 5.31MB/s]', '\\n', '\\rplain_text/test-00000-of-00001.parquet:   0%|\n| 0.00/20.5M [00:00<?, ?B/s]', '\\rplain_text/test-00000-of-00001.parquet:   0%|\n| 49.2k/20.5M [00:00<03:07, 109kB/s]',\n'\\rplain_text/test-00000-of-00001.parquet: 100%|##########| 20.5M/20.5M\n[00:01<00:00, 22.0MB/s]', '', '\\rplain_text/test-00000-of-00001.parquet:\n100%|##########| 20.5M/20.5M [00:01<00:00, 19.2MB/s]', '\\n',\n'\\rplain_text/unsupervised-00000-of-00001.p(\u2026):   0%|          | 0.00/42.0M\n[00:00<?, ?B/s]', '\\rplain_text/unsupervised-00000-of-00001.p(\u2026):  60%|######\n| 25.2M/42.0M [00:01<00:00, 17.8MB/s]',\n'\\rplain_text/unsupervised-00000-of-00001.p(\u2026): 100%|##########| 42.0M/42.0M\n[00:01<00:00, 22.8MB/s]', '', '\\rplain_text/unsupervised-00000-of-00001.p(\u2026):\n100%|##########| 42.0M/42.0M [00:01<00:00, 21.7MB/s]', '\\n', '\\rGenerating train\nsplit:   0%|          | 0/25000 [00:00<?, ? examples/s]', '\\rGenerating train\nsplit:  64%|######4   | 16000/25000 [00:00<00:00, 146848.46 examples/s]', '',\n'\\rGenerating train split: 100%|##########| 25000/25000 [00:00<00:00, 127985.47\nexamples/s]', '\\n', '\\rGenerating test split:   0%|          | 0/25000 [00:00<?,\n? examples/s]', '\\rGenerating test split:  68%|######8   | 17000/25000\n[00:00<00:00, 161466.97 examples/s]', '', '\\rGenerating test split:\n100%|##########| 25000/25000 [00:00<00:00, 80992.26 examples/s] ', '\\n',\n'\\rGenerating unsupervised split:   0%|          | 0/50000 [00:00<?, ?\nexamples/s]', '\\rGenerating unsupervised split:  26%|##6       | 13000/50000\n[00:00<00:00, 118794.22 examples/s]', '\\rGenerating unsupervised split:\n76%|#######6  | 38000/50000 [00:00<00:00, 186955.31 examples/s]', '',\n'\\rGenerating unsupervised split: 100%|##########| 50000/50000 [00:00<00:00,\n178871.49 examples/s]', '\\n', '\\rMap:   0%|          | 0/6250 [00:00<?, ?\nexamples/s]', '\\rMap:  49%|####9     | 3089/6250 [00:00<00:00, 30536.13\nexamples/s]', '\\rMap: 100%|##########| 6250/6250 [00:00<00:00, 28038.83\nexamples/s]', '', '\\rMap: 100%|##########| 6250/6250 [00:00<00:00, 26684.77\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/6250 [00:00<?, ? examples/s]',\n'\\rMap:  48%|####8     | 3000/6250 [00:00<00:00, 28321.39 examples/s]', '\\rMap:\n95%|#########4| 5937/6250 [00:00<00:00, 28917.72 examples/s]', '', '\\rMap:\n100%|##########| 6250/6250 [00:00<00:00, 24150.14 examples/s]', '\\n', '\\n====\nOverwrite phase: overwrite_wikitext ====', '\\n', '\\rMap:   0%|          | 0/9180\n[00:00<?, ? examples/s]', '\\rMap:  22%|##1       | 2000/9180 [00:00<00:01,\n4624.67 examples/s]', '\\rMap:  65%|######5   | 6000/9180 [00:00<00:00, 11004.01\nexamples/s]', '\\rMap:  98%|#########8| 9000/9180 [00:00<00:00, 13668.82\nexamples/s]', '', '\\rMap: 100%|##########| 9180/9180 [00:00<00:00, 11108.18\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/3760 [00:00<?, ? examples/s]',\n'\\rMap:  80%|#######9  | 3000/3760 [00:00<00:00, 20169.84 examples/s]', '',\n'\\rMap: 100%|##########| 3760/3760 [00:00<00:00, 15758.62 examples/s]', '\\n',\n'\\roverwrite_wikitext epoch 1/2:   0%|          | 0/302 [00:00<?, ?it/s]', '',\n'\\roverwrite_wikitext epoch 1/2:   0%|          | 0/302 [01:40<?, ?it/s]', '\\n',\n'Traceback (most recent call last):\\n  File \"runfile.py\", line 677, in\n<module>\\n    overwrite_phase(\\n  File \"runfile.py\", line 494, in\noverwrite_phase\\n    anchor_batch = next(anchor_iter)\\n\n^^^^^^^^^^^\\nUnboundLocalError: cannot access local variable \\'anchor_iter\\'\nwhere it is not associated with a value\\n', 'Execution time: 5 minutes seconds\n(time limit is 2 hours).']", "['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls: [' apple', '\ntable', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n34689.61 examples/s]', '\\n', '\\rMap:   0%|          | 0/300 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00, 28791.87\nexamples/s]', '\\n', '\\rTraining synthetic_injection epoch 1/1:   0%|          |\n0/21 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 1/1:   5%|4\n| 1/21 [00:54<18:15, 54.79s/it]', '\\rTraining synthetic_injection epoch 1/1:\n10%|9         | 2/21 [00:54<07:09, 22.63s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  14%|#4        | 3/21 [00:55<03:42, 12.35s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  19%|#9        | 4/21 [00:55<02:07,  7.52s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  24%|##3       | 5/21 [00:55<01:17,\n4.85s/it]', '\\rTraining synthetic_injection epoch 1/1:  29%|##8       | 6/21\n[00:55<00:48,  3.24s/it]', '\\rTraining synthetic_injection epoch 1/1:  33%|###3\n| 7/21 [00:55<00:31,  2.22s/it]', '\\rTraining synthetic_injection epoch 1/1:\n38%|###8      | 8/21 [00:55<00:20,  1.55s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  43%|####2     | 9/21 [00:55<00:13,  1.10s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  48%|####7     | 10/21 [00:55<00:08,  1.26it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  52%|#####2    | 11/21 [00:55<00:05,\n1.70it/s]', '\\rTraining synthetic_injection epoch 1/1:  57%|#####7    | 12/21\n[00:56<00:03,  2.25it/s]', '\\rTraining synthetic_injection epoch 1/1:\n62%|######1   | 13/21 [00:56<00:02,  2.90it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  67%|######6   | 14/21 [00:56<00:01,  3.63it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  71%|#######1  | 15/21 [00:56<00:01,  4.41it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  76%|#######6  | 16/21 [00:56<00:00,\n5.17it/s]', '\\rTraining synthetic_injection epoch 1/1:  81%|########  | 17/21\n[00:56<00:00,  5.89it/s]', '\\rTraining synthetic_injection epoch 1/1:\n86%|########5 | 18/21 [00:56<00:00,  6.52it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  90%|######### | 19/21 [00:56<00:00,  7.05it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  95%|#########5| 20/21 [00:56<00:00,  7.48it/s]',\n'', '\\rTraining synthetic_injection epoch 1/1: 100%|##########| 21/21\n[00:57<00:00,  2.76s/it]', '\\n', 'Epoch 1: validation_loss = 3.2268', '\\n',\n'\\rMap:   0%|          | 0/128 [00:00<?, ? examples/s]', '', '\\rMap:\n100%|##########| 128/128 [00:00<00:00, 17576.39 examples/s]', '\\n', '\\rMap:\n0%|          | 0/30000 [00:00<?, ? examples/s]', '\\rMap:  13%|#3        |\n3952/30000 [00:00<00:00, 39342.57 examples/s]', '\\rMap:  27%|##6       |\n8000/30000 [00:00<00:00, 38888.87 examples/s]', '\\rMap:  40%|####      |\n12075/30000 [00:00<00:00, 39330.90 examples/s]', '\\rMap:  60%|######    |\n18079/30000 [00:00<00:00, 39663.39 examples/s]', '\\rMap:  74%|#######4  |\n22243/30000 [00:00<00:00, 40286.04 examples/s]', '\\rMap:  94%|#########3|\n28163/30000 [00:00<00:00, 39946.27 examples/s]', '', '\\rMap: 100%|##########|\n30000/30000 [00:00<00:00, 38835.39 examples/s]', '\\n', '\\rMap:   0%|          |\n0/1900 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 1900/1900\n[00:00<00:00, 31853.27 examples/s]', '\\n', '\\n==== Overwrite phase:\noverwrite_wikitext ====', '\\n', '\\rMap:   0%|          | 0/2295 [00:00<?, ?\nexamples/s]', '\\rMap:  87%|########7 | 2000/2295 [00:00<00:00, 17132.54\nexamples/s]', '', '\\rMap: 100%|##########| 2295/2295 [00:00<00:00, 15338.51\nexamples/s]', '\\n', '\\roverwrite_wikitext epoch 1/2:   0%|          | 0/76\n[00:00<?, ?it/s]', '\\roverwrite_wikitext epoch 1/2:   1%|1         | 1/76\n[01:02<1:18:42, 62.96s/it]', '\\roverwrite_wikitext epoch 1/2:   4%|3         |\n3/76 [01:03<19:55, 16.37s/it]  ', '\\roverwrite_wikitext epoch 1/2:   7%|6\n| 5/76 [01:03<09:27,  7.99s/it]', '\\roverwrite_wikitext epoch 1/2:   9%|9\n| 7/76 [01:03<05:19,  4.63s/it]', '\\roverwrite_wikitext epoch 1/2:  12%|#1\n| 9/76 [01:03<03:15,  2.91s/it]', '\\roverwrite_wikitext epoch 1/2:  14%|#4\n| 11/76 [01:03<02:04,  1.92s/it]', '\\roverwrite_wikitext epoch 1/2:  17%|#7\n| 13/76 [01:03<01:21,  1.30s/it]', '\\roverwrite_wikitext epoch 1/2:  20%|#9\n| 15/76 [01:03<00:55,  1.11it/s]', '\\roverwrite_wikitext epoch 1/2:  22%|##2\n| 17/76 [01:04<00:37,  1.56it/s]', '\\roverwrite_wikitext epoch 1/2:  25%|##5\n| 19/76 [01:04<00:26,  2.17it/s]', '\\roverwrite_wikitext epoch 1/2:  28%|##7\n| 21/76 [01:04<00:18,  2.94it/s]', '\\roverwrite_wikitext epoch 1/2:  30%|###\n| 23/76 [01:04<00:13,  3.89it/s]', '\\roverwrite_wikitext epoch 1/2:  33%|###2\n| 25/76 [01:04<00:10,  5.02it/s]', '\\roverwrite_wikitext epoch 1/2:  36%|###5\n| 27/76 [01:04<00:07,  6.28it/s]', '\\roverwrite_wikitext epoch 1/2:  38%|###8\n| 29/76 [01:04<00:06,  7.60it/s]', '\\roverwrite_wikitext epoch 1/2:  41%|####\n| 31/76 [01:04<00:05,  8.88it/s]', '\\roverwrite_wikitext epoch 1/2:  43%|####3\n| 33/76 [01:05<00:04, 10.12it/s]', '\\roverwrite_wikitext epoch 1/2:  46%|####6\n| 35/76 [01:05<00:03, 11.22it/s]', '\\roverwrite_wikitext epoch 1/2:  49%|####8\n| 37/76 [01:05<00:03, 12.14it/s]', '\\roverwrite_wikitext epoch 1/2:  51%|#####1\n| 39/76 [01:05<00:02, 12.89it/s]', '\\roverwrite_wikitext epoch 1/2:  54%|#####3\n| 41/76 [01:05<00:02, 13.47it/s]', '\\roverwrite_wikitext epoch 1/2:  57%|#####6\n| 43/76 [01:05<00:02, 13.87it/s]', '\\roverwrite_wikitext epoch 1/2:  59%|#####9\n| 45/76 [01:05<00:02, 14.10it/s]', '\\roverwrite_wikitext epoch 1/2:  62%|######1\n| 47/76 [01:06<00:02, 14.36it/s]', '\\roverwrite_wikitext epoch 1/2:  64%|######4\n| 49/76 [01:06<00:01, 14.48it/s]', '\\roverwrite_wikitext epoch 1/2:  67%|######7\n| 51/76 [01:06<00:01, 14.61it/s]', '\\roverwrite_wikitext epoch 1/2:  70%|######9\n| 53/76 [01:06<00:01, 14.72it/s]', '\\roverwrite_wikitext epoch 1/2:\n72%|#######2  | 55/76 [01:06<00:01, 14.77it/s]', '\\roverwrite_wikitext epoch\n1/2:  75%|#######5  | 57/76 [01:06<00:01, 14.82it/s]', '\\roverwrite_wikitext\nepoch 1/2:  78%|#######7  | 59/76 [01:06<00:01, 14.85it/s]',\n'\\roverwrite_wikitext epoch 1/2:  80%|########  | 61/76 [01:06<00:01,\n14.91it/s]', '\\roverwrite_wikitext epoch 1/2:  83%|########2 | 63/76\n[01:07<00:00, 14.89it/s]', '\\roverwrite_wikitext epoch 1/2:  86%|########5 |\n65/76 [01:07<00:00, 14.88it/s]', '\\roverwrite_wikitext epoch 1/2:  88%|########8\n| 67/76 [01:07<00:00, 14.93it/s]', '\\roverwrite_wikitext epoch 1/2:\n91%|######### | 69/76 [01:07<00:00, 14.96it/s]', '\\roverwrite_wikitext epoch\n1/2:  93%|#########3| 71/76 [01:07<00:00, 14.99it/s]', '\\roverwrite_wikitext\nepoch 1/2:  96%|#########6| 73/76 [01:07<00:00, 14.95it/s]',\n'\\roverwrite_wikitext epoch 1/2:  99%|#########8| 75/76 [01:07<00:00,\n15.00it/s]', '', '\\roverwrite_wikitext epoch 1/2: 100%|##########| 76/76\n[01:08<00:00,  1.10it/s]', '\\n', 'Epoch 1: validation_loss = 3.6909', '\\n',\n'\\roverwrite_wikitext epoch 2/2:   0%|          | 0/76 [00:00<?, ?it/s]',\n'\\roverwrite_wikitext epoch 2/2:   1%|1         | 1/76 [01:07<1:23:50,\n67.08s/it]', '\\roverwrite_wikitext epoch 2/2:   4%|3         | 3/76\n[01:07<21:13, 17.45s/it]  ', '\\roverwrite_wikitext epoch 2/2:   7%|6         |\n5/76 [01:07<10:04,  8.51s/it]', '\\roverwrite_wikitext epoch 2/2:   9%|9\n| 7/76 [01:07<05:40,  4.94s/it]', '\\roverwrite_wikitext epoch 2/2:  12%|#1\n| 9/76 [01:07<03:27,  3.10s/it]', '\\roverwrite_wikitext epoch 2/2:  14%|#4\n| 11/76 [01:07<02:12,  2.04s/it]', '\\roverwrite_wikitext epoch 2/2:  17%|#7\n| 13/76 [01:07<01:27,  1.38s/it]', '\\roverwrite_wikitext epoch 2/2:  20%|#9\n| 15/76 [01:08<00:58,  1.04it/s]', '\\roverwrite_wikitext epoch 2/2:  22%|##2\n| 17/76 [01:08<00:40,  1.47it/s]', '\\roverwrite_wikitext epoch 2/2:  25%|##5\n| 19/76 [01:08<00:27,  2.04it/s]', '\\roverwrite_wikitext epoch 2/2:  28%|##7\n| 21/76 [01:08<00:19,  2.78it/s]', '\\roverwrite_wikitext epoch 2/2:  30%|###\n| 23/76 [01:08<00:14,  3.70it/s]', '\\roverwrite_wikitext epoch 2/2:  33%|###2\n| 25/76 [01:08<00:10,  4.80it/s]', '\\roverwrite_wikitext epoch 2/2:  36%|###5\n| 27/76 [01:08<00:08,  6.03it/s]', '\\roverwrite_wikitext epoch 2/2:  38%|###8\n| 29/76 [01:09<00:06,  7.35it/s]', '\\roverwrite_wikitext epoch 2/2:  41%|####\n| 31/76 [01:09<00:05,  8.59it/s]', '\\roverwrite_wikitext epoch 2/2:  43%|####3\n| 33/76 [01:09<00:04,  9.84it/s]', '\\roverwrite_wikitext epoch 2/2:  46%|####6\n| 35/76 [01:09<00:03, 10.89it/s]', '\\roverwrite_wikitext epoch 2/2:  49%|####8\n| 37/76 [01:09<00:03, 11.83it/s]', '\\roverwrite_wikitext epoch 2/2:  51%|#####1\n| 39/76 [01:09<00:02, 12.60it/s]', '\\roverwrite_wikitext epoch 2/2:  54%|#####3\n| 41/76 [01:09<00:02, 13.14it/s]', '\\roverwrite_wikitext epoch 2/2:  57%|#####6\n| 43/76 [01:09<00:02, 13.61it/s]', '\\roverwrite_wikitext epoch 2/2:  59%|#####9\n| 45/76 [01:10<00:02, 13.97it/s]', '\\roverwrite_wikitext epoch 2/2:  62%|######1\n| 47/76 [01:10<00:02, 14.21it/s]', '\\roverwrite_wikitext epoch 2/2:  64%|######4\n| 49/76 [01:10<00:01, 14.39it/s]', '\\roverwrite_wikitext epoch 2/2:  67%|######7\n| 51/76 [01:10<00:01, 14.50it/s]', '\\roverwrite_wikitext epoch 2/2:  70%|######9\n| 53/76 [01:10<00:01, 14.47it/s]', '\\roverwrite_wikitext epoch 2/2:\n72%|#######2  | 55/76 [01:10<00:01, 14.61it/s]', '\\roverwrite_wikitext epoch\n2/2:  75%|#######5  | 57/76 [01:10<00:01, 14.64it/s]', '\\roverwrite_wikitext\nepoch 2/2:  78%|#######7  | 59/76 [01:11<00:01, 14.49it/s]',\n'\\roverwrite_wikitext epoch 2/2:  80%|########  | 61/76 [01:11<00:01,\n14.52it/s]', '\\roverwrite_wikitext epoch 2/2:  83%|########2 | 63/76\n[01:11<00:00, 14.60it/s]', '\\roverwrite_wikitext epoch 2/2:  86%|########5 |\n65/76 [01:11<00:00, 14.71it/s]', '\\roverwrite_wikitext epoch 2/2:  88%|########8\n| 67/76 [01:11<00:00, 14.66it/s]', '\\roverwrite_wikitext epoch 2/2:\n91%|######### | 69/76 [01:11<00:00, 14.70it/s]', '\\roverwrite_wikitext epoch\n2/2:  93%|#########3| 71/76 [01:11<00:00, 14.70it/s]', '\\roverwrite_wikitext\nepoch 2/2:  96%|#########6| 73/76 [01:12<00:00, 14.74it/s]',\n'\\roverwrite_wikitext epoch 2/2:  99%|#########8| 75/76 [01:12<00:00,\n14.82it/s]', '', '\\roverwrite_wikitext epoch 2/2: 100%|##########| 76/76\n[01:13<00:00,  1.04it/s]', '\\n', 'Epoch 2: validation_loss = 3.7060', '\\n',\n'overwrite_wikitext PHR (median rare HL / median common HL) = 0.0000', '\\n',\n'\\n==== Overwrite phase: overwrite_agnews ====', '\\n', '\\rMap:   0%|          |\n0/7500 [00:00<?, ? examples/s]', '\\rMap:  27%|##6       | 2000/7500\n[00:00<00:00, 14898.70 examples/s]', '\\rMap:  53%|#####3    | 4000/7500\n[00:00<00:00, 16584.02 examples/s]', '\\rMap:  80%|########  | 6000/7500\n[00:00<00:00, 7440.51 examples/s] ', '', '\\rMap: 100%|##########| 7500/7500\n[00:00<00:00, 9425.35 examples/s]', '\\n', '\\rMap:   0%|          | 0/1900\n[00:00<?, ? examples/s]', '\\rMap: 100%|##########| 1900/1900 [00:00<00:00,\n18231.31 examples/s]', '', '\\rMap: 100%|##########| 1900/1900 [00:00<00:00,\n16969.34 examples/s]', '\\n', '\\roverwrite_agnews epoch 1/2:   0%|          |\n0/247 [00:00<?, ?it/s]', '\\roverwrite_agnews epoch 1/2:   0%|          | 1/247\n[01:06<4:33:30, 66.71s/it]', '\\roverwrite_agnews epoch 1/2:   1%|1         |\n3/247 [01:06<1:10:33, 17.35s/it]', '\\roverwrite_agnews epoch 1/2:   2%|2\n| 5/247 [01:06<34:08,  8.46s/it]  ', '\\roverwrite_agnews epoch 1/2:   3%|2\n| 7/247 [01:07<19:38,  4.91s/it]', '\\roverwrite_agnews epoch 1/2:   4%|3\n| 9/247 [01:07<12:14,  3.09s/it]', '\\roverwrite_agnews epoch 1/2:   4%|4\n| 11/247 [01:07<07:58,  2.03s/it]', '\\roverwrite_agnews epoch 1/2:   5%|5\n| 13/247 [01:07<05:21,  1.38s/it]', '\\roverwrite_agnews epoch 1/2:   6%|6\n| 15/247 [01:07<03:41,  1.05it/s]', '\\roverwrite_agnews epoch 1/2:   7%|6\n| 17/247 [01:07<02:35,  1.48it/s]', '\\roverwrite_agnews epoch 1/2:   8%|7\n| 19/247 [01:07<01:51,  2.05it/s]', '\\roverwrite_agnews epoch 1/2:   9%|8\n| 21/247 [01:08<01:21,  2.79it/s]', '\\roverwrite_agnews epoch 1/2:   9%|9\n| 23/247 [01:08<01:00,  3.69it/s]', '\\roverwrite_agnews epoch 1/2:  10%|#\n| 25/247 [01:08<00:46,  4.78it/s]', '\\roverwrite_agnews epoch 1/2:  11%|#\n| 27/247 [01:08<00:36,  6.02it/s]', '\\roverwrite_agnews epoch 1/2:  12%|#1\n| 29/247 [01:08<00:29,  7.32it/s]', '\\roverwrite_agnews epoch 1/2:  13%|#2\n| 31/247 [01:08<00:24,  8.65it/s]', '\\roverwrite_agnews epoch 1/2:  13%|#3\n| 33/247 [01:08<00:21,  9.82it/s]', '\\roverwrite_agnews epoch 1/2:  14%|#4\n| 35/247 [01:09<00:19, 10.80it/s]', '\\roverwrite_agnews epoch 1/2:  15%|#4\n| 37/247 [01:09<00:18, 11.63it/s]', '\\roverwrite_agnews epoch 1/2:  16%|#5\n| 39/247 [01:09<00:17, 12.18it/s]', '\\roverwrite_agnews epoch 1/2:  17%|#6\n| 41/247 [01:09<00:16, 12.84it/s]', '\\roverwrite_agnews epoch 1/2:  17%|#7\n| 43/247 [01:09<00:15, 13.01it/s]', '\\roverwrite_agnews epoch 1/2:  18%|#8\n| 45/247 [01:09<00:14, 13.51it/s]', '\\roverwrite_agnews epoch 1/2:  19%|#9\n| 47/247 [01:09<00:14, 13.86it/s]', '\\roverwrite_agnews epoch 1/2:  20%|#9\n| 49/247 [01:10<00:14, 14.11it/s]', '\\roverwrite_agnews epoch 1/2:  21%|##\n| 51/247 [01:10<00:13, 14.31it/s]', '\\roverwrite_agnews epoch 1/2:  21%|##1\n| 53/247 [01:10<00:13, 14.48it/s]', '\\roverwrite_agnews epoch 1/2:  22%|##2\n| 55/247 [01:10<00:13, 14.60it/s]', '\\roverwrite_agnews epoch 1/2:  23%|##3\n| 57/247 [01:10<00:12, 14.69it/s]', '\\roverwrite_agnews epoch 1/2:  24%|##3\n| 59/247 [01:10<00:12, 14.49it/s]', '\\roverwrite_agnews epoch 1/2:  25%|##4\n| 61/247 [01:10<00:12, 14.48it/s]', '\\roverwrite_agnews epoch 1/2:  26%|##5\n| 63/247 [01:10<00:12, 14.61it/s]', '\\roverwrite_agnews epoch 1/2:  26%|##6\n| 65/247 [01:11<00:12, 14.54it/s]', '\\roverwrite_agnews epoch 1/2:  27%|##7\n| 67/247 [01:11<00:12, 14.51it/s]', '\\roverwrite_agnews epoch 1/2:  28%|##7\n| 69/247 [01:11<00:12, 14.52it/s]', '\\roverwrite_agnews epoch 1/2:  29%|##8\n| 71/247 [01:11<00:12, 14.30it/s]', '\\roverwrite_agnews epoch 1/2:  30%|##9\n| 73/247 [01:11<00:12, 14.46it/s]', '\\roverwrite_agnews epoch 1/2:  30%|###\n| 75/247 [01:11<00:11, 14.34it/s]', '\\roverwrite_agnews epoch 1/2:  31%|###1\n| 77/247 [01:11<00:11, 14.40it/s]', '\\roverwrite_agnews epoch 1/2:  32%|###1\n| 79/247 [01:12<00:11, 14.56it/s]', '\\roverwrite_agnews epoch 1/2:  33%|###2\n| 81/247 [01:12<00:11, 14.47it/s]', '\\roverwrite_agnews epoch 1/2:  34%|###3\n| 83/247 [01:12<00:11, 14.58it/s]', '\\roverwrite_agnews epoch 1/2:  34%|###4\n| 85/247 [01:12<00:11, 14.55it/s]', '\\roverwrite_agnews epoch 1/2:  35%|###5\n| 87/247 [01:12<00:10, 14.55it/s]', '\\roverwrite_agnews epoch 1/2:  36%|###6\n| 89/247 [01:12<00:10, 14.62it/s]', '\\roverwrite_agnews epoch 1/2:  37%|###6\n| 91/247 [01:12<00:10, 14.32it/s]', '\\roverwrite_agnews epoch 1/2:  38%|###7\n| 93/247 [01:13<00:10, 14.44it/s]', '\\roverwrite_agnews epoch 1/2:  38%|###8\n| 95/247 [01:13<00:10, 14.55it/s]', '\\roverwrite_agnews epoch 1/2:  39%|###9\n| 97/247 [01:13<00:10, 14.62it/s]', '\\roverwrite_agnews epoch 1/2:  40%|####\n| 99/247 [01:13<00:10, 14.68it/s]', '\\roverwrite_agnews epoch 1/2:  41%|####\n| 101/247 [01:13<00:10, 14.43it/s]', '\\roverwrite_agnews epoch 1/2:  42%|####1\n| 103/247 [01:13<00:10, 14.35it/s]', '\\roverwrite_agnews epoch 1/2:  43%|####2\n| 105/247 [01:13<00:09, 14.45it/s]', '\\roverwrite_agnews epoch 1/2:  43%|####3\n| 107/247 [01:14<00:09, 14.56it/s]', '\\roverwrite_agnews epoch 1/2:  44%|####4\n| 109/247 [01:14<00:09, 14.53it/s]', '\\roverwrite_agnews epoch 1/2:  45%|####4\n| 111/247 [01:14<00:09, 14.62it/s]', '\\roverwrite_agnews epoch 1/2:  46%|####5\n| 113/247 [01:14<00:09, 14.58it/s]', '\\roverwrite_agnews epoch 1/2:  47%|####6\n| 115/247 [01:14<00:09, 14.65it/s]', '\\roverwrite_agnews epoch 1/2:  47%|####7\n| 117/247 [01:14<00:08, 14.61it/s]', '\\roverwrite_agnews epoch 1/2:  48%|####8\n| 119/247 [01:14<00:08, 14.66it/s]', '\\roverwrite_agnews epoch 1/2:  49%|####8\n| 121/247 [01:14<00:08, 14.64it/s]', '\\roverwrite_agnews epoch 1/2:  50%|####9\n| 123/247 [01:15<00:08, 14.43it/s]', '\\roverwrite_agnews epoch 1/2:  51%|#####\n| 125/247 [01:15<00:08, 14.56it/s]', '\\roverwrite_agnews epoch 1/2:  51%|#####1\n| 127/247 [01:15<00:08, 14.64it/s]', '\\roverwrite_agnews epoch 1/2:  52%|#####2\n| 129/247 [01:15<00:08, 14.62it/s]', '\\roverwrite_agnews epoch 1/2:  53%|#####3\n| 131/247 [01:15<00:07, 14.63it/s]', '\\roverwrite_agnews epoch 1/2:  54%|#####3\n| 133/247 [01:15<00:07, 14.57it/s]', '\\roverwrite_agnews epoch 1/2:  55%|#####4\n| 135/247 [01:15<00:07, 14.47it/s]', '\\roverwrite_agnews epoch 1/2:  55%|#####5\n| 137/247 [01:16<00:07, 14.62it/s]', '\\roverwrite_agnews epoch 1/2:  56%|#####6\n| 139/247 [01:16<00:07, 14.31it/s]', '\\roverwrite_agnews epoch 1/2:  57%|#####7\n| 141/247 [01:16<00:07, 14.24it/s]', '\\roverwrite_agnews epoch 1/2:  58%|#####7\n| 143/247 [01:16<00:07, 14.31it/s]', '\\roverwrite_agnews epoch 1/2:  59%|#####8\n| 145/247 [01:16<00:07, 14.28it/s]', '\\roverwrite_agnews epoch 1/2:  60%|#####9\n| 147/247 [01:16<00:06, 14.39it/s]', '\\roverwrite_agnews epoch 1/2:  60%|######\n| 149/247 [01:16<00:06, 14.30it/s]', '\\roverwrite_agnews epoch 1/2:  61%|######1\n| 151/247 [01:17<00:06, 14.40it/s]', '\\roverwrite_agnews epoch 1/2:  62%|######1\n| 153/247 [01:17<00:06, 14.53it/s]', '\\roverwrite_agnews epoch 1/2:  63%|######2\n| 155/247 [01:17<00:06, 14.63it/s]', '\\roverwrite_agnews epoch 1/2:  64%|######3\n| 157/247 [01:17<00:06, 14.71it/s]', '\\roverwrite_agnews epoch 1/2:  64%|######4\n| 159/247 [01:17<00:06, 14.64it/s]', '\\roverwrite_agnews epoch 1/2:  65%|######5\n| 161/247 [01:17<00:05, 14.71it/s]', '\\roverwrite_agnews epoch 1/2:  66%|######5\n| 163/247 [01:17<00:05, 14.72it/s]', '\\roverwrite_agnews epoch 1/2:  67%|######6\n| 165/247 [01:18<00:05, 14.54it/s]', '\\roverwrite_agnews epoch 1/2:  68%|######7\n| 167/247 [01:18<00:05, 14.64it/s]', '\\roverwrite_agnews epoch 1/2:  68%|######8\n| 169/247 [01:18<00:05, 14.62it/s]', '\\roverwrite_agnews epoch 1/2:  69%|######9\n| 171/247 [01:18<00:05, 14.52it/s]', '\\roverwrite_agnews epoch 1/2:  70%|#######\n| 173/247 [01:18<00:05, 14.60it/s]', '\\roverwrite_agnews epoch 1/2:  71%|#######\n| 175/247 [01:18<00:04, 14.69it/s]', '\\roverwrite_agnews epoch 1/2:\n72%|#######1  | 177/247 [01:18<00:04, 14.61it/s]', '\\roverwrite_agnews epoch\n1/2:  72%|#######2  | 179/247 [01:18<00:04, 14.67it/s]', '\\roverwrite_agnews\nepoch 1/2:  73%|#######3  | 181/247 [01:19<00:04, 14.67it/s]',\n'\\roverwrite_agnews epoch 1/2:  74%|#######4  | 183/247 [01:19<00:04,\n14.29it/s]', '\\roverwrite_agnews epoch 1/2:  75%|#######4  | 185/247\n[01:19<00:04, 14.40it/s]', '\\roverwrite_agnews epoch 1/2:  76%|#######5  |\n187/247 [01:19<00:04, 14.45it/s]', '\\roverwrite_agnews epoch 1/2:  77%|#######6\n| 189/247 [01:19<00:03, 14.52it/s]', '\\roverwrite_agnews epoch 1/2:\n77%|#######7  | 191/247 [01:19<00:03, 14.64it/s]', '\\roverwrite_agnews epoch\n1/2:  78%|#######8  | 193/247 [01:19<00:03, 14.65it/s]', '\\roverwrite_agnews\nepoch 1/2:  79%|#######8  | 195/247 [01:20<00:03, 14.72it/s]',\n'\\roverwrite_agnews epoch 1/2:  80%|#######9  | 197/247 [01:20<00:03,\n14.77it/s]', '\\roverwrite_agnews epoch 1/2:  81%|########  | 199/247\n[01:20<00:03, 14.72it/s]', '[2025-12-03 20:33:25] overwrite_agnews step 200:\navg_train_loss=3.5764', '\\n', '\\roverwrite_agnews epoch 1/2:  81%|########1 |\n201/247 [01:20<00:03, 14.80it/s]', '\\roverwrite_agnews epoch 1/2:  82%|########2\n| 203/247 [01:20<00:02, 14.72it/s]', '\\roverwrite_agnews epoch 1/2:\n83%|########2 | 205/247 [01:20<00:02, 14.61it/s]', '\\roverwrite_agnews epoch\n1/2:  84%|########3 | 207/247 [01:20<00:02, 14.63it/s]', '\\roverwrite_agnews\nepoch 1/2:  85%|########4 | 209/247 [01:21<00:02, 14.53it/s]',\n'\\roverwrite_agnews epoch 1/2:  85%|########5 | 211/247 [01:21<00:02,\n14.50it/s]', '\\roverwrite_agnews epoch 1/2:  86%|########6 | 213/247\n[01:21<00:02, 14.41it/s]', '\\roverwrite_agnews epoch 1/2:  87%|########7 |\n215/247 [01:21<00:02, 14.35it/s]', '\\roverwrite_agnews epoch 1/2:  88%|########7\n| 217/247 [01:21<00:02, 14.45it/s]', '\\roverwrite_agnews epoch 1/2:\n89%|########8 | 219/247 [01:21<00:01, 14.31it/s]', '\\roverwrite_agnews epoch\n1/2:  89%|########9 | 221/247 [01:21<00:01, 14.45it/s]', '\\roverwrite_agnews\nepoch 1/2:  90%|######### | 223/247 [01:22<00:01, 14.53it/s]',\n'\\roverwrite_agnews epoch 1/2:  91%|#########1| 225/247 [01:22<00:01,\n14.37it/s]', '\\roverwrite_agnews epoch 1/2:  92%|#########1| 227/247\n[01:22<00:01, 14.43it/s]', '\\roverwrite_agnews epoch 1/2:  93%|#########2|\n229/247 [01:22<00:01, 14.01it/s]', '\\roverwrite_agnews epoch 1/2:\n94%|#########3| 231/247 [01:22<00:01, 13.70it/s]', '\\roverwrite_agnews epoch\n1/2:  94%|#########4| 233/247 [01:22<00:01, 13.75it/s]', '\\roverwrite_agnews\nepoch 1/2:  95%|#########5| 235/247 [01:22<00:00, 13.93it/s]',\n'\\roverwrite_agnews epoch 1/2:  96%|#########5| 237/247 [01:23<00:00,\n14.04it/s]', '\\roverwrite_agnews epoch 1/2:  97%|#########6| 239/247\n[01:23<00:00, 14.05it/s]', '\\roverwrite_agnews epoch 1/2:  98%|#########7|\n241/247 [01:23<00:00, 14.27it/s]', '\\roverwrite_agnews epoch 1/2:\n98%|#########8| 243/247 [01:23<00:00, 13.94it/s]', '\\roverwrite_agnews epoch\n1/2:  99%|#########9| 245/247 [01:23<00:00, 14.10it/s]', '\\roverwrite_agnews\nepoch 1/2: 100%|##########| 247/247 [01:23<00:00, 14.19it/s]', '',\n'\\roverwrite_agnews epoch 1/2: 100%|##########| 247/247 [01:25<00:00,\n2.90it/s]', '\\n', 'Epoch 1: validation_loss = 3.3572', '\\n', '\\roverwrite_agnews\nepoch 2/2:   0%|          | 0/247 [00:00<?, ?it/s]', '\\roverwrite_agnews epoch\n2/2:   0%|          | 1/247 [01:11<4:54:09, 71.74s/it]', '\\roverwrite_agnews\nepoch 2/2:   1%|1         | 3/247 [01:11<1:15:52, 18.66s/it]',\n'\\roverwrite_agnews epoch 2/2:   2%|2         | 5/247 [01:12<36:42,  9.10s/it]\n', '\\roverwrite_agnews epoch 2/2:   3%|2         | 7/247 [01:12<21:06,\n5.28s/it]', '\\roverwrite_agnews epoch 2/2:   4%|3         | 9/247 [01:12<13:08,\n3.31s/it]', '\\roverwrite_agnews epoch 2/2:   4%|4         | 11/247 [01:12<08:33,\n2.18s/it]', '\\roverwrite_agnews epoch 2/2:   5%|5         | 13/247 [01:12<05:44,\n1.47s/it]', '\\roverwrite_agnews epoch 2/2:   6%|6         | 15/247 [01:12<03:56,\n1.02s/it]', '\\roverwrite_agnews epoch 2/2:   7%|6         | 17/247 [01:12<02:45,\n1.39it/s]', '\\roverwrite_agnews epoch 2/2:   8%|7         | 19/247 [01:12<01:57,\n1.93it/s]', '\\roverwrite_agnews epoch 2/2:   9%|8         | 21/247 [01:13<01:25,\n2.64it/s]', '\\roverwrite_agnews epoch 2/2:   9%|9         | 23/247 [01:13<01:03,\n3.52it/s]', '\\roverwrite_agnews epoch 2/2:  10%|#         | 25/247 [01:13<00:48,\n4.59it/s]', '\\roverwrite_agnews epoch 2/2:  11%|#         | 27/247 [01:13<00:37,\n5.81it/s]', '\\roverwrite_agnews epoch 2/2:  12%|#1        | 29/247 [01:13<00:30,\n7.11it/s]', '\\roverwrite_agnews epoch 2/2:  13%|#2        | 31/247 [01:13<00:25,\n8.37it/s]', '\\roverwrite_agnews epoch 2/2:  13%|#3        | 33/247 [01:13<00:22,\n9.63it/s]', '\\roverwrite_agnews epoch 2/2:  14%|#4        | 35/247 [01:14<00:19,\n10.66it/s]', '\\roverwrite_agnews epoch 2/2:  15%|#4        | 37/247\n[01:14<00:18, 11.64it/s]', '\\roverwrite_agnews epoch 2/2:  16%|#5        |\n39/247 [01:14<00:16, 12.37it/s]', '\\roverwrite_agnews epoch 2/2:  17%|#6\n| 41/247 [01:14<00:15, 12.99it/s]', '\\roverwrite_agnews epoch 2/2:  17%|#7\n| 43/247 [01:14<00:15, 13.48it/s]', '\\roverwrite_agnews epoch 2/2:  18%|#8\n| 45/247 [01:14<00:14, 13.84it/s]', '\\roverwrite_agnews epoch 2/2:  19%|#9\n| 47/247 [01:14<00:14, 14.08it/s]', '\\roverwrite_agnews epoch 2/2:  20%|#9\n| 49/247 [01:15<00:13, 14.28it/s]', '\\roverwrite_agnews epoch 2/2:  21%|##\n| 51/247 [01:15<00:13, 14.31it/s]', '\\roverwrite_agnews epoch 2/2:  21%|##1\n| 53/247 [01:15<00:13, 14.39it/s]', '\\roverwrite_agnews epoch 2/2:  22%|##2\n| 55/247 [01:15<00:13, 14.56it/s]', '\\roverwrite_agnews epoch 2/2:  23%|##3\n| 57/247 [01:15<00:13, 14.58it/s]', '\\roverwrite_agnews epoch 2/2:  24%|##3\n| 59/247 [01:15<00:12, 14.66it/s]', '\\roverwrite_agnews epoch 2/2:  25%|##4\n| 61/247 [01:15<00:12, 14.71it/s]', '\\roverwrite_agnews epoch 2/2:  26%|##5\n| 63/247 [01:15<00:12, 14.77it/s]', '\\roverwrite_agnews epoch 2/2:  26%|##6\n| 65/247 [01:16<00:12, 14.83it/s]', '\\roverwrite_agnews epoch 2/2:  27%|##7\n| 67/247 [01:16<00:12, 14.86it/s]', '\\roverwrite_agnews epoch 2/2:  28%|##7\n| 69/247 [01:16<00:11, 14.90it/s]', '\\roverwrite_agnews epoch 2/2:  29%|##8\n| 71/247 [01:16<00:11, 14.92it/s]', '\\roverwrite_agnews epoch 2/2:  30%|##9\n| 73/247 [01:16<00:11, 14.93it/s]', '\\roverwrite_agnews epoch 2/2:  30%|###\n| 75/247 [01:16<00:11, 14.94it/s]', '\\roverwrite_agnews epoch 2/2:  31%|###1\n| 77/247 [01:16<00:11, 14.95it/s]', '\\roverwrite_agnews epoch 2/2:  32%|###1\n| 79/247 [01:17<00:11, 14.93it/s]', '\\roverwrite_agnews epoch 2/2:  33%|###2\n| 81/247 [01:17<00:11, 14.95it/s]', '\\roverwrite_agnews epoch 2/2:  34%|###3\n| 83/247 [01:17<00:10, 14.95it/s]', '\\roverwrite_agnews epoch 2/2:  34%|###4\n| 85/247 [01:17<00:10, 14.93it/s]', '\\roverwrite_agnews epoch 2/2:  35%|###5\n| 87/247 [01:17<00:10, 14.93it/s]', '\\roverwrite_agnews epoch 2/2:  36%|###6\n| 89/247 [01:17<00:10, 14.95it/s]', '\\roverwrite_agnews epoch 2/2:  37%|###6\n| 91/247 [01:17<00:10, 14.94it/s]', '\\roverwrite_agnews epoch 2/2:  38%|###7\n| 93/247 [01:17<00:10, 14.96it/s]', '\\roverwrite_agnews epoch 2/2:  38%|###8\n| 95/247 [01:18<00:10, 14.85it/s]', '\\roverwrite_agnews epoch 2/2:  39%|###9\n| 97/247 [01:18<00:10, 14.87it/s]', '\\roverwrite_agnews epoch 2/2:  40%|####\n| 99/247 [01:18<00:09, 14.89it/s]', '\\roverwrite_agnews epoch 2/2:  41%|####\n| 101/247 [01:18<00:09, 14.90it/s]', '\\roverwrite_agnews epoch 2/2:  42%|####1\n| 103/247 [01:18<00:09, 14.92it/s]', '\\roverwrite_agnews epoch 2/2:  43%|####2\n| 105/247 [01:18<00:09, 14.94it/s]', '\\roverwrite_agnews epoch 2/2:  43%|####3\n| 107/247 [01:18<00:09, 14.96it/s]', '\\roverwrite_agnews epoch 2/2:  44%|####4\n| 109/247 [01:19<00:09, 14.97it/s]', '\\roverwrite_agnews epoch 2/2:  45%|####4\n| 111/247 [01:19<00:09, 14.97it/s]', '\\roverwrite_agnews epoch 2/2:  46%|####5\n| 113/247 [01:19<00:08, 14.96it/s]', '\\roverwrite_agnews epoch 2/2:  47%|####6\n| 115/247 [01:19<00:08, 14.89it/s]', '\\roverwrite_agnews epoch 2/2:  47%|####7\n| 117/247 [01:19<00:08, 14.90it/s]', '\\roverwrite_agnews epoch 2/2:  48%|####8\n| 119/247 [01:19<00:08, 14.89it/s]', '\\roverwrite_agnews epoch 2/2:  49%|####8\n| 121/247 [01:19<00:08, 14.91it/s]', '\\roverwrite_agnews epoch 2/2:  50%|####9\n| 123/247 [01:19<00:08, 14.89it/s]', '\\roverwrite_agnews epoch 2/2:  51%|#####\n| 125/247 [01:20<00:08, 14.94it/s]', '\\roverwrite_agnews epoch 2/2:  51%|#####1\n| 127/247 [01:20<00:08, 14.70it/s]', '\\roverwrite_agnews epoch 2/2:  52%|#####2\n| 129/247 [01:20<00:08, 14.56it/s]', '\\roverwrite_agnews epoch 2/2:  53%|#####3\n| 131/247 [01:20<00:07, 14.64it/s]', '\\roverwrite_agnews epoch 2/2:  54%|#####3\n| 133/247 [01:20<00:07, 14.73it/s]', '\\roverwrite_agnews epoch 2/2:  55%|#####4\n| 135/247 [01:20<00:07, 14.78it/s]', '\\roverwrite_agnews epoch 2/2:  55%|#####5\n| 137/247 [01:20<00:07, 14.77it/s]', '\\roverwrite_agnews epoch 2/2:  56%|#####6\n| 139/247 [01:21<00:07, 14.81it/s]', '\\roverwrite_agnews epoch 2/2:  57%|#####7\n| 141/247 [01:21<00:07, 14.86it/s]', '\\roverwrite_agnews epoch 2/2:  58%|#####7\n| 143/247 [01:21<00:06, 14.86it/s]', '\\roverwrite_agnews epoch 2/2:  59%|#####8\n| 145/247 [01:21<00:06, 14.89it/s]', '\\roverwrite_agnews epoch 2/2:  60%|#####9\n| 147/247 [01:21<00:06, 14.90it/s]', '\\roverwrite_agnews epoch 2/2:  60%|######\n| 149/247 [01:21<00:06, 14.92it/s]', '\\roverwrite_agnews epoch 2/2:  61%|######1\n| 151/247 [01:21<00:06, 14.93it/s]', '[2025-12-03 20:36:07] overwrite_agnews\nstep 400: avg_train_loss=3.0953', '\\n', '\\roverwrite_agnews epoch 2/2:\n62%|######1   | 153/247 [01:22<00:06, 14.94it/s]', '\\roverwrite_agnews epoch\n2/2:  63%|######2   | 155/247 [01:22<00:06, 14.93it/s]', '\\roverwrite_agnews\nepoch 2/2:  64%|######3   | 157/247 [01:22<00:06, 14.95it/s]',\n'\\roverwrite_agnews epoch 2/2:  64%|######4   | 159/247 [01:22<00:05,\n14.94it/s]', '\\roverwrite_agnews epoch 2/2:  65%|######5   | 161/247\n[01:22<00:05, 14.95it/s]', '\\roverwrite_agnews epoch 2/2:  66%|######5   |\n163/247 [01:22<00:05, 14.96it/s]', '\\roverwrite_agnews epoch 2/2:  67%|######6\n| 165/247 [01:22<00:05, 14.97it/s]', '\\roverwrite_agnews epoch 2/2:  68%|######7\n| 167/247 [01:22<00:05, 14.89it/s]', '\\roverwrite_agnews epoch 2/2:  68%|######8\n| 169/247 [01:23<00:05, 14.90it/s]', '\\roverwrite_agnews epoch 2/2:  69%|######9\n| 171/247 [01:23<00:05, 14.90it/s]', '\\roverwrite_agnews epoch 2/2:  70%|#######\n| 173/247 [01:23<00:04, 14.88it/s]', '\\roverwrite_agnews epoch 2/2:  71%|#######\n| 175/247 [01:23<00:04, 14.91it/s]', '\\roverwrite_agnews epoch 2/2:\n72%|#######1  | 177/247 [01:23<00:04, 14.92it/s]', '\\roverwrite_agnews epoch\n2/2:  72%|#######2  | 179/247 [01:23<00:04, 14.93it/s]', '\\roverwrite_agnews\nepoch 2/2:  73%|#######3  | 181/247 [01:23<00:04, 14.94it/s]',\n'\\roverwrite_agnews epoch 2/2:  74%|#######4  | 183/247 [01:24<00:04,\n14.92it/s]', '\\roverwrite_agnews epoch 2/2:  75%|#######4  | 185/247\n[01:24<00:04, 14.92it/s]', '\\roverwrite_agnews epoch 2/2:  76%|#######5  |\n187/247 [01:24<00:04, 14.87it/s]', '\\roverwrite_agnews epoch 2/2:  77%|#######6\n| 189/247 [01:24<00:03, 14.89it/s]', '\\roverwrite_agnews epoch 2/2:\n77%|#######7  | 191/247 [01:24<00:03, 14.88it/s]', '\\roverwrite_agnews epoch\n2/2:  78%|#######8  | 193/247 [01:24<00:03, 14.92it/s]', '\\roverwrite_agnews\nepoch 2/2:  79%|#######8  | 195/247 [01:24<00:03, 14.92it/s]',\n'\\roverwrite_agnews epoch 2/2:  80%|#######9  | 197/247 [01:24<00:03,\n14.92it/s]', '\\roverwrite_agnews epoch 2/2:  81%|########  | 199/247\n[01:25<00:03, 14.92it/s]', '\\roverwrite_agnews epoch 2/2:  81%|########1 |\n201/247 [01:25<00:03, 14.91it/s]', '\\roverwrite_agnews epoch 2/2:  82%|########2\n| 203/247 [01:25<00:03, 14.63it/s]', '\\roverwrite_agnews epoch 2/2:\n83%|########2 | 205/247 [01:25<00:02, 14.70it/s]', '\\roverwrite_agnews epoch\n2/2:  84%|########3 | 207/247 [01:25<00:02, 14.75it/s]', '\\roverwrite_agnews\nepoch 2/2:  85%|########4 | 209/247 [01:25<00:02, 14.78it/s]',\n'\\roverwrite_agnews epoch 2/2:  85%|########5 | 211/247 [01:25<00:02,\n14.77it/s]', '\\roverwrite_agnews epoch 2/2:  86%|########6 | 213/247\n[01:26<00:02, 14.81it/s]', '\\roverwrite_agnews epoch 2/2:  87%|########7 |\n215/247 [01:26<00:02, 14.85it/s]', '\\roverwrite_agnews epoch 2/2:  88%|########7\n| 217/247 [01:26<00:02, 14.84it/s]', '\\roverwrite_agnews epoch 2/2:\n89%|########8 | 219/247 [01:26<00:01, 14.84it/s]', '\\roverwrite_agnews epoch\n2/2:  89%|########9 | 221/247 [01:26<00:01, 14.84it/s]', '\\roverwrite_agnews\nepoch 2/2:  90%|######### | 223/247 [01:26<00:01, 14.86it/s]',\n'\\roverwrite_agnews epoch 2/2:  91%|#########1| 225/247 [01:26<00:01,\n14.91it/s]', '\\roverwrite_agnews epoch 2/2:  92%|#########1| 227/247\n[01:26<00:01, 14.90it/s]', '\\roverwrite_agnews epoch 2/2:  93%|#########2|\n229/247 [01:27<00:01, 14.91it/s]', '\\roverwrite_agnews epoch 2/2:\n94%|#########3| 231/247 [01:27<00:01, 14.90it/s]', '\\roverwrite_agnews epoch\n2/2:  94%|#########4| 233/247 [01:27<00:00, 14.92it/s]', '\\roverwrite_agnews\nepoch 2/2:  95%|#########5| 235/247 [01:27<00:00, 14.81it/s]',\n'\\roverwrite_agnews epoch 2/2:  96%|#########5| 237/247 [01:27<00:00,\n14.80it/s]', '\\roverwrite_agnews epoch 2/2:  97%|#########6| 239/247\n[01:27<00:00, 14.82it/s]', '\\roverwrite_agnews epoch 2/2:  98%|#########7|\n241/247 [01:27<00:00, 14.86it/s]', '\\roverwrite_agnews epoch 2/2:\n98%|#########8| 243/247 [01:28<00:00, 14.82it/s]', '\\roverwrite_agnews epoch\n2/2:  99%|#########9| 245/247 [01:28<00:00, 14.88it/s]', '\\roverwrite_agnews\nepoch 2/2: 100%|##########| 247/247 [01:28<00:00, 15.56it/s]', '',\n'\\roverwrite_agnews epoch 2/2: 100%|##########| 247/247 [01:29<00:00,\n2.77it/s]', '\\n', 'Epoch 2: validation_loss = 3.3232', '\\n', 'overwrite_agnews\nPHR (median rare HL / median common HL) = 0.0000', '\\n', '\\n==== Overwrite\nphase: overwrite_imdb ====', '\\n', '\\rMap:   0%|          | 0/1562 [00:00<?, ?\nexamples/s]', '\\rMap:  64%|######4   | 1000/1562 [00:00<00:00, 6575.23\nexamples/s]', '', '\\rMap: 100%|##########| 1562/1562 [00:00<00:00, 6046.65\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/6250 [00:00<?, ? examples/s]',\n'\\rMap:  16%|#6        | 1000/6250 [00:00<00:00, 6364.55 examples/s]', '\\rMap:\n32%|###2      | 2000/6250 [00:00<00:00, 6491.65 examples/s]', '\\rMap:  48%|####8\n| 3000/6250 [00:00<00:00, 6663.92 examples/s]', '\\rMap:  64%|######4   |\n4000/6250 [00:00<00:00, 6781.30 examples/s]', '\\rMap:  80%|########  | 5000/6250\n[00:00<00:00, 6968.82 examples/s]', '\\rMap:  96%|#########6| 6000/6250\n[00:00<00:00, 6961.85 examples/s]', '', '\\rMap: 100%|##########| 6250/6250\n[00:00<00:00, 6661.37 examples/s]', '\\n', '\\roverwrite_imdb epoch 1/2:   0%|\n| 0/52 [00:00<?, ?it/s]', '\\roverwrite_imdb epoch 1/2:   2%|1         | 1/52\n[00:59<50:43, 59.67s/it]', '\\roverwrite_imdb epoch 1/2:   6%|5         | 3/52\n[00:59<12:40, 15.52s/it]', '\\roverwrite_imdb epoch 1/2:  10%|9         | 5/52\n[00:59<05:56,  7.57s/it]', '\\roverwrite_imdb epoch 1/2:  13%|#3        | 7/52\n[01:00<03:17,  4.40s/it]', '\\roverwrite_imdb epoch 1/2:  17%|#7        | 9/52\n[01:00<01:58,  2.76s/it]', '\\roverwrite_imdb epoch 1/2:  21%|##1       | 11/52\n[01:00<01:14,  1.82s/it]', '\\roverwrite_imdb epoch 1/2:  25%|##5       | 13/52\n[01:00<00:48,  1.24s/it]', '\\roverwrite_imdb epoch 1/2:  29%|##8       | 15/52\n[01:00<00:31,  1.16it/s]', '\\roverwrite_imdb epoch 1/2:  33%|###2      | 17/52\n[01:00<00:21,  1.64it/s]', '\\roverwrite_imdb epoch 1/2:  37%|###6      | 19/52\n[01:00<00:14,  2.27it/s]', '\\roverwrite_imdb epoch 1/2:  40%|####      | 21/52\n[01:01<00:10,  3.07it/s]', '\\roverwrite_imdb epoch 1/2:  44%|####4     | 23/52\n[01:01<00:07,  4.05it/s]', '\\roverwrite_imdb epoch 1/2:  48%|####8     | 25/52\n[01:01<00:05,  5.21it/s]', '\\roverwrite_imdb epoch 1/2:  52%|#####1    | 27/52\n[01:01<00:03,  6.48it/s]', '\\roverwrite_imdb epoch 1/2:  56%|#####5    | 29/52\n[01:01<00:02,  7.82it/s]', '\\roverwrite_imdb epoch 1/2:  60%|#####9    | 31/52\n[01:01<00:02,  9.13it/s]', '\\roverwrite_imdb epoch 1/2:  63%|######3   | 33/52\n[01:01<00:01, 10.33it/s]', '\\roverwrite_imdb epoch 1/2:  67%|######7   | 35/52\n[01:01<00:01, 11.40it/s]', '\\roverwrite_imdb epoch 1/2:  71%|#######1  | 37/52\n[01:02<00:01, 12.27it/s]', '\\roverwrite_imdb epoch 1/2:  75%|#######5  | 39/52\n[01:02<00:01, 12.78it/s]', '\\roverwrite_imdb epoch 1/2:  79%|#######8  | 41/52\n[01:02<00:00, 13.24it/s]', '\\roverwrite_imdb epoch 1/2:  83%|########2 | 43/52\n[01:02<00:00, 13.66it/s]', '\\roverwrite_imdb epoch 1/2:  87%|########6 | 45/52\n[01:02<00:00, 13.99it/s]', '\\roverwrite_imdb epoch 1/2:  90%|######### | 47/52\n[01:02<00:00, 14.25it/s]', '\\roverwrite_imdb epoch 1/2:  94%|#########4| 49/52\n[01:02<00:00, 14.43it/s]', '\\roverwrite_imdb epoch 1/2:  98%|#########8| 51/52\n[01:03<00:00, 14.61it/s]', '', '\\roverwrite_imdb epoch 1/2: 100%|##########|\n52/52 [01:04<00:00,  1.23s/it]', '\\n', 'Epoch 1: validation_loss = 3.6876',\n'\\n', '\\roverwrite_imdb epoch 2/2:   0%|          | 0/52 [00:00<?, ?it/s]',\n'\\roverwrite_imdb epoch 2/2:   2%|1         | 1/52 [01:41<1:25:52, 101.04s/it]',\n'\\roverwrite_imdb epoch 2/2:   6%|5         | 3/52 [01:41<21:26, 26.25s/it]   ',\n'\\roverwrite_imdb epoch 2/2:  10%|9         | 5/52 [01:41<10:01, 12.79s/it]',\n'\\roverwrite_imdb epoch 2/2:  13%|#3        | 7/52 [01:41<05:33,  7.40s/it]',\n'\\roverwrite_imdb epoch 2/2:  17%|#7        | 9/52 [01:41<03:19,  4.64s/it]',\n'\\roverwrite_imdb epoch 2/2:  21%|##1       | 11/52 [01:41<02:04,  3.04s/it]',\n'\\roverwrite_imdb epoch 2/2:  25%|##5       | 13/52 [01:41<01:19,  2.05s/it]',\n'\\roverwrite_imdb epoch 2/2:  29%|##8       | 15/52 [01:42<00:52,  1.41s/it]',\n'\\roverwrite_imdb epoch 2/2:  33%|###2      | 17/52 [01:42<00:34,  1.01it/s]',\n'\\roverwrite_imdb epoch 2/2:  37%|###6      | 19/52 [01:42<00:23,  1.43it/s]',\n'\\roverwrite_imdb epoch 2/2:  40%|####      | 21/52 [01:42<00:15,  1.98it/s]',\n'\\roverwrite_imdb epoch 2/2:  44%|####4     | 23/52 [01:42<00:10,  2.68it/s]',\n'\\roverwrite_imdb epoch 2/2:  48%|####8     | 25/52 [01:42<00:07,  3.57it/s]',\n'\\roverwrite_imdb epoch 2/2:  52%|#####1    | 27/52 [01:42<00:05,  4.63it/s]',\n'\\roverwrite_imdb epoch 2/2:  56%|#####5    | 29/52 [01:42<00:03,  5.84it/s]',\n'\\roverwrite_imdb epoch 2/2:  60%|#####9    | 31/52 [01:43<00:02,  7.14it/s]',\n'\\roverwrite_imdb epoch 2/2:  63%|######3   | 33/52 [01:43<00:02,  8.45it/s]',\n'\\roverwrite_imdb epoch 2/2:  67%|######7   | 35/52 [01:43<00:01,  9.70it/s]',\n'\\roverwrite_imdb epoch 2/2:  71%|#######1  | 37/52 [01:43<00:01, 10.82it/s]',\n'\\roverwrite_imdb epoch 2/2:  75%|#######5  | 39/52 [01:43<00:01, 11.76it/s]',\n'\\roverwrite_imdb epoch 2/2:  79%|#######8  | 41/52 [01:43<00:00, 12.52it/s]',\n'\\roverwrite_imdb epoch 2/2:  83%|########2 | 43/52 [01:43<00:00, 13.11it/s]',\n'\\roverwrite_imdb epoch 2/2:  87%|########6 | 45/52 [01:44<00:00, 13.55it/s]',\n'\\roverwrite_imdb epoch 2/2:  90%|######### | 47/52 [01:44<00:00, 13.89it/s]',\n'\\roverwrite_imdb epoch 2/2:  94%|#########4| 49/52 [01:44<00:00, 14.18it/s]',\n'\\roverwrite_imdb epoch 2/2:  98%|#########8| 51/52 [01:44<00:00, 14.42it/s]',\n'', '\\roverwrite_imdb epoch 2/2: 100%|##########| 52/52 [01:45<00:00,\n2.03s/it]', '\\n', 'Epoch 2: validation_loss = 3.6922', '\\n', 'overwrite_imdb PHR\n(median rare HL / median common HL) = 0.0000', '\\n', 'Experiment complete.\nArtifacts saved to:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-4/working',\n'\\n', 'Execution time: 18 minutes seconds (time limit is 2 hours).']", "['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls: [' apple', '\ntable', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n25993.38 examples/s]', '\\n', '\\rMap:   0%|          | 0/300 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00, 33685.58\nexamples/s]', '\\n', '\\rTraining synthetic injection (1 epoch):   0%|          |\n0/21 [00:00<?, ?it/s]', '\\rTraining synthetic injection (1 epoch):   5%|4\n| 1/21 [00:46<15:36, 46.81s/it]', '\\rTraining synthetic injection (1 epoch):\n10%|9         | 2/21 [00:46<06:07, 19.34s/it]', '\\rTraining synthetic injection\n(1 epoch):  14%|#4        | 3/21 [00:47<03:10, 10.56s/it]', '\\rTraining\nsynthetic injection (1 epoch):  19%|#9        | 4/21 [00:47<01:49,  6.44s/it]',\n'\\rTraining synthetic injection (1 epoch):  24%|##3       | 5/21 [00:47<01:06,\n4.16s/it]', '\\rTraining synthetic injection (1 epoch):  29%|##8       | 6/21\n[00:47<00:41,  2.78s/it]', '\\rTraining synthetic injection (1 epoch):  33%|###3\n| 7/21 [00:47<00:26,  1.91s/it]', '\\rTraining synthetic injection (1 epoch):\n38%|###8      | 8/21 [00:47<00:17,  1.34s/it]', '\\rTraining synthetic injection\n(1 epoch):  43%|####2     | 9/21 [00:47<00:11,  1.05it/s]', '\\rTraining\nsynthetic injection (1 epoch):  48%|####7     | 10/21 [00:47<00:07,  1.44it/s]',\n'\\rTraining synthetic injection (1 epoch):  52%|#####2    | 11/21 [00:47<00:05,\n1.93it/s]', '\\rTraining synthetic injection (1 epoch):  57%|#####7    | 12/21\n[00:48<00:03,  2.53it/s]', '\\rTraining synthetic injection (1 epoch):\n62%|######1   | 13/21 [00:48<00:02,  3.21it/s]', '\\rTraining synthetic injection\n(1 epoch):  67%|######6   | 14/21 [00:48<00:01,  3.97it/s]', '\\rTraining\nsynthetic injection (1 epoch):  71%|#######1  | 15/21 [00:48<00:01,  4.75it/s]',\n'\\rTraining synthetic injection (1 epoch):  76%|#######6  | 16/21 [00:48<00:00,\n5.50it/s]', '\\rTraining synthetic injection (1 epoch):  81%|########  | 17/21\n[00:48<00:00,  6.17it/s]', '\\rTraining synthetic injection (1 epoch):\n86%|########5 | 18/21 [00:48<00:00,  6.76it/s]', '\\rTraining synthetic injection\n(1 epoch):  90%|######### | 19/21 [00:48<00:00,  7.25it/s]', '\\rTraining\nsynthetic injection (1 epoch):  95%|#########5| 20/21 [00:48<00:00,  7.63it/s]',\n'', '\\rTraining synthetic injection (1 epoch): 100%|##########| 21/21\n[00:49<00:00,  2.37s/it]', '\\n', 'Epoch 1: validation_loss = 3.2268', '\\n',\n'\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '', '\\rMap:\n100%|##########| 2000/2000 [00:00<00:00, 36590.57 examples/s]', '\\n', '\\n=====\nStarting overwrite_wikitext =====', '\\n', '\\rMap:   0%|          | 0/7344\n[00:00<?, ? examples/s]', '\\rMap:  27%|##7       | 2000/7344 [00:00<00:00,\n18506.21 examples/s]', '\\rMap:  82%|########1 | 6000/7344 [00:00<00:00, 21556.66\nexamples/s]', '', '\\rMap: 100%|##########| 7344/7344 [00:00<00:00, 19675.72\nexamples/s]', '\\n', '\\rMap:   0%|          | 0/3760 [00:00<?, ? examples/s]',\n'\\rMap:  80%|#######9  | 3000/3760 [00:00<00:00, 20808.69 examples/s]', '',\n'\\rMap: 100%|##########| 3760/3760 [00:00<00:00, 14317.14 examples/s]', '\\n',\n'\\roverwrite_wikitext epoch 1/3:   0%|          | 0/230 [00:00<?, ?it/s]',\n'\\roverwrite_wikitext epoch 1/3:   0%|          | 1/230 [02:33<9:44:13,\n153.07s/it]', '\\roverwrite_wikitext epoch 1/3:   1%|1         | 3/230\n[02:33<2:30:23, 39.75s/it] ', '\\roverwrite_wikitext epoch 1/3:   2%|2         |\n5/230 [02:33<1:12:35, 19.36s/it]', '\\roverwrite_wikitext epoch 1/3:   3%|3\n| 7/230 [02:33<41:37, 11.20s/it]  ', '\\roverwrite_wikitext epoch 1/3:   4%|3\n| 9/230 [02:33<25:49,  7.01s/it]', '\\roverwrite_wikitext epoch 1/3:   5%|4\n| 11/230 [02:33<16:44,  4.59s/it]', '\\roverwrite_wikitext epoch 1/3:   6%|5\n| 13/230 [02:34<11:09,  3.09s/it]', '\\roverwrite_wikitext epoch 1/3:   7%|6\n| 15/230 [02:34<07:35,  2.12s/it]', '\\roverwrite_wikitext epoch 1/3:   7%|7\n| 17/230 [02:34<05:14,  1.48s/it]', '\\roverwrite_wikitext epoch 1/3:   8%|8\n| 19/230 [02:34<03:40,  1.05s/it]', '\\roverwrite_wikitext epoch 1/3:   9%|9\n| 21/230 [02:34<02:37,  1.33it/s]', '\\roverwrite_wikitext epoch 1/3:  10%|#\n| 23/230 [02:35<01:53,  1.82it/s]', '\\roverwrite_wikitext epoch 1/3:  11%|#\n| 25/230 [02:35<01:23,  2.44it/s]', '\\roverwrite_wikitext epoch 1/3:  12%|#1\n| 27/230 [02:35<01:03,  3.20it/s]', '\\roverwrite_wikitext epoch 1/3:  13%|#2\n| 29/230 [02:35<00:49,  4.09it/s]', '\\roverwrite_wikitext epoch 1/3:  13%|#3\n| 31/230 [02:35<00:39,  5.07it/s]', '\\roverwrite_wikitext epoch 1/3:  14%|#4\n| 33/230 [02:35<00:32,  6.07it/s]', '\\roverwrite_wikitext epoch 1/3:  15%|#5\n| 35/230 [02:36<00:27,  7.06it/s]', '\\roverwrite_wikitext epoch 1/3:  16%|#6\n| 37/230 [02:36<00:24,  7.92it/s]', '\\roverwrite_wikitext epoch 1/3:  17%|#6\n| 39/230 [02:36<00:21,  8.70it/s]', '\\roverwrite_wikitext epoch 1/3:  18%|#7\n| 41/230 [02:36<00:20,  9.37it/s]', '\\roverwrite_wikitext epoch 1/3:  19%|#8\n| 43/230 [02:36<00:18,  9.89it/s]', '\\roverwrite_wikitext epoch 1/3:  20%|#9\n| 45/230 [02:36<00:17, 10.29it/s]', '\\roverwrite_wikitext epoch 1/3:  20%|##\n| 47/230 [02:37<00:17, 10.58it/s]', '\\roverwrite_wikitext epoch 1/3:  21%|##1\n| 49/230 [02:37<00:16, 10.83it/s]', '\\roverwrite_wikitext epoch 1/3:  22%|##2\n| 51/230 [02:37<00:16, 11.02it/s]', '\\roverwrite_wikitext epoch 1/3:  23%|##3\n| 53/230 [02:37<00:15, 11.12it/s]', '\\roverwrite_wikitext epoch 1/3:  24%|##3\n| 55/230 [02:37<00:15, 11.20it/s]', '\\roverwrite_wikitext epoch 1/3:  25%|##4\n| 57/230 [02:37<00:15, 11.27it/s]', '\\roverwrite_wikitext epoch 1/3:  26%|##5\n| 59/230 [02:38<00:15, 11.33it/s]', '\\roverwrite_wikitext epoch 1/3:  27%|##6\n| 61/230 [02:38<00:14, 11.35it/s]', '\\roverwrite_wikitext epoch 1/3:  27%|##7\n| 63/230 [02:38<00:14, 11.38it/s]', '\\roverwrite_wikitext epoch 1/3:  28%|##8\n| 65/230 [02:38<00:14, 11.39it/s]', '\\roverwrite_wikitext epoch 1/3:  29%|##9\n| 67/230 [02:38<00:14, 11.41it/s]', '\\roverwrite_wikitext epoch 1/3:  30%|###\n| 69/230 [02:39<00:14, 11.42it/s]', '\\roverwrite_wikitext epoch 1/3:  31%|###\n| 71/230 [02:39<00:13, 11.39it/s]', '\\roverwrite_wikitext epoch 1/3:  32%|###1\n| 73/230 [02:39<00:13, 11.40it/s]', '\\roverwrite_wikitext epoch 1/3:  33%|###2\n| 75/230 [02:39<00:13, 11.39it/s]', '\\roverwrite_wikitext epoch 1/3:  33%|###3\n| 77/230 [02:39<00:13, 11.42it/s]', '\\roverwrite_wikitext epoch 1/3:  34%|###4\n| 79/230 [02:39<00:13, 11.42it/s]', '\\roverwrite_wikitext epoch 1/3:  35%|###5\n| 81/230 [02:40<00:13, 11.42it/s]', '\\roverwrite_wikitext epoch 1/3:  36%|###6\n| 83/230 [02:40<00:12, 11.45it/s]', '\\roverwrite_wikitext epoch 1/3:  37%|###6\n| 85/230 [02:40<00:12, 11.45it/s]', '\\roverwrite_wikitext epoch 1/3:  38%|###7\n| 87/230 [02:40<00:12, 11.40it/s]', '\\roverwrite_wikitext epoch 1/3:  39%|###8\n| 89/230 [02:40<00:12, 11.39it/s]', '\\roverwrite_wikitext epoch 1/3:  40%|###9\n| 91/230 [02:40<00:12, 11.42it/s]', '\\roverwrite_wikitext epoch 1/3:  40%|####\n| 93/230 [02:41<00:11, 11.44it/s]', '\\roverwrite_wikitext epoch 1/3:  41%|####1\n| 95/230 [02:41<00:11, 11.44it/s]', '\\roverwrite_wikitext epoch 1/3:  42%|####2\n| 97/230 [02:41<00:11, 11.42it/s]', '\\roverwrite_wikitext epoch 1/3:  43%|####3\n| 99/230 [02:41<00:11, 11.43it/s]', '\\roverwrite_wikitext epoch 1/3:  44%|####3\n| 101/230 [02:41<00:11, 11.44it/s]', '\\roverwrite_wikitext epoch 1/3:  45%|####4\n| 103/230 [02:42<00:11, 11.44it/s]', '\\roverwrite_wikitext epoch 1/3:  46%|####5\n| 105/230 [02:42<00:10, 11.44it/s]', '\\roverwrite_wikitext epoch 1/3:  47%|####6\n| 107/230 [02:42<00:10, 11.42it/s]', '\\roverwrite_wikitext epoch 1/3:  47%|####7\n| 109/230 [02:42<00:10, 11.42it/s]', '\\roverwrite_wikitext epoch 1/3:  48%|####8\n| 111/230 [02:42<00:10, 11.43it/s]', '\\roverwrite_wikitext epoch 1/3:  49%|####9\n| 113/230 [02:42<00:10, 11.44it/s]', '\\roverwrite_wikitext epoch 1/3:  50%|#####\n| 115/230 [02:43<00:10, 11.45it/s]', '\\roverwrite_wikitext epoch 1/3:  51%|#####\n| 117/230 [02:43<00:09, 11.46it/s]', '\\roverwrite_wikitext epoch 1/3:\n52%|#####1    | 119/230 [02:43<00:09, 11.46it/s]', '\\roverwrite_wikitext epoch\n1/3:  53%|#####2    | 121/230 [02:43<00:09, 11.39it/s]', '\\roverwrite_wikitext\nepoch 1/3:  53%|#####3    | 123/230 [02:43<00:09, 11.42it/s]',\n'\\roverwrite_wikitext epoch 1/3:  54%|#####4    | 125/230 [02:43<00:08,\n11.74it/s]', '\\roverwrite_wikitext epoch 1/3:  54%|#####4    | 125/230\n[03:02<00:08, 11.74it/s]', '\\roverwrite_wikitext epoch 1/3:  55%|#####5    |\n127/230 [03:27<11:21,  6.62s/it]', '\\roverwrite_wikitext epoch 1/3:  56%|#####6\n| 129/230 [03:27<07:50,  4.66s/it]', '\\roverwrite_wikitext epoch 1/3:\n57%|#####6    | 131/230 [03:28<05:25,  3.29s/it]', '\\roverwrite_wikitext epoch\n1/3:  58%|#####7    | 133/230 [03:28<03:45,  2.33s/it]', '\\roverwrite_wikitext\nepoch 1/3:  59%|#####8    | 135/230 [03:28<02:37,  1.66s/it]',\n'\\roverwrite_wikitext epoch 1/3:  60%|#####9    | 137/230 [03:28<01:50,\n1.18s/it]', '\\roverwrite_wikitext epoch 1/3:  60%|######    | 139/230\n[03:28<01:17,  1.17it/s]', '\\roverwrite_wikitext epoch 1/3:  61%|######1   |\n141/230 [03:28<00:55,  1.60it/s]', '\\roverwrite_wikitext epoch 1/3:  62%|######2\n| 143/230 [03:29<00:40,  2.16it/s]', '\\roverwrite_wikitext epoch 1/3:\n63%|######3   | 145/230 [03:29<00:29,  2.85it/s]', '\\roverwrite_wikitext epoch\n1/3:  64%|######3   | 147/230 [03:29<00:22,  3.68it/s]', '\\roverwrite_wikitext\nepoch 1/3:  65%|######4   | 149/230 [03:29<00:17,  4.62it/s]',\n'\\roverwrite_wikitext epoch 1/3:  66%|######5   | 151/230 [03:29<00:14,\n5.63it/s]', '\\roverwrite_wikitext epoch 1/3:  67%|######6   | 153/230\n[03:29<00:11,  6.64it/s]', '\\roverwrite_wikitext epoch 1/3:  67%|######7   |\n155/230 [03:30<00:09,  7.59it/s]', '\\roverwrite_wikitext epoch 1/3:  68%|######8\n| 157/230 [03:30<00:08,  8.45it/s]', '\\roverwrite_wikitext epoch 1/3:\n69%|######9   | 159/230 [03:30<00:07,  9.17it/s]', '\\roverwrite_wikitext epoch\n1/3:  70%|#######   | 161/230 [03:30<00:07,  9.76it/s]', '\\roverwrite_wikitext\nepoch 1/3:  71%|#######   | 163/230 [03:30<00:06, 10.22it/s]',\n'\\roverwrite_wikitext epoch 1/3:  72%|#######1  | 165/230 [03:30<00:06,\n10.56it/s]', '\\roverwrite_wikitext epoch 1/3:  73%|#######2  | 167/230\n[03:31<00:05, 10.81it/s]', '\\roverwrite_wikitext epoch 1/3:  73%|#######3  |\n169/230 [03:31<00:05, 11.00it/s]', '\\roverwrite_wikitext epoch 1/3:\n74%|#######4  | 171/230 [03:31<00:05, 11.13it/s]', '\\roverwrite_wikitext epoch\n1/3:  75%|#######5  | 173/230 [03:31<00:05, 11.19it/s]', '\\roverwrite_wikitext\nepoch 1/3:  76%|#######6  | 175/230 [03:31<00:04, 11.18it/s]',\n'\\roverwrite_wikitext epoch 1/3:  77%|#######6  | 177/230 [03:32<00:04,\n11.26it/s]', '\\roverwrite_wikitext epoch 1/3:  78%|#######7  | 179/230\n[03:32<00:04, 11.30it/s]', '\\roverwrite_wikitext epoch 1/3:  79%|#######8  |\n181/230 [03:32<00:04, 11.34it/s]', '\\roverwrite_wikitext epoch 1/3:\n80%|#######9  | 183/230 [03:32<00:04, 11.37it/s]', '\\roverwrite_wikitext epoch\n1/3:  80%|########  | 185/230 [03:32<00:03, 11.40it/s]', '\\roverwrite_wikitext\nepoch 1/3:  81%|########1 | 187/230 [03:32<00:03, 11.42it/s]',\n'\\roverwrite_wikitext epoch 1/3:  82%|########2 | 189/230 [03:33<00:03,\n11.42it/s]', '\\roverwrite_wikitext epoch 1/3:  83%|########3 | 191/230\n[03:33<00:03, 11.43it/s]', '\\roverwrite_wikitext epoch 1/3:  84%|########3 |\n193/230 [03:33<00:03, 11.42it/s]', '\\roverwrite_wikitext epoch 1/3:\n85%|########4 | 195/230 [03:33<00:03, 11.36it/s]', '\\roverwrite_wikitext epoch\n1/3:  86%|########5 | 197/230 [03:33<00:02, 11.39it/s]', '\\roverwrite_wikitext\nepoch 1/3:  87%|########6 | 199/230 [03:33<00:02, 11.40it/s]', '[2025-12-03\n21:04:16] overwrite_wikitext step 200: avg_train_loss=5.0729', '\\n',\n'\\roverwrite_wikitext epoch 1/3:  87%|########7 | 201/230 [03:34<00:02,\n11.41it/s]', '\\roverwrite_wikitext epoch 1/3:  88%|########8 | 203/230\n[03:34<00:02, 11.42it/s]', '\\roverwrite_wikitext epoch 1/3:  89%|########9 |\n205/230 [03:34<00:02, 11.42it/s]', '\\roverwrite_wikitext epoch 1/3:\n90%|######### | 207/230 [03:34<00:02, 11.43it/s]', '\\roverwrite_wikitext epoch\n1/3:  91%|######### | 209/230 [03:34<00:01, 11.45it/s]', '\\roverwrite_wikitext\nepoch 1/3:  92%|#########1| 211/230 [03:35<00:01, 11.46it/s]',\n'\\roverwrite_wikitext epoch 1/3:  93%|#########2| 213/230 [03:35<00:01,\n11.47it/s]', '\\roverwrite_wikitext epoch 1/3:  93%|#########3| 215/230\n[03:35<00:01, 11.39it/s]', '\\roverwrite_wikitext epoch 1/3:  94%|#########4|\n217/230 [03:35<00:01, 11.40it/s]', '\\roverwrite_wikitext epoch 1/3:\n95%|#########5| 219/230 [03:35<00:00, 11.42it/s]', '\\roverwrite_wikitext epoch\n1/3:  96%|#########6| 221/230 [03:35<00:00, 11.43it/s]', '\\roverwrite_wikitext\nepoch 1/3:  97%|#########6| 223/230 [03:36<00:00, 11.44it/s]',\n'\\roverwrite_wikitext epoch 1/3:  98%|#########7| 225/230 [03:36<00:00,\n11.45it/s]', '\\roverwrite_wikitext epoch 1/3:  99%|#########8| 227/230\n[03:36<00:00, 11.46it/s]', '\\roverwrite_wikitext epoch 1/3: 100%|#########9|\n229/230 [03:36<00:00, 11.50it/s]', '', '\\roverwrite_wikitext epoch 1/3:\n100%|##########| 230/230 [03:37<00:00,  1.06it/s]', '\\n', 'Epoch 1:\nvalidation_loss = 3.7069', '\\n', '\\roverwrite_wikitext epoch 2/3:   0%|\n| 0/230 [00:00<?, ?it/s]', '\\roverwrite_wikitext epoch 2/3:   0%|          |\n1/230 [00:22<1:25:30, 22.40s/it]', '\\roverwrite_wikitext epoch 2/3:   1%|\n| 2/230 [00:23<36:23,  9.58s/it]  ', '\\roverwrite_wikitext epoch 2/3:   1%|1\n| 3/230 [00:23<19:53,  5.26s/it]', '\\roverwrite_wikitext epoch 2/3:   2%|2\n| 5/230 [00:23<08:44,  2.33s/it]', '\\roverwrite_wikitext epoch 2/3:   3%|3\n| 7/230 [00:23<04:56,  1.33s/it]', '\\roverwrite_wikitext epoch 2/3:   4%|3\n| 9/230 [00:23<03:06,  1.18it/s]', '\\roverwrite_wikitext epoch 2/3:   5%|4\n| 11/230 [00:23<02:05,  1.74it/s]', '\\roverwrite_wikitext epoch 2/3:   6%|5\n| 13/230 [00:23<01:28,  2.44it/s]', '\\roverwrite_wikitext epoch 2/3:   7%|6\n| 15/230 [00:24<01:05,  3.28it/s]', '\\roverwrite_wikitext epoch 2/3:   7%|7\n| 17/230 [00:24<00:50,  4.25it/s]', '\\roverwrite_wikitext epoch 2/3:   8%|8\n| 19/230 [00:24<00:39,  5.28it/s]', '\\roverwrite_wikitext epoch 2/3:   9%|9\n| 21/230 [00:24<00:32,  6.44it/s]', '\\roverwrite_wikitext epoch 2/3:   9%|9\n| 21/230 [00:39<00:32,  6.44it/s]', '\\roverwrite_wikitext epoch 2/3:  10%|#\n| 23/230 [01:07<23:09,  6.71s/it]', '\\roverwrite_wikitext epoch 2/3:  11%|#\n| 25/230 [01:08<16:03,  4.70s/it]', '\\roverwrite_wikitext epoch 2/3:  12%|#1\n| 27/230 [01:08<11:10,  3.30s/it]', '\\roverwrite_wikitext epoch 2/3:  13%|#2\n| 29/230 [01:08<07:48,  2.33s/it]', '\\roverwrite_wikitext epoch 2/3:  13%|#3\n| 31/230 [01:08<05:29,  1.66s/it]', '\\roverwrite_wikitext epoch 2/3:  14%|#4\n| 33/230 [01:08<03:53,  1.18s/it]', '\\roverwrite_wikitext epoch 2/3:  15%|#5\n| 35/230 [01:08<02:46,  1.17it/s]', '\\roverwrite_wikitext epoch 2/3:  16%|#6\n| 37/230 [01:09<02:00,  1.60it/s]', '\\roverwrite_wikitext epoch 2/3:  17%|#6\n| 39/230 [01:09<01:28,  2.16it/s]', '\\roverwrite_wikitext epoch 2/3:  18%|#7\n| 41/230 [01:09<01:06,  2.85it/s]', '\\roverwrite_wikitext epoch 2/3:  19%|#8\n| 43/230 [01:09<00:50,  3.69it/s]', '\\roverwrite_wikitext epoch 2/3:  20%|#9\n| 45/230 [01:09<00:39,  4.63it/s]', '\\roverwrite_wikitext epoch 2/3:  20%|##\n| 47/230 [01:09<00:32,  5.64it/s]', '\\roverwrite_wikitext epoch 2/3:  21%|##1\n| 49/230 [01:10<00:27,  6.65it/s]', '\\roverwrite_wikitext epoch 2/3:  22%|##2\n| 51/230 [01:10<00:23,  7.60it/s]', '\\roverwrite_wikitext epoch 2/3:  23%|##3\n| 53/230 [01:10<00:20,  8.45it/s]', '\\roverwrite_wikitext epoch 2/3:  24%|##3\n| 55/230 [01:10<00:19,  9.15it/s]', '\\roverwrite_wikitext epoch 2/3:  25%|##4\n| 57/230 [01:10<00:17,  9.74it/s]', '\\roverwrite_wikitext epoch 2/3:  26%|##5\n| 59/230 [01:11<00:16, 10.19it/s]', '\\roverwrite_wikitext epoch 2/3:  27%|##6\n| 61/230 [01:11<00:16, 10.55it/s]', '\\roverwrite_wikitext epoch 2/3:  27%|##7\n| 63/230 [01:11<00:15, 10.79it/s]', '\\roverwrite_wikitext epoch 2/3:  28%|##8\n| 65/230 [01:11<00:15, 10.98it/s]', '\\roverwrite_wikitext epoch 2/3:  29%|##9\n| 67/230 [01:11<00:14, 11.13it/s]', '\\roverwrite_wikitext epoch 2/3:  30%|###\n| 69/230 [01:11<00:14, 11.24it/s]', '\\roverwrite_wikitext epoch 2/3:  31%|###\n| 71/230 [01:12<00:14, 11.32it/s]', '\\roverwrite_wikitext epoch 2/3:  32%|###1\n| 73/230 [01:12<00:13, 11.36it/s]', '\\roverwrite_wikitext epoch 2/3:  33%|###2\n| 75/230 [01:12<00:13, 11.40it/s]', '\\roverwrite_wikitext epoch 2/3:  33%|###3\n| 77/230 [01:12<00:13, 11.42it/s]', '\\roverwrite_wikitext epoch 2/3:  34%|###4\n| 79/230 [01:12<00:13, 11.26it/s]', '\\roverwrite_wikitext epoch 2/3:  35%|###5\n| 81/230 [01:12<00:13, 11.31it/s]', '\\roverwrite_wikitext epoch 2/3:  36%|###6\n| 83/230 [01:13<00:12, 11.34it/s]', '\\roverwrite_wikitext epoch 2/3:  37%|###6\n| 85/230 [01:13<00:12, 11.38it/s]', '\\roverwrite_wikitext epoch 2/3:  38%|###7\n| 87/230 [01:13<00:12, 11.41it/s]', '\\roverwrite_wikitext epoch 2/3:  39%|###8\n| 89/230 [01:13<00:12, 11.42it/s]', '\\roverwrite_wikitext epoch 2/3:  40%|###9\n| 91/230 [01:13<00:12, 11.44it/s]', '\\roverwrite_wikitext epoch 2/3:  40%|####\n| 93/230 [01:14<00:11, 11.45it/s]', '\\roverwrite_wikitext epoch 2/3:  41%|####1\n| 95/230 [01:14<00:11, 11.46it/s]', '\\roverwrite_wikitext epoch 2/3:  42%|####2\n| 97/230 [01:14<00:11, 11.45it/s]', '\\roverwrite_wikitext epoch 2/3:  43%|####3\n| 99/230 [01:14<00:11, 11.47it/s]', '\\roverwrite_wikitext epoch 2/3:  44%|####3\n| 101/230 [01:14<00:11, 11.47it/s]', '\\roverwrite_wikitext epoch 2/3:  45%|####4\n| 103/230 [01:14<00:11, 11.47it/s]', '\\roverwrite_wikitext epoch 2/3:  46%|####5\n| 105/230 [01:15<00:10, 11.46it/s]', '\\roverwrite_wikitext epoch 2/3:  47%|####6\n| 107/230 [01:15<00:10, 11.34it/s]', '\\roverwrite_wikitext epoch 2/3:  47%|####7\n| 109/230 [01:15<00:10, 11.38it/s]', '\\roverwrite_wikitext epoch 2/3:  48%|####8\n| 111/230 [01:15<00:10, 11.41it/s]', '\\roverwrite_wikitext epoch 2/3:  49%|####9\n| 113/230 [01:15<00:10, 11.43it/s]', '\\roverwrite_wikitext epoch 2/3:  50%|#####\n| 115/230 [01:15<00:10, 11.45it/s]', '\\roverwrite_wikitext epoch 2/3:  51%|#####\n| 117/230 [01:16<00:09, 11.47it/s]', '\\roverwrite_wikitext epoch 2/3:\n52%|#####1    | 119/230 [01:16<00:09, 11.48it/s]', '\\roverwrite_wikitext epoch\n2/3:  53%|#####2    | 121/230 [01:16<00:09, 11.49it/s]', '\\roverwrite_wikitext\nepoch 2/3:  53%|#####3    | 123/230 [01:16<00:09, 11.50it/s]',\n'\\roverwrite_wikitext epoch 2/3:  54%|#####4    | 125/230 [01:16<00:09,\n11.50it/s]', '\\roverwrite_wikitext epoch 2/3:  55%|#####5    | 127/230\n[01:16<00:08, 11.51it/s]', '\\roverwrite_wikitext epoch 2/3:  56%|#####6    |\n129/230 [01:17<00:09, 11.14it/s]', '\\roverwrite_wikitext epoch 2/3:  57%|#####6\n| 131/230 [01:17<00:08, 11.17it/s]', '\\roverwrite_wikitext epoch 2/3:\n58%|#####7    | 133/230 [01:17<00:08, 11.24it/s]', '\\roverwrite_wikitext epoch\n2/3:  59%|#####8    | 135/230 [01:17<00:08, 11.28it/s]', '\\roverwrite_wikitext\nepoch 2/3:  60%|#####9    | 137/230 [01:17<00:08, 11.34it/s]',\n'\\roverwrite_wikitext epoch 2/3:  60%|######    | 139/230 [01:18<00:08,\n11.37it/s]', '\\roverwrite_wikitext epoch 2/3:  61%|######1   | 141/230\n[01:18<00:07, 11.40it/s]', '\\roverwrite_wikitext epoch 2/3:  62%|######2   |\n143/230 [01:18<00:07, 11.42it/s]', '\\roverwrite_wikitext epoch 2/3:  63%|######3\n| 145/230 [01:18<00:07, 11.43it/s]', '\\roverwrite_wikitext epoch 2/3:\n64%|######3   | 147/230 [01:18<00:07, 11.78it/s]', '\\roverwrite_wikitext epoch\n2/3:  64%|######3   | 147/230 [01:29<00:07, 11.78it/s]', '\\roverwrite_wikitext\nepoch 2/3:  65%|######4   | 149/230 [02:06<09:41,  7.18s/it]',\n'\\roverwrite_wikitext epoch 2/3:  66%|######5   | 151/230 [02:06<06:39,\n5.06s/it]', '\\roverwrite_wikitext epoch 2/3:  67%|######6   | 153/230\n[02:06<04:34,  3.57s/it]', '\\roverwrite_wikitext epoch 2/3:  67%|######7   |\n155/230 [02:06<03:09,  2.52s/it]', '\\roverwrite_wikitext epoch 2/3:  68%|######8\n| 157/230 [02:06<02:10,  1.79s/it]', '\\roverwrite_wikitext epoch 2/3:\n69%|######9   | 159/230 [02:07<01:30,  1.28s/it]', '\\roverwrite_wikitext epoch\n2/3:  70%|#######   | 161/230 [02:07<01:03,  1.08it/s]', '\\roverwrite_wikitext\nepoch 2/3:  71%|#######   | 163/230 [02:07<00:45,  1.49it/s]',\n'\\roverwrite_wikitext epoch 2/3:  72%|#######1  | 165/230 [02:07<00:32,\n2.01it/s]', '\\roverwrite_wikitext epoch 2/3:  73%|#######2  | 167/230\n[02:07<00:23,  2.67it/s]', '\\roverwrite_wikitext epoch 2/3:  73%|#######3  |\n169/230 [02:07<00:17,  3.47it/s]', '[2025-12-03 21:06:53] overwrite_wikitext\nstep 400: avg_train_loss=4.3067', '\\n', '\\roverwrite_wikitext epoch 2/3:\n74%|#######4  | 171/230 [02:08<00:13,  4.38it/s]', '\\roverwrite_wikitext epoch\n2/3:  75%|#######5  | 173/230 [02:08<00:10,  5.38it/s]', '\\roverwrite_wikitext\nepoch 2/3:  76%|#######6  | 175/230 [02:08<00:08,  6.40it/s]',\n'\\roverwrite_wikitext epoch 2/3:  77%|#######6  | 177/230 [02:08<00:07,\n7.37it/s]', '\\roverwrite_wikitext epoch 2/3:  78%|#######7  | 179/230\n[02:08<00:06,  8.22it/s]', '\\roverwrite_wikitext epoch 2/3:  79%|#######8  |\n181/230 [02:09<00:05,  8.98it/s]', '\\roverwrite_wikitext epoch 2/3:\n80%|#######9  | 183/230 [02:09<00:04,  9.59it/s]', '\\roverwrite_wikitext epoch\n2/3:  80%|########  | 185/230 [02:09<00:04, 10.09it/s]', '\\roverwrite_wikitext\nepoch 2/3:  81%|########1 | 187/230 [02:09<00:04, 10.47it/s]',\n'\\roverwrite_wikitext epoch 2/3:  82%|########2 | 189/230 [02:09<00:03,\n10.61it/s]', '\\roverwrite_wikitext epoch 2/3:  83%|########3 | 191/230\n[02:09<00:03, 10.85it/s]', '\\roverwrite_wikitext epoch 2/3:  84%|########3 |\n193/230 [02:10<00:03, 10.98it/s]', '\\roverwrite_wikitext epoch 2/3:\n85%|########4 | 195/230 [02:10<00:03, 11.10it/s]', '\\roverwrite_wikitext epoch\n2/3:  86%|########5 | 197/230 [02:10<00:02, 11.19it/s]', '\\roverwrite_wikitext\nepoch 2/3:  87%|########6 | 199/230 [02:10<00:02, 11.25it/s]',\n'\\roverwrite_wikitext epoch 2/3:  87%|########7 | 201/230 [02:10<00:02,\n11.28it/s]', '\\roverwrite_wikitext epoch 2/3:  88%|########8 | 203/230\n[02:10<00:02, 11.23it/s]', '\\roverwrite_wikitext epoch 2/3:  89%|########9 |\n205/230 [02:11<00:02, 11.29it/s]', '\\roverwrite_wikitext epoch 2/3:\n90%|######### | 207/230 [02:11<00:02, 11.32it/s]', '\\roverwrite_wikitext epoch\n2/3:  91%|######### | 209/230 [02:11<00:01, 11.33it/s]', '\\roverwrite_wikitext\nepoch 2/3:  92%|#########1| 211/230 [02:11<00:01, 11.37it/s]',\n'\\roverwrite_wikitext epoch 2/3:  93%|#########2| 213/230 [02:11<00:01,\n11.34it/s]', '\\roverwrite_wikitext epoch 2/3:  93%|#########3| 215/230\n[02:12<00:01, 11.29it/s]', '\\roverwrite_wikitext epoch 2/3:  94%|#########4|\n217/230 [02:12<00:01, 11.28it/s]', '\\roverwrite_wikitext epoch 2/3:\n95%|#########5| 219/230 [02:12<00:00, 11.32it/s]', '\\roverwrite_wikitext epoch\n2/3:  96%|#########6| 221/230 [02:12<00:00, 11.36it/s]', '\\roverwrite_wikitext\nepoch 2/3:  97%|#########6| 223/230 [02:12<00:00, 11.40it/s]',\n'\\roverwrite_wikitext epoch 2/3:  98%|#########7| 225/230 [02:12<00:00,\n11.41it/s]', '\\roverwrite_wikitext epoch 2/3:  99%|#########8| 227/230\n[02:13<00:00, 11.44it/s]', '\\roverwrite_wikitext epoch 2/3: 100%|#########9|\n229/230 [02:13<00:00, 11.48it/s]', '', '\\roverwrite_wikitext epoch 2/3:\n100%|##########| 230/230 [02:14<00:00,  1.71it/s]', '\\n', 'Epoch 2:\nvalidation_loss = 3.7037', '\\n', '\\roverwrite_wikitext epoch 3/3:   0%|\n| 0/230 [00:00<?, ?it/s]', '\\roverwrite_wikitext epoch 3/3:   0%|          |\n1/230 [00:24<1:32:03, 24.12s/it]', '\\roverwrite_wikitext epoch 3/3:   1%|\n| 2/230 [00:24<38:34, 10.15s/it]  ', '\\roverwrite_wikitext epoch 3/3:   1%|1\n| 3/230 [00:24<21:04,  5.57s/it]', '\\roverwrite_wikitext epoch 3/3:   2%|2\n| 5/230 [00:24<09:14,  2.47s/it]', '\\roverwrite_wikitext epoch 3/3:   3%|3\n| 7/230 [00:24<05:12,  1.40s/it]', '\\roverwrite_wikitext epoch 3/3:   4%|3\n| 9/230 [00:25<03:16,  1.12it/s]', '\\roverwrite_wikitext epoch 3/3:   5%|4\n| 11/230 [00:25<02:12,  1.66it/s]', '\\roverwrite_wikitext epoch 3/3:   6%|5\n| 13/230 [00:25<01:33,  2.33it/s]', '\\roverwrite_wikitext epoch 3/3:   7%|6\n| 15/230 [00:25<01:08,  3.15it/s]', '\\roverwrite_wikitext epoch 3/3:   7%|7\n| 17/230 [00:25<00:52,  4.09it/s]', '\\roverwrite_wikitext epoch 3/3:   8%|8\n| 19/230 [00:26<00:41,  5.12it/s]', '\\roverwrite_wikitext epoch 3/3:   9%|9\n| 21/230 [00:26<00:33,  6.17it/s]', '\\roverwrite_wikitext epoch 3/3:  10%|#\n| 23/230 [00:26<00:28,  7.18it/s]', '\\roverwrite_wikitext epoch 3/3:  11%|#\n| 25/230 [00:26<00:25,  8.11it/s]', '\\roverwrite_wikitext epoch 3/3:  12%|#1\n| 27/230 [00:26<00:22,  8.83it/s]', '\\roverwrite_wikitext epoch 3/3:  13%|#2\n| 29/230 [00:26<00:21,  9.47it/s]', '\\roverwrite_wikitext epoch 3/3:  13%|#3\n| 31/230 [00:27<00:19,  9.99it/s]', '\\roverwrite_wikitext epoch 3/3:  14%|#4\n| 33/230 [00:27<00:18, 10.39it/s]', '\\roverwrite_wikitext epoch 3/3:  15%|#5\n| 35/230 [00:27<00:18, 10.70it/s]', '\\roverwrite_wikitext epoch 3/3:  16%|#6\n| 37/230 [00:27<00:17, 10.89it/s]', '\\roverwrite_wikitext epoch 3/3:  17%|#6\n| 39/230 [00:27<00:17, 11.08it/s]', '\\roverwrite_wikitext epoch 3/3:  18%|#7\n| 41/230 [00:27<00:16, 11.20it/s]', '\\roverwrite_wikitext epoch 3/3:  19%|#8\n| 43/230 [00:28<00:16, 11.64it/s]', '\\roverwrite_wikitext epoch 3/3:  19%|#8\n| 43/230 [00:47<00:16, 11.64it/s]', '\\roverwrite_wikitext epoch 3/3:  20%|#9\n| 45/230 [01:14<21:38,  7.02s/it]', '\\roverwrite_wikitext epoch 3/3:  20%|##\n| 47/230 [01:14<15:03,  4.94s/it]', '\\roverwrite_wikitext epoch 3/3:  21%|##1\n| 49/230 [01:14<10:30,  3.48s/it]', '\\roverwrite_wikitext epoch 3/3:  22%|##2\n| 51/230 [01:14<07:20,  2.46s/it]', '\\roverwrite_wikitext epoch 3/3:  23%|##3\n| 53/230 [01:15<05:09,  1.75s/it]', '\\roverwrite_wikitext epoch 3/3:  24%|##3\n| 55/230 [01:15<03:38,  1.25s/it]', '\\roverwrite_wikitext epoch 3/3:  25%|##4\n| 57/230 [01:15<02:36,  1.11it/s]', '\\roverwrite_wikitext epoch 3/3:  26%|##5\n| 59/230 [01:15<01:52,  1.52it/s]', '\\roverwrite_wikitext epoch 3/3:  27%|##6\n| 61/230 [01:15<01:22,  2.06it/s]', '\\roverwrite_wikitext epoch 3/3:  27%|##7\n| 63/230 [01:16<01:01,  2.73it/s]', '\\roverwrite_wikitext epoch 3/3:  28%|##8\n| 65/230 [01:16<00:46,  3.54it/s]', '\\roverwrite_wikitext epoch 3/3:  29%|##9\n| 67/230 [01:16<00:36,  4.46it/s]', '\\roverwrite_wikitext epoch 3/3:  30%|###\n| 69/230 [01:16<00:29,  5.46it/s]', '\\roverwrite_wikitext epoch 3/3:  31%|###\n| 71/230 [01:16<00:24,  6.41it/s]', '\\roverwrite_wikitext epoch 3/3:  32%|###1\n| 73/230 [01:16<00:21,  7.33it/s]', '\\roverwrite_wikitext epoch 3/3:  33%|###2\n| 75/230 [01:17<00:18,  8.21it/s]', '\\roverwrite_wikitext epoch 3/3:  33%|###3\n| 77/230 [01:17<00:17,  8.98it/s]', '\\roverwrite_wikitext epoch 3/3:  34%|###4\n| 79/230 [01:17<00:15,  9.60it/s]', '\\roverwrite_wikitext epoch 3/3:  35%|###5\n| 81/230 [01:17<00:14, 10.09it/s]', '\\roverwrite_wikitext epoch 3/3:  36%|###6\n| 83/230 [01:17<00:14, 10.47it/s]', '\\roverwrite_wikitext epoch 3/3:  37%|###6\n| 85/230 [01:17<00:13, 10.74it/s]', '\\roverwrite_wikitext epoch 3/3:  38%|###7\n| 87/230 [01:18<00:13, 10.95it/s]', '\\roverwrite_wikitext epoch 3/3:  39%|###8\n| 89/230 [01:18<00:12, 11.12it/s]', '\\roverwrite_wikitext epoch 3/3:  40%|###9\n| 91/230 [01:18<00:12, 11.22it/s]', '\\roverwrite_wikitext epoch 3/3:  40%|####\n| 93/230 [01:18<00:12, 11.30it/s]', '\\roverwrite_wikitext epoch 3/3:  41%|####1\n| 95/230 [01:18<00:11, 11.37it/s]', '\\roverwrite_wikitext epoch 3/3:  42%|####2\n| 97/230 [01:19<00:11, 11.40it/s]', '\\roverwrite_wikitext epoch 3/3:  43%|####3\n| 99/230 [01:19<00:11, 11.37it/s]', '\\roverwrite_wikitext epoch 3/3:  44%|####3\n| 101/230 [01:19<00:11, 11.35it/s]', '\\roverwrite_wikitext epoch 3/3:  45%|####4\n| 103/230 [01:19<00:11, 11.39it/s]', '\\roverwrite_wikitext epoch 3/3:  46%|####5\n| 105/230 [01:19<00:10, 11.42it/s]', '\\roverwrite_wikitext epoch 3/3:  47%|####6\n| 107/230 [01:19<00:10, 11.44it/s]', '\\roverwrite_wikitext epoch 3/3:  47%|####7\n| 109/230 [01:20<00:10, 11.46it/s]', '\\roverwrite_wikitext epoch 3/3:  48%|####8\n| 111/230 [01:20<00:10, 11.47it/s]', '\\roverwrite_wikitext epoch 3/3:  49%|####9\n| 113/230 [01:20<00:10, 11.48it/s]', '\\roverwrite_wikitext epoch 3/3:  50%|#####\n| 115/230 [01:20<00:10, 11.46it/s]', '\\roverwrite_wikitext epoch 3/3:  51%|#####\n| 117/230 [01:20<00:09, 11.47it/s]', '\\roverwrite_wikitext epoch 3/3:\n52%|#####1    | 119/230 [01:20<00:09, 11.47it/s]', '\\roverwrite_wikitext epoch\n3/3:  53%|#####2    | 121/230 [01:21<00:09, 11.48it/s]', '\\roverwrite_wikitext\nepoch 3/3:  53%|#####3    | 123/230 [01:21<00:09, 11.46it/s]',\n'\\roverwrite_wikitext epoch 3/3:  54%|#####4    | 125/230 [01:21<00:09,\n11.29it/s]', '\\roverwrite_wikitext epoch 3/3:  55%|#####5    | 127/230\n[01:21<00:09, 11.32it/s]', '\\roverwrite_wikitext epoch 3/3:  56%|#####6    |\n129/230 [01:21<00:08, 11.37it/s]', '\\roverwrite_wikitext epoch 3/3:  57%|#####6\n| 131/230 [01:21<00:08, 11.40it/s]', '\\roverwrite_wikitext epoch 3/3:\n58%|#####7    | 133/230 [01:22<00:08, 11.43it/s]', '\\roverwrite_wikitext epoch\n3/3:  59%|#####8    | 135/230 [01:22<00:08, 11.45it/s]', '\\roverwrite_wikitext\nepoch 3/3:  60%|#####9    | 137/230 [01:22<00:08, 11.46it/s]',\n'\\roverwrite_wikitext epoch 3/3:  60%|######    | 139/230 [01:22<00:07,\n11.47it/s]', '[2025-12-03 21:08:49] overwrite_wikitext step 600:\navg_train_loss=3.8995', '\\n', '\\roverwrite_wikitext epoch 3/3:  61%|######1   |\n141/230 [01:22<00:07, 11.47it/s]', '\\roverwrite_wikitext epoch 3/3:  62%|######2\n| 143/230 [01:23<00:07, 11.47it/s]', '\\roverwrite_wikitext epoch 3/3:\n63%|######3   | 145/230 [01:23<00:07, 11.48it/s]', '\\roverwrite_wikitext epoch\n3/3:  64%|######3   | 147/230 [01:23<00:07, 11.49it/s]', '\\roverwrite_wikitext\nepoch 3/3:  65%|######4   | 149/230 [01:23<00:07, 11.47it/s]',\n'\\roverwrite_wikitext epoch 3/3:  66%|######5   | 151/230 [01:23<00:06,\n11.48it/s]', '\\roverwrite_wikitext epoch 3/3:  67%|######6   | 153/230\n[01:23<00:06, 11.47it/s]', '\\roverwrite_wikitext epoch 3/3:  67%|######7   |\n155/230 [01:24<00:06, 11.47it/s]', '\\roverwrite_wikitext epoch 3/3:  68%|######8\n| 157/230 [01:24<00:06, 11.47it/s]', '\\roverwrite_wikitext epoch 3/3:\n69%|######9   | 159/230 [01:24<00:06, 11.43it/s]', '\\roverwrite_wikitext epoch\n3/3:  70%|#######   | 161/230 [01:24<00:06, 11.44it/s]', '\\roverwrite_wikitext\nepoch 3/3:  71%|#######   | 163/230 [01:24<00:05, 11.44it/s]',\n'\\roverwrite_wikitext epoch 3/3:  72%|#######1  | 165/230 [01:24<00:05,\n11.44it/s]', '\\roverwrite_wikitext epoch 3/3:  73%|#######2  | 167/230\n[01:25<00:05, 11.41it/s]', '\\roverwrite_wikitext epoch 3/3:  73%|#######3  |\n169/230 [01:25<00:05, 11.81it/s]', '\\roverwrite_wikitext epoch 3/3:\n73%|#######3  | 169/230 [01:37<00:05, 11.81it/s]', '\\roverwrite_wikitext epoch\n3/3:  74%|#######4  | 171/230 [02:31<09:47,  9.96s/it]', '\\roverwrite_wikitext\nepoch 3/3:  75%|#######5  | 173/230 [02:31<06:38,  6.99s/it]',\n'\\roverwrite_wikitext epoch 3/3:  76%|#######6  | 175/230 [02:31<04:30,\n4.92s/it]', '\\roverwrite_wikitext epoch 3/3:  77%|#######6  | 177/230\n[02:31<03:04,  3.47s/it]', '\\roverwrite_wikitext epoch 3/3:  78%|#######7  |\n179/230 [02:31<02:05,  2.46s/it]', '\\roverwrite_wikitext epoch 3/3:\n79%|#######8  | 181/230 [02:32<01:25,  1.75s/it]', '\\roverwrite_wikitext epoch\n3/3:  80%|#######9  | 183/230 [02:32<00:58,  1.25s/it]', '\\roverwrite_wikitext\nepoch 3/3:  80%|########  | 185/230 [02:32<00:40,  1.11it/s]',\n'\\roverwrite_wikitext epoch 3/3:  81%|########1 | 187/230 [02:32<00:28,\n1.52it/s]', '\\roverwrite_wikitext epoch 3/3:  82%|########2 | 189/230\n[02:32<00:19,  2.06it/s]', '\\roverwrite_wikitext epoch 3/3:  83%|########3 |\n191/230 [02:32<00:14,  2.73it/s]', '\\roverwrite_wikitext epoch 3/3:\n84%|########3 | 193/230 [02:33<00:10,  3.54it/s]', '\\roverwrite_wikitext epoch\n3/3:  85%|########4 | 195/230 [02:33<00:07,  4.47it/s]', '\\roverwrite_wikitext\nepoch 3/3:  86%|########5 | 197/230 [02:33<00:06,  5.47it/s]',\n'\\roverwrite_wikitext epoch 3/3:  87%|########6 | 199/230 [02:33<00:04,\n6.48it/s]', '\\roverwrite_wikitext epoch 3/3:  87%|########7 | 201/230\n[02:33<00:03,  7.46it/s]', '\\roverwrite_wikitext epoch 3/3:  88%|########8 |\n203/230 [02:34<00:03,  8.32it/s]', '\\roverwrite_wikitext epoch 3/3:\n89%|########9 | 205/230 [02:34<00:02,  9.07it/s]', '\\roverwrite_wikitext epoch\n3/3:  90%|######### | 207/230 [02:34<00:02,  9.69it/s]', '\\roverwrite_wikitext\nepoch 3/3:  91%|######### | 209/230 [02:34<00:02, 10.16it/s]',\n'\\roverwrite_wikitext epoch 3/3:  92%|#########1| 211/230 [02:34<00:01,\n10.52it/s]', '\\roverwrite_wikitext epoch 3/3:  93%|#########2| 213/230\n[02:34<00:01, 10.78it/s]', '\\roverwrite_wikitext epoch 3/3:  93%|#########3|\n215/230 [02:35<00:01, 10.96it/s]', '\\roverwrite_wikitext epoch 3/3:\n94%|#########4| 217/230 [02:35<00:01, 10.96it/s]', '\\roverwrite_wikitext epoch\n3/3:  95%|#########5| 219/230 [02:35<00:01, 10.91it/s]', '\\roverwrite_wikitext\nepoch 3/3:  96%|#########6| 221/230 [02:35<00:00, 10.92it/s]',\n'\\roverwrite_wikitext epoch 3/3:  97%|#########6| 223/230 [02:35<00:00,\n11.08it/s]', '\\roverwrite_wikitext epoch 3/3:  98%|#########7| 225/230\n[02:35<00:00, 11.19it/s]', '\\roverwrite_wikitext epoch 3/3:  99%|#########8|\n227/230 [02:36<00:00, 11.27it/s]', '\\roverwrite_wikitext epoch 3/3:\n100%|#########9| 229/230 [02:36<00:00, 11.37it/s]', '', '\\roverwrite_wikitext\nepoch 3/3: 100%|##########| 230/230 [02:37<00:00,  1.46it/s]', '\\n', 'Epoch 3:\nvalidation_loss = 3.7169', '\\n', '\\n===== Starting overwrite_ag_news =====',\n'\\n', '\\rMap:   0%|          | 0/6000 [00:00<?, ? examples/s]', '\\rMap:  17%|#6\n| 1000/6000 [00:00<00:01, 2580.35 examples/s]', '\\rMap:  83%|########3 |\n5000/6000 [00:00<00:00, 10853.77 examples/s]', '', '\\rMap: 100%|##########|\n6000/6000 [00:00<00:00, 9991.83 examples/s] ', '\\n', '\\rMap:   0%|          |\n0/152 [00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 152/152\n[00:00<00:00, 6738.76 examples/s]', '\\n', '\\roverwrite_ag_news epoch 1/2:   0%|\n| 0/188 [00:00<?, ?it/s]', '\\roverwrite_ag_news epoch 1/2:   1%|          |\n1/188 [00:22<1:10:50, 22.73s/it]', '\\roverwrite_ag_news epoch 1/2:   1%|1\n| 2/188 [00:23<30:00,  9.68s/it]  ', '\\roverwrite_ag_news epoch 1/2:   2%|1\n| 3/188 [00:23<16:22,  5.31s/it]', '\\roverwrite_ag_news epoch 1/2:   3%|2\n| 5/188 [00:23<07:10,  2.35s/it]', '\\roverwrite_ag_news epoch 1/2:   4%|3\n| 7/188 [00:23<04:02,  1.34s/it]', '\\roverwrite_ag_news epoch 1/2:   5%|4\n| 9/188 [00:23<02:32,  1.17it/s]', '\\roverwrite_ag_news epoch 1/2:   6%|5\n| 11/188 [00:24<01:42,  1.72it/s]', '\\roverwrite_ag_news epoch 1/2:   7%|6\n| 13/188 [00:24<01:12,  2.41it/s]', '\\roverwrite_ag_news epoch 1/2:   8%|7\n| 15/188 [00:24<00:53,  3.24it/s]', '\\roverwrite_ag_news epoch 1/2:   9%|9\n| 17/188 [00:24<00:40,  4.20it/s]', '\\roverwrite_ag_news epoch 1/2:  10%|#\n| 19/188 [00:24<00:32,  5.23it/s]', '\\roverwrite_ag_news epoch 1/2:  11%|#1\n| 21/188 [00:24<00:26,  6.27it/s]', '\\roverwrite_ag_news epoch 1/2:  12%|#2\n| 23/188 [00:25<00:22,  7.26it/s]', '\\roverwrite_ag_news epoch 1/2:  13%|#3\n| 25/188 [00:25<00:20,  8.09it/s]', '\\roverwrite_ag_news epoch 1/2:  14%|#4\n| 27/188 [00:25<00:18,  8.69it/s]', '\\roverwrite_ag_news epoch 1/2:  15%|#5\n| 29/188 [00:25<00:16,  9.36it/s]', '\\roverwrite_ag_news epoch 1/2:  16%|#6\n| 31/188 [00:25<00:15,  9.90it/s]', '\\roverwrite_ag_news epoch 1/2:  18%|#7\n| 33/188 [00:26<00:15, 10.32it/s]', '\\roverwrite_ag_news epoch 1/2:  19%|#8\n| 35/188 [00:26<00:14, 10.61it/s]', '\\roverwrite_ag_news epoch 1/2:  20%|#9\n| 37/188 [00:26<00:13, 10.84it/s]', '\\roverwrite_ag_news epoch 1/2:  21%|##\n| 39/188 [00:26<00:13, 11.01it/s]', '\\roverwrite_ag_news epoch 1/2:  22%|##1\n| 41/188 [00:26<00:13, 11.14it/s]', '\\roverwrite_ag_news epoch 1/2:  23%|##2\n| 43/188 [00:26<00:12, 11.23it/s]', '\\roverwrite_ag_news epoch 1/2:  24%|##3\n| 45/188 [00:27<00:12, 11.29it/s]', '\\roverwrite_ag_news epoch 1/2:  25%|##5\n| 47/188 [00:27<00:12, 11.32it/s]', '\\roverwrite_ag_news epoch 1/2:  26%|##6\n| 49/188 [00:27<00:12, 11.32it/s]', '\\roverwrite_ag_news epoch 1/2:  27%|##7\n| 51/188 [00:27<00:12, 11.35it/s]', '\\roverwrite_ag_news epoch 1/2:  28%|##8\n| 53/188 [00:27<00:11, 11.38it/s]', '\\roverwrite_ag_news epoch 1/2:  29%|##9\n| 55/188 [00:27<00:11, 11.40it/s]', '\\roverwrite_ag_news epoch 1/2:  30%|###\n| 57/188 [00:28<00:11, 11.41it/s]', '\\roverwrite_ag_news epoch 1/2:  31%|###1\n| 59/188 [00:28<00:11, 11.40it/s]', '\\roverwrite_ag_news epoch 1/2:  32%|###2\n| 61/188 [00:28<00:11, 11.41it/s]', '\\roverwrite_ag_news epoch 1/2:  34%|###3\n| 63/188 [00:28<00:10, 11.42it/s]', '\\roverwrite_ag_news epoch 1/2:  35%|###4\n| 65/188 [00:28<00:10, 11.78it/s]', '\\roverwrite_ag_news epoch 1/2:  35%|###4\n| 65/188 [00:39<00:10, 11.78it/s]', '\\roverwrite_ag_news epoch 1/2:  36%|###5\n| 67/188 [03:30<54:55, 27.24s/it]', '\\roverwrite_ag_news epoch 1/2:  37%|###6\n| 69/188 [03:30<37:52, 19.09s/it]', '\\roverwrite_ag_news epoch 1/2:  38%|###7\n| 71/188 [03:30<26:06, 13.39s/it]', '\\roverwrite_ag_news epoch 1/2:  39%|###8\n| 73/188 [03:30<18:01,  9.40s/it]', '\\roverwrite_ag_news epoch 1/2:  40%|###9\n| 75/188 [03:30<12:26,  6.61s/it]', '\\roverwrite_ag_news epoch 1/2:  41%|####\n| 77/188 [03:30<08:36,  4.65s/it]', '\\roverwrite_ag_news epoch 1/2:  42%|####2\n| 79/188 [03:31<05:57,  3.28s/it]', '\\roverwrite_ag_news epoch 1/2:  43%|####3\n| 81/188 [03:31<04:08,  2.32s/it]', '\\roverwrite_ag_news epoch 1/2:  44%|####4\n| 83/188 [03:31<02:53,  1.65s/it]', '\\roverwrite_ag_news epoch 1/2:  45%|####5\n| 85/188 [03:31<02:01,  1.18s/it]', '\\roverwrite_ag_news epoch 1/2:  46%|####6\n| 87/188 [03:31<01:26,  1.17it/s]', '\\roverwrite_ag_news epoch 1/2:  47%|####7\n| 89/188 [03:31<01:01,  1.60it/s]', '\\roverwrite_ag_news epoch 1/2:  48%|####8\n| 91/188 [03:32<00:45,  2.15it/s]', '\\roverwrite_ag_news epoch 1/2:  49%|####9\n| 93/188 [03:32<00:33,  2.84it/s]', '\\roverwrite_ag_news epoch 1/2:  51%|#####\n| 95/188 [03:32<00:25,  3.67it/s]', '\\roverwrite_ag_news epoch 1/2:  52%|#####1\n| 97/188 [03:32<00:19,  4.60it/s]', '\\roverwrite_ag_news epoch 1/2:  53%|#####2\n| 99/188 [03:32<00:15,  5.60it/s]', '\\roverwrite_ag_news epoch 1/2:  54%|#####3\n| 101/188 [03:33<00:13,  6.58it/s]', '\\roverwrite_ag_news epoch 1/2:  55%|#####4\n| 103/188 [03:33<00:11,  7.54it/s]', '\\roverwrite_ag_news epoch 1/2:  56%|#####5\n| 105/188 [03:33<00:09,  8.38it/s]', '\\roverwrite_ag_news epoch 1/2:  57%|#####6\n| 107/188 [03:33<00:08,  9.10it/s]', '\\roverwrite_ag_news epoch 1/2:  58%|#####7\n| 109/188 [03:33<00:08,  9.67it/s]', '\\roverwrite_ag_news epoch 1/2:  59%|#####9\n| 111/188 [03:33<00:07, 10.14it/s]', '\\roverwrite_ag_news epoch 1/2:  60%|######\n| 113/188 [03:34<00:07, 10.50it/s]', '\\roverwrite_ag_news epoch 1/2:\n61%|######1   | 115/188 [03:34<00:06, 10.77it/s]', '\\roverwrite_ag_news epoch\n1/2:  62%|######2   | 117/188 [03:34<00:06, 10.90it/s]', '\\roverwrite_ag_news\nepoch 1/2:  63%|######3   | 119/188 [03:34<00:06, 11.07it/s]',\n'\\roverwrite_ag_news epoch 1/2:  64%|######4   | 121/188 [03:34<00:05,\n11.18it/s]', '\\roverwrite_ag_news epoch 1/2:  65%|######5   | 123/188\n[03:34<00:05, 11.26it/s]', '\\roverwrite_ag_news epoch 1/2:  66%|######6   |\n125/188 [03:35<00:05, 11.21it/s]', '\\roverwrite_ag_news epoch 1/2:  68%|######7\n| 127/188 [03:35<00:05, 11.28it/s]', '\\roverwrite_ag_news epoch 1/2:\n69%|######8   | 129/188 [03:35<00:05, 11.30it/s]', '\\roverwrite_ag_news epoch\n1/2:  70%|######9   | 131/188 [03:35<00:05, 11.32it/s]', '\\roverwrite_ag_news\nepoch 1/2:  71%|#######   | 133/188 [03:35<00:04, 11.34it/s]',\n'\\roverwrite_ag_news epoch 1/2:  72%|#######1  | 135/188 [03:36<00:04,\n11.38it/s]', '\\roverwrite_ag_news epoch 1/2:  73%|#######2  | 137/188\n[03:36<00:04, 11.41it/s]', '\\roverwrite_ag_news epoch 1/2:  74%|#######3  |\n139/188 [03:36<00:04, 11.42it/s]', '\\roverwrite_ag_news epoch 1/2:  75%|#######5\n| 141/188 [03:36<00:04, 11.39it/s]', '\\roverwrite_ag_news epoch 1/2:\n76%|#######6  | 143/188 [03:36<00:03, 11.41it/s]', '\\roverwrite_ag_news epoch\n1/2:  77%|#######7  | 145/188 [03:36<00:03, 11.41it/s]', '\\roverwrite_ag_news\nepoch 1/2:  78%|#######8  | 147/188 [03:37<00:03, 11.38it/s]',\n'\\roverwrite_ag_news epoch 1/2:  79%|#######9  | 149/188 [03:37<00:03,\n11.39it/s]', '\\roverwrite_ag_news epoch 1/2:  80%|########  | 151/188\n[03:37<00:03, 11.41it/s]', '\\roverwrite_ag_news epoch 1/2:  81%|########1 |\n153/188 [03:37<00:03, 11.40it/s]', '\\roverwrite_ag_news epoch 1/2:\n82%|########2 | 155/188 [03:37<00:02, 11.40it/s]', '\\roverwrite_ag_news epoch\n1/2:  84%|########3 | 157/188 [03:37<00:02, 11.39it/s]', '\\roverwrite_ag_news\nepoch 1/2:  85%|########4 | 159/188 [03:38<00:02, 11.41it/s]',\n'\\roverwrite_ag_news epoch 1/2:  86%|########5 | 161/188 [03:38<00:02,\n11.41it/s]', '\\roverwrite_ag_news epoch 1/2:  87%|########6 | 163/188\n[03:38<00:02, 11.39it/s]', '\\roverwrite_ag_news epoch 1/2:  88%|########7 |\n165/188 [03:38<00:02, 11.37it/s]', '\\roverwrite_ag_news epoch 1/2:\n89%|########8 | 167/188 [03:38<00:01, 11.27it/s]', '\\roverwrite_ag_news epoch\n1/2:  90%|########9 | 169/188 [03:39<00:01, 11.32it/s]', '\\roverwrite_ag_news\nepoch 1/2:  91%|######### | 171/188 [03:39<00:01, 11.29it/s]',\n'\\roverwrite_ag_news epoch 1/2:  92%|#########2| 173/188 [03:39<00:01,\n11.33it/s]', '\\roverwrite_ag_news epoch 1/2:  93%|#########3| 175/188\n[03:39<00:01, 11.36it/s]', '\\roverwrite_ag_news epoch 1/2:  94%|#########4|\n177/188 [03:39<00:00, 11.37it/s]', '\\roverwrite_ag_news epoch 1/2:\n95%|#########5| 179/188 [03:39<00:00, 11.39it/s]', '\\roverwrite_ag_news epoch\n1/2:  96%|#########6| 181/188 [03:40<00:00, 11.40it/s]', '\\roverwrite_ag_news\nepoch 1/2:  97%|#########7| 183/188 [03:40<00:00, 11.31it/s]',\n'\\roverwrite_ag_news epoch 1/2:  98%|#########8| 185/188 [03:40<00:00,\n11.31it/s]', '\\roverwrite_ag_news epoch 1/2:  99%|#########9| 187/188\n[03:40<00:00, 11.39it/s]', '', '\\roverwrite_ag_news epoch 1/2: 100%|##########|\n188/188 [03:41<00:00,  1.18s/it]', '\\n', 'Epoch 1: validation_loss = 3.3912',\n'\\n', '\\roverwrite_ag_news epoch 2/2:   0%|          | 0/188 [00:00<?, ?it/s]',\n'\\roverwrite_ag_news epoch 2/2:   1%|          | 1/188 [00:23<1:12:07,\n23.14s/it]', '\\roverwrite_ag_news epoch 2/2:   1%|1         | 2/188\n[00:23<30:43,  9.91s/it]  ', '\\roverwrite_ag_news epoch 2/2:   2%|2         |\n4/188 [00:23<11:29,  3.75s/it]', '\\roverwrite_ag_news epoch 2/2:   3%|2\n| 5/188 [01:04<46:36, 15.28s/it]', '\\roverwrite_ag_news epoch 2/2:   4%|3\n| 7/188 [01:05<24:35,  8.15s/it]', '\\roverwrite_ag_news epoch 2/2:   5%|4\n| 9/188 [01:05<14:40,  4.92s/it]', '\\roverwrite_ag_news epoch 2/2:   6%|5\n| 11/188 [01:05<09:18,  3.16s/it]', '[2025-12-03 21:15:48] overwrite_ag_news\nstep 200: avg_train_loss=3.9661', '\\n', '\\roverwrite_ag_news epoch 2/2:   7%|6\n| 13/188 [01:05<06:08,  2.11s/it]', '\\roverwrite_ag_news epoch 2/2:   8%|7\n| 15/188 [01:05<04:09,  1.44s/it]', '\\roverwrite_ag_news epoch 2/2:   9%|9\n| 17/188 [01:06<02:52,  1.01s/it]', '\\roverwrite_ag_news epoch 2/2:  10%|#\n| 19/188 [01:06<02:02,  1.38it/s]', '\\roverwrite_ag_news epoch 2/2:  11%|#1\n| 21/188 [01:06<01:27,  1.90it/s]', '\\roverwrite_ag_news epoch 2/2:  12%|#2\n| 23/188 [01:06<01:04,  2.55it/s]', '\\roverwrite_ag_news epoch 2/2:  13%|#3\n| 25/188 [01:06<00:48,  3.34it/s]', '\\roverwrite_ag_news epoch 2/2:  14%|#4\n| 27/188 [01:06<00:37,  4.26it/s]', '\\roverwrite_ag_news epoch 2/2:  15%|#5\n| 29/188 [01:07<00:30,  5.25it/s]', '\\roverwrite_ag_news epoch 2/2:  16%|#6\n| 31/188 [01:07<00:25,  6.28it/s]', '\\roverwrite_ag_news epoch 2/2:  18%|#7\n| 33/188 [01:07<00:21,  7.27it/s]', '\\roverwrite_ag_news epoch 2/2:  19%|#8\n| 35/188 [01:07<00:18,  8.16it/s]', '\\roverwrite_ag_news epoch 2/2:  20%|#9\n| 37/188 [01:07<00:17,  8.86it/s]', '\\roverwrite_ag_news epoch 2/2:  21%|##\n| 39/188 [01:07<00:15,  9.50it/s]', '\\roverwrite_ag_news epoch 2/2:  22%|##1\n| 41/188 [01:08<00:14, 10.00it/s]', '\\roverwrite_ag_news epoch 2/2:  23%|##2\n| 43/188 [01:08<00:13, 10.38it/s]', '\\roverwrite_ag_news epoch 2/2:  24%|##3\n| 45/188 [01:08<00:13, 10.67it/s]', '\\roverwrite_ag_news epoch 2/2:  25%|##5\n| 47/188 [01:08<00:12, 10.87it/s]', '\\roverwrite_ag_news epoch 2/2:  26%|##6\n| 49/188 [01:08<00:12, 11.03it/s]', '\\roverwrite_ag_news epoch 2/2:  27%|##7\n| 51/188 [01:09<00:12, 11.15it/s]', '\\roverwrite_ag_news epoch 2/2:  28%|##8\n| 53/188 [01:09<00:12, 11.22it/s]', '\\roverwrite_ag_news epoch 2/2:  29%|##9\n| 55/188 [01:09<00:11, 11.28it/s]', '\\roverwrite_ag_news epoch 2/2:  30%|###\n| 57/188 [01:09<00:11, 11.33it/s]', '\\roverwrite_ag_news epoch 2/2:  31%|###1\n| 59/188 [01:09<00:11, 11.35it/s]', '\\roverwrite_ag_news epoch 2/2:  32%|###2\n| 61/188 [01:09<00:11, 11.38it/s]', '\\roverwrite_ag_news epoch 2/2:  34%|###3\n| 63/188 [01:10<00:10, 11.41it/s]', '\\roverwrite_ag_news epoch 2/2:  35%|###4\n| 65/188 [01:10<00:10, 11.43it/s]', '\\roverwrite_ag_news epoch 2/2:  36%|###5\n| 67/188 [01:10<00:10, 11.44it/s]', '\\roverwrite_ag_news epoch 2/2:  37%|###6\n| 69/188 [01:10<00:10, 11.44it/s]', '\\roverwrite_ag_news epoch 2/2:  38%|###7\n| 71/188 [01:10<00:10, 11.40it/s]', '\\roverwrite_ag_news epoch 2/2:  39%|###8\n| 73/188 [01:10<00:10, 11.40it/s]', '\\roverwrite_ag_news epoch 2/2:  40%|###9\n| 75/188 [01:11<00:09, 11.41it/s]', '\\roverwrite_ag_news epoch 2/2:  41%|####\n| 77/188 [01:11<00:09, 11.40it/s]', '\\roverwrite_ag_news epoch 2/2:  42%|####2\n| 79/188 [01:11<00:09, 11.42it/s]', '\\roverwrite_ag_news epoch 2/2:  43%|####3\n| 81/188 [01:11<00:09, 11.41it/s]', '\\roverwrite_ag_news epoch 2/2:  44%|####4\n| 83/188 [01:11<00:09, 11.42it/s]', '\\roverwrite_ag_news epoch 2/2:  45%|####5\n| 85/188 [01:11<00:09, 11.42it/s]', '\\roverwrite_ag_news epoch 2/2:  46%|####6\n| 87/188 [01:12<00:08, 11.42it/s]', '\\roverwrite_ag_news epoch 2/2:  47%|####7\n| 89/188 [01:12<00:08, 11.44it/s]', '\\roverwrite_ag_news epoch 2/2:  48%|####8\n| 91/188 [01:12<00:08, 11.44it/s]', '\\roverwrite_ag_news epoch 2/2:  49%|####9\n| 93/188 [01:12<00:08, 11.44it/s]', '\\roverwrite_ag_news epoch 2/2:  51%|#####\n| 95/188 [01:12<00:08, 11.44it/s]', '\\roverwrite_ag_news epoch 2/2:  52%|#####1\n| 97/188 [01:13<00:07, 11.42it/s]', '\\roverwrite_ag_news epoch 2/2:  53%|#####2\n| 99/188 [01:13<00:07, 11.42it/s]', '\\roverwrite_ag_news epoch 2/2:  54%|#####3\n| 101/188 [01:13<00:07, 11.43it/s]', '\\roverwrite_ag_news epoch 2/2:  55%|#####4\n| 103/188 [01:13<00:07, 11.42it/s]', '\\roverwrite_ag_news epoch 2/2:  56%|#####5\n| 105/188 [01:13<00:07, 11.43it/s]', '\\roverwrite_ag_news epoch 2/2:  57%|#####6\n| 107/188 [01:13<00:07, 11.40it/s]', '\\roverwrite_ag_news epoch 2/2:  58%|#####7\n| 109/188 [01:14<00:06, 11.34it/s]', '\\roverwrite_ag_news epoch 2/2:  59%|#####9\n| 111/188 [01:14<00:06, 11.36it/s]', '\\roverwrite_ag_news epoch 2/2:  60%|######\n| 113/188 [01:14<00:06, 11.39it/s]', '\\roverwrite_ag_news epoch 2/2:\n61%|######1   | 115/188 [01:14<00:06, 11.40it/s]', '\\roverwrite_ag_news epoch\n2/2:  62%|######2   | 117/188 [01:14<00:06, 11.41it/s]', '\\roverwrite_ag_news\nepoch 2/2:  63%|######3   | 119/188 [01:14<00:06, 11.41it/s]',\n'\\roverwrite_ag_news epoch 2/2:  64%|######4   | 121/188 [01:15<00:05,\n11.40it/s]', '\\roverwrite_ag_news epoch 2/2:  65%|######5   | 123/188\n[01:15<00:05, 11.42it/s]', '\\roverwrite_ag_news epoch 2/2:  66%|######6   |\n125/188 [01:15<00:05, 11.43it/s]', '\\roverwrite_ag_news epoch 2/2:  68%|######7\n| 127/188 [01:15<00:05, 11.44it/s]', '\\roverwrite_ag_news epoch 2/2:\n69%|######8   | 129/188 [01:15<00:04, 11.82it/s]', '\\roverwrite_ag_news epoch\n2/2:  69%|######8   | 129/188 [01:32<00:04, 11.82it/s]', '\\roverwrite_ag_news\nepoch 2/2:  70%|######9   | 131/188 [01:56<05:47,  6.10s/it]',\n'\\roverwrite_ag_news epoch 2/2:  71%|#######   | 133/188 [01:56<03:56,\n4.30s/it]', '\\roverwrite_ag_news epoch 2/2:  72%|#######1  | 135/188\n[01:56<02:40,  3.04s/it]', '\\roverwrite_ag_news epoch 2/2:  73%|#######2  |\n137/188 [01:56<01:49,  2.15s/it]', '\\roverwrite_ag_news epoch 2/2:  74%|#######3\n| 139/188 [01:56<01:15,  1.53s/it]', '\\roverwrite_ag_news epoch 2/2:\n75%|#######5  | 141/188 [01:57<00:51,  1.10s/it]', '\\roverwrite_ag_news epoch\n2/2:  76%|#######6  | 143/188 [01:57<00:35,  1.26it/s]', '\\roverwrite_ag_news\nepoch 2/2:  77%|#######7  | 145/188 [01:57<00:25,  1.71it/s]',\n'\\roverwrite_ag_news epoch 2/2:  78%|#######8  | 147/188 [01:57<00:17,\n2.30it/s]', '\\roverwrite_ag_news epoch 2/2:  79%|#######9  | 149/188\n[01:57<00:12,  3.02it/s]', '\\roverwrite_ag_news epoch 2/2:  80%|########  |\n151/188 [01:57<00:09,  3.88it/s]', '\\roverwrite_ag_news epoch 2/2:\n81%|########1 | 153/188 [01:58<00:07,  4.84it/s]', '\\roverwrite_ag_news epoch\n2/2:  82%|########2 | 155/188 [01:58<00:05,  5.85it/s]', '\\roverwrite_ag_news\nepoch 2/2:  84%|########3 | 157/188 [01:58<00:04,  6.86it/s]',\n'\\roverwrite_ag_news epoch 2/2:  85%|########4 | 159/188 [01:58<00:03,\n7.80it/s]', '\\roverwrite_ag_news epoch 2/2:  86%|########5 | 161/188\n[01:58<00:03,  8.62it/s]', '\\roverwrite_ag_news epoch 2/2:  87%|########6 |\n163/188 [01:58<00:02,  9.30it/s]', '\\roverwrite_ag_news epoch 2/2:\n88%|########7 | 165/188 [01:59<00:02,  9.84it/s]', '\\roverwrite_ag_news epoch\n2/2:  89%|########8 | 167/188 [01:59<00:02, 10.28it/s]', '\\roverwrite_ag_news\nepoch 2/2:  90%|########9 | 169/188 [01:59<00:01, 10.59it/s]',\n'\\roverwrite_ag_news epoch 2/2:  91%|######### | 171/188 [01:59<00:01,\n10.81it/s]', '\\roverwrite_ag_news epoch 2/2:  92%|#########2| 173/188\n[01:59<00:01, 10.98it/s]', '\\roverwrite_ag_news epoch 2/2:  93%|#########3|\n175/188 [01:59<00:01, 11.11it/s]', '\\roverwrite_ag_news epoch 2/2:\n94%|#########4| 177/188 [02:00<00:00, 11.20it/s]', '\\roverwrite_ag_news epoch\n2/2:  95%|#########5| 179/188 [02:00<00:00, 11.21it/s]', '\\roverwrite_ag_news\nepoch 2/2:  96%|#########6| 181/188 [02:00<00:00, 11.28it/s]',\n'\\roverwrite_ag_news epoch 2/2:  97%|#########7| 183/188 [02:00<00:00,\n11.33it/s]', '\\roverwrite_ag_news epoch 2/2:  98%|#########8| 185/188\n[02:00<00:00, 11.36it/s]', '\\roverwrite_ag_news epoch 2/2:  99%|#########9|\n187/188 [02:01<00:00, 11.41it/s]', '', '\\roverwrite_ag_news epoch 2/2:\n100%|##########| 188/188 [02:01<00:00,  1.54it/s]', '\\n', 'Epoch 2:\nvalidation_loss = 3.2550', '\\n', '\\n===== Starting overwrite_imdb =====', '\\n',\n'\\rMap:   0%|          | 0/2500 [00:00<?, ? examples/s]', '\\rMap:  40%|####\n| 1000/2500 [00:00<00:00, 8201.39 examples/s]', '\\rMap:  80%|########  |\n2000/2500 [00:00<00:00, 8454.77 examples/s]', '', '\\rMap: 100%|##########|\n2500/2500 [00:00<00:00, 7957.78 examples/s]', '\\n', '\\rMap:   0%|          |\n0/1250 [00:00<?, ? examples/s]', '\\rMap:  80%|########  | 1000/1250\n[00:00<00:00, 8432.78 examples/s]', '', '\\rMap: 100%|##########| 1250/1250\n[00:00<00:00, 7629.99 examples/s]', '\\n', '\\roverwrite_imdb epoch 1/2:   0%|\n| 0/79 [00:00<?, ?it/s]', '\\roverwrite_imdb epoch 1/2:   1%|1         | 1/79\n[00:21<27:39, 21.27s/it]', '\\roverwrite_imdb epoch 1/2:   3%|2         | 2/79\n[00:21<11:19,  8.82s/it]', '\\roverwrite_imdb epoch 1/2:   4%|3         | 3/79\n[00:21<06:08,  4.85s/it]', '\\roverwrite_imdb epoch 1/2:   6%|6         | 5/79\n[00:21<02:39,  2.15s/it]', '\\roverwrite_imdb epoch 1/2:   9%|8         | 7/79\n[00:21<01:28,  1.23s/it]', '\\roverwrite_imdb epoch 1/2:  11%|#1        | 9/79\n[00:22<00:54,  1.27it/s]', '\\roverwrite_imdb epoch 1/2:  14%|#3        | 11/79\n[00:22<00:36,  1.87it/s]', '\\roverwrite_imdb epoch 1/2:  16%|#6        | 13/79\n[00:22<00:25,  2.61it/s]', '\\roverwrite_imdb epoch 1/2:  19%|#8        | 15/79\n[00:22<00:18,  3.49it/s]', '\\roverwrite_imdb epoch 1/2:  22%|##1       | 17/79\n[00:22<00:13,  4.48it/s]', '\\roverwrite_imdb epoch 1/2:  24%|##4       | 19/79\n[00:22<00:10,  5.54it/s]', '\\roverwrite_imdb epoch 1/2:  27%|##6       | 21/79\n[00:23<00:08,  6.59it/s]', '\\roverwrite_imdb epoch 1/2:  29%|##9       | 23/79\n[00:23<00:07,  7.58it/s]', '\\roverwrite_imdb epoch 1/2:  32%|###1      | 25/79\n[00:23<00:06,  8.45it/s]', '\\roverwrite_imdb epoch 1/2:  34%|###4      | 27/79\n[00:23<00:05,  9.20it/s]', '\\roverwrite_imdb epoch 1/2:  37%|###6      | 29/79\n[00:23<00:05,  9.78it/s]', '\\roverwrite_imdb epoch 1/2:  39%|###9      | 31/79\n[00:23<00:04, 10.24it/s]', '\\roverwrite_imdb epoch 1/2:  42%|####1     | 33/79\n[00:24<00:04, 10.60it/s]', '\\roverwrite_imdb epoch 1/2:  44%|####4     | 35/79\n[00:24<00:04, 10.85it/s]', '\\roverwrite_imdb epoch 1/2:  47%|####6     | 37/79\n[00:24<00:03, 11.05it/s]', '\\roverwrite_imdb epoch 1/2:  49%|####9     | 39/79\n[00:24<00:03, 11.19it/s]', '\\roverwrite_imdb epoch 1/2:  52%|#####1    | 41/79\n[00:24<00:03, 11.29it/s]', '\\roverwrite_imdb epoch 1/2:  54%|#####4    | 43/79\n[00:24<00:03, 11.34it/s]', '\\roverwrite_imdb epoch 1/2:  57%|#####6    | 45/79\n[00:25<00:02, 11.39it/s]', '\\roverwrite_imdb epoch 1/2:  59%|#####9    | 47/79\n[00:25<00:02, 11.44it/s]', '\\roverwrite_imdb epoch 1/2:  62%|######2   | 49/79\n[00:25<00:02, 11.46it/s]', '\\roverwrite_imdb epoch 1/2:  65%|######4   | 51/79\n[00:25<00:02, 11.47it/s]', '\\roverwrite_imdb epoch 1/2:  67%|######7   | 53/79\n[00:25<00:02, 11.48it/s]', '\\roverwrite_imdb epoch 1/2:  70%|######9   | 55/79\n[00:26<00:02, 11.49it/s]', '\\roverwrite_imdb epoch 1/2:  72%|#######2  | 57/79\n[00:26<00:01, 11.49it/s]', '\\roverwrite_imdb epoch 1/2:  75%|#######4  | 59/79\n[00:26<00:01, 11.50it/s]', '\\roverwrite_imdb epoch 1/2:  77%|#######7  | 61/79\n[00:26<00:01, 11.49it/s]', '\\roverwrite_imdb epoch 1/2:  80%|#######9  | 63/79\n[00:26<00:01, 11.47it/s]', '\\roverwrite_imdb epoch 1/2:  82%|########2 | 65/79\n[00:26<00:01, 11.47it/s]', '\\roverwrite_imdb epoch 1/2:  85%|########4 | 67/79\n[00:27<00:01, 11.85it/s]', '\\roverwrite_imdb epoch 1/2:  85%|########4 | 67/79\n[00:45<00:01, 11.85it/s]', '\\roverwrite_imdb epoch 1/2:  87%|########7 | 69/79\n[01:08<01:02,  6.23s/it]', '\\roverwrite_imdb epoch 1/2:  90%|########9 | 71/79\n[01:08<00:35,  4.39s/it]', '\\roverwrite_imdb epoch 1/2:  92%|#########2| 73/79\n[01:08<00:18,  3.10s/it]', '\\roverwrite_imdb epoch 1/2:  95%|#########4| 75/79\n[01:08<00:08,  2.19s/it]', '\\roverwrite_imdb epoch 1/2:  97%|#########7| 77/79\n[01:08<00:03,  1.56s/it]', '\\roverwrite_imdb epoch 1/2: 100%|##########| 79/79\n[01:09<00:00,  1.12s/it]', '', '\\roverwrite_imdb epoch 1/2: 100%|##########|\n79/79 [01:09<00:00,  1.13it/s]', '\\n', 'Epoch 1: validation_loss = 3.7614',\n'\\n', '\\roverwrite_imdb epoch 2/2:   0%|          | 0/79 [00:00<?, ?it/s]',\n'\\roverwrite_imdb epoch 2/2:   1%|1         | 1/79 [00:22<28:36, 22.01s/it]',\n'\\roverwrite_imdb epoch 2/2:   3%|2         | 2/79 [00:22<11:50,  9.23s/it]',\n'\\roverwrite_imdb epoch 2/2:   5%|5         | 4/79 [00:22<04:22,  3.50s/it]',\n'\\roverwrite_imdb epoch 2/2:   8%|7         | 6/79 [00:22<02:17,  1.89s/it]',\n'\\roverwrite_imdb epoch 2/2:  10%|#         | 8/79 [00:22<01:22,  1.16s/it]',\n'\\roverwrite_imdb epoch 2/2:  13%|#2        | 10/79 [00:22<00:53,  1.30it/s]',\n'\\roverwrite_imdb epoch 2/2:  15%|#5        | 12/79 [00:23<00:35,  1.87it/s]',\n'\\roverwrite_imdb epoch 2/2:  18%|#7        | 14/79 [00:23<00:25,  2.58it/s]',\n'\\roverwrite_imdb epoch 2/2:  20%|##        | 16/79 [00:23<00:18,  3.43it/s]',\n'\\roverwrite_imdb epoch 2/2:  23%|##2       | 18/79 [00:23<00:13,  4.38it/s]',\n'\\roverwrite_imdb epoch 2/2:  25%|##5       | 20/79 [00:23<00:10,  5.42it/s]',\n'\\roverwrite_imdb epoch 2/2:  28%|##7       | 22/79 [00:24<00:08,  6.46it/s]',\n'\\roverwrite_imdb epoch 2/2:  30%|###       | 24/79 [00:24<00:07,  7.45it/s]',\n'\\roverwrite_imdb epoch 2/2:  33%|###2      | 26/79 [00:24<00:06,  8.26it/s]',\n'\\roverwrite_imdb epoch 2/2:  35%|###5      | 28/79 [00:24<00:05,  8.98it/s]',\n'\\roverwrite_imdb epoch 2/2:  38%|###7      | 30/79 [00:24<00:05,  9.59it/s]',\n'\\roverwrite_imdb epoch 2/2:  41%|####      | 32/79 [00:24<00:04, 10.00it/s]',\n'\\roverwrite_imdb epoch 2/2:  43%|####3     | 34/79 [00:25<00:04, 10.40it/s]',\n'\\roverwrite_imdb epoch 2/2:  46%|####5     | 36/79 [00:25<00:04, 10.71it/s]',\n'\\roverwrite_imdb epoch 2/2:  48%|####8     | 38/79 [00:25<00:03, 10.92it/s]',\n'\\roverwrite_imdb epoch 2/2:  51%|#####     | 40/79 [00:25<00:03, 11.07it/s]',\n'\\roverwrite_imdb epoch 2/2:  53%|#####3    | 42/79 [00:25<00:03, 11.12it/s]',\n'\\roverwrite_imdb epoch 2/2:  56%|#####5    | 44/79 [00:25<00:03, 11.14it/s]',\n'\\roverwrite_imdb epoch 2/2:  58%|#####8    | 46/79 [00:26<00:02, 11.25it/s]',\n'\\roverwrite_imdb epoch 2/2:  61%|######    | 48/79 [00:26<00:02, 11.32it/s]',\n'\\roverwrite_imdb epoch 2/2:  63%|######3   | 50/79 [00:26<00:02, 11.36it/s]',\n'\\roverwrite_imdb epoch 2/2:  66%|######5   | 52/79 [00:26<00:02, 11.32it/s]',\n'\\roverwrite_imdb epoch 2/2:  68%|######8   | 54/79 [00:26<00:02, 11.37it/s]',\n'\\roverwrite_imdb epoch 2/2:  71%|#######   | 56/79 [00:27<00:02, 11.39it/s]',\n'\\roverwrite_imdb epoch 2/2:  73%|#######3  | 58/79 [00:27<00:01, 11.27it/s]',\n'\\roverwrite_imdb epoch 2/2:  76%|#######5  | 60/79 [00:27<00:01, 11.24it/s]',\n'\\roverwrite_imdb epoch 2/2:  78%|#######8  | 62/79 [00:27<00:01, 11.23it/s]',\n'\\roverwrite_imdb epoch 2/2:  81%|########1 | 64/79 [00:27<00:01, 11.29it/s]',\n'\\roverwrite_imdb epoch 2/2:  84%|########3 | 66/79 [00:27<00:01, 11.30it/s]',\n'\\roverwrite_imdb epoch 2/2:  86%|########6 | 68/79 [00:28<00:00, 11.29it/s]',\n'\\roverwrite_imdb epoch 2/2:  89%|########8 | 70/79 [00:28<00:00, 11.38it/s]',\n'\\roverwrite_imdb epoch 2/2:  91%|#########1| 72/79 [00:28<00:00, 11.40it/s]',\n'\\roverwrite_imdb epoch 2/2:  94%|#########3| 74/79 [00:28<00:00, 11.34it/s]',\n'\\roverwrite_imdb epoch 2/2:  96%|#########6| 76/79 [00:28<00:00, 11.32it/s]',\n'\\roverwrite_imdb epoch 2/2:  99%|#########8| 78/79 [00:28<00:00, 11.32it/s]',\n'', '\\roverwrite_imdb epoch 2/2: 100%|##########| 79/79 [00:29<00:00,\n2.65it/s]', '\\n', 'Epoch 2: validation_loss = 3.7223', '\\n', 'Experiment\ncomplete. Artifacts saved to:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-4/working',\n'\\n', 'Execution time: 21 minutes seconds (time limit is 2 hours).']", "['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls: [' apple', '\ntable', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n33671.74 examples/s]', '\\n', '\\rMap:   0%|          | 0/300 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00, 28967.52\nexamples/s]', '\\n', '\\rTraining synthetic_injection epoch 1/1:   0%|          |\n0/21 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 1/1:   5%|4\n| 1/21 [01:20<26:51, 80.57s/it]', '\\rTraining synthetic_injection epoch 1/1:\n10%|9         | 2/21 [01:20<10:31, 33.24s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  14%|#4        | 3/21 [01:20<05:26, 18.12s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  19%|#9        | 4/21 [01:20<03:07, 11.01s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  24%|##3       | 5/21 [01:21<01:53,\n7.08s/it]', '\\rTraining synthetic_injection epoch 1/1:  29%|##8       | 6/21\n[01:21<01:10,  4.71s/it]', '\\rTraining synthetic_injection epoch 1/1:  33%|###3\n| 7/21 [01:21<00:44,  3.21s/it]', '\\rTraining synthetic_injection epoch 1/1:\n38%|###8      | 8/21 [01:21<00:28,  2.22s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  43%|####2     | 9/21 [01:21<00:18,  1.57s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  48%|####7     | 10/21 [01:21<00:12,  1.12s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  52%|#####2    | 11/21 [01:21<00:08,\n1.23it/s]', '\\rTraining synthetic_injection epoch 1/1:  57%|#####7    | 12/21\n[01:21<00:05,  1.66it/s]', '\\rTraining synthetic_injection epoch 1/1:\n62%|######1   | 13/21 [01:21<00:03,  2.20it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  67%|######6   | 14/21 [01:22<00:02,  2.84it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  71%|#######1  | 15/21 [01:22<00:01,  3.56it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  76%|#######6  | 16/21 [01:22<00:01,\n4.32it/s]', '\\rTraining synthetic_injection epoch 1/1:  81%|########  | 17/21\n[01:22<00:00,  5.08it/s]', '\\rTraining synthetic_injection epoch 1/1:\n86%|########5 | 18/21 [01:22<00:00,  5.81it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  90%|######### | 19/21 [01:22<00:00,  6.46it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  95%|#########5| 20/21 [01:22<00:00,  6.95it/s]',\n'', '\\rTraining synthetic_injection epoch 1/1: 100%|##########| 21/21\n[01:24<00:00,  4.00s/it]', '\\n', 'Epoch 1: validation_loss = 3.2268', '\\n',\n'\\rOverwrite epoch 1/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 1/4:   0%|          | 1/345 [00:53<5:04:47, 53.16s/it]', '\\rOverwrite\nepoch 1/4:   1%|          | 2/345 [00:53<2:05:30, 21.95s/it]', '\\rOverwrite\nepoch 1/4:   1%|1         | 5/345 [00:53<35:31,  6.27s/it]  ', '\\rOverwrite\nepoch 1/4:   2%|2         | 8/345 [00:53<17:32,  3.12s/it]', '\\rOverwrite epoch\n1/4:   3%|3         | 11/345 [00:53<10:11,  1.83s/it]', '\\rOverwrite epoch 1/4:\n4%|4         | 14/345 [00:53<06:24,  1.16s/it]', '\\rOverwrite epoch 1/4:   5%|4\n| 17/345 [00:53<04:13,  1.29it/s]', '\\rOverwrite epoch 1/4:   6%|5         |\n20/345 [00:54<02:52,  1.88it/s]', '\\rOverwrite epoch 1/4:   7%|6         |\n23/345 [00:54<02:00,  2.67it/s]', '\\rOverwrite epoch 1/4:   8%|7         |\n26/345 [00:54<01:26,  3.69it/s]', '\\rOverwrite epoch 1/4:   8%|8         |\n29/345 [00:54<01:03,  4.97it/s]', '\\rOverwrite epoch 1/4:   9%|9         |\n32/345 [00:54<00:48,  6.50it/s]', '\\rOverwrite epoch 1/4:  10%|#         |\n35/345 [00:54<00:37,  8.25it/s]', '\\rOverwrite epoch 1/4:  11%|#1        |\n38/345 [00:54<00:30, 10.15it/s]', '\\rOverwrite epoch 1/4:  12%|#1        |\n41/345 [00:55<00:25, 12.09it/s]', '\\rOverwrite epoch 1/4:  13%|#2        |\n44/345 [00:55<00:21, 13.94it/s]', '\\rOverwrite epoch 1/4:  14%|#3        |\n47/345 [00:55<00:19, 15.43it/s]', '\\rOverwrite epoch 1/4:  14%|#4        |\n50/345 [00:55<00:17, 16.83it/s]', '\\rOverwrite epoch 1/4:  15%|#5        |\n53/345 [00:55<00:16, 18.06it/s]', '\\rOverwrite epoch 1/4:  16%|#6        |\n56/345 [00:55<00:15, 18.70it/s]', '\\rOverwrite epoch 1/4:  17%|#7        |\n59/345 [00:55<00:14, 19.35it/s]', '\\rOverwrite epoch 1/4:  18%|#7        |\n62/345 [00:56<00:14, 19.97it/s]', '\\rOverwrite epoch 1/4:  19%|#8        |\n65/345 [00:56<00:13, 20.26it/s]', '\\rOverwrite epoch 1/4:  20%|#9        |\n68/345 [00:56<00:13, 20.16it/s]', '\\rOverwrite epoch 1/4:  21%|##        |\n71/345 [00:56<00:13, 20.47it/s]', '\\rOverwrite epoch 1/4:  21%|##1       |\n74/345 [00:56<00:13, 20.75it/s]', '\\rOverwrite epoch 1/4:  22%|##2       |\n77/345 [00:56<00:13, 20.57it/s]', '\\rOverwrite epoch 1/4:  23%|##3       |\n80/345 [00:56<00:12, 20.49it/s]', '\\rOverwrite epoch 1/4:  24%|##4       |\n83/345 [00:57<00:12, 20.38it/s]', '\\rOverwrite epoch 1/4:  25%|##4       |\n86/345 [00:57<00:12, 20.42it/s]', '\\rOverwrite epoch 1/4:  26%|##5       |\n89/345 [00:57<00:12, 20.62it/s]', '\\rOverwrite epoch 1/4:  27%|##6       |\n92/345 [00:57<00:12, 20.96it/s]', '\\rOverwrite epoch 1/4:  28%|##7       |\n95/345 [00:57<00:11, 21.17it/s]', '\\rOverwrite epoch 1/4:  28%|##8       |\n98/345 [00:57<00:11, 21.21it/s]', '\\rOverwrite epoch 1/4:  29%|##9       |\n101/345 [00:57<00:11, 21.02it/s]', '\\rOverwrite epoch 1/4:  30%|###       |\n104/345 [00:58<00:11, 20.97it/s]', '\\rOverwrite epoch 1/4:  31%|###1      |\n107/345 [00:58<00:11, 20.67it/s]', '\\rOverwrite epoch 1/4:  32%|###1      |\n110/345 [00:58<00:11, 20.57it/s]', '\\rOverwrite epoch 1/4:  33%|###2      |\n113/345 [00:58<00:11, 20.77it/s]', '\\rOverwrite epoch 1/4:  34%|###3      |\n116/345 [00:58<00:10, 21.08it/s]', '\\rOverwrite epoch 1/4:  34%|###4      |\n119/345 [00:58<00:10, 21.21it/s]', '\\rOverwrite epoch 1/4:  35%|###5      |\n122/345 [00:58<00:10, 21.25it/s]', '\\rOverwrite epoch 1/4:  36%|###6      |\n125/345 [00:59<00:10, 21.06it/s]', '\\rOverwrite epoch 1/4:  37%|###7      |\n128/345 [00:59<00:10, 21.23it/s]', '\\rOverwrite epoch 1/4:  38%|###7      |\n131/345 [00:59<00:10, 21.29it/s]', '\\rOverwrite epoch 1/4:  39%|###8      |\n134/345 [00:59<00:09, 21.39it/s]', '\\rOverwrite epoch 1/4:  40%|###9      |\n137/345 [00:59<00:09, 21.35it/s]', '\\rOverwrite epoch 1/4:  41%|####      |\n140/345 [00:59<00:09, 21.29it/s]', '\\rOverwrite epoch 1/4:  41%|####1     |\n143/345 [00:59<00:09, 21.13it/s]', '\\rOverwrite epoch 1/4:  42%|####2     |\n146/345 [01:00<00:09, 20.62it/s]', '\\rOverwrite epoch 1/4:  43%|####3     |\n149/345 [01:00<00:09, 20.78it/s]', '\\rOverwrite epoch 1/4:  44%|####4     |\n152/345 [01:00<00:09, 20.80it/s]', '\\rOverwrite epoch 1/4:  45%|####4     |\n155/345 [01:00<00:09, 20.79it/s]', '\\rOverwrite epoch 1/4:  46%|####5     |\n158/345 [01:00<00:08, 20.84it/s]', '\\rOverwrite epoch 1/4:  47%|####6     |\n161/345 [01:00<00:08, 21.02it/s]', '\\rOverwrite epoch 1/4:  48%|####7     |\n164/345 [01:00<00:08, 20.99it/s]', '\\rOverwrite epoch 1/4:  48%|####8     |\n167/345 [01:01<00:08, 21.10it/s]', '\\rOverwrite epoch 1/4:  49%|####9     |\n170/345 [01:01<00:08, 21.29it/s]', '\\rOverwrite epoch 1/4:  50%|#####     |\n173/345 [01:01<00:08, 21.42it/s]', '\\rOverwrite epoch 1/4:  51%|#####1    |\n176/345 [01:01<00:07, 21.35it/s]', '\\rOverwrite epoch 1/4:  52%|#####1    |\n179/345 [01:01<00:07, 21.36it/s]', '\\rOverwrite epoch 1/4:  53%|#####2    |\n182/345 [01:01<00:07, 20.90it/s]', '\\rOverwrite epoch 1/4:  54%|#####3    |\n185/345 [01:01<00:07, 20.82it/s]', '\\rOverwrite epoch 1/4:  54%|#####4    |\n188/345 [01:02<00:07, 20.77it/s]', '\\rOverwrite epoch 1/4:  55%|#####5    |\n191/345 [01:02<00:07, 21.04it/s]', '\\rOverwrite epoch 1/4:  56%|#####6    |\n194/345 [01:02<00:07, 21.23it/s]', '\\rOverwrite epoch 1/4:  57%|#####7    |\n197/345 [01:02<00:06, 21.29it/s]', '[2025-12-03 21:39:18] Overwrite step 200:\navg_train_loss=3.8327', '\\n', '\\rOverwrite epoch 1/4:  58%|#####7    | 200/345\n[01:02<00:06, 21.29it/s]', '\\rOverwrite epoch 1/4:  59%|#####8    | 203/345\n[01:02<00:06, 21.26it/s]', '\\rOverwrite epoch 1/4:  60%|#####9    | 206/345\n[01:02<00:06, 21.39it/s]', '\\rOverwrite epoch 1/4:  61%|######    | 209/345\n[01:03<00:06, 21.43it/s]', '\\rOverwrite epoch 1/4:  61%|######1   | 212/345\n[01:03<00:06, 21.44it/s]', '\\rOverwrite epoch 1/4:  62%|######2   | 215/345\n[01:03<00:06, 21.30it/s]', '\\rOverwrite epoch 1/4:  63%|######3   | 218/345\n[01:03<00:06, 20.64it/s]', '\\rOverwrite epoch 1/4:  64%|######4   | 221/345\n[01:03<00:05, 20.72it/s]', '\\rOverwrite epoch 1/4:  65%|######4   | 224/345\n[01:03<00:05, 20.44it/s]', '\\rOverwrite epoch 1/4:  66%|######5   | 227/345\n[01:03<00:05, 20.46it/s]', '\\rOverwrite epoch 1/4:  67%|######6   | 230/345\n[01:04<00:05, 20.40it/s]', '\\rOverwrite epoch 1/4:  68%|######7   | 233/345\n[01:04<00:05, 20.20it/s]', '\\rOverwrite epoch 1/4:  68%|######8   | 236/345\n[01:04<00:05, 20.18it/s]', '\\rOverwrite epoch 1/4:  69%|######9   | 239/345\n[01:04<00:05, 20.39it/s]', '\\rOverwrite epoch 1/4:  70%|#######   | 242/345\n[01:04<00:05, 20.58it/s]', '\\rOverwrite epoch 1/4:  71%|#######1  | 245/345\n[01:04<00:04, 20.81it/s]', '\\rOverwrite epoch 1/4:  72%|#######1  | 248/345\n[01:04<00:04, 21.04it/s]', '\\rOverwrite epoch 1/4:  73%|#######2  | 251/345\n[01:05<00:04, 21.20it/s]', '\\rOverwrite epoch 1/4:  74%|#######3  | 254/345\n[01:05<00:04, 20.99it/s]', '\\rOverwrite epoch 1/4:  74%|#######4  | 257/345\n[01:05<00:04, 20.44it/s]', '\\rOverwrite epoch 1/4:  75%|#######5  | 260/345\n[01:05<00:04, 20.47it/s]', '\\rOverwrite epoch 1/4:  76%|#######6  | 263/345\n[01:05<00:04, 20.10it/s]', '\\rOverwrite epoch 1/4:  77%|#######7  | 266/345\n[01:05<00:03, 20.28it/s]', '\\rOverwrite epoch 1/4:  78%|#######7  | 269/345\n[01:05<00:03, 19.99it/s]', '\\rOverwrite epoch 1/4:  79%|#######8  | 272/345\n[01:06<00:03, 19.96it/s]', '\\rOverwrite epoch 1/4:  80%|#######9  | 275/345\n[01:06<00:03, 20.03it/s]', '\\rOverwrite epoch 1/4:  81%|########  | 278/345\n[01:06<00:03, 20.00it/s]', '\\rOverwrite epoch 1/4:  81%|########1 | 281/345\n[01:06<00:03, 20.06it/s]', '\\rOverwrite epoch 1/4:  82%|########2 | 284/345\n[01:06<00:03, 20.14it/s]', '\\rOverwrite epoch 1/4:  83%|########3 | 287/345\n[01:06<00:02, 20.42it/s]', '\\rOverwrite epoch 1/4:  84%|########4 | 290/345\n[01:07<00:02, 20.77it/s]', '\\rOverwrite epoch 1/4:  85%|########4 | 293/345\n[01:07<00:02, 20.83it/s]', '\\rOverwrite epoch 1/4:  86%|########5 | 296/345\n[01:07<00:02, 20.83it/s]', '\\rOverwrite epoch 1/4:  87%|########6 | 299/345\n[01:07<00:02, 20.79it/s]', '\\rOverwrite epoch 1/4:  88%|########7 | 302/345\n[01:07<00:02, 20.79it/s]', '\\rOverwrite epoch 1/4:  88%|########8 | 305/345\n[01:07<00:01, 20.76it/s]', '\\rOverwrite epoch 1/4:  89%|########9 | 308/345\n[01:07<00:01, 20.62it/s]', '\\rOverwrite epoch 1/4:  90%|######### | 311/345\n[01:08<00:01, 20.66it/s]', '\\rOverwrite epoch 1/4:  91%|#########1| 314/345\n[01:08<00:01, 20.67it/s]', '\\rOverwrite epoch 1/4:  92%|#########1| 317/345\n[01:08<00:01, 20.97it/s]', '\\rOverwrite epoch 1/4:  93%|#########2| 320/345\n[01:08<00:01, 21.10it/s]', '\\rOverwrite epoch 1/4:  94%|#########3| 323/345\n[01:08<00:01, 21.22it/s]', '\\rOverwrite epoch 1/4:  94%|#########4| 326/345\n[01:08<00:00, 21.13it/s]', '\\rOverwrite epoch 1/4:  95%|#########5| 329/345\n[01:08<00:00, 20.82it/s]', '\\rOverwrite epoch 1/4:  96%|#########6| 332/345\n[01:09<00:00, 20.89it/s]', '\\rOverwrite epoch 1/4:  97%|#########7| 335/345\n[01:09<00:00, 20.89it/s]', '\\rOverwrite epoch 1/4:  98%|#########7| 338/345\n[01:09<00:00, 21.00it/s]', '\\rOverwrite epoch 1/4:  99%|#########8| 341/345\n[01:09<00:00, 21.15it/s]', '\\rOverwrite epoch 1/4: 100%|#########9| 344/345\n[01:09<00:00, 21.36it/s]', '', '\\rOverwrite epoch 1/4: 100%|##########| 345/345\n[01:10<00:00,  4.88it/s]', '\\n', 'Epoch 1: validation_loss = 3.6236', '\\n',\n'\\rOverwrite epoch 2/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 2/4:   0%|          | 1/345 [01:11<6:47:36, 71.09s/it]', '\\rOverwrite\nepoch 2/4:   1%|          | 3/345 [01:11<1:45:21, 18.48s/it]', '\\rOverwrite\nepoch 2/4:   2%|1         | 6/345 [01:11<40:31,  7.17s/it]  ', '\\rOverwrite\nepoch 2/4:   3%|2         | 9/345 [01:11<21:31,  3.84s/it]', '\\rOverwrite epoch\n2/4:   3%|3         | 12/345 [01:11<12:53,  2.32s/it]', '\\rOverwrite epoch 2/4:\n4%|4         | 15/345 [01:11<08:13,  1.50s/it]', '\\rOverwrite epoch 2/4:   5%|5\n| 18/345 [01:11<05:26,  1.00it/s]', '\\rOverwrite epoch 2/4:   6%|6         |\n21/345 [01:12<03:42,  1.46it/s]', '\\rOverwrite epoch 2/4:   7%|6         |\n24/345 [01:12<02:34,  2.07it/s]', '\\rOverwrite epoch 2/4:   8%|7         |\n27/345 [01:12<01:49,  2.89it/s]', '\\rOverwrite epoch 2/4:   9%|8         |\n30/345 [01:12<01:19,  3.95it/s]', '\\rOverwrite epoch 2/4:  10%|9         |\n33/345 [01:12<00:59,  5.26it/s]', '\\rOverwrite epoch 2/4:  10%|#         |\n36/345 [01:12<00:45,  6.84it/s]', '\\rOverwrite epoch 2/4:  11%|#1        |\n39/345 [01:12<00:35,  8.63it/s]', '\\rOverwrite epoch 2/4:  12%|#2        |\n42/345 [01:13<00:28, 10.56it/s]', '\\rOverwrite epoch 2/4:  13%|#3        |\n45/345 [01:13<00:23, 12.50it/s]', '\\rOverwrite epoch 2/4:  14%|#3        |\n48/345 [01:13<00:20, 14.35it/s]', '\\rOverwrite epoch 2/4:  15%|#4        |\n51/345 [01:13<00:18, 16.01it/s]', '\\rOverwrite epoch 2/4:  16%|#5        |\n54/345 [01:13<00:16, 17.44it/s]', '[2025-12-03 21:42:50] Overwrite step 400:\navg_train_loss=3.3332', '\\n', '\\rOverwrite epoch 2/4:  17%|#6        | 57/345\n[01:13<00:15, 18.56it/s]', '\\rOverwrite epoch 2/4:  17%|#7        | 60/345\n[01:13<00:14, 19.44it/s]', '\\rOverwrite epoch 2/4:  18%|#8        | 63/345\n[01:13<00:14, 20.11it/s]', '\\rOverwrite epoch 2/4:  19%|#9        | 66/345\n[01:14<00:13, 20.65it/s]', '\\rOverwrite epoch 2/4:  20%|##        | 69/345\n[01:14<00:13, 20.99it/s]', '\\rOverwrite epoch 2/4:  21%|##        | 72/345\n[01:14<00:12, 21.23it/s]', '\\rOverwrite epoch 2/4:  22%|##1       | 75/345\n[01:14<00:12, 21.39it/s]', '\\rOverwrite epoch 2/4:  23%|##2       | 78/345\n[01:14<00:12, 21.50it/s]', '\\rOverwrite epoch 2/4:  23%|##3       | 81/345\n[01:14<00:12, 21.56it/s]', '\\rOverwrite epoch 2/4:  24%|##4       | 84/345\n[01:14<00:12, 21.53it/s]', '\\rOverwrite epoch 2/4:  25%|##5       | 87/345\n[01:15<00:11, 21.56it/s]', '\\rOverwrite epoch 2/4:  26%|##6       | 90/345\n[01:15<00:11, 21.45it/s]', '\\rOverwrite epoch 2/4:  27%|##6       | 93/345\n[01:15<00:11, 21.52it/s]', '\\rOverwrite epoch 2/4:  28%|##7       | 96/345\n[01:15<00:11, 21.63it/s]', '\\rOverwrite epoch 2/4:  29%|##8       | 99/345\n[01:15<00:11, 21.62it/s]', '\\rOverwrite epoch 2/4:  30%|##9       | 102/345\n[01:15<00:11, 21.68it/s]', '\\rOverwrite epoch 2/4:  30%|###       | 105/345\n[01:15<00:11, 21.73it/s]', '\\rOverwrite epoch 2/4:  31%|###1      | 108/345\n[01:16<00:10, 21.80it/s]', '\\rOverwrite epoch 2/4:  32%|###2      | 111/345\n[01:16<00:10, 21.80it/s]', '\\rOverwrite epoch 2/4:  33%|###3      | 114/345\n[01:16<00:10, 21.80it/s]', '\\rOverwrite epoch 2/4:  34%|###3      | 117/345\n[01:16<00:10, 21.72it/s]', '\\rOverwrite epoch 2/4:  35%|###4      | 120/345\n[01:16<00:10, 21.45it/s]', '\\rOverwrite epoch 2/4:  36%|###5      | 123/345\n[01:16<00:10, 21.28it/s]', '\\rOverwrite epoch 2/4:  37%|###6      | 126/345\n[01:16<00:10, 21.30it/s]', '\\rOverwrite epoch 2/4:  37%|###7      | 129/345\n[01:17<00:10, 21.47it/s]', '\\rOverwrite epoch 2/4:  38%|###8      | 132/345\n[01:17<00:09, 21.59it/s]', '\\rOverwrite epoch 2/4:  39%|###9      | 135/345\n[01:17<00:09, 21.67it/s]', '\\rOverwrite epoch 2/4:  40%|####      | 138/345\n[01:17<00:09, 21.67it/s]', '\\rOverwrite epoch 2/4:  41%|####      | 141/345\n[01:17<00:09, 21.66it/s]', '\\rOverwrite epoch 2/4:  42%|####1     | 144/345\n[01:17<00:09, 21.75it/s]', '\\rOverwrite epoch 2/4:  43%|####2     | 147/345\n[01:17<00:09, 21.82it/s]', '\\rOverwrite epoch 2/4:  43%|####3     | 150/345\n[01:18<00:08, 21.89it/s]', '\\rOverwrite epoch 2/4:  44%|####4     | 153/345\n[01:18<00:08, 21.92it/s]', '\\rOverwrite epoch 2/4:  45%|####5     | 156/345\n[01:18<00:08, 21.87it/s]', '\\rOverwrite epoch 2/4:  46%|####6     | 159/345\n[01:18<00:08, 21.80it/s]', '\\rOverwrite epoch 2/4:  47%|####6     | 162/345\n[01:18<00:08, 21.34it/s]', '\\rOverwrite epoch 2/4:  48%|####7     | 165/345\n[01:18<00:08, 21.08it/s]', '\\rOverwrite epoch 2/4:  49%|####8     | 168/345\n[01:18<00:08, 21.14it/s]', '\\rOverwrite epoch 2/4:  50%|####9     | 171/345\n[01:18<00:08, 21.26it/s]', '\\rOverwrite epoch 2/4:  50%|#####     | 174/345\n[01:19<00:07, 21.40it/s]', '\\rOverwrite epoch 2/4:  51%|#####1    | 177/345\n[01:19<00:07, 21.49it/s]', '\\rOverwrite epoch 2/4:  52%|#####2    | 180/345\n[01:19<00:07, 21.57it/s]', '\\rOverwrite epoch 2/4:  53%|#####3    | 183/345\n[01:19<00:07, 21.54it/s]', '\\rOverwrite epoch 2/4:  54%|#####3    | 186/345\n[01:19<00:07, 21.54it/s]', '\\rOverwrite epoch 2/4:  55%|#####4    | 189/345\n[01:19<00:07, 21.61it/s]', '\\rOverwrite epoch 2/4:  56%|#####5    | 192/345\n[01:19<00:07, 21.69it/s]', '\\rOverwrite epoch 2/4:  57%|#####6    | 195/345\n[01:20<00:06, 21.73it/s]', '\\rOverwrite epoch 2/4:  57%|#####7    | 198/345\n[01:20<00:06, 21.79it/s]', '\\rOverwrite epoch 2/4:  58%|#####8    | 201/345\n[01:20<00:06, 21.80it/s]', '\\rOverwrite epoch 2/4:  59%|#####9    | 204/345\n[01:20<00:06, 21.81it/s]', '\\rOverwrite epoch 2/4:  60%|######    | 207/345\n[01:20<00:06, 21.31it/s]', '\\rOverwrite epoch 2/4:  61%|######    | 210/345\n[01:20<00:06, 21.37it/s]', '\\rOverwrite epoch 2/4:  62%|######1   | 213/345\n[01:20<00:06, 21.50it/s]', '\\rOverwrite epoch 2/4:  63%|######2   | 216/345\n[01:21<00:05, 21.60it/s]', '\\rOverwrite epoch 2/4:  63%|######3   | 219/345\n[01:21<00:05, 21.40it/s]', '\\rOverwrite epoch 2/4:  64%|######4   | 222/345\n[01:21<00:05, 21.33it/s]', '\\rOverwrite epoch 2/4:  65%|######5   | 225/345\n[01:21<00:05, 21.36it/s]', '\\rOverwrite epoch 2/4:  66%|######6   | 228/345\n[01:21<00:05, 21.50it/s]', '\\rOverwrite epoch 2/4:  67%|######6   | 231/345\n[01:21<00:05, 21.47it/s]', '\\rOverwrite epoch 2/4:  68%|######7   | 234/345\n[01:21<00:05, 21.24it/s]', '\\rOverwrite epoch 2/4:  69%|######8   | 237/345\n[01:22<00:05, 21.38it/s]', '\\rOverwrite epoch 2/4:  70%|######9   | 240/345\n[01:22<00:04, 21.45it/s]', '\\rOverwrite epoch 2/4:  70%|#######   | 243/345\n[01:22<00:04, 21.33it/s]', '\\rOverwrite epoch 2/4:  71%|#######1  | 246/345\n[01:22<00:04, 21.44it/s]', '\\rOverwrite epoch 2/4:  72%|#######2  | 249/345\n[01:22<00:04, 21.54it/s]', '\\rOverwrite epoch 2/4:  73%|#######3  | 252/345\n[01:22<00:04, 21.59it/s]', '[2025-12-03 21:42:59] Overwrite step 600:\navg_train_loss=3.3203', '\\n', '\\rOverwrite epoch 2/4:  74%|#######3  | 255/345\n[01:22<00:04, 21.35it/s]', '\\rOverwrite epoch 2/4:  75%|#######4  | 258/345\n[01:23<00:04, 21.06it/s]', '\\rOverwrite epoch 2/4:  76%|#######5  | 261/345\n[01:23<00:03, 21.08it/s]', '\\rOverwrite epoch 2/4:  77%|#######6  | 264/345\n[01:23<00:03, 21.25it/s]', '\\rOverwrite epoch 2/4:  77%|#######7  | 267/345\n[01:23<00:03, 21.42it/s]', '\\rOverwrite epoch 2/4:  78%|#######8  | 270/345\n[01:23<00:03, 21.52it/s]', '\\rOverwrite epoch 2/4:  79%|#######9  | 273/345\n[01:23<00:03, 21.13it/s]', '\\rOverwrite epoch 2/4:  80%|########  | 276/345\n[01:23<00:03, 21.35it/s]', '\\rOverwrite epoch 2/4:  81%|########  | 279/345\n[01:24<00:03, 21.36it/s]', '\\rOverwrite epoch 2/4:  82%|########1 | 282/345\n[01:24<00:02, 21.44it/s]', '\\rOverwrite epoch 2/4:  83%|########2 | 285/345\n[01:24<00:02, 21.35it/s]', '\\rOverwrite epoch 2/4:  83%|########3 | 288/345\n[01:24<00:02, 21.35it/s]', '\\rOverwrite epoch 2/4:  84%|########4 | 291/345\n[01:24<00:02, 21.26it/s]', '\\rOverwrite epoch 2/4:  85%|########5 | 294/345\n[01:24<00:02, 21.30it/s]', '\\rOverwrite epoch 2/4:  86%|########6 | 297/345\n[01:24<00:02, 21.47it/s]', '\\rOverwrite epoch 2/4:  87%|########6 | 300/345\n[01:25<00:02, 21.58it/s]', '\\rOverwrite epoch 2/4:  88%|########7 | 303/345\n[01:25<00:01, 21.67it/s]', '\\rOverwrite epoch 2/4:  89%|########8 | 306/345\n[01:25<00:01, 21.68it/s]', '\\rOverwrite epoch 2/4:  90%|########9 | 309/345\n[01:25<00:01, 21.73it/s]', '\\rOverwrite epoch 2/4:  90%|######### | 312/345\n[01:25<00:01, 21.47it/s]', '\\rOverwrite epoch 2/4:  91%|#########1| 315/345\n[01:25<00:01, 21.52it/s]', '\\rOverwrite epoch 2/4:  92%|#########2| 318/345\n[01:25<00:01, 21.58it/s]', '\\rOverwrite epoch 2/4:  93%|#########3| 321/345\n[01:25<00:01, 21.60it/s]', '\\rOverwrite epoch 2/4:  94%|#########3| 324/345\n[01:26<00:00, 21.62it/s]', '\\rOverwrite epoch 2/4:  95%|#########4| 327/345\n[01:26<00:00, 21.67it/s]', '\\rOverwrite epoch 2/4:  96%|#########5| 330/345\n[01:26<00:00, 21.69it/s]', '\\rOverwrite epoch 2/4:  97%|#########6| 333/345\n[01:26<00:00, 21.68it/s]', '\\rOverwrite epoch 2/4:  97%|#########7| 336/345\n[01:26<00:00, 21.65it/s]', '\\rOverwrite epoch 2/4:  98%|#########8| 339/345\n[01:26<00:00, 21.69it/s]', '\\rOverwrite epoch 2/4:  99%|#########9| 342/345\n[01:26<00:00, 21.66it/s]', '\\rOverwrite epoch 2/4: 100%|##########| 345/345\n[01:27<00:00, 22.65it/s]', '', '\\rOverwrite epoch 2/4: 100%|##########| 345/345\n[01:28<00:00,  3.92it/s]', '\\n', 'Epoch 2: validation_loss = 3.6392', '\\n',\n'\\rOverwrite epoch 3/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 3/4:   0%|          | 1/345 [00:50<4:47:00, 50.06s/it]', '\\rOverwrite\nepoch 3/4:   1%|          | 3/345 [00:50<1:14:16, 13.03s/it]', '\\rOverwrite\nepoch 3/4:   2%|1         | 6/345 [00:50<28:36,  5.06s/it]  ', '\\rOverwrite\nepoch 3/4:   3%|2         | 9/345 [00:50<15:14,  2.72s/it]', '\\rOverwrite epoch\n3/4:   3%|3         | 12/345 [00:50<09:09,  1.65s/it]', '\\rOverwrite epoch 3/4:\n4%|4         | 15/345 [00:50<05:52,  1.07s/it]', '\\rOverwrite epoch 3/4:   5%|5\n| 18/345 [00:50<03:54,  1.39it/s]', '\\rOverwrite epoch 3/4:   6%|6         |\n21/345 [00:51<02:41,  2.01it/s]', '\\rOverwrite epoch 3/4:   7%|6         |\n24/345 [00:51<01:53,  2.82it/s]', '\\rOverwrite epoch 3/4:   8%|7         |\n27/345 [00:51<01:22,  3.88it/s]', '\\rOverwrite epoch 3/4:   9%|8         |\n30/345 [00:51<01:00,  5.20it/s]', '\\rOverwrite epoch 3/4:  10%|9         |\n33/345 [00:51<00:46,  6.78it/s]', '\\rOverwrite epoch 3/4:  10%|#         |\n36/345 [00:51<00:36,  8.56it/s]', '\\rOverwrite epoch 3/4:  11%|#1        |\n39/345 [00:51<00:29, 10.42it/s]', '\\rOverwrite epoch 3/4:  12%|#2        |\n42/345 [00:52<00:24, 12.28it/s]', '\\rOverwrite epoch 3/4:  13%|#3        |\n45/345 [00:52<00:21, 14.01it/s]', '\\rOverwrite epoch 3/4:  14%|#3        |\n48/345 [00:52<00:19, 15.49it/s]', '\\rOverwrite epoch 3/4:  15%|#4        |\n51/345 [00:52<00:17, 16.88it/s]', '\\rOverwrite epoch 3/4:  16%|#5        |\n54/345 [00:52<00:16, 17.97it/s]', '\\rOverwrite epoch 3/4:  17%|#6        |\n57/345 [00:52<00:15, 18.87it/s]', '\\rOverwrite epoch 3/4:  17%|#7        |\n60/345 [00:52<00:14, 19.38it/s]', '\\rOverwrite epoch 3/4:  18%|#8        |\n63/345 [00:53<00:14, 19.81it/s]', '\\rOverwrite epoch 3/4:  19%|#9        |\n66/345 [00:53<00:13, 20.23it/s]', '\\rOverwrite epoch 3/4:  20%|##        |\n69/345 [00:53<00:13, 20.69it/s]', '\\rOverwrite epoch 3/4:  21%|##        |\n72/345 [00:53<00:12, 21.02it/s]', '\\rOverwrite epoch 3/4:  22%|##1       |\n75/345 [00:53<00:12, 21.05it/s]', '\\rOverwrite epoch 3/4:  23%|##2       |\n78/345 [00:53<00:12, 20.96it/s]', '\\rOverwrite epoch 3/4:  23%|##3       |\n81/345 [00:53<00:12, 21.15it/s]', '\\rOverwrite epoch 3/4:  24%|##4       |\n84/345 [00:54<00:12, 21.38it/s]', '\\rOverwrite epoch 3/4:  25%|##5       |\n87/345 [00:54<00:11, 21.51it/s]', '\\rOverwrite epoch 3/4:  26%|##6       |\n90/345 [00:54<00:11, 21.44it/s]', '\\rOverwrite epoch 3/4:  27%|##6       |\n93/345 [00:54<00:11, 21.54it/s]', '\\rOverwrite epoch 3/4:  28%|##7       |\n96/345 [00:54<00:11, 21.43it/s]', '\\rOverwrite epoch 3/4:  29%|##8       |\n99/345 [00:54<00:11, 21.52it/s]', '\\rOverwrite epoch 3/4:  30%|##9       |\n102/345 [00:54<00:11, 21.59it/s]', '\\rOverwrite epoch 3/4:  30%|###       |\n105/345 [00:54<00:11, 21.42it/s]', '\\rOverwrite epoch 3/4:  31%|###1      |\n108/345 [00:55<00:11, 21.19it/s]', '[2025-12-03 21:44:55] Overwrite step 800:\navg_train_loss=3.0696', '\\n', '\\rOverwrite epoch 3/4:  32%|###2      | 111/345\n[00:55<00:11, 21.25it/s]', '\\rOverwrite epoch 3/4:  33%|###3      | 114/345\n[00:55<00:10, 21.38it/s]', '\\rOverwrite epoch 3/4:  34%|###3      | 117/345\n[00:55<00:10, 21.41it/s]', '\\rOverwrite epoch 3/4:  35%|###4      | 120/345\n[00:55<00:10, 21.35it/s]', '\\rOverwrite epoch 3/4:  36%|###5      | 123/345\n[00:55<00:10, 21.30it/s]', '\\rOverwrite epoch 3/4:  37%|###6      | 126/345\n[00:55<00:10, 21.27it/s]', '\\rOverwrite epoch 3/4:  37%|###7      | 129/345\n[00:56<00:10, 21.10it/s]', '\\rOverwrite epoch 3/4:  38%|###8      | 132/345\n[00:56<00:09, 21.34it/s]', '\\rOverwrite epoch 3/4:  39%|###9      | 135/345\n[00:56<00:09, 21.41it/s]', '\\rOverwrite epoch 3/4:  40%|####      | 138/345\n[00:56<00:09, 21.45it/s]', '\\rOverwrite epoch 3/4:  41%|####      | 141/345\n[00:56<00:09, 21.48it/s]', '\\rOverwrite epoch 3/4:  42%|####1     | 144/345\n[00:56<00:09, 21.45it/s]', '\\rOverwrite epoch 3/4:  43%|####2     | 147/345\n[00:56<00:09, 21.56it/s]', '\\rOverwrite epoch 3/4:  43%|####3     | 150/345\n[00:57<00:09, 21.54it/s]', '\\rOverwrite epoch 3/4:  44%|####4     | 153/345\n[00:57<00:08, 21.49it/s]', '\\rOverwrite epoch 3/4:  45%|####5     | 156/345\n[00:57<00:08, 21.21it/s]', '\\rOverwrite epoch 3/4:  46%|####6     | 159/345\n[00:57<00:08, 21.42it/s]', '\\rOverwrite epoch 3/4:  47%|####6     | 162/345\n[00:57<00:08, 21.61it/s]', '\\rOverwrite epoch 3/4:  48%|####7     | 165/345\n[00:57<00:08, 21.53it/s]', '\\rOverwrite epoch 3/4:  49%|####8     | 168/345\n[00:57<00:08, 21.42it/s]', '\\rOverwrite epoch 3/4:  50%|####9     | 171/345\n[00:58<00:08, 21.00it/s]', '\\rOverwrite epoch 3/4:  50%|#####     | 174/345\n[00:58<00:08, 20.69it/s]', '\\rOverwrite epoch 3/4:  51%|#####1    | 177/345\n[00:58<00:08, 20.40it/s]', '\\rOverwrite epoch 3/4:  52%|#####2    | 180/345\n[00:58<00:08, 20.55it/s]', '\\rOverwrite epoch 3/4:  53%|#####3    | 183/345\n[00:58<00:07, 20.96it/s]', '\\rOverwrite epoch 3/4:  54%|#####3    | 186/345\n[00:58<00:07, 21.22it/s]', '\\rOverwrite epoch 3/4:  55%|#####4    | 189/345\n[00:58<00:07, 21.40it/s]', '\\rOverwrite epoch 3/4:  56%|#####5    | 192/345\n[00:59<00:07, 21.33it/s]', '\\rOverwrite epoch 3/4:  57%|#####6    | 195/345\n[00:59<00:06, 21.47it/s]', '\\rOverwrite epoch 3/4:  57%|#####7    | 198/345\n[00:59<00:06, 21.59it/s]', '\\rOverwrite epoch 3/4:  58%|#####8    | 201/345\n[00:59<00:06, 21.50it/s]', '\\rOverwrite epoch 3/4:  59%|#####9    | 204/345\n[00:59<00:06, 21.61it/s]', '\\rOverwrite epoch 3/4:  60%|######    | 207/345\n[00:59<00:06, 21.49it/s]', '\\rOverwrite epoch 3/4:  61%|######    | 210/345\n[00:59<00:06, 21.54it/s]', '\\rOverwrite epoch 3/4:  62%|######1   | 213/345\n[01:00<00:06, 21.40it/s]', '\\rOverwrite epoch 3/4:  63%|######2   | 216/345\n[01:00<00:06, 21.13it/s]', '\\rOverwrite epoch 3/4:  63%|######3   | 219/345\n[01:00<00:05, 21.32it/s]', '\\rOverwrite epoch 3/4:  64%|######4   | 222/345\n[01:00<00:05, 21.41it/s]', '\\rOverwrite epoch 3/4:  65%|######5   | 225/345\n[01:00<00:05, 21.43it/s]', '\\rOverwrite epoch 3/4:  66%|######6   | 228/345\n[01:00<00:05, 21.45it/s]', '\\rOverwrite epoch 3/4:  67%|######6   | 231/345\n[01:00<00:05, 21.50it/s]', '\\rOverwrite epoch 3/4:  68%|######7   | 234/345\n[01:01<00:05, 21.60it/s]', '\\rOverwrite epoch 3/4:  69%|######8   | 237/345\n[01:01<00:04, 21.67it/s]', '\\rOverwrite epoch 3/4:  70%|######9   | 240/345\n[01:01<00:04, 21.74it/s]', '\\rOverwrite epoch 3/4:  70%|#######   | 243/345\n[01:01<00:04, 21.75it/s]', '\\rOverwrite epoch 3/4:  71%|#######1  | 246/345\n[01:01<00:04, 21.65it/s]', '\\rOverwrite epoch 3/4:  72%|#######2  | 249/345\n[01:01<00:04, 21.68it/s]', '\\rOverwrite epoch 3/4:  73%|#######3  | 252/345\n[01:01<00:04, 21.65it/s]', '\\rOverwrite epoch 3/4:  74%|#######3  | 255/345\n[01:01<00:04, 21.62it/s]', '\\rOverwrite epoch 3/4:  75%|#######4  | 258/345\n[01:02<00:04, 21.62it/s]', '\\rOverwrite epoch 3/4:  76%|#######5  | 261/345\n[01:02<00:03, 21.68it/s]', '\\rOverwrite epoch 3/4:  77%|#######6  | 264/345\n[01:02<00:03, 21.69it/s]', '\\rOverwrite epoch 3/4:  77%|#######7  | 267/345\n[01:02<00:03, 21.78it/s]', '\\rOverwrite epoch 3/4:  78%|#######8  | 270/345\n[01:02<00:03, 21.83it/s]', '\\rOverwrite epoch 3/4:  79%|#######9  | 273/345\n[01:02<00:03, 21.85it/s]', '\\rOverwrite epoch 3/4:  80%|########  | 276/345\n[01:02<00:03, 21.83it/s]', '\\rOverwrite epoch 3/4:  81%|########  | 279/345\n[01:03<00:03, 21.74it/s]', '\\rOverwrite epoch 3/4:  82%|########1 | 282/345\n[01:03<00:02, 21.70it/s]', '\\rOverwrite epoch 3/4:  83%|########2 | 285/345\n[01:03<00:02, 21.70it/s]', '\\rOverwrite epoch 3/4:  83%|########3 | 288/345\n[01:03<00:02, 21.63it/s]', '\\rOverwrite epoch 3/4:  84%|########4 | 291/345\n[01:03<00:02, 21.63it/s]', '\\rOverwrite epoch 3/4:  85%|########5 | 294/345\n[01:03<00:02, 21.53it/s]', '\\rOverwrite epoch 3/4:  86%|########6 | 297/345\n[01:03<00:02, 21.62it/s]', '\\rOverwrite epoch 3/4:  87%|########6 | 300/345\n[01:04<00:02, 21.68it/s]', '\\rOverwrite epoch 3/4:  88%|########7 | 303/345\n[01:04<00:01, 21.74it/s]', '\\rOverwrite epoch 3/4:  89%|########8 | 306/345\n[01:04<00:01, 21.77it/s]', '\\rOverwrite epoch 3/4:  90%|########9 | 309/345\n[01:04<00:01, 21.83it/s]', '[2025-12-03 21:45:04] Overwrite step 1000:\navg_train_loss=3.0832', '\\n', '\\rOverwrite epoch 3/4:  90%|######### | 312/345\n[01:04<00:01, 21.84it/s]', '\\rOverwrite epoch 3/4:  91%|#########1| 315/345\n[01:04<00:01, 21.86it/s]', '\\rOverwrite epoch 3/4:  92%|#########2| 318/345\n[01:04<00:01, 21.86it/s]', '\\rOverwrite epoch 3/4:  93%|#########3| 321/345\n[01:05<00:01, 21.87it/s]', '\\rOverwrite epoch 3/4:  94%|#########3| 324/345\n[01:05<00:00, 21.83it/s]', '\\rOverwrite epoch 3/4:  95%|#########4| 327/345\n[01:05<00:00, 21.82it/s]', '\\rOverwrite epoch 3/4:  96%|#########5| 330/345\n[01:05<00:00, 21.76it/s]', '\\rOverwrite epoch 3/4:  97%|#########6| 333/345\n[01:05<00:00, 21.75it/s]', '\\rOverwrite epoch 3/4:  97%|#########7| 336/345\n[01:05<00:00, 21.79it/s]', '\\rOverwrite epoch 3/4:  98%|#########8| 339/345\n[01:05<00:00, 21.84it/s]', '\\rOverwrite epoch 3/4:  99%|#########9| 342/345\n[01:05<00:00, 21.76it/s]', '\\rOverwrite epoch 3/4: 100%|##########| 345/345\n[01:06<00:00, 22.61it/s]', '', '\\rOverwrite epoch 3/4: 100%|##########| 345/345\n[01:07<00:00,  5.14it/s]', '\\n', 'Epoch 3: validation_loss = 3.6716', '\\n',\n'\\rOverwrite epoch 4/4:   0%|          | 0/345 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 4/4:   0%|          | 1/345 [00:46<4:24:23, 46.12s/it]', '\\rOverwrite\nepoch 4/4:   1%|          | 3/345 [00:46<1:08:25, 12.00s/it]', '\\rOverwrite\nepoch 4/4:   2%|1         | 6/345 [00:46<26:22,  4.67s/it]  ', '\\rOverwrite\nepoch 4/4:   3%|2         | 9/345 [00:46<14:03,  2.51s/it]', '\\rOverwrite epoch\n4/4:   3%|3         | 12/345 [00:46<08:27,  1.52s/it]', '\\rOverwrite epoch 4/4:\n4%|4         | 15/345 [00:46<05:25,  1.01it/s]', '\\rOverwrite epoch 4/4:   5%|5\n| 18/345 [00:46<03:37,  1.50it/s]', '\\rOverwrite epoch 4/4:   6%|6         |\n21/345 [00:47<02:29,  2.17it/s]', '\\rOverwrite epoch 4/4:   7%|6         |\n24/345 [00:47<01:45,  3.04it/s]', '\\rOverwrite epoch 4/4:   8%|7         |\n27/345 [00:47<01:16,  4.16it/s]', '\\rOverwrite epoch 4/4:   9%|8         |\n30/345 [00:47<00:56,  5.54it/s]', '\\rOverwrite epoch 4/4:  10%|9         |\n33/345 [00:47<00:43,  7.18it/s]', '\\rOverwrite epoch 4/4:  10%|#         |\n36/345 [00:47<00:34,  9.02it/s]', '\\rOverwrite epoch 4/4:  11%|#1        |\n39/345 [00:47<00:27, 10.97it/s]', '\\rOverwrite epoch 4/4:  12%|#2        |\n42/345 [00:48<00:23, 12.90it/s]', '\\rOverwrite epoch 4/4:  13%|#3        |\n45/345 [00:48<00:20, 14.72it/s]', '\\rOverwrite epoch 4/4:  14%|#3        |\n48/345 [00:48<00:18, 16.31it/s]', '\\rOverwrite epoch 4/4:  15%|#4        |\n51/345 [00:48<00:16, 17.63it/s]', '\\rOverwrite epoch 4/4:  16%|#5        |\n54/345 [00:48<00:15, 18.65it/s]', '\\rOverwrite epoch 4/4:  17%|#6        |\n57/345 [00:48<00:14, 19.44it/s]', '\\rOverwrite epoch 4/4:  17%|#7        |\n60/345 [00:48<00:14, 20.10it/s]', '\\rOverwrite epoch 4/4:  18%|#8        |\n63/345 [00:49<00:13, 20.59it/s]', '\\rOverwrite epoch 4/4:  19%|#9        |\n66/345 [00:49<00:13, 20.95it/s]', '\\rOverwrite epoch 4/4:  20%|##        |\n69/345 [00:49<00:13, 21.22it/s]', '\\rOverwrite epoch 4/4:  21%|##        |\n72/345 [00:49<00:12, 21.32it/s]', '\\rOverwrite epoch 4/4:  22%|##1       |\n75/345 [00:49<00:12, 21.39it/s]', '\\rOverwrite epoch 4/4:  23%|##2       |\n78/345 [00:49<00:12, 21.52it/s]', '\\rOverwrite epoch 4/4:  23%|##3       |\n81/345 [00:49<00:12, 21.59it/s]', '\\rOverwrite epoch 4/4:  24%|##4       |\n84/345 [00:49<00:12, 21.60it/s]', '\\rOverwrite epoch 4/4:  25%|##5       |\n87/345 [00:50<00:11, 21.66it/s]', '\\rOverwrite epoch 4/4:  26%|##6       |\n90/345 [00:50<00:11, 21.68it/s]', '\\rOverwrite epoch 4/4:  27%|##6       |\n93/345 [00:50<00:11, 21.73it/s]', '\\rOverwrite epoch 4/4:  28%|##7       |\n96/345 [00:50<00:11, 21.72it/s]', '\\rOverwrite epoch 4/4:  29%|##8       |\n99/345 [00:50<00:11, 21.74it/s]', '\\rOverwrite epoch 4/4:  30%|##9       |\n102/345 [00:50<00:11, 21.67it/s]', '\\rOverwrite epoch 4/4:  30%|###       |\n105/345 [00:50<00:11, 21.69it/s]', '\\rOverwrite epoch 4/4:  31%|###1      |\n108/345 [00:51<00:10, 21.73it/s]', '\\rOverwrite epoch 4/4:  32%|###2      |\n111/345 [00:51<00:10, 21.77it/s]', '\\rOverwrite epoch 4/4:  33%|###3      |\n114/345 [00:51<00:10, 21.70it/s]', '\\rOverwrite epoch 4/4:  34%|###3      |\n117/345 [00:51<00:10, 21.70it/s]', '\\rOverwrite epoch 4/4:  35%|###4      |\n120/345 [00:51<00:10, 21.68it/s]', '\\rOverwrite epoch 4/4:  36%|###5      |\n123/345 [00:51<00:10, 21.63it/s]', '\\rOverwrite epoch 4/4:  37%|###6      |\n126/345 [00:51<00:10, 21.66it/s]', '\\rOverwrite epoch 4/4:  37%|###7      |\n129/345 [00:52<00:10, 21.55it/s]', '\\rOverwrite epoch 4/4:  38%|###8      |\n132/345 [00:52<00:09, 21.53it/s]', '\\rOverwrite epoch 4/4:  39%|###9      |\n135/345 [00:52<00:09, 21.53it/s]', '\\rOverwrite epoch 4/4:  40%|####      |\n138/345 [00:52<00:09, 21.56it/s]', '\\rOverwrite epoch 4/4:  41%|####      |\n141/345 [00:52<00:09, 21.61it/s]', '\\rOverwrite epoch 4/4:  42%|####1     |\n144/345 [00:52<00:09, 21.68it/s]', '\\rOverwrite epoch 4/4:  43%|####2     |\n147/345 [00:52<00:09, 21.68it/s]', '\\rOverwrite epoch 4/4:  43%|####3     |\n150/345 [00:53<00:08, 21.71it/s]', '\\rOverwrite epoch 4/4:  44%|####4     |\n153/345 [00:53<00:08, 21.71it/s]', '\\rOverwrite epoch 4/4:  45%|####5     |\n156/345 [00:53<00:08, 21.75it/s]', '\\rOverwrite epoch 4/4:  46%|####6     |\n159/345 [00:53<00:08, 21.77it/s]', '\\rOverwrite epoch 4/4:  47%|####6     |\n162/345 [00:53<00:08, 21.59it/s]', '[2025-12-03 21:46:50] Overwrite step 1200:\navg_train_loss=2.8707', '\\n', '\\rOverwrite epoch 4/4:  48%|####7     | 165/345\n[00:53<00:08, 21.68it/s]', '\\rOverwrite epoch 4/4:  49%|####8     | 168/345\n[00:53<00:08, 21.76it/s]', '\\rOverwrite epoch 4/4:  50%|####9     | 171/345\n[00:53<00:08, 21.56it/s]', '\\rOverwrite epoch 4/4:  50%|#####     | 174/345\n[00:54<00:07, 21.70it/s]', '\\rOverwrite epoch 4/4:  51%|#####1    | 177/345\n[00:54<00:07, 21.77it/s]', '\\rOverwrite epoch 4/4:  52%|#####2    | 180/345\n[00:54<00:07, 21.64it/s]', '\\rOverwrite epoch 4/4:  53%|#####3    | 183/345\n[00:54<00:07, 21.75it/s]', '\\rOverwrite epoch 4/4:  54%|#####3    | 186/345\n[00:54<00:07, 21.79it/s]', '\\rOverwrite epoch 4/4:  55%|#####4    | 189/345\n[00:54<00:07, 21.83it/s]', '\\rOverwrite epoch 4/4:  56%|#####5    | 192/345\n[00:54<00:07, 21.84it/s]', '\\rOverwrite epoch 4/4:  57%|#####6    | 195/345\n[00:55<00:06, 21.79it/s]', '\\rOverwrite epoch 4/4:  57%|#####7    | 198/345\n[00:55<00:06, 21.55it/s]', '\\rOverwrite epoch 4/4:  58%|#####8    | 201/345\n[00:55<00:06, 21.43it/s]', '\\rOverwrite epoch 4/4:  59%|#####9    | 204/345\n[00:55<00:06, 21.46it/s]', '\\rOverwrite epoch 4/4:  60%|######    | 207/345\n[00:55<00:06, 21.51it/s]', '\\rOverwrite epoch 4/4:  61%|######    | 210/345\n[00:55<00:06, 21.06it/s]', '\\rOverwrite epoch 4/4:  62%|######1   | 213/345\n[00:55<00:06, 21.18it/s]', '\\rOverwrite epoch 4/4:  63%|######2   | 216/345\n[00:56<00:06, 21.35it/s]', '\\rOverwrite epoch 4/4:  63%|######3   | 219/345\n[00:56<00:05, 21.45it/s]', '\\rOverwrite epoch 4/4:  64%|######4   | 222/345\n[00:56<00:05, 21.41it/s]', '\\rOverwrite epoch 4/4:  65%|######5   | 225/345\n[00:56<00:05, 21.37it/s]', '\\rOverwrite epoch 4/4:  66%|######6   | 228/345\n[00:56<00:05, 21.34it/s]', '\\rOverwrite epoch 4/4:  67%|######6   | 231/345\n[00:56<00:05, 21.43it/s]', '\\rOverwrite epoch 4/4:  68%|######7   | 234/345\n[00:56<00:05, 21.49it/s]', '\\rOverwrite epoch 4/4:  69%|######8   | 237/345\n[00:57<00:05, 21.56it/s]', '\\rOverwrite epoch 4/4:  70%|######9   | 240/345\n[00:57<00:04, 21.58it/s]', '\\rOverwrite epoch 4/4:  70%|#######   | 243/345\n[00:57<00:04, 21.35it/s]', '\\rOverwrite epoch 4/4:  71%|#######1  | 246/345\n[00:57<00:04, 21.50it/s]', '\\rOverwrite epoch 4/4:  72%|#######2  | 249/345\n[00:57<00:04, 21.54it/s]', '\\rOverwrite epoch 4/4:  73%|#######3  | 252/345\n[00:57<00:04, 21.65it/s]', '\\rOverwrite epoch 4/4:  74%|#######3  | 255/345\n[00:57<00:04, 21.67it/s]', '\\rOverwrite epoch 4/4:  75%|#######4  | 258/345\n[00:58<00:04, 21.68it/s]', '\\rOverwrite epoch 4/4:  76%|#######5  | 261/345\n[00:58<00:03, 21.70it/s]', '\\rOverwrite epoch 4/4:  77%|#######6  | 264/345\n[00:58<00:03, 21.68it/s]', '\\rOverwrite epoch 4/4:  77%|#######7  | 267/345\n[00:58<00:03, 21.64it/s]', '\\rOverwrite epoch 4/4:  78%|#######8  | 270/345\n[00:58<00:03, 21.45it/s]', '\\rOverwrite epoch 4/4:  79%|#######9  | 273/345\n[00:58<00:03, 21.51it/s]', '\\rOverwrite epoch 4/4:  80%|########  | 276/345\n[00:58<00:03, 21.63it/s]', '\\rOverwrite epoch 4/4:  81%|########  | 279/345\n[00:59<00:03, 21.72it/s]', '\\rOverwrite epoch 4/4:  82%|########1 | 282/345\n[00:59<00:02, 21.78it/s]', '\\rOverwrite epoch 4/4:  83%|########2 | 285/345\n[00:59<00:02, 21.80it/s]', '\\rOverwrite epoch 4/4:  83%|########3 | 288/345\n[00:59<00:02, 21.83it/s]', '\\rOverwrite epoch 4/4:  84%|########4 | 291/345\n[00:59<00:02, 21.78it/s]', '\\rOverwrite epoch 4/4:  85%|########5 | 294/345\n[00:59<00:02, 21.83it/s]', '\\rOverwrite epoch 4/4:  86%|########6 | 297/345\n[00:59<00:02, 21.81it/s]', '\\rOverwrite epoch 4/4:  87%|########6 | 300/345\n[00:59<00:02, 21.77it/s]', '\\rOverwrite epoch 4/4:  88%|########7 | 303/345\n[01:00<00:01, 21.80it/s]', '\\rOverwrite epoch 4/4:  89%|########8 | 306/345\n[01:00<00:01, 21.77it/s]', '\\rOverwrite epoch 4/4:  90%|########9 | 309/345\n[01:00<00:01, 21.75it/s]', '\\rOverwrite epoch 4/4:  90%|######### | 312/345\n[01:00<00:01, 21.73it/s]', '\\rOverwrite epoch 4/4:  91%|#########1| 315/345\n[01:00<00:01, 21.74it/s]', '\\rOverwrite epoch 4/4:  92%|#########2| 318/345\n[01:00<00:01, 21.78it/s]', '\\rOverwrite epoch 4/4:  93%|#########3| 321/345\n[01:00<00:01, 21.81it/s]', '\\rOverwrite epoch 4/4:  94%|#########3| 324/345\n[01:01<00:00, 21.82it/s]', '\\rOverwrite epoch 4/4:  95%|#########4| 327/345\n[01:01<00:00, 21.84it/s]', '\\rOverwrite epoch 4/4:  96%|#########5| 330/345\n[01:01<00:00, 21.85it/s]', '\\rOverwrite epoch 4/4:  97%|#########6| 333/345\n[01:01<00:00, 21.78it/s]', '\\rOverwrite epoch 4/4:  97%|#########7| 336/345\n[01:01<00:00, 21.69it/s]', '\\rOverwrite epoch 4/4:  98%|#########8| 339/345\n[01:01<00:00, 21.33it/s]', '\\rOverwrite epoch 4/4:  99%|#########9| 342/345\n[01:01<00:00, 21.08it/s]', '\\rOverwrite epoch 4/4: 100%|##########| 345/345\n[01:02<00:00, 21.79it/s]', '', '\\rOverwrite epoch 4/4: 100%|##########| 345/345\n[01:02<00:00,  5.48it/s]', '\\n', 'Epoch 4: validation_loss = 3.7202', '\\n',\n'Experiment complete. Artifacts saved to:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-4/working',\n'\\n', 'Execution time: 12 minutes seconds (time limit is 2 hours).']"], "analysis": ["", "", "Run failed with UnboundLocalError: anchor_iter referenced before assignment\ninside overwrite_phase. Cause: assigning to anchor_iter within the function (on\nStopIteration) makes it a local variable; Python marks it local for the entire\nfunction scope, so the first next(anchor_iter) tries to read an uninitialized\nlocal. Fix: keep the anchor iterator local to overwrite_phase and initialize it\nbefore the training loop, then reinitialize it on exhaustion (or use\nitertools.cycle). Example fix:  # at top of overwrite_phase, before the training\nloop collator_anch = DataCollatorForLanguageModeling(tokenizer, mlm=False)\nanchor_loader_local = DataLoader(anchor_ds, batch_size=32, shuffle=True,\ncollate_fn=collator_anch) anchor_iter = iter(anchor_loader_local)  # inside the\ntraining loop try:     anchor_batch = next(anchor_iter) except StopIteration:\nanchor_iter = iter(anchor_loader_local)     anchor_batch = next(anchor_iter)\nAlternatively: from itertools import cycle anchor_cycle =\ncycle(anchor_loader_local) ... anchor_batch = next(anchor_cycle)  Minor issues\nto consider (non-blocking): - max_train_pct is not actually applied in\noverwrite_phase; if desired, subsample with\nselect(range(int(max_train_pct*len(raw_train)))) on raw_train. - The duplicate\nremove_columns calls around tokenization can be simplified.", "", "", ""], "exc_type": [null, null, "UnboundLocalError", null, null, null], "exc_info": [null, null, {"args": ["cannot access local variable 'anchor_iter' where it is not associated with a value"], "name": "None"}, null, null, null], "exc_stack": [null, null, [["/workspace/AE-Scientist/research_pipeline/ai_scientist/treesearch/interpreter.py", 264, "_repl_run_session", "exec(compile(code, agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 677, "<module>", "overwrite_phase("], ["runfile.py", 494, "overwrite_phase", "anchor_batch = next(anchor_iter)"]], null, null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train average loss", "lower_is_better": true, "description": "Average training loss over the training set.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.085596, "best_value": 3.085596}, {"dataset_name": "overwrite_wikitext", "final_value": 2.876351, "best_value": 2.876351}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss measured on the validation set.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.226789, "best_value": 3.226789}, {"dataset_name": "overwrite_wikitext", "final_value": 3.623615, "best_value": 3.623615}]}, {"metric_name": "RCRG@50", "lower_is_better": false, "description": "RCRG metric at cutoff 50; higher indicates better retrieval/recall performance.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "common_recall@50", "lower_is_better": false, "description": "Recall at 50 for common items.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.2, "best_value": 0.2}]}, {"metric_name": "rare_recall@50", "lower_is_better": false, "description": "Recall at 50 for rare items.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train average loss", "lower_is_better": true, "description": "Average training loss over the training data.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.027933, "best_value": 3.027933}, {"dataset_name": "overwrite_wikitext", "final_value": 3.367259, "best_value": 3.367259}, {"dataset_name": "overwrite_ag_news", "final_value": 3.215548, "best_value": 3.215548}, {"dataset_name": "overwrite_imdb", "final_value": 3.670691, "best_value": 3.670691}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss computed on the validation set.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.390247, "best_value": 3.390247}, {"dataset_name": "overwrite_wikitext", "final_value": 3.665937, "best_value": 3.665937}, {"dataset_name": "overwrite_ag_news", "final_value": 3.169449, "best_value": 3.169449}, {"dataset_name": "overwrite_imdb", "final_value": 3.691671, "best_value": 3.691671}]}, {"metric_name": "RCRG@50", "lower_is_better": false, "description": "RCRG evaluated at cutoff 50.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": -0.4, "best_value": -0.4}, {"dataset_name": "overwrite_ag_news", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "rare recall@50", "lower_is_better": false, "description": "Recall for rare items at top-50.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_ag_news", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "common recall@50", "lower_is_better": false, "description": "Recall for common items at top-50.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.6, "best_value": 0.6}, {"dataset_name": "overwrite_ag_news", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "value", "lower_is_better": true, "description": "", "data": [{"dataset_name": "default", "final_value": null, "best_value": null}]}]}, {"metric_names": [{"metric_name": "train loss", "lower_is_better": true, "description": "Training loss at the end of training.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.085596, "best_value": 3.085596}, {"dataset_name": "overwrite_wikitext", "final_value": 3.221727, "best_value": 3.221727}, {"dataset_name": "overwrite_agnews", "final_value": 3.072489, "best_value": 3.072489}, {"dataset_name": "overwrite_imdb", "final_value": 3.691378, "best_value": 3.691378}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss at the end of training.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.226789, "best_value": 3.226789}, {"dataset_name": "overwrite_wikitext", "final_value": 3.706039, "best_value": 3.706039}, {"dataset_name": "overwrite_agnews", "final_value": 3.323182, "best_value": 3.323182}, {"dataset_name": "overwrite_imdb", "final_value": 3.692236, "best_value": 3.692236}]}, {"metric_name": "RCRG@50", "lower_is_better": false, "description": "RCRG measured at cutoff 50.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": -0.2, "best_value": -0.2}, {"dataset_name": "overwrite_agnews", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "common_recall@50", "lower_is_better": false, "description": "Recall on common items at cutoff 50.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.2, "best_value": 0.2}, {"dataset_name": "overwrite_agnews", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "rare_recall@50", "lower_is_better": false, "description": "Recall on rare items at cutoff 50.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_agnews", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "PHR", "lower_is_better": false, "description": "Hit rate (PHR) at the evaluated cutoff.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_agnews", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train average loss", "lower_is_better": true, "description": "Average training loss at the best checkpoint; lower indicates better fit.", "data": [{"dataset_name": "synthetic_injection", "final_value": 0.0, "best_value": 3.085596}, {"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 3.866405}, {"dataset_name": "overwrite_ag_news", "final_value": 0.0, "best_value": 3.88239}, {"dataset_name": "overwrite_imdb", "final_value": 0.0, "best_value": 4.229976}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Validation loss at the best checkpoint; lower indicates better generalization.", "data": [{"dataset_name": "synthetic_injection", "final_value": 0.0, "best_value": 3.226789}, {"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 3.703722}, {"dataset_name": "overwrite_ag_news", "final_value": 0.0, "best_value": 3.255014}, {"dataset_name": "overwrite_imdb", "final_value": 0.0, "best_value": 3.722343}]}, {"metric_name": "RCRG@50", "lower_is_better": false, "description": "Recall-like metric at cutoff 50; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 1.0, "best_value": 0.0}, {"dataset_name": "overwrite_ag_news", "final_value": 1.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 1.0, "best_value": 0.0}]}, {"metric_name": "rare_recall@50", "lower_is_better": false, "description": "Recall for rare items at cutoff 50; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 1.0, "best_value": 0.0}, {"dataset_name": "overwrite_ag_news", "final_value": 1.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 1.0, "best_value": 0.0}]}, {"metric_name": "common_recall@50", "lower_is_better": false, "description": "Recall for common items at cutoff 50; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_ag_news", "final_value": 0.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "rare_censored_pct", "lower_is_better": true, "description": "Percentage of rare items censored; lower is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 100.0, "best_value": 0.0}, {"dataset_name": "overwrite_ag_news", "final_value": 100.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 100.0, "best_value": 0.0}]}, {"metric_name": "common_censored_pct", "lower_is_better": true, "description": "Percentage of common items censored; lower is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 100.0, "best_value": 0.0}, {"dataset_name": "overwrite_ag_news", "final_value": 100.0, "best_value": 0.0}, {"dataset_name": "overwrite_imdb", "final_value": 100.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train average loss", "lower_is_better": true, "description": "Average training loss over the training set; lower values indicate better fit.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.085596, "best_value": 0.0}, {"dataset_name": "overwrite_wikitext", "final_value": 2.876351, "best_value": 0.0}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set; lower values indicate better generalization.", "data": [{"dataset_name": "synthetic_injection", "final_value": 0.0, "best_value": 3.226789}, {"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 3.623615}]}, {"metric_name": "RCRG@50", "lower_is_better": false, "description": "Retrieval/recall metric at top-50; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "common_recall@50", "lower_is_better": false, "description": "Recall at top-50 for common items; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.2}]}, {"metric_name": "rare_recall@50", "lower_is_better": false, "description": "Recall at top-50 for rare items; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}]}], "is_best_node": [false, false, false, true, false, false], "plots": [[], ["../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_phr_over_checkpoints.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_recalls_over_checkpoints.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_recall_metrics_over_epochs.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_phr_over_checkpoints.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_recalls_over_checkpoints.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_recall_metrics_over_epochs.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_phr_over_checkpoints.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_recalls_over_checkpoints.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_recall_metrics_over_epochs.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/synthetic_injection_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/synthetic_injection_baseline_recalls_common.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/synthetic_injection_baseline_recalls_rare.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/phr_all_checkpoints.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/recalls_all_checkpoints.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/embedding_retention_common.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/embedding_retention_rare.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_phr_over_epochs.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_recalls_over_epochs.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_common_post_overwrite_imdb.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_rare_post_overwrite_imdb.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_phr_over_epochs.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_recalls_over_epochs.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_common_post_overwrite_ag_news.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_rare_post_overwrite_ag_news.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_phr_over_epochs.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_recalls_over_epochs.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_common_post_overwrite_wikitext.png", "../../logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_rare_post_overwrite_wikitext.png"], [], ["../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/phr_summary_overwrite_datasets.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/synthetic_injection_baseline_recall_vs_k.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_imdb_generated_hit_counts.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_ag_news_generated_hit_counts.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_wikitext2_generated_hit_counts.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_imdb_rcrg_recall50.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_ag_news_rcrg_recall50.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_wikitext2_rcrg_recall50.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_imdb_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_ag_news_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_wikitext2_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/synthetic_injection_synthetic_injection_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/embedding_retention_common_final.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/embedding_retention_rare_final.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_gen_counts_common.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_gen_counts_rare.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_rcrg_recalls.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_gen_counts_common.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_gen_counts_rare.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_rcrg_recalls.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_gen_counts_common.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_gen_counts_rare.png", "../../logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_rcrg_recalls.png"], ["../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_imdb_aux_recall_curves.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_imdb_val_metrics.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_imdb_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_ag_news_aux_recall_curves.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_ag_news_val_metrics.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_ag_news_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_wikitext_aux_recall_curves.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_wikitext_val_metrics.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_wikitext_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/synthetic_injection_train_val_loss.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_imdb_rcrg_over_epochs.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_ag_news_rcrg_over_epochs.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_wikitext_rcrg_over_epochs.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/gen_counts_common_post.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/gen_counts_rare_post.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/embedding_retention_common.png", "../../logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/embedding_retention_rare.png"], ["../../logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/rcrg_over_epochs.png", "../../logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/gen_counts_common_post.png", "../../logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/gen_counts_rare_post.png", "../../logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/embedding_retention_common.png", "../../logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/embedding_retention_rare.png"]], "plot_paths": [[], ["workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_phr_over_checkpoints.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_recalls_over_checkpoints.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_recall_metrics_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_phr_over_checkpoints.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_recalls_over_checkpoints.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_recall_metrics_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_phr_over_checkpoints.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_recalls_over_checkpoints.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_recall_metrics_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/synthetic_injection_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/synthetic_injection_baseline_recalls_common.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/synthetic_injection_baseline_recalls_rare.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/phr_all_checkpoints.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/recalls_all_checkpoints.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/embedding_retention_common.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/embedding_retention_rare.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_phr_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_recalls_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_common_post_overwrite_imdb.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_rare_post_overwrite_imdb.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_phr_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_recalls_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_common_post_overwrite_ag_news.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_rare_post_overwrite_ag_news.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_phr_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_recalls_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_common_post_overwrite_wikitext.png", "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/gen_counts_rare_post_overwrite_wikitext.png"], [], ["workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/phr_summary_overwrite_datasets.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/synthetic_injection_baseline_recall_vs_k.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_imdb_generated_hit_counts.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_ag_news_generated_hit_counts.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_wikitext2_generated_hit_counts.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_imdb_rcrg_recall50.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_ag_news_rcrg_recall50.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_wikitext2_rcrg_recall50.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_imdb_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_ag_news_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_wikitext2_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/synthetic_injection_synthetic_injection_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/embedding_retention_common_final.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/embedding_retention_rare_final.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_gen_counts_common.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_gen_counts_rare.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_rcrg_recalls.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_gen_counts_common.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_gen_counts_rare.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_rcrg_recalls.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_gen_counts_common.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_gen_counts_rare.png", "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_rcrg_recalls.png"], ["workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_imdb_aux_recall_curves.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_imdb_val_metrics.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_imdb_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_ag_news_aux_recall_curves.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_ag_news_val_metrics.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_ag_news_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_wikitext_aux_recall_curves.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_wikitext_val_metrics.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_wikitext_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/synthetic_injection_train_val_loss.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_imdb_rcrg_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_ag_news_rcrg_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_wikitext_rcrg_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/gen_counts_common_post.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/gen_counts_rare_post.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/embedding_retention_common.png", "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/embedding_retention_rare.png"], ["workspaces/logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/rcrg_over_epochs.png", "workspaces/logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/gen_counts_common_post.png", "workspaces/logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/gen_counts_rare_post.png", "workspaces/logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/embedding_retention_common.png", "workspaces/logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/embedding_retention_rare.png"]], "plot_analyses": [[], [{"analysis": "Recall metrics across overwrite checkpoints show Common recall@50 decaying from a clearly positive value to ~0 by later checkpoints, while RCRG@50 rises from a negative value toward ~0. Rare recall@50 remains at 0 throughout. This suggests the overwrite procedure progressively neutralizes recall signals rather than enhancing rare-token recall, indicating potential forgetting or a shift toward uniformly lower recall without targeted gains for rare items.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_phr_over_checkpoints.png"}, {"analysis": "PHR across overwrite checkpoints is flat at 0 for both the lower-bound and the estimate, with an odd x-axis range, indicating no detected paraphrase hits and/or a plotting/evaluation artifact. This likely reflects either absence of paraphrase overlap, overly strict thresholds, or an evaluation pipeline issue.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_recalls_over_checkpoints.png"}, {"analysis": "Embedding Retention (Cosine) for common tokens is ~1.0 across all shown indices, implying no measurable change to these embeddings. Any behavioral changes in recall are therefore unlikely to be caused by drift in these token embeddings.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_recall_metrics_over_epochs.png"}, {"analysis": "Embedding Retention (Cosine) for rare tokens is also ~1.0 across all shown indices, suggesting rare-token embeddings are likewise unchanged. This points to changes occurring in other parts of the model (e.g., higher layers, output head) or from evaluation artifacts rather than embedding alteration.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_phr_over_checkpoints.png"}, {"analysis": "Validation recall metrics over epochs for overwrite_imdb are all at 0 (RCRG@50, Rare recall@50, Common recall@50), indicating no measurable signal. This could be due to dataset-target mismatch (targets not present), insufficient sample size, tokenization/case issues, or a metric computation failure.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_recalls_over_checkpoints.png"}, {"analysis": "PHR over epochs for overwrite_imdb remains at 0 for both curves, reinforcing that paraphrase matches were not detected. This aligns with the recall metrics showing no signal and suggests revisiting paraphrase sets and matching criteria.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_ag_news_recall_metrics_over_epochs.png"}, {"analysis": "Validation recall metrics over epochs for overwrite_ag_news are all at 0, mirroring the imdb behavior. Again, this points to potential dataset coverage issues for the target lexicon or evaluation pipeline problems (e.g., filtering, thresholds, k, or generation settings).", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_phr_over_checkpoints.png"}, {"analysis": "PHR over epochs for overwrite_ag_news is flat at 0, consistent with the absence of recall signal and suggesting the paraphrase evaluation is not capturing matches under current settings.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_recalls_over_checkpoints.png"}, {"analysis": "Validation recall metrics over epochs for overwrite_wikitext show a non-zero dynamic: Common recall@50 declines over epochs, RCRG@50 moves upward (less negative), and Rare recall@50 stays at 0. The training appears to reduce common-token recall while partially alleviating the negative RCRG signal, yet it does not produce gains for rare tokens. This suggests the overwrite is dampening recall overall without achieving the intended rare-token improvements.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_wikitext_recall_metrics_over_epochs.png"}, {"analysis": "PHR over epochs for overwrite_wikitext remains at 0, indicating no paraphrase detection despite observable changes in recall metrics. This further points to either limited paraphrase coverage or matching criteria that are too strict.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_216c7cda026741659cb0ff690201d244_proc_24127/overwrite_imdb_train_val_loss.png"}], [], [{"analysis": "PHR Summary across Overwrite Datasets: The figure appears empty (no plotted values). This is non-diagnostic; cannot infer rare-to-common head loss ratios. Recommend re-generating or checking data export.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/phr_summary_overwrite_datasets.png"}, {"analysis": "RCRG and Recalls \u2014 overwrite_imdb: At epoch 1, common recall@50 \u2248 0.2, rare recall@50 = 0, yielding negative RCRG (~\u22120.2). By epoch 2, all three metrics collapse to ~0. This indicates no measurable improvement for rare tokens and a regression for common tokens, suggesting the method did not enhance retrieval and may have degraded it.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/synthetic_injection_baseline_recall_vs_k.png"}, {"analysis": "RCRG and Recalls \u2014 overwrite_agnews: All metrics (RCRG@50, rare recall@50, common recall@50) remain at 0 across epochs. The procedure produces no detectable retrieval signal for either token group.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_imdb_generated_hit_counts.png"}, {"analysis": "RCRG and Recalls \u2014 overwrite_wikitext: Common recall@50 is stable around 0.2; rare recall@50 stays at 0; RCRG@50 remains negative (~\u22120.2) across epochs. This reflects a persistent gap favoring common tokens with no training-time progress.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_ag_news_generated_hit_counts.png"}, {"analysis": "Embedding Retention (Cosine) \u2014 Common Tokens (final): Cosine similarity is 1.0 for all common tokens, implying embeddings are unchanged. This indicates either the embedding layer was frozen or gradients/updates did not reach it.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_wikitext2_generated_hit_counts.png"}, {"analysis": "Embedding Retention (Cosine) \u2014 Rare Tokens (final): Cosine similarity is 1.0 for all rare tokens as well. There is no evidence of overwrite or adaptation in rare token embeddings, aligning with the flat recall metrics.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_imdb_rcrg_recall50.png"}, {"analysis": "Baseline Recall vs k \u2014 synthetic_injection: Mean per-token recall for both rare and common remains at 0 across all tested k (up to ~200). Even generous retrieval depth yields no hits, suggesting an evaluation or data-pipeline issue, or that the model lacks any capacity to recover injected tokens under the current setup.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_ag_news_rcrg_recall50.png"}, {"analysis": "Training and Validation Loss \u2014 overwrite_imdb: Training loss decreases (\u22483.94 \u2192 \u22483.67) while validation loss is essentially flat (\u22483.68). Despite reduced training loss, retrieval metrics do not improve and even deteriorate, indicating the optimized objective is not aligned with the recall targets or that changes are not affecting relevant components.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_wikitext_wikitext2_rcrg_recall50.png"}, {"analysis": "Training and Validation Loss \u2014 overwrite_agnews: Both training and validation losses drop modestly (\u22483.50 \u2192 \u22483.08 train; \u22483.30 \u2192 \u22483.28 val). However, recall metrics remain at 0, showing a disconnect between the loss and the retrieval outcomes.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_imdb_imdb_train_val_loss.png"}, {"analysis": "Training and Validation Loss \u2014 overwrite_wikitext: Training loss decreases substantially (\u22483.78 \u2192 \u22483.25) while validation loss increases slightly (\u22483.70 \u2192 \u22483.75), indicating overfitting. Consistent with other plots, recall metrics remain unchanged, pointing to ineffective updates for the retrieval objective.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_1de88a67053341f7b8c8f1e4489fb9b5_proc_24127/overwrite_agnews_ag_news_train_val_loss.png"}], [{"analysis": "Validation metrics for overwrite_imdb are largely degenerate: rare/common recall@50 and PHR remain near zero across epochs, rare/common median half-life is near zero, while rare/common censored_pct is saturated near 100%. RCRG@50 appears flat with no dynamics. This pattern indicates the evaluation is dominated by censoring and the model is not surfacing positives, or the metric pipeline/candidate pool is misconfigured. The lack of change between epochs suggests no measurable improvement.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_imdb_aux_recall_curves.png"}, {"analysis": "Validation metrics for overwrite_ag_news mirror the same degeneracy: near-zero rare/common recall@50 and PHR, near-zero median half-lives, and ~100% censored percentages for both rare and common groups. RCRG@50 is flat. This strongly suggests a systemic issue (evaluation setup or data partitioning), not dataset-specific behavior.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_imdb_val_metrics.png"}, {"analysis": "Validation metrics for overwrite_wikitext across three epochs also show the same saturation: near-zero recall@50/PHR and median half-lives with ~100% censored_pct for both rare and common. No meaningful epoch-to-epoch change. The persistence across more epochs reinforces that the observed behavior is not due to early training dynamics but likely an evaluation or data/labeling mismatch.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_imdb_train_val_loss.png"}, {"analysis": "Train/validation loss for overwrite_imdb: train loss decreases noticeably across epochs, while validation loss decreases only slightly and remains consistently lower than the training loss. This inversion (val < train) is atypical and points to a mismatch between training and evaluation losses (e.g., different loss definitions, regularization/noise during training, or batch/label smoothing differences). Generalization improvement is minimal despite training loss reduction.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_ag_news_aux_recall_curves.png"}, {"analysis": "Train/validation loss for overwrite_ag_news: similar pattern to imdb. Training loss drops substantially; validation loss improves only marginally and remains below the training loss. The gap direction suggests a metric mismatch or heavy regularization/noise during training rather than genuine generalization gains.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_ag_news_val_metrics.png"}, {"analysis": "Train/validation loss for overwrite_wikitext: training loss decreases steadily over three epochs; validation loss is essentially flat and lower than training across all epochs. This indicates the model is optimizing on the training objective without corresponding validation gains, consistent with a mismatch in loss computation or an evaluation setup that is not sensitive to the learned changes.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_ag_news_train_val_loss.png"}, {"analysis": "Recall and RCRG over epochs for overwrite_imdb show rare recall@50 pinned at 1.0, common recall@50 at 0.0, and RCRG@50 at 1.0 across epochs. This is highly implausible and directly conflicts with the earlier validation-metrics panel (where rare/common recall@50 appeared near zero). The discrepancy suggests a bug or inconsistency in metric computation/plotting (e.g., scale mismatch, index misalignment, or class-split logic).", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_wikitext_aux_recall_curves.png"}, {"analysis": "Recall and RCRG over epochs for overwrite_ag_news replicate the same improbable pattern: rare recall@50 = 1.0, common recall@50 = 0.0, RCRG@50 = 1.0, with no dynamics. This further supports a systemic metric or plotting issue rather than a dataset-specific phenomenon.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_wikitext_val_metrics.png"}, {"analysis": "Recall and RCRG over epochs for overwrite_wikitext again display rare recall@50 = 1.0, common recall@50 = 0.0, and RCRG@50 = 1.0 across three epochs, with no temporal variation. The uniformity across datasets and epochs strongly indicates a metrics pipeline problem (e.g., degenerate rare/common partitioning, empty candidate sets for common, or incorrect normalization/clipping producing constant 1.0 ratios).", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/overwrite_wikitext_train_val_loss.png"}, {"analysis": "Embedding Retention (Cosine) for rare tokens shows cosine similarity of 1.0 for all reported tokens, implying the rare-token embeddings are unchanged by training. This strongly suggests embeddings were frozen or otherwise not updated (e.g., zero or extremely small LR, incorrect parameter groups, or the creative intervention not applied). The lack of embedding movement aligns with the absence of meaningful changes in downstream metrics.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_2fca63294806446fbafc2e7ccda0fa3c_proc_24127/synthetic_injection_train_val_loss.png"}], [{"analysis": "RCRG@50 is ~-0.2 at overwrite epoch 1 because common recall@50 (~0.2) exceeds rare recall@50 (~0). From epochs 2\u20134, both rare and common recall@50 drop to ~0, making the gap appear closed only due to collapsed performance. This indicates the overwrite destabilized retrieval for both token groups; the gap metric alone is misleading without absolute recalls.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/rcrg_over_epochs.png"}, {"analysis": "Post-overwrite generation counts for common tokens are sparse and skewed: 'apple' (3), 'water' (2), 'table' (1), while 'green' and 'house' are 0. The model concentrates probability mass on a few common continuations at the first token, consistent with a general recall collapse and potential logit suppression for many vocabulary items.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/gen_counts_common_post.png"}, {"analysis": "Post-overwrite generation counts for rare tokens are 0 across all listed items. Rare tokens are effectively never selected as the first generated token after the overwrite, suggesting strong suppression of rare-token logits or decoding biases, contrary to the intended improvement for rarity.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/gen_counts_rare_post.png"}, {"analysis": "Embedding retention (cosine) for common tokens is ~1.0 across indices, indicating embeddings were virtually unchanged by the overwrite. The behavioral degradation likely originates beyond the embedding layer (e.g., output projection, intermediate transformer blocks, or altered logit biases/normalization).", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/embedding_retention_common.png"}, {"analysis": "Embedding retention (cosine) for rare tokens is also ~1.0, yet none are generated post-overwrite. This dissociation implies downstream logit path changes (e.g., output head row norms or bias terms for rare tokens) rather than embedding drift; weight tying or head-specific regularization may need verification.", "plot_path": "workspaces/logs/0-run/experiment_results/experiment_fc926ba412da4f32bd3ef2b5e7429c99_proc_24127/embedding_retention_rare.png"}]], "vlm_feedback_summary": ["[]", "['Only two visuals carry meaningful dynamics: the across-checkpoint recall\nsummary and the wikitext validation curves. They consistently show common-token\nrecall decreasing toward zero and RCRG moving from negative toward zero, while\nrare-token recall remains at zero. Embedding retention for both common and rare\ntokens is ~1.0, implying embeddings were not altered; changes likely arise from\nother components or evaluation configuration. For imdb and ag_news, all recall\nand PHR metrics are identically zero, suggesting dataset-target mismatch or a\nmetric/generation pipeline issue. Recommended actions: verify target lexicon\ncoverage per dataset, check tokenization/casing/punctuation normalization,\nconfirm generation/sample sizes and top-k settings, inspect thresholds and\ndefinitions for recall/PHR, and audit the output head or higher-layer changes\nsince embeddings are unchanged. If the goal is to boost rare-token recall,\nintroduce targeted objectives or sampling strategies that explicitly reward\nrare-token usage while monitoring trade-offs with common-token recall.']", "[]", "['Across datasets, retrieval metrics for rare tokens are consistently zero and\ndo not improve, with occasional initial common-token recall that later collapses\nor remains modest. RCRG stays negative or zero, indicating no progress in\nclosing the rare\u2013common gap. Embedding retention is perfect (cosine = 1.0) for\nboth rare and common tokens, strongly suggesting the embedding layer is frozen\nor otherwise unaffected by training; this likely explains the flat recall\ncurves. Although training loss drops (and sometimes validation loss too), these\nimprovements do not translate into recall gains, implying objective\u2013metric\nmisalignment or that updates are bypassing the components critical for\nretrieval. The synthetic injection baseline shows zero recall even at large k,\nflagging a potential evaluation/pipeline issue. Recommended actions: verify that\nembeddings and relevant layers are unfrozen; confirm gradients reach token\nembeddings; instrument to ensure the overwrite/data injection is actually\npresent in batches; sanity-check the recall@k computation and candidate set\nconstruction; extend training beyond 2 epochs; re-run the PHR summary plot with\npopulated data.']", "['Across datasets, training loss decreases but validation loss is largely flat\nand remains lower than training, indicating a loss/metric mismatch rather than\ngenuine generalization gains. Validation metrics are degenerate: recall@50 and\nPHR near zero with ~100% censoring for both rare and common groups, and no\nepoch-to-epoch improvement. In contrast, the recall-only panels show rare\nrecall@50 = 1.0, common recall@50 = 0.0, and RCRG@50 = 1.0 across all\nepochs/datasets\u2014an implausible and contradictory outcome that points to a\nmetrics computation/plotting bug (e.g., mis-specified rare/common splits, index\nmisalignment, or clipping/normalization artifacts). Embedding retention for rare\ntokens is identically 1.0, indicating embeddings did not change (likely frozen\nor misconfigured optimizer parameter groups), which would explain the\nstagnant/degenerate behavior. Immediate actions: verify rare/common partitioning\nand candidate pools; audit recall@k and RCRG implementations and scales;\nreconcile train/val loss definitions; confirm that rare-token embeddings and\nassociated modules are included in trainable parameters with a non-zero\neffective learning rate.']", "['Gap parity was achieved by degeneracy: both rare and common recall@50 fell to\n~0 after overwrite, not by improving rare recall. Generated samples corroborate\nthis: common tokens appear only in a few cases (apple, water, table), while rare\ntokens never appear as the first token. Embeddings for both groups are retained\n(~cosine 1.0), so the failure likely resides in the output projection/logit\npathway or decoding configuration. Consider safeguarding absolute performance\nwith regularizers (e.g., KL to baseline, L2 on output head rows/biases), verify\nweight tying and temperature/top-k settings, apply layer-wise LR or constrained\nfine-tuning, and report confidence intervals and absolute recalls alongside gap\nmetrics.']"], "exec_time": [670.1359784603119, 997.4787209033966, 321.8231899738312, 1109.674371957779, 1289.146823644638, 759.4814119338989], "exec_time_feedback": ["", "", "", "", "", ""], "datasets_successfully_tested": [["synthetic_injection", "overwrite_wikitext"], ["overwrite_wikitext"], [], [""], [""], [""]], "plot_code": [null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nworking_dir = os.path.join(os.getcwd(), 'working')\n\n# Ensure working directory exists\nos.makedirs(working_dir, exist_ok=True)\n\n# GPU/device enforcement with CPU fallback\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.set_device(0)\n        device = torch.device('cuda:0')\n    else:\n        device = torch.device('cpu')\nexcept Exception as e:\n    # Torch might not be available in plotting environment; proceed on CPU\n    device = None\n    print(f\"Torch device setup warning: {e}\")\n\n# Load experiment data\nexperiment_data = None\ntry:\n    experiment_data = np.load(os.path.join(working_dir, 'experiment_data.npy'), allow_pickle=True).item()\nexcept Exception as e:\n    print(f'Error loading experiment data: {e}')\n\nif experiment_data is None:\n    raise SystemExit(0)\n\n# Access root container\nroot_key = 'hyperparam_tuning_type_1'\nif root_key not in experiment_data:\n    print(f\"Missing root key '{root_key}' in experiment_data.\")\n    raise SystemExit(0)\n\nall_tags = list(experiment_data[root_key].keys())\n\n# Helper to safely extract epoch-x and value-y arrays from list of dicts\ndef extract_xy(dict_list, xkey, ykey):\n    xs, ys = [], []\n    for d in dict_list:\n        if isinstance(d, dict) and (xkey in d) and (ykey in d):\n            xs.append(d[xkey])\n            ys.append(d[ykey])\n    return xs, ys\n\n# 1) Synthetic injection: baseline per-token recalls (rare/common)\nif 'synthetic_injection' in all_tags:\n    inj = experiment_data[root_key].get('synthetic_injection', {})\n    aux = inj.get('aux', {}) if isinstance(inj, dict) else {}\n    baseline_recalls = aux.get('baseline_recalls', {}) if isinstance(aux, dict) else {}\n    rare_tokens = aux.get('rare_tokens', []) if isinstance(aux, dict) else []\n    control_tokens = aux.get('control_tokens', []) if isinstance(aux, dict) else []\n\n    # Baseline recalls - rare tokens\n    try:\n        if isinstance(baseline_recalls, dict) and isinstance(rare_tokens, list) and len(rare_tokens) > 0:\n            vals = [baseline_recalls.get(t, np.nan) for t in rare_tokens]\n            if np.any(np.isfinite(vals)):\n                plt.figure(figsize=(8, 4))\n                plt.bar(range(len(rare_tokens)), vals, color='tab:blue')\n                xt = [t.strip() if isinstance(t, str) else str(t) for t in rare_tokens]\n                plt.xticks(range(len(rare_tokens)), xt, rotation=45, ha='right')\n                title_main = 'Baseline per-token recall@50 - Rare tokens'\n                subtitle = 'Dataset: synthetic_injection | Left: Ground Truth, Right: Generated Samples'\n                plt.title(title_main + '\\n' + subtitle)\n                plt.ylabel('Recall@50')\n                plt.tight_layout()\n                plt.savefig(os.path.join(working_dir, 'synthetic_injection_baseline_recalls_rare.png'))\n                plt.close()\n            else:\n                # Ensure we close even if nothing to plot\n                plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating baseline rare recall plot: {e}\")\n        plt.close()\n\n    # Baseline recalls - common/control tokens\n    try:\n        if isinstance(baseline_recalls, dict) and isinstance(control_tokens, list) and len(control_tokens) > 0:\n            vals = [baseline_recalls.get(t, np.nan) for t in control_tokens]\n            if np.any(np.isfinite(vals)):\n                plt.figure(figsize=(8, 4))\n                plt.bar(range(len(control_tokens)), vals, color='tab:orange')\n                xt = [t.strip() if isinstance(t, str) else str(t) for t in control_tokens]\n                plt.xticks(range(len(control_tokens)), xt, rotation=45, ha='right')\n                title_main = 'Baseline per-token recall@50 - Common tokens'\n                subtitle = 'Dataset: synthetic_injection | Left: Ground Truth, Right: Generated Samples'\n                plt.title(title_main + '\\n' + subtitle)\n                plt.ylabel('Recall@50')\n                plt.tight_layout()\n                plt.savefig(os.path.join(working_dir, 'synthetic_injection_baseline_recalls_common.png'))\n                plt.close()\n            else:\n                plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating baseline common recall plot: {e}\")\n        plt.close()\n\n# 2) Per-dataset training/validation loss curves\nfor tag in all_tags:\n    ds = experiment_data[root_key].get(tag, {})\n    losses = ds.get('losses', {}) if isinstance(ds, dict) else {}\n    train_list = losses.get('train', []) if isinstance(losses, dict) else []\n    val_list = losses.get('val', []) if isinstance(losses, dict) else []\n\n    try:\n        tx, ty = extract_xy(train_list, 'epoch', 'loss')\n        vx, vy = extract_xy(val_list, 'epoch', 'loss')\n        if len(tx) > 0 or len(vx) > 0:\n            plt.figure(figsize=(7, 4))\n            if len(tx) > 0:\n                plt.plot(tx, ty, marker='o', label='Train loss')\n            if len(vx) > 0:\n                plt.plot(vx, vy, marker='s', label='Val loss')\n            plt.xlabel('Epoch')\n            plt.ylabel('Loss')\n            title_main = 'Training/Validation Loss Curves'\n            subtitle = f'Dataset: {tag} | Left: Ground Truth, Right: Generated Samples'\n            plt.title(title_main + '\\n' + subtitle)\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f'{tag}_train_val_loss.png'))\n            plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating loss curves for {tag}: {e}\")\n        plt.close()\n\n# 3) Overwrite datasets: plot recall metrics and PHR over epochs (from metrics and aux histories)\noverwrite_tags = [t for t in all_tags if t.startswith('overwrite_')]\nfor tag in overwrite_tags:\n    ds = experiment_data[root_key].get(tag, {})\n    metrics = ds.get('metrics', {}) if isinstance(ds, dict) else {}\n    val_metrics = metrics.get('val', []) if isinstance(metrics, dict) else []\n\n    # Extract per-epoch metrics if present\n    epochs_vm = []\n    rcrg_vm, rare_vm, common_vm, phr_lb_vm, phr_est_vm = [], [], [], [], []\n    for m in val_metrics:\n        if isinstance(m, dict) and 'epoch' in m:\n            epochs_vm.append(m.get('epoch'))\n            rcrg_vm.append(m.get('RCRG@50', np.nan))\n            rare_vm.append(m.get('rare_recall@50', np.nan))\n            common_vm.append(m.get('common_recall@50', np.nan))\n            phr_lb_vm.append(m.get('PHR_lb', np.nan))\n            phr_est_vm.append(m.get('PHR_est', np.nan))\n\n    # Plot recalls and RCRG over epochs (from val metrics)\n    try:\n        if len(epochs_vm) > 0 and (np.any(np.isfinite(rcrg_vm)) or np.any(np.isfinite(rare_vm)) or np.any(np.isfinite(common_vm))):\n            plt.figure(figsize=(7, 4))\n            if np.any(np.isfinite(rcrg_vm)):\n                plt.plot(epochs_vm, rcrg_vm, marker='o', label='RCRG@50')\n            if np.any(np.isfinite(rare_vm)):\n                plt.plot(epochs_vm, rare_vm, marker='s', label='Rare recall@50')\n            if np.any(np.isfinite(common_vm)):\n                plt.plot(epochs_vm, common_vm, marker='^', label='Common recall@50')\n            plt.xlabel('Epoch')\n            plt.ylabel('Score')\n            title_main = 'Recall metrics over epochs (validation)'\n            subtitle = f'Dataset: {tag} | Left: Ground Truth, Right: Generated Samples'\n            plt.title(title_main + '\\n' + subtitle)\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f'{tag}_recall_metrics_over_epochs.png'))\n            plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating recall metrics plot for {tag}: {e}\")\n        plt.close()\n\n    # Plot PHR over epochs (from val metrics)\n    try:\n        if len(epochs_vm) > 0 and (np.any(np.isfinite(phr_lb_vm)) or np.any(np.isfinite(phr_est_vm))):\n            plt.figure(figsize=(7, 4))\n            if np.any(np.isfinite(phr_lb_vm)):\n                plt.plot(epochs_vm, phr_lb_vm, marker='o', label='PHR lower-bound')\n            if np.any(np.isfinite(phr_est_vm)):\n                plt.plot(epochs_vm, phr_est_vm, marker='s', label='PHR estimate')\n            plt.xlabel('Epoch')\n            plt.ylabel('PHR')\n            title_main = 'PHR over epochs (validation)'\n            subtitle = f'Dataset: {tag} | Left: Ground Truth, Right: Generated Samples'\n            plt.title(title_main + '\\n' + subtitle)\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f'{tag}_phr_over_epochs.png'))\n            plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating PHR plot for {tag}: {e}\")\n        plt.close()\n\n    # Also plot from aux histories if available (steps-based)\n    aux = ds.get('aux', {}) if isinstance(ds, dict) else {}\n    histories = aux.get('histories', {}) if isinstance(aux, dict) else {}\n    steps = histories.get('steps', []) if isinstance(histories, dict) else []\n    rcrg = histories.get('rcrg', []) if isinstance(histories, dict) else []\n    rare_hist = histories.get('rare_recall', []) if isinstance(histories, dict) else []\n    common_hist = histories.get('common_recall', []) if isinstance(histories, dict) else []\n    phr_lb_hist = histories.get('phr_lb', []) if isinstance(histories, dict) else []\n    phr_est_hist = histories.get('phr_est', []) if isinstance(histories, dict) else []\n\n    # Steps-based recalls and RCRG\n    try:\n        if len(steps) > 0 and (len(rcrg) > 0 or len(rare_hist) > 0 or len(common_hist) > 0):\n            plt.figure(figsize=(7, 4))\n            if len(rcrg) > 0:\n                plt.plot(steps, rcrg, marker='o', label='RCRG@50')\n            if len(rare_hist) > 0:\n                plt.plot(steps, rare_hist, marker='s', label='Rare recall@50')\n            if len(common_hist) > 0:\n                plt.plot(steps, common_hist, marker='^', label='Common recall@50')\n            plt.xlabel('Global step')\n            plt.ylabel('Score')\n            title_main = 'Recall metrics across overwrite checkpoints'\n            subtitle = f'Dataset: {tag} | Left: Ground Truth, Right: Generated Samples'\n            plt.title(title_main + '\\n' + subtitle)\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f'{tag}_recalls_over_checkpoints.png'))\n            plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating recalls-over-checkpoints plot for {tag}: {e}\")\n        plt.close()\n\n    # Steps-based PHR\n    try:\n        if len(steps) > 0 and (len(phr_lb_hist) > 0 or len(phr_est_hist) > 0):\n            plt.figure(figsize=(7, 4))\n            if len(phr_lb_hist) > 0:\n                plt.plot(steps, phr_lb_hist, marker='o', label='PHR lower-bound')\n            if len(phr_est_hist) > 0:\n                plt.plot(steps, phr_est_hist, marker='s', label='PHR estimate')\n            plt.xlabel('Global step')\n            plt.ylabel('PHR')\n            title_main = 'PHR across overwrite checkpoints'\n            subtitle = f'Dataset: {tag} | Left: Ground Truth, Right: Generated Samples'\n            plt.title(title_main + '\\n' + subtitle)\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.savefig(os.path.join(working_dir, f'{tag}_phr_over_checkpoints.png'))\n            plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f\"Error creating PHR-over-checkpoints plot for {tag}: {e}\")\n        plt.close()\n\nprint('Plotting complete. Figures saved to:', working_dir)", null, "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nworking_dir = os.path.join(os.getcwd(), 'working')\n\nos.makedirs(working_dir, exist_ok=True)\n\n# GPU/Device setup (as required)\ntry:\n    import torch\n    try:\n        torch.cuda.set_device(0)\n        device = torch.device('cuda:0')\n    except Exception:\n        device = torch.device('cpu')\n    print(f'Using device: {device}')\nexcept Exception:\n    # If torch is not available, continue without device specifics\n    device = None\n\n# Load experiment data\nexperiment_data = None\ntry:\n    experiment_data = np.load(os.path.join(working_dir, 'experiment_data.npy'), allow_pickle=True).item()\nexcept Exception as e:\n    print(f'Error loading experiment data: {e}')\n    experiment_data = {}\n\n# Helpers\nDATASET_DISPLAY = {\n    'synthetic_injection': 'Synthetic Injection',\n    'overwrite_wikitext': 'WikiText-2',\n    'overwrite_agnews': 'AG News',\n    'overwrite_imdb': 'IMDb',\n}\n\ndef display_name(tag: str) -> str:\n    return DATASET_DISPLAY.get(tag, tag)\n\ndef sanitize(name: str) -> str:\n    return name.lower().replace(' ', '_').replace('/', '_').replace('-', '')\n\n# 1) Training/Validation loss curves for all datasets (if available)\nfor tag, container in experiment_data.items():\n    try:\n        losses = container.get('losses', {})\n        train_entries = losses.get('train', []) or []\n        val_entries = losses.get('val', []) or []\n        if len(train_entries) == 0 and len(val_entries) == 0:\n            continue\n\n        train_epochs = [e.get('epoch', i + 1) for i, e in enumerate(train_entries)]\n        val_epochs = [e.get('epoch', i + 1) for i, e in enumerate(val_entries)]\n        train_vals = [e.get('loss', None) for e in train_entries]\n        val_vals = [e.get('loss', None) for e in val_entries]\n\n        plt.figure(figsize=(7, 4))\n        if len(train_vals) > 0:\n            plt.plot(train_epochs, train_vals, marker='o', label='Train Loss')\n        if len(val_vals) > 0:\n            plt.plot(val_epochs, val_vals, marker='s', label='Validation Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        title_main = f'Training and Validation Loss \u2014 {tag}'\n        title_sub = f'Dataset: {display_name(tag)}'\n        plt.title(title_main + '\\n' + title_sub)\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        fname = f'{sanitize(tag)}_{sanitize(display_name(tag))}_train_val_loss.png'\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f'Error creating train/val loss plot for {tag}: {e}')\n        try:\n            plt.close()\n        except Exception:\n            pass\n\n# 2) RCRG and Recall@50 curves for overwrite datasets (if available in aux)\nfor tag, container in experiment_data.items():\n    try:\n        aux = container.get('aux', {}) or {}\n        rcrg_hist = aux.get('rcrg_history', None)\n        recall_hist_by_k = aux.get('recall_hist_by_k', None)\n        if rcrg_hist is None or recall_hist_by_k is None:\n            continue\n        # Handle int or string keys for k\n        rare50 = None\n        common50 = None\n        if isinstance(recall_hist_by_k, dict):\n            if 50 in recall_hist_by_k:\n                rare50 = recall_hist_by_k[50].get('rare', [])\n                common50 = recall_hist_by_k[50].get('common', [])\n            elif '50' in recall_hist_by_k:\n                rare50 = recall_hist_by_k['50'].get('rare', [])\n                common50 = recall_hist_by_k['50'].get('common', [])\n        if rare50 is None or common50 is None:\n            continue\n\n        epochs = list(range(1, max(len(rcrg_hist), len(rare50), len(common50)) + 1))\n        plt.figure(figsize=(7, 4))\n        if len(rcrg_hist) > 0:\n            plt.plot(range(1, len(rcrg_hist) + 1), rcrg_hist, marker='o', label='RCRG@50')\n        if len(rare50) > 0:\n            plt.plot(range(1, len(rare50) + 1), rare50, marker='s', label='Rare recall@50')\n        if len(common50) > 0:\n            plt.plot(range(1, len(common50) + 1), common50, marker='^', label='Common recall@50')\n        plt.xlabel('Epoch')\n        plt.ylabel('Score')\n        title_main = f'RCRG and Recalls \u2014 {tag}'\n        title_sub = f'Dataset: {display_name(tag)}'\n        plt.title(title_main + '\\n' + title_sub)\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        fname = f'{sanitize(tag)}_{sanitize(display_name(tag))}_rcrg_recall50.png'\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\n    except Exception as e:\n        print(f'Error creating RCRG/Recall plot for {tag}: {e}')\n        try:\n            plt.close()\n        except Exception:\n            pass\n\n# 3) Generated samples vs ground truth (token hit counts) \u2014 side-by-side plots\nfor tag, container in experiment_data.items():\n    try:\n        preds = container.get('predictions', None)\n        gt = container.get('ground_truth', None)\n        if preds is None or gt is None or not isinstance(gt, dict):\n            continue\n        rare_tokens = gt.get('rare', []) or []\n        common_tokens = gt.get('common', []) or []\n        if len(rare_tokens) == 0 and len(common_tokens) == 0:\n            continue\n        preds_list = list(preds) if isinstance(preds, (list, tuple)) else []\n\n        def count_hits(pred_list, tokens):\n            return [sum(1 for p in pred_list if p == t) for t in tokens]\n\n        rare_counts = count_hits(preds_list, rare_tokens)\n        common_counts = count_hits(preds_list, common_tokens)\n\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n        # Left: Rare token counts\n        axes[0].bar(range(len(rare_tokens)), rare_counts, color='tab:blue')\n        axes[0].set_xticks(range(len(rare_tokens)))\n        axes[0].set_xticklabels([t.strip() for t in rare_tokens], rotation=45, ha='right')\n        axes[0].set_ylabel('Hit Count')\n        axes[0].set_title('Rare Tokens')\n\n        # Right: Common token counts\n        axes[1].bar(range(len(common_tokens)), common_counts, color='tab:orange')\n        axes[1].set_xticks(range(len(common_tokens)))\n        axes[1].set_xticklabels([t.strip() for t in common_tokens], rotation=45, ha='right')\n        axes[1].set_title('Common Tokens')\n\n        fig.suptitle(f'Generated Token Hit Counts \u2014 {tag}\\nLeft: Rare Tokens, Right: Common Tokens \u2014 Dataset: {display_name(tag)}')\n        fig.tight_layout(rect=[0, 0.03, 1, 0.90])\n        fname = f'{sanitize(tag)}_{sanitize(display_name(tag))}_generated_hit_counts.png'\n        fig.savefig(os.path.join(working_dir, fname))\n        plt.close(fig)\n    except Exception as e:\n        print(f'Error creating generated-vs-ground-truth plot for {tag}: {e}')\n        try:\n            plt.close()\n        except Exception:\n            pass\n\n# 4) Baseline per-k recalls (Synthetic Injection)\ntry:\n    syn = experiment_data.get('synthetic_injection', {})\n    aux = syn.get('aux', {}) or {}\n    rare_base = aux.get('rare_baseline_per_k', None)\n    common_base = aux.get('common_baseline_per_k', None)\n    if rare_base is not None and common_base is not None and isinstance(rare_base, dict) and isinstance(common_base, dict):\n        # Sort ks numerically if possible\n        def to_int_keys(d):\n            out = {}\n            for k, v in d.items():\n                try:\n                    out[int(k)] = v\n                except Exception:\n                    pass\n            return dict(sorted(out.items(), key=lambda x: x[0]))\n        rb = to_int_keys(rare_base)\n        cb = to_int_keys(common_base)\n        ks = sorted(set(rb.keys()).intersection(cb.keys()))\n        if len(ks) > 0:\n            rare_means = [float(np.mean(rb[k])) if len(rb[k]) > 0 else 0.0 for k in ks]\n            common_means = [float(np.mean(cb[k])) if len(cb[k]) > 0 else 0.0 for k in ks]\n            plt.figure(figsize=(7, 4))\n            plt.plot(ks, rare_means, marker='o', label='Rare (mean)')\n            plt.plot(ks, common_means, marker='s', label='Common (mean)')\n            plt.xlabel('k')\n            plt.ylabel('Per-token recall')\n            title_main = 'Baseline Recall vs k \u2014 synthetic_injection'\n            title_sub = f'Dataset: {display_name(\"synthetic_injection\")}'\n            plt.title(title_main + '\\n' + title_sub)\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            fname = 'synthetic_injection_baseline_recall_vs_k.png'\n            plt.savefig(os.path.join(working_dir, fname))\n            plt.close()\nexcept Exception as e:\n    print(f'Error creating baseline per-k recall plot: {e}')\n    try:\n        plt.close()\n    except Exception:\n        pass\n\n# 5) PHR summary across overwrite datasets\ntry:\n    tags = ['overwrite_wikitext', 'overwrite_agnews', 'overwrite_imdb']\n    labels = []\n    vals = []\n    for tag in tags:\n        cont = experiment_data.get(tag, {})\n        aux = cont.get('aux', {}) if isinstance(cont, dict) else {}\n        phr = aux.get('PHR', None)\n        if phr is not None:\n            labels.append(display_name(tag))\n            vals.append(float(phr))\n    if len(vals) > 0:\n        plt.figure(figsize=(7, 4))\n        x = np.arange(len(vals))\n        plt.bar(x, vals, color='tab:green')\n        plt.xticks(x, labels, rotation=0)\n        plt.ylabel('PHR (median rare HL / median common HL)')\n        title_main = 'PHR Summary across Overwrite Datasets'\n        title_sub = 'Datasets: ' + ', '.join(labels)\n        plt.title(title_main + '\\n' + title_sub)\n        plt.ylim(0, max(1.0, max(vals) * 1.1))\n        plt.tight_layout()\n        fname = 'phr_summary_overwrite_datasets.png'\n        plt.savefig(os.path.join(working_dir, fname))\n        plt.close()\nexcept Exception as e:\n    print(f'Error creating PHR summary plot: {e}')\n    try:\n        plt.close()\n    except Exception:\n        pass\n\nprint('Plotting complete. Figures saved to:', working_dir)", "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nworking_dir = os.path.join(os.getcwd(), 'working')\n\nimport torch\n\n# Ensure output dir exists\nos.makedirs(working_dir, exist_ok=True)\n\n# Enforce GPU index when available (CPU fallback allowed)\ntry:\n    if torch.cuda.is_available():\n        torch.cuda.set_device(0)\n        device = torch.device('cuda:0')\n    else:\n        device = torch.device('cpu')\nexcept Exception:\n    device = torch.device('cpu')\n\n# Load experiment data\nexperiment_data = None\ntry:\n    experiment_data = np.load(os.path.join(working_dir, 'experiment_data.npy'), allow_pickle=True).item()\nexcept Exception as e:\n    print(f'Error loading experiment data: {e}')\n\nif experiment_data is None or not isinstance(experiment_data, dict):\n    raise SystemExit('No experiment data available to plot.')\n\n# Navigate to expected structure\nroot_key = 'rare_token_persistence'\nif root_key not in experiment_data:\n    raise SystemExit(f'Missing key {root_key} in experiment_data; nothing to plot.')\n\nroot = experiment_data[root_key]\nif not isinstance(root, dict) or len(root) == 0:\n    raise SystemExit('No dataset entries found in experiment_data.')\n\n# Helper to safely extract numeric series\ndef extract_series(entries, value_keys):\n    xs, ys = [], []\n    for i, rec in enumerate(entries):\n        if not isinstance(rec, dict):\n            continue\n        x = rec.get('epoch', i + 1)\n        y = None\n        for k in value_keys:\n            if k in rec and rec[k] is not None:\n                y = rec[k]\n                break\n        if y is None:\n            continue\n        try:\n            y_float = float(y)\n            xs.append(x)\n            ys.append(y_float)\n        except Exception:\n            # skip non-numeric\n            pass\n    return xs, ys\n\n# Iterate over dataset tags and plot\nfor tag, container in root.items():\n    dataset_name = tag\n\n    # 1) Train/Val loss curves\n    try:\n        losses = container.get('losses', {}) if isinstance(container, dict) else {}\n        train_losses = losses.get('train', []) if isinstance(losses, dict) else []\n        val_losses = losses.get('val', []) if isinstance(losses, dict) else []\n\n        x_tr, y_tr = extract_series(train_losses, ['loss', 'avg_loss'])\n        x_va, y_va = extract_series(val_losses, ['loss', 'val_loss'])\n\n        if len(x_tr) > 0 or len(x_va) > 0:\n            fig, ax = plt.subplots(figsize=(7, 4))\n            if len(x_tr) > 0:\n                ax.plot(x_tr, y_tr, marker='o', label='Train loss')\n            if len(x_va) > 0:\n                ax.plot(x_va, y_va, marker='s', label='Validation loss')\n            ax.set_xlabel('Epoch')\n            ax.set_ylabel('Loss')\n            ax.set_title(f'Train/Validation Loss - {dataset_name}')\n            plt.suptitle(f'Dataset: {dataset_name.replace(\"_\", \" \")}', y=0.98, fontsize=10)\n            ax.grid(True, alpha=0.3)\n            ax.legend()\n            plt.tight_layout(rect=[0, 0, 1, 0.96])\n            plt.savefig(os.path.join(working_dir, f'{dataset_name}_train_val_loss.png'))\n            plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f'Error creating loss plot for {dataset_name}: {e}')\n        plt.close()\n\n    # 2) Validation metrics over epochs (if present)\n    try:\n        metrics = container.get('metrics', {}) if isinstance(container, dict) else {}\n        val_metrics = metrics.get('val', []) if isinstance(metrics, dict) else []\n        if isinstance(val_metrics, list) and len(val_metrics) > 0:\n            # Determine which metric keys are present beyond basic loss keys\n            exclude = {'epoch', 'ts', 'loss', 'val_loss', 'avg_loss'}\n            metric_keys = []\n            seen = set()\n            for rec in val_metrics:\n                if not isinstance(rec, dict):\n                    continue\n                for k, v in rec.items():\n                    if k in exclude:\n                        continue\n                    # include only numeric\n                    try:\n                        _ = float(v)\n                        if k not in seen:\n                            seen.add(k)\n                            metric_keys.append(k)\n                    except Exception:\n                        pass\n            if len(metric_keys) > 0:\n                fig, ax = plt.subplots(figsize=(7, 4))\n                epochs = [rec.get('epoch', i + 1) if isinstance(rec, dict) else (i + 1) for i, rec in enumerate(val_metrics)]\n                for mk in metric_keys:\n                    ys = []\n                    for rec in val_metrics:\n                        if not isinstance(rec, dict) or mk not in rec:\n                            ys.append(np.nan)\n                        else:\n                            try:\n                                ys.append(float(rec[mk]))\n                            except Exception:\n                                ys.append(np.nan)\n                    ax.plot(epochs, ys, marker='o', label=mk)\n                ax.set_xlabel('Epoch')\n                ax.set_ylabel('Metric value')\n                ax.set_title(f'Validation Metrics - {dataset_name}')\n                plt.suptitle(f'Dataset: {dataset_name.replace(\"_\", \" \")}', y=0.98, fontsize=10)\n                ax.grid(True, alpha=0.3)\n                ax.legend()\n                plt.tight_layout(rect=[0, 0, 1, 0.96])\n                plt.savefig(os.path.join(working_dir, f'{dataset_name}_val_metrics.png'))\n                plt.close()\n            else:\n                plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f'Error creating metrics plot for {dataset_name}: {e}')\n        plt.close()\n\n    # 3) Recall averages from aux histories (if arrays are present in experiment_data)\n    try:\n        aux = container.get('aux', {}) if isinstance(container, dict) else {}\n        rare_hist = aux.get('rare_hist', None)\n        common_hist = aux.get('common_hist', None)\n\n        have_rare = isinstance(rare_hist, np.ndarray) and rare_hist.size > 0\n        have_common = isinstance(common_hist, np.ndarray) and common_hist.size > 0\n\n        if have_rare or have_common:\n            fig, ax = plt.subplots(figsize=(7, 4))\n            if have_rare:\n                rare_avg = np.nanmean(rare_hist, axis=1)\n                ax.plot(np.arange(1, len(rare_avg) + 1), rare_avg, marker='o', label='Rare recall@50 (aux)')\n            if have_common:\n                common_avg = np.nanmean(common_hist, axis=1)\n                ax.plot(np.arange(1, len(common_avg) + 1), common_avg, marker='s', label='Common recall@50 (aux)')\n            if have_rare and have_common and len(rare_hist) == len(common_hist):\n                rcrg = np.nanmean(rare_hist, axis=1) - np.nanmean(common_hist, axis=1)\n                ax.plot(np.arange(1, len(rcrg) + 1), rcrg, marker='^', label='RCRG@50 (aux)')\n            ax.set_xlabel('Epoch')\n            ax.set_ylabel('Score')\n            ax.set_title(f'Recall Averages from Aux - {dataset_name}')\n            plt.suptitle(f'Dataset: {dataset_name.replace(\"_\", \" \")}', y=0.98, fontsize=10)\n            ax.grid(True, alpha=0.3)\n            ax.legend()\n            plt.tight_layout(rect=[0, 0, 1, 0.96])\n            plt.savefig(os.path.join(working_dir, f'{dataset_name}_aux_recall_curves.png'))\n            plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f'Error creating aux recall plot for {dataset_name}: {e}')\n        plt.close()\n\n    # 4) Ground Truth vs Predictions (text) if present; limit to 5 pairs\n    try:\n        preds = container.get('predictions', []) if isinstance(container, dict) else []\n        gts = container.get('ground_truth', []) if isinstance(container, dict) else []\n        n = min(5, len(preds), len(gts))\n        if n > 0:\n            fig, axes = plt.subplots(nrows=n, ncols=2, figsize=(10, 2 * n))\n            if n == 1:\n                axes = np.array([axes])\n            for i in range(n):\n                gt_text = str(gts[i])\n                pr_text = str(preds[i])\n                axes[i, 0].axis('off')\n                axes[i, 0].text(0, 0.5, gt_text, va='center', ha='left', wrap=True)\n                axes[i, 1].axis('off')\n                axes[i, 1].text(0, 0.5, pr_text, va='center', ha='left', wrap=True)\n            plt.suptitle(f'Left: Ground Truth, Right: Generated Samples | Dataset: {dataset_name.replace(\"_\", \" \")}', y=0.98)\n            plt.title(f'Ground Truth vs Predictions - {dataset_name}')\n            plt.tight_layout(rect=[0, 0, 1, 0.95])\n            plt.savefig(os.path.join(working_dir, f'{dataset_name}_ground_truth_vs_predictions.png'))\n            plt.close()\n        else:\n            plt.close()\n    except Exception as e:\n        print(f'Error creating predictions comparison plot for {dataset_name}: {e}')\n        plt.close()\n\nprint('Plotting complete. Figures saved to:', working_dir)", null], "plot_plan": [null, "Load experiment_data.npy from the working directory and extract available metrics per dataset tag under hyperparam_tuning_type_1. For each dataset, plot training/validation loss curves over epochs. For overwrite datasets, also plot recall-related metrics (RCRG, rare/common recall) and PHR across epochs using the stored metrics and histories in aux. For synthetic_injection, plot baseline per-token recalls for rare and common tokens if present. Enforce CUDA device index 0 when available, include clear titles and subtitles mentioning the dataset, save all plots to working_dir with descriptive filenames, ensure each plot is wrapped in a separate try-except and figures are closed in all cases, and avoid generating or simulating any data beyond what is stored in experiment_data.npy.", null, "Load experiment_data.npy from working_dir, parse available metrics, and create standard plots without fabricating any values. For each dataset: plot training/validation loss curves, RCRG and Recall@50 (if present), and side-by-side bar charts of generated-token hit counts (rare vs common) computed from predictions and ground truth. Additionally, plot baseline per-k recall for the synthetic injection phase and a summary bar chart of PHR across overwrite datasets. Save every figure to working_dir, wrap each plot in its own try-except, always close figures, include clear titles and subtitles that mention dataset type, and keep to basic matplotlib. Also include the required CUDA device setup with CPU fallback.", "Load experiment_data.npy from the working directory, parse per-dataset containers, and create standard plots that only use values present in the file. For each dataset tag, plot train/validation loss curves and any available validation metrics (e.g., RCRG@50, PHR) over epochs. If auxiliary histories (rare_hist/common_hist) are present, derive and plot average recall curves from those arrays. If predictions and ground truth exist, show up to five paired examples in a side-by-side text figure. Each plot will be created in its own try-except block, saved to working_dir with descriptive names, and figures will always be closed. Enforce CUDA device 0 when available, with CPU fallback.", null], "ablation_name": [null, null, null, null, null, null], "hyperparam_name": ["Reduce overwrite-phase batch size to 32", null, null, null, null, null], "is_seed_node": [false, false, false, false, false, true], "is_seed_agg_node": [false, false, false, false, false, false], "parse_metrics_plan": ["Load the saved experiment_data.npy from the working directory, parse the nested\nstructure under hyperparam_tuning_type_1, and for each dataset\n(synthetic_injection and overwrite_wikitext) print the final train average loss,\nthe best validation loss, and any additional validation metrics (RCRG@50,\nrare_recall@50, common_recall@50) using their best values. Enforce GPU device 0\nwhen CUDA is available with a CPU fallback. Keep code at global scope/functions\nand execute immediately without any plots.", "Load the saved experiment_data.npy from the working directory, parse the\nhierarchical structure, and compute the best (or final if best not applicable)\nvalues for key metrics per dataset. Use min for losses and max for recall/ratio\nmetrics. Ensure GPU index enforcement if CUDA is available. Print dataset name\nfollowed by metric names and their best values. No plots and no __main__ guard;\nrun at import time.", "Load experiment_data.npy from the working directory, parse the hierarchical\nstructure created in the original code, and print key metrics per dataset. For\nloss metrics, report the best (minimum) train and validation loss from the\ndedicated losses containers. For other metrics stored in metrics['val'] (e.g.,\nRCRG@50, rare/common recall), print the final value from the last epoch. Also\nprint PHR from aux if available. Ensure GPU index 0 is selected if CUDA is\navailable, and avoid any plotting. Implement parsing in helper functions and\nexecute immediately at the global scope.", "Load experiment_data.npy from the working directory, enforce GPU device\nselection when available, and parse the nested structure for each dataset. For\neach dataset, extract the final values for training loss, validation loss, and\nany final RCRG/recall metrics present. Also print PHR if available in aux.\nEnsure printing order is per dataset with clear metric labels. No plotting, and\ncode runs immediately without using an if __name__ == '__main__' block.", "Load the saved experiment_data.npy from the working directory. Navigate the\nnested structure under rare_token_persistence and, for each dataset, compute\nbest train average loss (minimum across epochs) and best validation loss\n(minimum). For overwrite datasets, also extract the final (last-epoch) values\nfor RCRG@50, recall metrics, PHR, half-life statistics, and censored\npercentages. Print the dataset name first, then metric names with precise labels\nand corresponding values. Enforce GPU device 0 when CUDA is available, with CPU\nfallback.", "Load the saved experiment_data.npy from the working directory, parse the nested\nstructure under hyperparam_tuning_type_1, and for each dataset\n(synthetic_injection and overwrite_wikitext) print the final train average loss,\nthe best validation loss, and any additional validation metrics (RCRG@50,\nrare_recall@50, common_recall@50) using their best values. Enforce GPU device 0\nwhen CUDA is available with a CPU fallback. Keep code at global scope/functions\nand execute immediately without any plots."], "parse_metrics_code": ["import os\nimport numpy as np\n\n# Optional torch/device setup as required (GPU index enforcement with CPU fallback)\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.set_device(0)\n        device = torch.device('cuda:0')\n    else:\n        device = torch.device('cpu')\nexcept Exception:\n    torch = None\n    device = None\n\n\ndef load_experiment_data(working_dir: str):\n    path = os.path.join(working_dir, 'experiment_data.npy')\n    data = np.load(path, allow_pickle=True).item()\n    return data\n\n\ndef get_final_train_avg_loss(train_metrics_list):\n    # Expect entries like {'epoch': int, 'avg_loss': float, 'ts': str}\n    if not train_metrics_list:\n        return None, None\n    last = train_metrics_list[-1]\n    return last.get('avg_loss', None), last.get('epoch', None)\n\n\ndef get_best_val_loss(val_metrics_list):\n    # Expect entries like {'epoch': int, 'val_loss': float, 'ts': str, ...}\n    if not val_metrics_list:\n        return None, None\n    filtered = [m for m in val_metrics_list if 'val_loss' in m]\n    if not filtered:\n        return None, None\n    best = min(filtered, key=lambda x: x.get('val_loss', float('inf')))\n    return best.get('val_loss', None), best.get('epoch', None)\n\n\ndef get_additional_val_metrics_best(val_metrics_list):\n    # Identify additional metrics beyond epoch/ts/val_loss; choose best as max\n    results = {}\n    if not val_metrics_list:\n        return results\n    ignore_keys = {'epoch', 'ts', 'val_loss'}\n    # Collect candidate metric names\n    metric_names = set()\n    for m in val_metrics_list:\n        for k, v in m.items():\n            if k not in ignore_keys and isinstance(v, (int, float, np.floating)):\n                metric_names.add(k)\n    # For each metric, find the max value and its epoch\n    for name in metric_names:\n        best_entry = None\n        best_val = None\n        for m in val_metrics_list:\n            if name in m and isinstance(m[name], (int, float, np.floating)):\n                val = float(m[name])\n                if best_val is None or val > best_val:\n                    best_val = val\n                    best_entry = m\n        if best_entry is not None:\n            results[name] = (best_val, best_entry.get('epoch', None))\n    return results\n\n\ndef print_metrics_for_dataset(name: str, ds: dict):\n    metrics = ds.get('metrics', {})\n    train_metrics = metrics.get('train', [])\n    val_metrics = metrics.get('val', [])\n\n    print(f'Dataset: {name}')\n\n    # Final train average loss\n    train_avg_loss, train_epoch = get_final_train_avg_loss(train_metrics)\n    if train_avg_loss is not None:\n        print(f'train average loss (final): {train_avg_loss:.6f}')\n\n    # Best validation loss\n    best_val_loss, best_val_epoch = get_best_val_loss(val_metrics)\n    if best_val_loss is not None:\n        print(f'validation loss (best): {best_val_loss:.6f}')\n\n    # Additional validation metrics (best)\n    extra_best = get_additional_val_metrics_best(val_metrics)\n    for metric_name in sorted(extra_best.keys()):\n        val, ep = extra_best[metric_name]\n        # Keep original metric naming exactly (e.g., RCRG@50)\n        print(f'{metric_name} (best): {val:.6f}')\n\n\ndef main():\n    working_dir = os.path.join(os.getcwd(), 'working')\n    experiment_data = load_experiment_data(working_dir)\n\n    # Navigate to the expected container\n    root_key = 'hyperparam_tuning_type_1'\n    if root_key not in experiment_data:\n        # Fallback: if structure differs, try to infer top key\n        keys = list(experiment_data.keys())\n        if len(keys) == 1 and isinstance(experiment_data[keys[0]], dict):\n            root_key = keys[0]\n        else:\n            # Print nothing if malformed, but attempt graceful exit\n            return\n\n    container = experiment_data[root_key]\n\n    for dataset_name, dataset_obj in container.items():\n        if isinstance(dataset_obj, dict):\n            print_metrics_for_dataset(dataset_name, dataset_obj)\n\n\n# Execute immediately\nmain()", "import os\nimport numpy as np\nimport math\nimport torch\nfrom typing import Any, Dict, List, Optional\n\n# -----------------------------------------------------------------------------\n# Device setup (GPU index enforced when available)\n# -----------------------------------------------------------------------------\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\n\n# -----------------------------------------------------------------------------\n# Constants and helpers\n# -----------------------------------------------------------------------------\nTOP_LEVEL_KEY = 'hyperparam_tuning_type_1'\n\nMETRIC_DIRECTIONS = {\n    'avg_loss': 'min',          # train\n    'val_loss': 'min',          # validation\n    'RCRG@50': 'max',\n    'rare_recall@50': 'max',\n    'common_recall@50': 'max',\n    'PHR_lb': 'max',\n    'PHR_est': 'max',\n}\n\nMETRIC_LABELS = {\n    'avg_loss': 'train average loss',\n    'val_loss': 'validation loss',\n    'RCRG@50': 'RCRG@50',\n    'rare_recall@50': 'rare recall@50',\n    'common_recall@50': 'common recall@50',\n    'PHR_lb': 'PHR lower-bound',\n    'PHR_est': 'PHR estimate',\n}\n\nDATASET_ORDER = [\n    'synthetic_injection',\n    'overwrite_wikitext',\n    'overwrite_ag_news',\n    'overwrite_imdb',\n]\n\n\ndef is_number(x: Any) -> bool:\n    try:\n        return x is not None and not (isinstance(x, float) and math.isnan(x))\n    except Exception:\n        return False\n\n\ndef best_value(values: List[Any], direction: str) -> Optional[float]:\n    filtered = [v for v in values if is_number(v)]\n    if not filtered:\n        return None\n    if direction == 'min':\n        return float(min(filtered))\n    if direction == 'max':\n        return float(max(filtered))\n    # Fallback to final\n    return float(filtered[-1])\n\n\ndef extract_best_train_loss(dset: Dict[str, Any]) -> Optional[float]:\n    # Prefer metrics['train'][*]['avg_loss'], fallback to losses['train'][*]['loss']\n    vals = []\n    train_metrics = dset.get('metrics', {}).get('train', [])\n    for m in train_metrics:\n        if 'avg_loss' in m:\n            vals.append(m.get('avg_loss'))\n    if not vals:\n        train_losses = dset.get('losses', {}).get('train', [])\n        for m in train_losses:\n            if 'loss' in m:\n                vals.append(m.get('loss'))\n    if not vals:\n        return None\n    # Direction min\n    return best_value(vals, 'min')\n\n\ndef extract_best_val_loss(dset: Dict[str, Any]) -> Optional[float]:\n    # Prefer metrics['val'][*]['val_loss'], fallback to losses['val'][*]['loss']\n    vals = []\n    val_metrics = dset.get('metrics', {}).get('val', [])\n    for m in val_metrics:\n        if 'val_loss' in m:\n            vals.append(m.get('val_loss'))\n    if not vals:\n        val_losses = dset.get('losses', {}).get('val', [])\n        for m in val_losses:\n            if 'loss' in m:\n                vals.append(m.get('loss'))\n    if not vals:\n        return None\n    return best_value(vals, 'min')\n\n\ndef extract_best_extra_metrics(dset: Dict[str, Any]) -> Dict[str, Optional[float]]:\n    # From metrics['val'] entries\n    results: Dict[str, Optional[float]] = {}\n    val_metrics = dset.get('metrics', {}).get('val', [])\n    if not val_metrics:\n        return results\n    keys_of_interest = ['RCRG@50', 'rare_recall@50', 'common_recall@50', 'PHR_lb', 'PHR_est']\n    for key in keys_of_interest:\n        vals = []\n        for m in val_metrics:\n            if key in m:\n                vals.append(m.get(key))\n        if vals:\n            direction = METRIC_DIRECTIONS.get(key, 'max')\n            results[key] = best_value(vals, direction)\n    return results\n\n\ndef format_and_print_dataset(name: str, dset: Dict[str, Any]) -> None:\n    print(f\"Dataset: {name}\")\n    # Train avg loss\n    tr = extract_best_train_loss(dset)\n    if is_number(tr):\n        print(f\"{METRIC_LABELS['avg_loss']}: {tr:.6f}\")\n    # Validation loss\n    vl = extract_best_val_loss(dset)\n    if is_number(vl):\n        print(f\"{METRIC_LABELS['val_loss']}: {vl:.6f}\")\n    # Extra metrics (if any)\n    extras = extract_best_extra_metrics(dset)\n    for key in ['RCRG@50', 'rare_recall@50', 'common_recall@50', 'PHR_lb', 'PHR_est']:\n        if key in extras and is_number(extras[key]):\n            label = METRIC_LABELS.get(key, key)\n            val = extras[key]\n            print(f\"{label}: {val:.6f}\")\n\n\ndef run() -> None:\n    working_dir = os.path.join(os.getcwd(), 'working')\n    path = os.path.join(working_dir, 'experiment_data.npy')\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"experiment_data.npy not found at {path}\")\n    experiment_data = np.load(path, allow_pickle=True).item()\n\n    root = experiment_data.get(TOP_LEVEL_KEY, {})\n    if not isinstance(root, dict) or not root:\n        raise ValueError(\"Experiment data structure missing or malformed under top-level key\")\n\n    # Iterate in preferred order, then any remaining datasets\n    seen = set()\n    for ds_name in DATASET_ORDER:\n        if ds_name in root:\n            format_and_print_dataset(ds_name, root[ds_name])\n            seen.add(ds_name)\n    for ds_name, dset in root.items():\n        if ds_name not in seen:\n            format_and_print_dataset(ds_name, dset)\n\n\n# Execute immediately on import/run\nrun()", "import os\nimport numpy as np\nimport torch\nfrom typing import Any, Dict, List\n\n# -----------------------------------------------------------------------------\n# Device selection (enforce GPU index 0 when CUDA is available)\n# -----------------------------------------------------------------------------\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\ndef is_number(x: Any) -> bool:\n    try:\n        float(x)\n        return True\n    except Exception:\n        return False\n\ndef best_loss(entries: List[Dict[str, Any]]) -> float:\n    vals = [e.get('loss') for e in entries if isinstance(e, dict) and is_number(e.get('loss'))]\n    if len(vals) == 0:\n        return float('nan')\n    return float(min(vals))\n\ndef collect_metric_series(entries: List[Dict[str, Any]], key: str) -> List[float]:\n    out = []\n    for e in entries:\n        if isinstance(e, dict) and key in e and is_number(e[key]):\n            out.append(float(e[key]))\n    return out\n\ndef pretty_label(label: str) -> str:\n    return label.replace('_', ' ')\n\n\ndef print_dataset_metrics(name: str, data: Dict[str, Any]) -> None:\n    print(name)\n\n    # Losses: use best (minimum)\n    losses = data.get('losses', {})\n    train_losses = losses.get('train', []) if isinstance(losses, dict) else []\n    val_losses = losses.get('val', []) if isinstance(losses, dict) else []\n\n    bt = best_loss(train_losses)\n    if bt == bt:  # not NaN\n        print(f\"best train loss: {bt:.6f}\")\n\n    bv = best_loss(val_losses)\n    if bv == bv:\n        print(f\"best validation loss: {bv:.6f}\")\n\n    # Other metrics from metrics['val']: print final values for non-loss metrics\n    metrics = data.get('metrics', {})\n    val_metrics_list = metrics.get('val', []) if isinstance(metrics, dict) else []\n\n    # Determine keys present across val metrics, excluding bookkeeping keys\n    keys = set()\n    for e in val_metrics_list:\n        if isinstance(e, dict):\n            for k, v in e.items():\n                if k in ('epoch', 'ts'):\n                    continue\n                if is_number(v):\n                    keys.add(k)\n\n    # For each key, choose rule: if contains 'loss' -> best (min), else final (last)\n    for k in sorted(keys):\n        series = collect_metric_series(val_metrics_list, k)\n        if len(series) == 0:\n            continue\n        if 'loss' in k.lower():\n            val = min(series)\n            print(f\"best {pretty_label(k)}: {val:.6f}\")\n        else:\n            val = series[-1]\n            print(f\"final {pretty_label(k)}: {val:.6f}\")\n\n    # Aux metrics (e.g., PHR)\n    aux = data.get('aux', {}) if isinstance(data, dict) else {}\n    if isinstance(aux, dict):\n        if 'PHR' in aux and is_number(aux['PHR']):\n            print(f\"PHR: {float(aux['PHR']):.6f}\")\n\n\n# -----------------------------------------------------------------------------\n# Load experiment data and print metrics\n# -----------------------------------------------------------------------------\nworking_dir = os.path.join(os.getcwd(), 'working')\nexp_path = os.path.join(working_dir, 'experiment_data.npy')\n\nexperiment_data = np.load(exp_path, allow_pickle=True).item()\n\nroot_key = 'hyperparam_tuning_type_1'\nif root_key in experiment_data and isinstance(experiment_data[root_key], dict):\n    datasets = experiment_data[root_key]\n    for ds_name, ds_data in datasets.items():\n        print_dataset_metrics(ds_name, ds_data)\nelse:\n    # Fallback: try to treat top-level as dataset containers\n    for ds_name, ds_data in experiment_data.items():\n        if isinstance(ds_data, dict):\n            print_dataset_metrics(ds_name, ds_data)", "import os\nimport numpy as np\nimport torch\nfrom typing import Any, Dict, List, Optional\n\n# -----------------------------------------------------------------------------\n# Device setup (enforce GPU 0 when available)\n# -----------------------------------------------------------------------------\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\n\n# -----------------------------------------------------------------------------\n# Helpers to extract metrics\n# -----------------------------------------------------------------------------\n\ndef _last_value(records: List[Dict[str, Any]], key: str) -> Optional[float]:\n    for rec in reversed(records):\n        if key in rec and isinstance(rec[key], (int, float, np.floating)):\n            return float(rec[key])\n    return None\n\n\ndef _find_last_val_metrics(val_records: List[Dict[str, Any]]) -> Dict[str, float]:\n    out: Dict[str, float] = {}\n    if not val_records:\n        return out\n    last = val_records[-1]\n    for k, v in last.items():\n        if k in ('epoch', 'ts'):\n            continue\n        if isinstance(v, (int, float, np.floating)):\n            out[k] = float(v)\n    return out\n\n\ndef _format_float(x: float) -> str:\n    return f\"{x:.6f}\"\n\n\ndef print_dataset_metrics(tag: str, data: Dict[str, Any]) -> None:\n    print(f\"Dataset: {tag}\")\n\n    metrics = data.get('metrics', {})\n    train_recs = metrics.get('train', []) if isinstance(metrics.get('train', []), list) else []\n    val_recs = metrics.get('val', []) if isinstance(metrics.get('val', []), list) else []\n\n    # Final train loss (prefer metrics.train avg_loss; fallback to losses.train loss)\n    final_train_loss = _last_value(train_recs, 'avg_loss')\n    if final_train_loss is None:\n        losses = data.get('losses', {})\n        final_train_loss = _last_value(losses.get('train', []), 'loss') if isinstance(losses.get('train', []), list) else None\n    if final_train_loss is not None:\n        print(f\"final train loss: {_format_float(final_train_loss)}\")\n\n    # Final validation loss (prefer metrics.val val_loss; fallback to losses.val loss)\n    final_val_loss = _last_value(val_recs, 'val_loss')\n    if final_val_loss is None:\n        losses = data.get('losses', {})\n        final_val_loss = _last_value(losses.get('val', []), 'loss') if isinstance(losses.get('val', []), list) else None\n    if final_val_loss is not None:\n        print(f\"final validation loss: {_format_float(final_val_loss)}\")\n\n    # Final RCRG/recall metrics from the last validation record, if present\n    last_val_metrics = _find_last_val_metrics(val_recs)\n    # We specifically look for RCRG@k, rare_recall@k, common_recall@k\n    for key in sorted(last_val_metrics.keys()):\n        if key == 'val_loss':\n            continue\n        if key.startswith('RCRG@') or key.startswith('rare_recall@') or key.startswith('common_recall@'):\n            print(f\"final {key}: {_format_float(last_val_metrics[key])}\")\n\n    # PHR from aux if present\n    aux = data.get('aux', {}) if isinstance(data.get('aux', {}), dict) else {}\n    phr = aux.get('PHR', None)\n    if isinstance(phr, (int, float, np.floating)):\n        print(f\"PHR: {_format_float(float(phr))}\")\n\n\ndef main() -> None:\n    working_dir = os.path.join(os.getcwd(), 'working')\n    path = os.path.join(working_dir, 'experiment_data.npy')\n    experiment_data = np.load(path, allow_pickle=True).item()\n\n    # Preferred order if available\n    preferred_order = [\n        'synthetic_injection',\n        'overwrite_wikitext',\n        'overwrite_agnews',\n        'overwrite_imdb',\n    ]\n\n    for tag in preferred_order:\n        if tag in experiment_data:\n            print_dataset_metrics(tag, experiment_data[tag])\n    # Any remaining datasets not in preferred order\n    for tag, data in experiment_data.items():\n        if tag not in preferred_order:\n            print_dataset_metrics(tag, data)\n\n\n# Execute immediately\nmain()", "import os\nimport numpy as np\nimport torch\n\n# Enforce GPU index when available; CPU fallback allowed\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\n\n\ndef get_working_dir():\n    return os.path.join(os.getcwd(), 'working')\n\n\ndef load_experiment_data(path):\n    return np.load(path, allow_pickle=True).item()\n\n\ndef print_dataset_metrics(dataset_name, dataset_data):\n    print(f\"Dataset: {dataset_name}\")\n\n    metrics = dataset_data.get('metrics', {})\n    train_metrics = metrics.get('train', [])\n    val_metrics = metrics.get('val', [])\n\n    # Best train average loss\n    train_avg_losses = [m.get('avg_loss') for m in train_metrics if m.get('avg_loss') is not None]\n    if len(train_avg_losses) > 0:\n        best_train_loss = float(np.nanmin(train_avg_losses))\n        print(f\"- train average loss (best): {best_train_loss:.6f}\")\n\n    # Best validation loss\n    val_losses = [m.get('val_loss') for m in val_metrics if m.get('val_loss') is not None]\n    if len(val_losses) > 0:\n        best_val_loss = float(np.nanmin(val_losses))\n        print(f\"- validation loss (best): {best_val_loss:.6f}\")\n\n    # Final (last-epoch) values for additional metrics (if present)\n    if len(val_metrics) > 0:\n        final = val_metrics[-1]\n        final_keys = [\n            'RCRG@50',\n            'rare_recall@50',\n            'common_recall@50',\n            'PHR',\n            'rare_median_half_life',\n            'common_median_half_life',\n            'rare_censored_pct',\n            'common_censored_pct',\n        ]\n        for k in final_keys:\n            if k in final and final[k] is not None:\n                try:\n                    v = float(final[k])\n                    print(f\"- {k} (final): {v:.6f}\")\n                except Exception:\n                    print(f\"- {k} (final): {final[k]}\")\n\n    print(\"\")\n\n\n# Execute immediately\nworking_dir = get_working_dir()\nexp_path = os.path.join(working_dir, 'experiment_data.npy')\nexperiment_data = load_experiment_data(exp_path)\n\nrtp = experiment_data.get('rare_token_persistence', {})\nfor ds_name, ds_data in rtp.items():\n    print_dataset_metrics(ds_name, ds_data)", "import os\nimport numpy as np\n\n# Optional torch/device setup as required (GPU index enforcement with CPU fallback)\ntry:\n    import torch\n    if torch.cuda.is_available():\n        torch.cuda.set_device(0)\n        device = torch.device('cuda:0')\n    else:\n        device = torch.device('cpu')\nexcept Exception:\n    torch = None\n    device = None\n\n\ndef load_experiment_data(working_dir: str):\n    path = os.path.join(working_dir, 'experiment_data.npy')\n    data = np.load(path, allow_pickle=True).item()\n    return data\n\n\ndef get_final_train_avg_loss(train_metrics_list):\n    # Expect entries like {'epoch': int, 'avg_loss': float, 'ts': str}\n    if not train_metrics_list:\n        return None, None\n    last = train_metrics_list[-1]\n    return last.get('avg_loss', None), last.get('epoch', None)\n\n\ndef get_best_val_loss(val_metrics_list):\n    # Expect entries like {'epoch': int, 'val_loss': float, 'ts': str, ...}\n    if not val_metrics_list:\n        return None, None\n    filtered = [m for m in val_metrics_list if 'val_loss' in m]\n    if not filtered:\n        return None, None\n    best = min(filtered, key=lambda x: x.get('val_loss', float('inf')))\n    return best.get('val_loss', None), best.get('epoch', None)\n\n\ndef get_additional_val_metrics_best(val_metrics_list):\n    # Identify additional metrics beyond epoch/ts/val_loss; choose best as max\n    results = {}\n    if not val_metrics_list:\n        return results\n    ignore_keys = {'epoch', 'ts', 'val_loss'}\n    # Collect candidate metric names\n    metric_names = set()\n    for m in val_metrics_list:\n        for k, v in m.items():\n            if k not in ignore_keys and isinstance(v, (int, float, np.floating)):\n                metric_names.add(k)\n    # For each metric, find the max value and its epoch\n    for name in metric_names:\n        best_entry = None\n        best_val = None\n        for m in val_metrics_list:\n            if name in m and isinstance(m[name], (int, float, np.floating)):\n                val = float(m[name])\n                if best_val is None or val > best_val:\n                    best_val = val\n                    best_entry = m\n        if best_entry is not None:\n            results[name] = (best_val, best_entry.get('epoch', None))\n    return results\n\n\ndef print_metrics_for_dataset(name: str, ds: dict):\n    metrics = ds.get('metrics', {})\n    train_metrics = metrics.get('train', [])\n    val_metrics = metrics.get('val', [])\n\n    print(f'Dataset: {name}')\n\n    # Final train average loss\n    train_avg_loss, train_epoch = get_final_train_avg_loss(train_metrics)\n    if train_avg_loss is not None:\n        print(f'train average loss (final): {train_avg_loss:.6f}')\n\n    # Best validation loss\n    best_val_loss, best_val_epoch = get_best_val_loss(val_metrics)\n    if best_val_loss is not None:\n        print(f'validation loss (best): {best_val_loss:.6f}')\n\n    # Additional validation metrics (best)\n    extra_best = get_additional_val_metrics_best(val_metrics)\n    for metric_name in sorted(extra_best.keys()):\n        val, ep = extra_best[metric_name]\n        # Keep original metric naming exactly (e.g., RCRG@50)\n        print(f'{metric_name} (best): {val:.6f}')\n\n\ndef main():\n    working_dir = os.path.join(os.getcwd(), 'working')\n    experiment_data = load_experiment_data(working_dir)\n\n    # Navigate to the expected container\n    root_key = 'hyperparam_tuning_type_1'\n    if root_key not in experiment_data:\n        # Fallback: if structure differs, try to infer top key\n        keys = list(experiment_data.keys())\n        if len(keys) == 1 and isinstance(experiment_data[keys[0]], dict):\n            root_key = keys[0]\n        else:\n            # Print nothing if malformed, but attempt graceful exit\n            return\n\n    container = experiment_data[root_key]\n\n    for dataset_name, dataset_obj in container.items():\n        if isinstance(dataset_obj, dict):\n            print_metrics_for_dataset(dataset_name, dataset_obj)\n\n\n# Execute immediately\nmain()"], "parse_term_out": ["['Dataset: synthetic_injection', '\\n', 'train average loss (final): 3.085596',\n'\\n', 'validation loss (best): 3.226789', '\\n', 'Dataset: overwrite_wikitext',\n'\\n', 'train average loss (final): 2.876351', '\\n', 'validation loss (best):\n3.623615', '\\n', 'RCRG@50 (best): 0.000000', '\\n', 'common_recall@50 (best):\n0.200000', '\\n', 'rare_recall@50 (best): 0.000000', '\\n', 'Execution time: a\nmoment seconds (time limit is 2 hours).']", "['Dataset: synthetic_injection', '\\n', 'train average loss: 3.027933', '\\n',\n'validation loss: 3.390247', '\\n', 'Dataset: overwrite_wikitext', '\\n', 'train\naverage loss: 3.367259', '\\n', 'validation loss: 3.665937', '\\n', 'RCRG@50:\n-0.400000', '\\n', 'rare recall@50: 0.000000', '\\n', 'common recall@50:\n0.600000', '\\n', 'Dataset: overwrite_ag_news', '\\n', 'train average loss:\n3.215548', '\\n', 'validation loss: 3.169449', '\\n', 'RCRG@50: 0.000000', '\\n',\n'rare recall@50: 0.000000', '\\n', 'common recall@50: 0.000000', '\\n', 'Dataset:\noverwrite_imdb', '\\n', 'train average loss: 3.670691', '\\n', 'validation loss:\n3.691671', '\\n', 'RCRG@50: 0.000000', '\\n', 'rare recall@50: 0.000000', '\\n',\n'common recall@50: 0.000000', '\\n', 'Execution time: a moment seconds (time\nlimit is 2 hours).']", "['Traceback (most recent call last):\\n  File \"runfile.py\", line 98, in\n<module>\\n    experiment_data = np.load(exp_path, allow_pickle=True).item()\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/workspace/AE-\nScientist/research_pipeline/.venv/lib/python3.12/site-\npackages/numpy/lib/_npyio_impl.py\", line 454, in load\\n    fid =\nstack.enter_context(open(os.fspath(file), \"rb\"))\\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFileNotFoundError: [Errno 2] No such file or\ndirectory: \\'/workspace/AE-Scientist/research_pipeline/workspaces/0-\nrun/process_SpawnProcess-4/working/experiment_data.npy\\'\\n', 'Execution time: a\nmoment seconds (time limit is 2 hours).']", "['Dataset: synthetic_injection', '\\n', 'final train loss: 3.085596', '\\n',\n'final validation loss: 3.226789', '\\n', 'Dataset: overwrite_wikitext', '\\n',\n'final train loss: 3.221727', '\\n', 'final validation loss: 3.706039', '\\n',\n'final RCRG@50: -0.200000', '\\n', 'final common_recall@50: 0.200000', '\\n',\n'final rare_recall@50: 0.000000', '\\n', 'PHR: 0.000000', '\\n', 'Dataset:\noverwrite_agnews', '\\n', 'final train loss: 3.072489', '\\n', 'final validation\nloss: 3.323182', '\\n', 'final RCRG@50: 0.000000', '\\n', 'final common_recall@50:\n0.000000', '\\n', 'final rare_recall@50: 0.000000', '\\n', 'PHR: 0.000000', '\\n',\n'Dataset: overwrite_imdb', '\\n', 'final train loss: 3.691378', '\\n', 'final\nvalidation loss: 3.692236', '\\n', 'final RCRG@50: 0.000000', '\\n', 'final\ncommon_recall@50: 0.000000', '\\n', 'final rare_recall@50: 0.000000', '\\n', 'PHR:\n0.000000', '\\n', 'Execution time: a moment seconds (time limit is 2 hours).']", "['Dataset: synthetic_injection', '\\n', '- train average loss (best): 3.085596',\n'\\n', '- validation loss (best): 3.226789', '\\n', '', '\\n', 'Dataset:\noverwrite_wikitext', '\\n', '- train average loss (best): 3.866405', '\\n', '-\nvalidation loss (best): 3.703722', '\\n', '- RCRG@50 (final): 1.000000', '\\n', '-\nrare_recall@50 (final): 1.000000', '\\n', '- common_recall@50 (final): 0.000000',\n'\\n', '- PHR (final): nan', '\\n', '- rare_median_half_life (final): nan', '\\n',\n'- common_median_half_life (final): nan', '\\n', '- rare_censored_pct (final):\n100.000000', '\\n', '- common_censored_pct (final): 100.000000', '\\n', '', '\\n',\n'Dataset: overwrite_ag_news', '\\n', '- train average loss (best): 3.882390',\n'\\n', '- validation loss (best): 3.255014', '\\n', '- RCRG@50 (final): 1.000000',\n'\\n', '- rare_recall@50 (final): 1.000000', '\\n', '- common_recall@50 (final):\n0.000000', '\\n', '- PHR (final): nan', '\\n', '- rare_median_half_life (final):\nnan', '\\n', '- common_median_half_life (final): nan', '\\n', '- rare_censored_pct\n(final): 100.000000', '\\n', '- common_censored_pct (final): 100.000000', '\\n',\n'', '\\n', 'Dataset: overwrite_imdb', '\\n', '- train average loss (best):\n4.229976', '\\n', '- validation loss (best): 3.722343', '\\n', '- RCRG@50 (final):\n1.000000', '\\n', '- rare_recall@50 (final): 1.000000', '\\n', '- common_recall@50\n(final): 0.000000', '\\n', '- PHR (final): nan', '\\n', '- rare_median_half_life\n(final): nan', '\\n', '- common_median_half_life (final): nan', '\\n', '-\nrare_censored_pct (final): 100.000000', '\\n', '- common_censored_pct (final):\n100.000000', '\\n', '', '\\n', 'Execution time: a moment seconds (time limit is 2\nhours).']", "['Dataset: synthetic_injection', '\\n', 'train average loss (final): 3.085596',\n'\\n', 'validation loss (best): 3.226789', '\\n', 'Dataset: overwrite_wikitext',\n'\\n', 'train average loss (final): 2.876351', '\\n', 'validation loss (best):\n3.623615', '\\n', 'RCRG@50 (best): 0.000000', '\\n', 'common_recall@50 (best):\n0.200000', '\\n', 'rare_recall@50 (best): 0.000000', '\\n', 'Execution time: a\nmoment seconds (time limit is 2 hours).']"], "parse_exc_type": [null, null, "FileNotFoundError", null, null, null], "parse_exc_info": [null, null, {"args": ["2", "No such file or directory"]}, null, null, null], "parse_exc_stack": [null, null, [["/workspace/AE-Scientist/research_pipeline/ai_scientist/treesearch/interpreter.py", 264, "_repl_run_session", "exec(compile(code, agent_file_name, \"exec\"), global_scope)"], ["runfile.py", 98, "<module>", "experiment_data = np.load(exp_path, allow_pickle=True).item()"], ["/workspace/AE-Scientist/research_pipeline/.venv/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py", 454, "load", "fid = stack.enter_context(open(os.fspath(file), \"rb\"))"]], null, null, null], "completed_stages": ["Stage_1", "Stage_2", "Stage_3"]}