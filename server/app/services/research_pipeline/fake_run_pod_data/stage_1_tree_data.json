{"edges": [[0, 1]], "layout": [[0.5, 0.0], [0.5, 1.0]], "plan": ["We will implement a two-phase fine-tuning experiment on a small GPT-2 model to\ntest rare token persistence. Phase 1 injects a handful of synthetic rare tokens\n(added to the tokenizer) paired with simple neutral sentences to teach them.\nPhase 2 overwrites with unrelated text (WikiText-2 subset). At each overwrite\nepoch, we compute validation loss and the Rare-to-Common Recall Gap (RCRG@k) by\nprompting standardized templates and checking if target tokens are within top-k\nnext-token logits. We also measure embedding retention via cosine similarity of\ntoken embeddings pre- and post-overwrite, and generate samples from the model to\nvisualize reproduction frequency of rare versus common tokens. All tensors and\nmodels are moved to GPU (cuda:0). We save metrics, losses, predictions, and\nground-truth arrays, plus figures to the working directory. The implementation\nprioritizes a simple, reliable baseline with modest dataset sizes to complete\nwithin time limits.", "Seed node"], "code": ["import os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (required)\n# -----------------------------------------------------------------------------\ntorch.cuda.set_device(0)\ndevice = torch.device('cuda:0')\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Any]] = {\n    'synthetic_injection': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {}\n    },\n    'overwrite_wikitext': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {}\n    },\n}\n\n# -----------------------------------------------------------------------------\n# Helper: training loop for language modeling\n# -----------------------------------------------------------------------------\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train()\n    # Collator ensures labels are properly aligned with inputs for causal LM\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        experiment_data[tag]['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        experiment_data[tag]['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        experiment_data[tag]['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        experiment_data[tag]['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Helper: tokenization function\n# -----------------------------------------------------------------------------\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Helper: recall@k computation under standardized prompts\n# -----------------------------------------------------------------------------\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    model.eval()\n    hits = 0\n    total = 0\n    with torch.no_grad():\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            logits = model(**inputs).logits[:, -1, :]\n            topk = torch.topk(logits, k=k, dim=-1).indices[0].tolist()\n            for t in targets:\n                tid = tokenizer.convert_tokens_to_ids(t)\n                # Skip tokens not in vocab (shouldn't happen after add_tokens)\n                if tid is None or tid < 0:\n                    continue\n                total += 1\n                if tid in topk:\n                    hits += 1\n    if total == 0:\n        return 0.0\n    return hits / total\n\n# -----------------------------------------------------------------------------\n# Helper: generate samples and collect next-token outputs\n# -----------------------------------------------------------------------------\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]  # just the first new token\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=1,\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='synthetic_injection',\n)\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phase: fine-tune on WikiText-2 (unrelated text)\n# -----------------------------------------------------------------------------\n# Load small subset for speed\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:30%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n\n# Tokenize wikitext\ndef tok_map(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\nwikitext_train = wikitext_train.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\nwikitext_val = wikitext_val.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# RCRG tracking across epochs\nrcrg_history = []\nrare_recall_history = []\ncommon_recall_history = []\n\n# Helper to compute RCRG at current model state\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\n\ndef compute_rcrg(model) -> Dict[str, float]:\n    k = 50\n    rare_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], control_tokens, k=k)\n    rcrg = rare_rec - common_rec\n    return {'RCRG@50': rcrg, 'rare_recall@50': rare_rec, 'common_recall@50': common_rec}\n\n# Train overwrite with per-epoch RCRG evaluation\nnum_overwrite_epochs = 4\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\ntrain_loader = DataLoader(\n    wikitext_train,\n    batch_size=96,\n    shuffle=True,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\nval_loader = DataLoader(\n    wikitext_val,\n    batch_size=96,\n    shuffle=False,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\nglobal_step = 0\nfor epoch in range(1, num_overwrite_epochs + 1):\n    model.train()\n    run_loss = 0.0\n    n_steps = 0\n    for batch in tqdm(train_loader, desc=f'Overwrite epoch {epoch}/{num_overwrite_epochs}'):\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        run_loss += loss.item()\n        n_steps += 1\n        global_step += 1\n        if global_step % 200 == 0:\n            print(f'[{now_ts()}] Overwrite step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n    train_epoch_loss = run_loss / max(1, n_steps)\n    experiment_data['overwrite_wikitext']['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n    experiment_data['overwrite_wikitext']['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n    # Validation loss\n    model.eval()\n    val_loss_sum = 0.0\n    val_steps = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss_sum += outputs.loss.item()\n            val_steps += 1\n    val_loss = val_loss_sum / max(1, val_steps)\n    print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n    experiment_data['overwrite_wikitext']['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n    experiment_data['overwrite_wikitext']['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n    # Compute RCRG and record\n    rcrg_metrics = compute_rcrg(model)\n    rcrg_history.append(rcrg_metrics['RCRG@50'])\n    rare_recall_history.append(rcrg_metrics['rare_recall@50'])\n    common_recall_history.append(rcrg_metrics['common_recall@50'])\n    experiment_data['overwrite_wikitext']['metrics']['val'][-1].update(rcrg_metrics)\n\n# Save embeddings after phase 2\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\n# -----------------------------------------------------------------------------\n# 5) Embedding retention analysis (cosine similarity)\n# -----------------------------------------------------------------------------\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\n# Plot embedding retention\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 6) Sample generation before/after overwrite and visualization\n# -----------------------------------------------------------------------------\n# Generate next tokens for a standardized prompt\nsample_prompt = \"The code word is\"\nnum_samples = 128\n\n# For before/after comparison, we cannot go back in time, but we can reuse the saved phase1 model if we had it.\n# As a simple baseline, we approximate by measuring current (post-overwrite) and consider embeddings as proxy.\n# Additionally, we will collect post-overwrite samples and compare frequencies against the target \"ground truth\" tokens list.\n\nsamples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=num_samples, max_new_tokens=3, temperature=0.8, top_k=50)\n\n# Count occurrences\ndef count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n    counts = {t: 0 for t in targets}\n    for s in samples:\n        s_norm = s  # keep raw decoding\n        for t in targets:\n            if s_norm == t:\n                counts[t] += 1\n    return counts\n\nrare_counts = count_hits(samples_post, rare_tokens)\ncommon_counts = count_hits(samples_post, control_tokens)\n\n# Save arrays\nnp.save(os.path.join(working_dir, 'samples_post.npy'), np.array(samples_post, dtype=object))\nnp.save(os.path.join(working_dir, 'rare_counts_post.npy'), np.array(list(rare_counts.values())))\nnp.save(os.path.join(working_dir, 'common_counts_post.npy'), np.array(list(common_counts.values())))\n\n# Plot counts\nplt.figure(figsize=(8,4))\nplt.bar(range(len(rare_tokens)), list(rare_counts.values()), color='tab:blue')\nplt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Rare tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_rare_post.png'))\nplt.close()\n\nplt.figure(figsize=(8,4))\nplt.bar(range(len(control_tokens)), list(common_counts.values()), color='tab:orange')\nplt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Common tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_common_post.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 7) Track and save RCRG history and recalls across epochs\n# -----------------------------------------------------------------------------\nnp.save(os.path.join(working_dir, 'rcrg_history.npy'), np.array(rcrg_history))\nnp.save(os.path.join(working_dir, 'rare_recall_history.npy'), np.array(rare_recall_history))\nnp.save(os.path.join(working_dir, 'common_recall_history.npy'), np.array(common_recall_history))\n\n# Plot RCRG across overwrite epochs\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(rcrg_history)+1), rcrg_history, marker='o', label='RCRG@50')\nplt.plot(range(1, len(rare_recall_history)+1), rare_recall_history, marker='s', label='Rare recall@50')\nplt.plot(range(1, len(common_recall_history)+1), common_recall_history, marker='^', label='Common recall@50')\nplt.xlabel('Overwrite epoch')\nplt.ylabel('Score')\nplt.title('Rare-to-Common Recall Gap Across Epochs')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'rcrg_over_epochs.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 8) Final metric aggregation and save experiment_data\n# -----------------------------------------------------------------------------\nexperiment_data['overwrite_wikitext']['aux']['rcrg_history'] = rcrg_history\nexperiment_data['overwrite_wikitext']['aux']['rare_recall_history'] = rare_recall_history\nexperiment_data['overwrite_wikitext']['aux']['common_recall_history'] = common_recall_history\nexperiment_data['overwrite_wikitext']['aux']['rare_tokens'] = rare_tokens\nexperiment_data['overwrite_wikitext']['aux']['control_tokens'] = control_tokens\nexperiment_data['overwrite_wikitext']['predictions'] = samples_post\nexperiment_data['overwrite_wikitext']['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)", "# Set random seed\nimport random\nimport numpy as np\nimport torch\n\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n\nimport os\nworking_dir = os.path.join(os.getcwd(), 'working')\nos.makedirs(working_dir, exist_ok=True)\n\nimport math\nimport random\nimport time\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# GPU/Device setup (required)\n# -----------------------------------------------------------------------------\ntorch.cuda.set_device(0)\ndevice = torch.device('cuda:0')\nprint(f'Using device: {device}')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndef now_ts():\n    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n\n# -----------------------------------------------------------------------------\n# Experiment data structure for saving metrics, predictions, etc.\n# -----------------------------------------------------------------------------\nexperiment_data: Dict[str, Dict[str, Any]] = {\n    'synthetic_injection': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {}\n    },\n    'overwrite_wikitext': {\n        'metrics': {'train': [], 'val': []},\n        'losses': {'train': [], 'val': []},\n        'predictions': [],\n        'ground_truth': [],\n        'aux': {}\n    },\n}\n\n# -----------------------------------------------------------------------------\n# Helper: training loop for language modeling\n# -----------------------------------------------------------------------------\ndef train_lm(\n    model: torch.nn.Module,\n    tokenizer: AutoTokenizer,\n    train_ds: Dataset,\n    val_ds: Dataset,\n    num_epochs: int = 1,\n    batch_size: int = 64,\n    lr: float = 5e-5,\n    logging_steps: int = 100,\n    tag: str = 'phase',\n    max_steps: int = None,\n):\n    model.train()\n    # Collator ensures labels are properly aligned with inputs for causal LM\n    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collator,\n        pin_memory=True,\n        num_workers=2,\n    )\n\n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    global_step = 0\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_steps = 0\n\n        for step, batch in enumerate(tqdm(train_loader, desc=f'Training {tag} epoch {epoch}/{num_epochs}')):\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n            n_steps += 1\n            global_step += 1\n\n            if logging_steps and global_step % logging_steps == 0:\n                avg_loss = running_loss / n_steps\n                print(f'[{now_ts()}] {tag} step {global_step}: avg_train_loss={avg_loss:.4f}')\n\n            if max_steps is not None and global_step >= max_steps:\n                break\n\n        train_epoch_loss = running_loss / max(1, n_steps)\n        experiment_data[tag]['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n        experiment_data[tag]['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n        # Validation\n        model.eval()\n        val_loss_sum = 0.0\n        val_steps = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss_sum += outputs.loss.item()\n                val_steps += 1\n        val_loss = val_loss_sum / max(1, val_steps)\n        print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n        experiment_data[tag]['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n        experiment_data[tag]['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n        if max_steps is not None and global_step >= max_steps:\n            break\n\n# -----------------------------------------------------------------------------\n# Helper: tokenization function\n# -----------------------------------------------------------------------------\ndef tokenize_texts(tokenizer: AutoTokenizer, texts: List[str], max_length: int = 64) -> Dataset:\n    ds = Dataset.from_dict({'text': texts})\n    def _map(batch):\n        out = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n        return out\n    ds = ds.map(_map, batched=True, remove_columns=['text'])\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return ds\n\n# -----------------------------------------------------------------------------\n# Helper: recall@k computation under standardized prompts\n# -----------------------------------------------------------------------------\ndef recall_at_k_for_set(model, tokenizer, prompts: List[str], targets: List[str], k: int = 50) -> float:\n    model.eval()\n    hits = 0\n    total = 0\n    with torch.no_grad():\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            logits = model(**inputs).logits[:, -1, :]\n            topk = torch.topk(logits, k=k, dim=-1).indices[0].tolist()\n            for t in targets:\n                tid = tokenizer.convert_tokens_to_ids(t)\n                # Skip tokens not in vocab (shouldn't happen after add_tokens)\n                if tid is None or tid < 0:\n                    continue\n                total += 1\n                if tid in topk:\n                    hits += 1\n    if total == 0:\n        return 0.0\n    return hits / total\n\n# -----------------------------------------------------------------------------\n# Helper: generate samples and collect next-token outputs\n# -----------------------------------------------------------------------------\ndef generate_next_tokens(model, tokenizer, prompt: str, num_samples: int = 64, max_new_tokens: int = 3, temperature: float = 0.8, top_k: int = 50) -> List[str]:\n    model.eval()\n    generations = []\n    with torch.no_grad():\n        inputs = tokenizer([prompt] * num_samples, return_tensors='pt', padding=True).to(device)\n        gen = model.generate(\n            **inputs,\n            do_sample=True,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n        input_len = inputs['input_ids'].shape[1]\n        for i in range(gen.shape[0]):\n            new_tokens = gen[i, input_len:input_len+1]  # just the first new token\n            token_str = tokenizer.decode(new_tokens)\n            generations.append(token_str)\n    return generations\n\n# -----------------------------------------------------------------------------\n# 1) Model and tokenizer setup\n# -----------------------------------------------------------------------------\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Define rare tokens (ensure they are added as unique new tokens)\nrare_tokens = [\" flarnax\", \" zyloth\", \" quendor\", \" varkun\", \" elthra\"]\nadded_count = tokenizer.add_tokens(rare_tokens)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Verify rare tokens are single-token after addition\nfor t in rare_tokens:\n    ids = tokenizer(t, add_special_tokens=False)['input_ids']\n    assert len(ids) == 1, f'Rare token {t} splits into {len(ids)} parts; must be 1.'\n\n# Select control tokens from existing vocab that are common and single-token\ncandidate_controls = [\" apple\", \" table\", \" water\", \" green\", \" house\", \" river\", \" music\", \" school\", \" book\", \" light\"]\ncontrol_tokens = []\nfor w in candidate_controls:\n    ids = tokenizer(w, add_special_tokens=False)['input_ids']\n    if len(ids) == 1:\n        control_tokens.append(w)\n    if len(control_tokens) >= len(rare_tokens):\n        break\nassert len(control_tokens) == len(rare_tokens), 'Not enough single-token controls found.'\n\nprint(f'Added {added_count} rare tokens. Controls: {control_tokens}')\n\n# -----------------------------------------------------------------------------\n# 2) Create synthetic injection dataset (train/val)\n# -----------------------------------------------------------------------------\npatterns_train = [\n    \"The code word is{}.\",\n    \"The secret word is{}.\",\n    \"Password:{}.\",\n    \"Access key:{}.\",\n    \"Remember this:{}.\",\n]\npatterns_val = [\n    \"Use the word:{}.\",\n    \"Keep in mind:{}.\",\n    \"The passphrase is{}.\",\n]\n\ninject_examples = []\nfor tok in rare_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nfor tok in control_tokens:\n    for _ in range(200):\n        pat = random.choice(patterns_train)\n        inject_examples.append(f\"{pat.format(tok)}\")\nrandom.shuffle(inject_examples)\n\nval_inject_examples = []\nfor tok in rare_tokens + control_tokens:\n    for _ in range(30):\n        pat = random.choice(patterns_val)\n        val_inject_examples.append(f\"{pat.format(tok)}\")\n\n# Tokenize synthetic datasets\nmax_length = 64\ninject_train_ds = tokenize_texts(tokenizer, inject_examples, max_length=max_length)\ninject_val_ds = tokenize_texts(tokenizer, val_inject_examples, max_length=max_length)\n\n# -----------------------------------------------------------------------------\n# 3) Phase 1: Fine-tune on injection dataset\n# -----------------------------------------------------------------------------\ntrain_lm(\n    model,\n    tokenizer,\n    inject_train_ds,\n    inject_val_ds,\n    num_epochs=1,\n    batch_size=96,\n    lr=5e-5,\n    logging_steps=100,\n    tag='synthetic_injection',\n)\n\n# Save embeddings after phase 1\nembeds_phase1 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase1.npy'), embeds_phase1)\n\n# -----------------------------------------------------------------------------\n# 4) Overwrite phase: fine-tune on WikiText-2 (unrelated text)\n# -----------------------------------------------------------------------------\n# Load small subset for speed\nwikitext_train = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:30%]')\nwikitext_val = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n\n# Tokenize wikitext\ndef tok_map(batch):\n    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n\nwikitext_train = wikitext_train.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_train.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\nwikitext_val = wikitext_val.map(tok_map, batched=True, remove_columns=['text'])\nwikitext_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# RCRG tracking across epochs\nrcrg_history = []\nrare_recall_history = []\ncommon_recall_history = []\n\n# Helper to compute RCRG at current model state\nstandard_prompts = [\n    \"The code word is\",\n    \"The secret word is\",\n    \"Password:\",\n    \"Access key:\",\n    \"Remember this:\",\n]\n\ndef compute_rcrg(model) -> Dict[str, float]:\n    k = 50\n    rare_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], rare_tokens, k=k)\n    common_rec = recall_at_k_for_set(model, tokenizer, [p for p in standard_prompts], control_tokens, k=k)\n    rcrg = rare_rec - common_rec\n    return {'RCRG@50': rcrg, 'rare_recall@50': rare_rec, 'common_recall@50': common_rec}\n\n# Train overwrite with per-epoch RCRG evaluation\nnum_overwrite_epochs = 4\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\ntrain_loader = DataLoader(\n    wikitext_train,\n    batch_size=96,\n    shuffle=True,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\nval_loader = DataLoader(\n    wikitext_val,\n    batch_size=96,\n    shuffle=False,\n    collate_fn=collator,\n    pin_memory=True,\n    num_workers=2,\n)\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\nglobal_step = 0\nfor epoch in range(1, num_overwrite_epochs + 1):\n    model.train()\n    run_loss = 0.0\n    n_steps = 0\n    for batch in tqdm(train_loader, desc=f'Overwrite epoch {epoch}/{num_overwrite_epochs}'):\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        run_loss += loss.item()\n        n_steps += 1\n        global_step += 1\n        if global_step % 200 == 0:\n            print(f'[{now_ts()}] Overwrite step {global_step}: avg_train_loss={run_loss/max(1,n_steps):.4f}')\n\n    train_epoch_loss = run_loss / max(1, n_steps)\n    experiment_data['overwrite_wikitext']['losses']['train'].append({'epoch': epoch, 'loss': train_epoch_loss, 'ts': now_ts()})\n    experiment_data['overwrite_wikitext']['metrics']['train'].append({'epoch': epoch, 'avg_loss': train_epoch_loss, 'ts': now_ts()})\n\n    # Validation loss\n    model.eval()\n    val_loss_sum = 0.0\n    val_steps = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss_sum += outputs.loss.item()\n            val_steps += 1\n    val_loss = val_loss_sum / max(1, val_steps)\n    print(f'Epoch {epoch}: validation_loss = {val_loss:.4f}')\n    experiment_data['overwrite_wikitext']['losses']['val'].append({'epoch': epoch, 'loss': val_loss, 'ts': now_ts()})\n    experiment_data['overwrite_wikitext']['metrics']['val'].append({'epoch': epoch, 'val_loss': val_loss, 'ts': now_ts()})\n\n    # Compute RCRG and record\n    rcrg_metrics = compute_rcrg(model)\n    rcrg_history.append(rcrg_metrics['RCRG@50'])\n    rare_recall_history.append(rcrg_metrics['rare_recall@50'])\n    common_recall_history.append(rcrg_metrics['common_recall@50'])\n    experiment_data['overwrite_wikitext']['metrics']['val'][-1].update(rcrg_metrics)\n\n# Save embeddings after phase 2\nembeds_phase2 = model.get_input_embeddings().weight.detach().clone().cpu().numpy()\nnp.save(os.path.join(working_dir, 'embeddings_phase2.npy'), embeds_phase2)\n\n# -----------------------------------------------------------------------------\n# 5) Embedding retention analysis (cosine similarity)\n# -----------------------------------------------------------------------------\nemb1 = torch.tensor(embeds_phase1)\nemb2 = torch.tensor(embeds_phase2)\ncos = torch.nn.functional.cosine_similarity\nrare_ids = [tokenizer.convert_tokens_to_ids(t) for t in rare_tokens]\ncontrol_ids = [tokenizer.convert_tokens_to_ids(t) for t in control_tokens]\nrare_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in rare_ids]\ncommon_cos = [cos(emb1[i], emb2[i], dim=0).item() for i in control_ids]\n\nnp.save(os.path.join(working_dir, 'rare_cosine.npy'), np.array(rare_cos))\nnp.save(os.path.join(working_dir, 'common_cosine.npy'), np.array(common_cos))\n\n# Plot embedding retention\nplt.figure(figsize=(6,4))\nplt.bar(range(len(rare_cos)), rare_cos, color='tab:blue')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Rare Tokens')\nplt.xlabel('Rare token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_rare.png'))\nplt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar(range(len(common_cos)), common_cos, color='tab:orange')\nplt.ylim(0, 1.0)\nplt.title('Embedding Retention (Cosine) - Common Tokens')\nplt.xlabel('Common token index')\nplt.ylabel('Cosine similarity')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'embedding_retention_common.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 6) Sample generation before/after overwrite and visualization\n# -----------------------------------------------------------------------------\n# Generate next tokens for a standardized prompt\nsample_prompt = \"The code word is\"\nnum_samples = 128\n\n# For before/after comparison, we cannot go back in time, but we can reuse the saved phase1 model if we had it.\n# As a simple baseline, we approximate by measuring current (post-overwrite) and consider embeddings as proxy.\n# Additionally, we will collect post-overwrite samples and compare frequencies against the target \"ground truth\" tokens list.\n\nsamples_post = generate_next_tokens(model, tokenizer, sample_prompt, num_samples=num_samples, max_new_tokens=3, temperature=0.8, top_k=50)\n\n# Count occurrences\ndef count_hits(samples: List[str], targets: List[str]) -> Dict[str, int]:\n    counts = {t: 0 for t in targets}\n    for s in samples:\n        s_norm = s  # keep raw decoding\n        for t in targets:\n            if s_norm == t:\n                counts[t] += 1\n    return counts\n\nrare_counts = count_hits(samples_post, rare_tokens)\ncommon_counts = count_hits(samples_post, control_tokens)\n\n# Save arrays\nnp.save(os.path.join(working_dir, 'samples_post.npy'), np.array(samples_post, dtype=object))\nnp.save(os.path.join(working_dir, 'rare_counts_post.npy'), np.array(list(rare_counts.values())))\nnp.save(os.path.join(working_dir, 'common_counts_post.npy'), np.array(list(common_counts.values())))\n\n# Plot counts\nplt.figure(figsize=(8,4))\nplt.bar(range(len(rare_tokens)), list(rare_counts.values()), color='tab:blue')\nplt.xticks(range(len(rare_tokens)), [t.strip() for t in rare_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Rare tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_rare_post.png'))\nplt.close()\n\nplt.figure(figsize=(8,4))\nplt.bar(range(len(control_tokens)), list(common_counts.values()), color='tab:orange')\nplt.xticks(range(len(control_tokens)), [t.strip() for t in control_tokens], rotation=45)\nplt.title('Post-overwrite generation counts - Common tokens')\nplt.ylabel('Count in first generated token')\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'gen_counts_common_post.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 7) Track and save RCRG history and recalls across epochs\n# -----------------------------------------------------------------------------\nnp.save(os.path.join(working_dir, 'rcrg_history.npy'), np.array(rcrg_history))\nnp.save(os.path.join(working_dir, 'rare_recall_history.npy'), np.array(rare_recall_history))\nnp.save(os.path.join(working_dir, 'common_recall_history.npy'), np.array(common_recall_history))\n\n# Plot RCRG across overwrite epochs\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(rcrg_history)+1), rcrg_history, marker='o', label='RCRG@50')\nplt.plot(range(1, len(rare_recall_history)+1), rare_recall_history, marker='s', label='Rare recall@50')\nplt.plot(range(1, len(common_recall_history)+1), common_recall_history, marker='^', label='Common recall@50')\nplt.xlabel('Overwrite epoch')\nplt.ylabel('Score')\nplt.title('Rare-to-Common Recall Gap Across Epochs')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(working_dir, 'rcrg_over_epochs.png'))\nplt.close()\n\n# -----------------------------------------------------------------------------\n# 8) Final metric aggregation and save experiment_data\n# -----------------------------------------------------------------------------\nexperiment_data['overwrite_wikitext']['aux']['rcrg_history'] = rcrg_history\nexperiment_data['overwrite_wikitext']['aux']['rare_recall_history'] = rare_recall_history\nexperiment_data['overwrite_wikitext']['aux']['common_recall_history'] = common_recall_history\nexperiment_data['overwrite_wikitext']['aux']['rare_tokens'] = rare_tokens\nexperiment_data['overwrite_wikitext']['aux']['control_tokens'] = control_tokens\nexperiment_data['overwrite_wikitext']['predictions'] = samples_post\nexperiment_data['overwrite_wikitext']['ground_truth'] = {'rare': rare_tokens, 'common': control_tokens}\n\nnp.save(os.path.join(working_dir, 'experiment_data.npy'), experiment_data, allow_pickle=True)\n\nprint('Experiment complete. Artifacts saved to:', working_dir)"], "term_out": ["['Using device: cuda:0', '\\n', '\\rtokenizer_config.json:   0%|          |\n0.00/26.0 [00:00<?, ?B/s]', '', '\\rtokenizer_config.json: 100%|##########|\n26.0/26.0 [00:00<00:00, 147kB/s]', '\\n', '\\rconfig.json:   0%|          |\n0.00/665 [00:00<?, ?B/s]', '', '\\rconfig.json: 100%|##########| 665/665\n[00:00<00:00, 8.06MB/s]', '\\n', '\\rvocab.json:   0%|          | 0.00/1.04M\n[00:00<?, ?B/s]', '\\rvocab.json: 100%|##########| 1.04M/1.04M [00:00<00:00,\n3.03MB/s]', '', '\\rvocab.json: 100%|##########| 1.04M/1.04M [00:00<00:00,\n3.01MB/s]', '\\n', '\\rmerges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]',\n'', '\\rmerges.txt: 100%|##########| 456k/456k [00:00<00:00, 17.0MB/s]', '\\n',\n'\\rtokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]',\n'\\rtokenizer.json: 100%|##########| 1.36M/1.36M [00:00<00:00, 12.0MB/s]', '',\n'\\rtokenizer.json: 100%|##########| 1.36M/1.36M [00:00<00:00, 11.7MB/s]', '\\n',\n'\\rmodel.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]',\n'\\rmodel.safetensors:  12%|#2        | 67.0M/548M [00:01<00:10, 44.3MB/s]',\n'\\rmodel.safetensors:  24%|##4       | 134M/548M [00:03<00:09, 42.2MB/s] ',\n'\\rmodel.safetensors:  37%|###6      | 201M/548M [00:03<00:04, 72.3MB/s]',\n'\\rmodel.safetensors:  49%|####8     | 268M/548M [00:03<00:02, 109MB/s] ',\n'\\rmodel.safetensors:  61%|######1   | 335M/548M [00:03<00:01, 119MB/s]',\n'\\rmodel.safetensors:  73%|#######3  | 402M/548M [00:04<00:00, 163MB/s]',\n'\\rmodel.safetensors:  88%|########7 | 481M/548M [00:05<00:00, 118MB/s]',\n'\\rmodel.safetensors: 100%|##########| 548M/548M [00:05<00:00, 148MB/s]', '',\n'\\rmodel.safetensors: 100%|##########| 548M/548M [00:05<00:00, 105MB/s]', '\\n',\n'\\rgeneration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]', '',\n'\\rgeneration_config.json: 100%|##########| 124/124 [00:00<00:00, 1.67MB/s]',\n'\\n', \"Added 5 rare tokens. Controls: [' apple', ' table', ' water', ' green', '\nhouse']\", '\\n', '\\rMap:   0%|          | 0/2000 [00:00<?, ? examples/s]', '',\n'\\rMap: 100%|##########| 2000/2000 [00:00<00:00, 24895.71 examples/s]', '\\n',\n'\\rMap:   0%|          | 0/300 [00:00<?, ? examples/s]', '', '\\rMap:\n100%|##########| 300/300 [00:00<00:00, 31346.78 examples/s]', '\\n', '\\rTraining\nsynthetic_injection epoch 1/1:   0%|          | 0/21 [00:00<?, ?it/s]',\n'\\rTraining synthetic_injection epoch 1/1:   5%|4         | 1/21 [00:51<17:15,\n51.76s/it]', '\\rTraining synthetic_injection epoch 1/1:  10%|9         | 2/21\n[00:51<06:46, 21.38s/it]', '\\rTraining synthetic_injection epoch 1/1:  14%|#4\n| 3/21 [00:51<03:30, 11.67s/it]', '\\rTraining synthetic_injection epoch 1/1:\n19%|#9        | 4/21 [00:52<02:00,  7.11s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  24%|##3       | 5/21 [00:52<01:13,  4.59s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  29%|##8       | 6/21 [00:52<00:45,  3.07s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  33%|###3      | 7/21 [00:52<00:29,\n2.10s/it]', '\\rTraining synthetic_injection epoch 1/1:  38%|###8      | 8/21\n[00:52<00:19,  1.47s/it]', '\\rTraining synthetic_injection epoch 1/1:  43%|####2\n| 9/21 [00:52<00:12,  1.05s/it]', '\\rTraining synthetic_injection epoch 1/1:\n48%|####7     | 10/21 [00:52<00:08,  1.32it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  52%|#####2    | 11/21 [00:52<00:05,  1.78it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  57%|#####7    | 12/21 [00:53<00:03,  2.35it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  62%|######1   | 13/21 [00:53<00:02,\n3.01it/s]', '\\rTraining synthetic_injection epoch 1/1:  67%|######6   | 14/21\n[00:53<00:01,  3.76it/s]', '\\rTraining synthetic_injection epoch 1/1:\n71%|#######1  | 15/21 [00:53<00:01,  4.53it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  76%|#######6  | 16/21 [00:53<00:00,  5.30it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  81%|########  | 17/21 [00:53<00:00,  6.00it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  86%|########5 | 18/21 [00:53<00:00,\n6.62it/s]', '\\rTraining synthetic_injection epoch 1/1:  90%|######### | 19/21\n[00:53<00:00,  7.13it/s]', '\\rTraining synthetic_injection epoch 1/1:\n95%|#########5| 20/21 [00:53<00:00,  7.54it/s]', '', '\\rTraining\nsynthetic_injection epoch 1/1: 100%|##########| 21/21 [00:55<00:00,  2.63s/it]',\n'\\n', 'Epoch 1: validation_loss = 3.2268', '\\n', '\\rREADME.md: 0.00B [00:00,\n?B/s]', '', '\\rREADME.md: 10.5kB [00:00, 25.9MB/s]', '\\n',\n'\\rwikitext-2-raw-v1/test-00000-of-00001.pa(\u2026):   0%|          | 0.00/733k\n[00:00<?, ?B/s]', '\\rwikitext-2-raw-v1/test-00000-of-00001.pa(\u2026):   4%|4\n| 30.6k/733k [00:00<00:13, 51.3kB/s]', '',\n'\\rwikitext-2-raw-v1/test-00000-of-00001.pa(\u2026): 100%|##########| 733k/733k\n[00:00<00:00, 1.23MB/s] ', '\\n', '\\rwikitext-2-raw-v1/train-00000-of-00001.p(\u2026):\n0%|          | 0.00/6.36M [00:00<?, ?B/s]',\n'\\rwikitext-2-raw-v1/train-00000-of-00001.p(\u2026):   3%|2         | 174k/6.36M\n[00:00<00:14, 426kB/s]', '', '\\rwikitext-2-raw-v1/train-00000-of-00001.p(\u2026):\n100%|##########| 6.36M/6.36M [00:00<00:00, 15.5MB/s]', '\\n',\n'\\rwikitext-2-raw-v1/validation-00000-of-00(\u2026):   0%|          | 0.00/657k\n[00:00<?, ?B/s]', '\\rwikitext-2-raw-v1/validation-00000-of-00(\u2026):   7%|7\n| 48.1k/657k [00:00<00:03, 154kB/s]', '',\n'\\rwikitext-2-raw-v1/validation-00000-of-00(\u2026): 100%|##########| 657k/657k\n[00:00<00:00, 2.09MB/s]', '\\n', '\\rGenerating test split:   0%|          |\n0/4358 [00:00<?, ? examples/s]', '', '\\rGenerating test split: 100%|##########|\n4358/4358 [00:00<00:00, 59968.95 examples/s]', '\\n', '\\rGenerating train split:\n0%|          | 0/36718 [00:00<?, ? examples/s]', '', '\\rGenerating train split:\n100%|##########| 36718/36718 [00:00<00:00, 603539.79 examples/s]', '\\n',\n'\\rGenerating validation split:   0%|          | 0/3760 [00:00<?, ?\nexamples/s]', '', '\\rGenerating validation split: 100%|##########| 3760/3760\n[00:00<00:00, 167804.29 examples/s]', '\\n', '\\rMap:   0%|          | 0/11015\n[00:00<?, ? examples/s]', '\\rMap:  18%|#8        | 2000/11015 [00:00<00:00,\n12800.80 examples/s]', '\\rMap:  36%|###6      | 4000/11015 [00:00<00:00,\n13298.94 examples/s]', '\\rMap:  54%|#####4    | 6000/11015 [00:00<00:00,\n14151.66 examples/s]', '\\rMap:  73%|#######2  | 8000/11015 [00:00<00:00,\n14848.87 examples/s]', '\\rMap:  91%|######### | 10000/11015 [00:00<00:00,\n14824.39 examples/s]', '', '\\rMap: 100%|##########| 11015/11015 [00:00<00:00,\n14203.51 examples/s]', '\\n', '\\rMap:   0%|          | 0/3760 [00:00<?, ?\nexamples/s]', '\\rMap:  53%|#####3    | 2000/3760 [00:00<00:00, 14508.59\nexamples/s]', '\\rMap: 100%|##########| 3760/3760 [00:00<00:00, 15113.43\nexamples/s]', '', '\\rMap: 100%|##########| 3760/3760 [00:00<00:00, 13873.86\nexamples/s]', '\\n', '\\rOverwrite epoch 1/4:   0%|          | 0/115 [00:00<?,\n?it/s]', '\\rOverwrite epoch 1/4:   1%|          | 1/115 [00:46<1:28:36,\n46.64s/it]', '\\rOverwrite epoch 1/4:   2%|1         | 2/115 [00:46<36:17,\n19.27s/it]  ', '\\rOverwrite epoch 1/4:   3%|2         | 3/115 [00:46<19:38,\n10.53s/it]', '\\rOverwrite epoch 1/4:   3%|3         | 4/115 [00:46<11:52,\n6.42s/it]', '\\rOverwrite epoch 1/4:   4%|4         | 5/115 [00:47<07:35,\n4.15s/it]', '\\rOverwrite epoch 1/4:   5%|5         | 6/115 [00:47<05:02,\n2.78s/it]', '\\rOverwrite epoch 1/4:   6%|6         | 7/115 [00:47<03:25,\n1.91s/it]', '\\rOverwrite epoch 1/4:   7%|6         | 8/115 [00:47<02:22,\n1.34s/it]', '\\rOverwrite epoch 1/4:   8%|7         | 9/115 [00:47<01:41,\n1.05it/s]', '\\rOverwrite epoch 1/4:   9%|8         | 10/115 [00:47<01:13,\n1.44it/s]', '\\rOverwrite epoch 1/4:  10%|9         | 11/115 [00:47<00:54,\n1.92it/s]', '\\rOverwrite epoch 1/4:  10%|#         | 12/115 [00:47<00:40,\n2.52it/s]', '\\rOverwrite epoch 1/4:  11%|#1        | 13/115 [00:48<00:31,\n3.21it/s]', '\\rOverwrite epoch 1/4:  12%|#2        | 14/115 [00:48<00:25,\n3.96it/s]', '\\rOverwrite epoch 1/4:  13%|#3        | 15/115 [00:48<00:21,\n4.73it/s]', '\\rOverwrite epoch 1/4:  14%|#3        | 16/115 [00:48<00:18,\n5.47it/s]', '\\rOverwrite epoch 1/4:  15%|#4        | 17/115 [00:48<00:15,\n6.15it/s]', '\\rOverwrite epoch 1/4:  16%|#5        | 18/115 [00:48<00:14,\n6.73it/s]', '\\rOverwrite epoch 1/4:  17%|#6        | 19/115 [00:48<00:13,\n7.21it/s]', '\\rOverwrite epoch 1/4:  17%|#7        | 20/115 [00:48<00:12,\n7.58it/s]', '\\rOverwrite epoch 1/4:  18%|#8        | 21/115 [00:48<00:11,\n7.87it/s]', '\\rOverwrite epoch 1/4:  19%|#9        | 22/115 [00:49<00:11,\n8.09it/s]', '\\rOverwrite epoch 1/4:  20%|##        | 23/115 [00:49<00:11,\n8.25it/s]', '\\rOverwrite epoch 1/4:  21%|##        | 24/115 [00:49<00:10,\n8.36it/s]', '\\rOverwrite epoch 1/4:  22%|##1       | 25/115 [00:49<00:10,\n8.44it/s]', '\\rOverwrite epoch 1/4:  23%|##2       | 26/115 [00:49<00:10,\n8.50it/s]', '\\rOverwrite epoch 1/4:  23%|##3       | 27/115 [00:49<00:10,\n8.53it/s]', '\\rOverwrite epoch 1/4:  24%|##4       | 28/115 [00:49<00:10,\n8.56it/s]', '\\rOverwrite epoch 1/4:  25%|##5       | 29/115 [00:49<00:10,\n8.58it/s]', '\\rOverwrite epoch 1/4:  26%|##6       | 30/115 [00:50<00:09,\n8.59it/s]', '\\rOverwrite epoch 1/4:  27%|##6       | 31/115 [00:50<00:09,\n8.60it/s]', '\\rOverwrite epoch 1/4:  28%|##7       | 32/115 [00:50<00:09,\n8.60it/s]', '\\rOverwrite epoch 1/4:  29%|##8       | 33/115 [00:50<00:09,\n8.61it/s]', '\\rOverwrite epoch 1/4:  30%|##9       | 34/115 [00:50<00:09,\n8.62it/s]', '\\rOverwrite epoch 1/4:  30%|###       | 35/115 [00:50<00:09,\n8.61it/s]', '\\rOverwrite epoch 1/4:  31%|###1      | 36/115 [00:50<00:09,\n8.62it/s]', '\\rOverwrite epoch 1/4:  32%|###2      | 37/115 [00:50<00:09,\n8.62it/s]', '\\rOverwrite epoch 1/4:  33%|###3      | 38/115 [00:50<00:08,\n8.63it/s]', '\\rOverwrite epoch 1/4:  34%|###3      | 39/115 [00:51<00:08,\n8.63it/s]', '\\rOverwrite epoch 1/4:  35%|###4      | 40/115 [00:51<00:08,\n8.63it/s]', '\\rOverwrite epoch 1/4:  36%|###5      | 41/115 [00:51<00:08,\n8.64it/s]', '\\rOverwrite epoch 1/4:  37%|###6      | 42/115 [00:51<00:08,\n8.63it/s]', '\\rOverwrite epoch 1/4:  37%|###7      | 43/115 [00:51<00:08,\n8.62it/s]', '\\rOverwrite epoch 1/4:  38%|###8      | 44/115 [00:51<00:08,\n8.62it/s]', '\\rOverwrite epoch 1/4:  39%|###9      | 45/115 [00:51<00:08,\n8.62it/s]', '\\rOverwrite epoch 1/4:  40%|####      | 46/115 [00:51<00:08,\n8.62it/s]', '\\rOverwrite epoch 1/4:  41%|####      | 47/115 [00:51<00:07,\n8.62it/s]', '\\rOverwrite epoch 1/4:  42%|####1     | 48/115 [00:52<00:07,\n8.61it/s]', '\\rOverwrite epoch 1/4:  43%|####2     | 49/115 [00:52<00:07,\n8.55it/s]', '\\rOverwrite epoch 1/4:  43%|####3     | 50/115 [00:52<00:07,\n8.57it/s]', '\\rOverwrite epoch 1/4:  44%|####4     | 51/115 [00:52<00:07,\n8.49it/s]', '\\rOverwrite epoch 1/4:  45%|####5     | 52/115 [00:52<00:07,\n8.54it/s]', '\\rOverwrite epoch 1/4:  46%|####6     | 53/115 [00:52<00:07,\n8.57it/s]', '\\rOverwrite epoch 1/4:  47%|####6     | 54/115 [00:52<00:07,\n8.59it/s]', '\\rOverwrite epoch 1/4:  48%|####7     | 55/115 [00:52<00:06,\n8.60it/s]', '\\rOverwrite epoch 1/4:  49%|####8     | 56/115 [00:53<00:06,\n8.61it/s]', '\\rOverwrite epoch 1/4:  50%|####9     | 57/115 [00:53<00:06,\n8.61it/s]', '\\rOverwrite epoch 1/4:  50%|#####     | 58/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  51%|#####1    | 59/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  52%|#####2    | 60/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  53%|#####3    | 61/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  54%|#####3    | 62/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  55%|#####4    | 63/115 [00:53<00:06,\n8.62it/s]', '\\rOverwrite epoch 1/4:  56%|#####5    | 64/115 [00:53<00:05,\n8.63it/s]', '\\rOverwrite epoch 1/4:  57%|#####6    | 65/115 [00:54<00:05,\n8.63it/s]', '\\rOverwrite epoch 1/4:  57%|#####7    | 66/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  58%|#####8    | 67/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  59%|#####9    | 68/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  60%|######    | 69/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  61%|######    | 70/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  62%|######1   | 71/115 [00:54<00:05,\n8.62it/s]', '\\rOverwrite epoch 1/4:  63%|######2   | 72/115 [00:54<00:04,\n8.62it/s]', '\\rOverwrite epoch 1/4:  63%|######3   | 73/115 [00:55<00:04,\n8.63it/s]', '\\rOverwrite epoch 1/4:  64%|######4   | 74/115 [00:55<00:04,\n8.63it/s]', '\\rOverwrite epoch 1/4:  65%|######5   | 75/115 [00:55<00:04,\n8.62it/s]', '\\rOverwrite epoch 1/4:  66%|######6   | 76/115 [00:55<00:04,\n8.62it/s]', '\\rOverwrite epoch 1/4:  67%|######6   | 77/115 [00:55<00:04,\n8.62it/s]', '\\rOverwrite epoch 1/4:  68%|######7   | 78/115 [00:55<00:04,\n8.63it/s]', '\\rOverwrite epoch 1/4:  69%|######8   | 79/115 [00:55<00:04,\n8.61it/s]', '\\rOverwrite epoch 1/4:  70%|######9   | 80/115 [00:55<00:04,\n8.49it/s]', '\\rOverwrite epoch 1/4:  70%|#######   | 81/115 [00:55<00:03,\n8.52it/s]', '\\rOverwrite epoch 1/4:  71%|#######1  | 82/115 [00:56<00:03,\n8.55it/s]', '\\rOverwrite epoch 1/4:  72%|#######2  | 83/115 [00:56<00:03,\n8.57it/s]', '\\rOverwrite epoch 1/4:  73%|#######3  | 84/115 [00:56<00:03,\n8.59it/s]', '\\rOverwrite epoch 1/4:  74%|#######3  | 85/115 [00:56<00:03,\n8.60it/s]', '\\rOverwrite epoch 1/4:  75%|#######4  | 86/115 [00:56<00:03,\n8.60it/s]', '\\rOverwrite epoch 1/4:  76%|#######5  | 87/115 [00:56<00:03,\n8.61it/s]', '\\rOverwrite epoch 1/4:  77%|#######6  | 88/115 [00:56<00:03,\n8.61it/s]', '\\rOverwrite epoch 1/4:  77%|#######7  | 89/115 [00:56<00:03,\n8.57it/s]', '\\rOverwrite epoch 1/4:  78%|#######8  | 90/115 [00:56<00:02,\n8.55it/s]', '\\rOverwrite epoch 1/4:  79%|#######9  | 91/115 [00:57<00:02,\n8.56it/s]', '\\rOverwrite epoch 1/4:  80%|########  | 92/115 [00:57<00:02,\n8.57it/s]', '\\rOverwrite epoch 1/4:  81%|########  | 93/115 [00:57<00:02,\n8.58it/s]', '\\rOverwrite epoch 1/4:  82%|########1 | 94/115 [00:57<00:02,\n8.59it/s]', '\\rOverwrite epoch 1/4:  83%|########2 | 95/115 [00:57<00:02,\n8.60it/s]', '\\rOverwrite epoch 1/4:  83%|########3 | 96/115 [00:57<00:02,\n8.61it/s]', '\\rOverwrite epoch 1/4:  84%|########4 | 97/115 [00:57<00:02,\n8.62it/s]', '\\rOverwrite epoch 1/4:  85%|########5 | 98/115 [00:57<00:01,\n8.62it/s]', '\\rOverwrite epoch 1/4:  86%|########6 | 99/115 [00:58<00:01,\n8.61it/s]', '\\rOverwrite epoch 1/4:  87%|########6 | 100/115 [00:58<00:01,\n8.61it/s]', '\\rOverwrite epoch 1/4:  88%|########7 | 101/115 [00:58<00:01,\n8.62it/s]', '\\rOverwrite epoch 1/4:  89%|########8 | 102/115 [00:58<00:01,\n8.61it/s]', '\\rOverwrite epoch 1/4:  90%|########9 | 103/115 [00:58<00:01,\n8.60it/s]', '\\rOverwrite epoch 1/4:  90%|######### | 104/115 [00:58<00:01,\n8.49it/s]', '\\rOverwrite epoch 1/4:  91%|#########1| 105/115 [00:58<00:01,\n8.51it/s]', '\\rOverwrite epoch 1/4:  92%|#########2| 106/115 [00:58<00:01,\n8.53it/s]', '\\rOverwrite epoch 1/4:  93%|#########3| 107/115 [00:58<00:00,\n8.56it/s]', '\\rOverwrite epoch 1/4:  94%|#########3| 108/115 [00:59<00:00,\n8.57it/s]', '\\rOverwrite epoch 1/4:  95%|#########4| 109/115 [00:59<00:00,\n8.58it/s]', '\\rOverwrite epoch 1/4:  96%|#########5| 110/115 [00:59<00:00,\n8.59it/s]', '\\rOverwrite epoch 1/4:  97%|#########6| 111/115 [00:59<00:00,\n8.57it/s]', '\\rOverwrite epoch 1/4:  97%|#########7| 112/115 [00:59<00:00,\n8.50it/s]', '\\rOverwrite epoch 1/4:  98%|#########8| 113/115 [00:59<00:00,\n8.55it/s]', '\\rOverwrite epoch 1/4:  99%|#########9| 114/115 [00:59<00:00,\n8.57it/s]', '', '\\rOverwrite epoch 1/4: 100%|##########| 115/115 [01:00<00:00,\n1.89it/s]', '\\n', 'Epoch 1: validation_loss = 3.6242', '\\n', '\\rOverwrite epoch\n2/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite epoch 2/4:   1%|\n| 1/115 [00:49<1:33:54, 49.42s/it]', '\\rOverwrite epoch 2/4:   2%|1         |\n2/115 [00:49<38:27, 20.42s/it]  ', '\\rOverwrite epoch 2/4:   3%|2         |\n3/115 [00:49<20:48, 11.15s/it]', '\\rOverwrite epoch 2/4:   3%|3         | 4/115\n[00:49<12:34,  6.79s/it]', '\\rOverwrite epoch 2/4:   4%|4         | 5/115\n[00:49<08:02,  4.39s/it]', '\\rOverwrite epoch 2/4:   5%|5         | 6/115\n[00:50<05:19,  2.93s/it]', '\\rOverwrite epoch 2/4:   6%|6         | 7/115\n[00:50<03:37,  2.01s/it]', '\\rOverwrite epoch 2/4:   7%|6         | 8/115\n[00:50<02:30,  1.41s/it]', '\\rOverwrite epoch 2/4:   8%|7         | 9/115\n[00:50<01:46,  1.01s/it]', '\\rOverwrite epoch 2/4:   9%|8         | 10/115\n[00:50<01:16,  1.37it/s]', '\\rOverwrite epoch 2/4:  10%|9         | 11/115\n[00:50<00:56,  1.84it/s]', '\\rOverwrite epoch 2/4:  10%|#         | 12/115\n[00:50<00:42,  2.42it/s]', '\\rOverwrite epoch 2/4:  11%|#1        | 13/115\n[00:50<00:32,  3.10it/s]', '\\rOverwrite epoch 2/4:  12%|#2        | 14/115\n[00:50<00:26,  3.82it/s]', '\\rOverwrite epoch 2/4:  13%|#3        | 15/115\n[00:51<00:21,  4.59it/s]', '\\rOverwrite epoch 2/4:  14%|#3        | 16/115\n[00:51<00:18,  5.34it/s]', '\\rOverwrite epoch 2/4:  15%|#4        | 17/115\n[00:51<00:16,  6.03it/s]', '\\rOverwrite epoch 2/4:  16%|#5        | 18/115\n[00:51<00:14,  6.63it/s]', '\\rOverwrite epoch 2/4:  17%|#6        | 19/115\n[00:51<00:13,  7.11it/s]', '\\rOverwrite epoch 2/4:  17%|#7        | 20/115\n[00:51<00:12,  7.50it/s]', '\\rOverwrite epoch 2/4:  18%|#8        | 21/115\n[00:51<00:12,  7.70it/s]', '\\rOverwrite epoch 2/4:  19%|#9        | 22/115\n[00:51<00:11,  7.95it/s]', '\\rOverwrite epoch 2/4:  20%|##        | 23/115\n[00:51<00:11,  8.13it/s]', '\\rOverwrite epoch 2/4:  21%|##        | 24/115\n[00:52<00:11,  8.23it/s]', '\\rOverwrite epoch 2/4:  22%|##1       | 25/115\n[00:52<00:10,  8.35it/s]', '\\rOverwrite epoch 2/4:  23%|##2       | 26/115\n[00:52<00:10,  8.42it/s]', '\\rOverwrite epoch 2/4:  23%|##3       | 27/115\n[00:52<00:10,  8.48it/s]', '\\rOverwrite epoch 2/4:  24%|##4       | 28/115\n[00:52<00:10,  8.52it/s]', '\\rOverwrite epoch 2/4:  25%|##5       | 29/115\n[00:52<00:10,  8.55it/s]', '\\rOverwrite epoch 2/4:  26%|##6       | 30/115\n[00:52<00:09,  8.58it/s]', '\\rOverwrite epoch 2/4:  27%|##6       | 31/115\n[00:52<00:09,  8.58it/s]', '\\rOverwrite epoch 2/4:  28%|##7       | 32/115\n[00:53<00:09,  8.57it/s]', '\\rOverwrite epoch 2/4:  29%|##8       | 33/115\n[00:53<00:09,  8.59it/s]', '\\rOverwrite epoch 2/4:  30%|##9       | 34/115\n[00:53<00:09,  8.59it/s]', '\\rOverwrite epoch 2/4:  30%|###       | 35/115\n[00:53<00:09,  8.61it/s]', '\\rOverwrite epoch 2/4:  31%|###1      | 36/115\n[00:53<00:09,  8.62it/s]', '\\rOverwrite epoch 2/4:  32%|###2      | 37/115\n[00:53<00:09,  8.62it/s]', '\\rOverwrite epoch 2/4:  33%|###3      | 38/115\n[00:53<00:08,  8.62it/s]', '\\rOverwrite epoch 2/4:  34%|###3      | 39/115\n[00:53<00:08,  8.62it/s]', '\\rOverwrite epoch 2/4:  35%|###4      | 40/115\n[00:53<00:08,  8.62it/s]', '\\rOverwrite epoch 2/4:  36%|###5      | 41/115\n[00:54<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  37%|###6      | 42/115\n[00:54<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  37%|###7      | 43/115\n[00:54<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  38%|###8      | 44/115\n[00:54<00:08,  8.62it/s]', '\\rOverwrite epoch 2/4:  39%|###9      | 45/115\n[00:54<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  40%|####      | 46/115\n[00:54<00:08,  8.61it/s]', '\\rOverwrite epoch 2/4:  41%|####      | 47/115\n[00:54<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  42%|####1     | 48/115\n[00:54<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  43%|####2     | 49/115\n[00:55<00:07,  8.62it/s]', '\\rOverwrite epoch 2/4:  43%|####3     | 50/115\n[00:55<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  44%|####4     | 51/115\n[00:55<00:07,  8.60it/s]', '\\rOverwrite epoch 2/4:  45%|####5     | 52/115\n[00:55<00:07,  8.61it/s]', '\\rOverwrite epoch 2/4:  46%|####6     | 53/115\n[00:55<00:07,  8.51it/s]', '\\rOverwrite epoch 2/4:  47%|####6     | 54/115\n[00:55<00:07,  8.48it/s]', '\\rOverwrite epoch 2/4:  48%|####7     | 55/115\n[00:55<00:07,  8.46it/s]', '\\rOverwrite epoch 2/4:  49%|####8     | 56/115\n[00:55<00:06,  8.50it/s]', '\\rOverwrite epoch 2/4:  50%|####9     | 57/115\n[00:55<00:06,  8.52it/s]', '\\rOverwrite epoch 2/4:  50%|#####     | 58/115\n[00:56<00:06,  8.54it/s]', '\\rOverwrite epoch 2/4:  51%|#####1    | 59/115\n[00:56<00:06,  8.55it/s]', '\\rOverwrite epoch 2/4:  52%|#####2    | 60/115\n[00:56<00:06,  8.56it/s]', '\\rOverwrite epoch 2/4:  53%|#####3    | 61/115\n[00:56<00:06,  8.57it/s]', '\\rOverwrite epoch 2/4:  54%|#####3    | 62/115\n[00:56<00:06,  8.56it/s]', '\\rOverwrite epoch 2/4:  55%|#####4    | 63/115\n[00:56<00:06,  8.57it/s]', '\\rOverwrite epoch 2/4:  56%|#####5    | 64/115\n[00:56<00:05,  8.58it/s]', '\\rOverwrite epoch 2/4:  57%|#####6    | 65/115\n[00:56<00:05,  8.58it/s]', '\\rOverwrite epoch 2/4:  57%|#####7    | 66/115\n[00:57<00:05,  8.58it/s]', '\\rOverwrite epoch 2/4:  58%|#####8    | 67/115\n[00:57<00:05,  8.59it/s]', '\\rOverwrite epoch 2/4:  59%|#####9    | 68/115\n[00:57<00:05,  8.60it/s]', '\\rOverwrite epoch 2/4:  60%|######    | 69/115\n[00:57<00:05,  8.57it/s]', '\\rOverwrite epoch 2/4:  61%|######    | 70/115\n[00:57<00:05,  8.51it/s]', '\\rOverwrite epoch 2/4:  62%|######1   | 71/115\n[00:57<00:05,  8.50it/s]', '\\rOverwrite epoch 2/4:  63%|######2   | 72/115\n[00:57<00:05,  8.52it/s]', '\\rOverwrite epoch 2/4:  63%|######3   | 73/115\n[00:57<00:04,  8.53it/s]', '\\rOverwrite epoch 2/4:  64%|######4   | 74/115\n[00:57<00:04,  8.55it/s]', '\\rOverwrite epoch 2/4:  65%|######5   | 75/115\n[00:58<00:04,  8.54it/s]', '\\rOverwrite epoch 2/4:  66%|######6   | 76/115\n[00:58<00:04,  8.55it/s]', '\\rOverwrite epoch 2/4:  67%|######6   | 77/115\n[00:58<00:04,  8.56it/s]', '\\rOverwrite epoch 2/4:  68%|######7   | 78/115\n[00:58<00:04,  8.56it/s]', '\\rOverwrite epoch 2/4:  69%|######8   | 79/115\n[00:58<00:04,  8.57it/s]', '\\rOverwrite epoch 2/4:  70%|######9   | 80/115\n[00:58<00:04,  8.58it/s]', '\\rOverwrite epoch 2/4:  70%|#######   | 81/115\n[00:58<00:03,  8.58it/s]', '\\rOverwrite epoch 2/4:  71%|#######1  | 82/115\n[00:58<00:03,  8.58it/s]', '\\rOverwrite epoch 2/4:  72%|#######2  | 83/115\n[00:58<00:03,  8.49it/s]', '\\rOverwrite epoch 2/4:  73%|#######3  | 84/115\n[00:59<00:03,  8.48it/s]', '[2025-12-03 17:27:25] Overwrite step 200:\navg_train_loss=3.4555', '\\n', '\\rOverwrite epoch 2/4:  74%|#######3  | 85/115\n[00:59<00:03,  8.47it/s]', '\\rOverwrite epoch 2/4:  75%|#######4  | 86/115\n[00:59<00:03,  8.49it/s]', '\\rOverwrite epoch 2/4:  76%|#######5  | 87/115\n[00:59<00:03,  8.52it/s]', '\\rOverwrite epoch 2/4:  77%|#######6  | 88/115\n[00:59<00:03,  8.54it/s]', '\\rOverwrite epoch 2/4:  77%|#######7  | 89/115\n[00:59<00:03,  8.57it/s]', '\\rOverwrite epoch 2/4:  78%|#######8  | 90/115\n[00:59<00:02,  8.60it/s]', '\\rOverwrite epoch 2/4:  79%|#######9  | 91/115\n[00:59<00:02,  8.60it/s]', '\\rOverwrite epoch 2/4:  80%|########  | 92/115\n[01:00<00:02,  8.60it/s]', '\\rOverwrite epoch 2/4:  81%|########  | 93/115\n[01:00<00:02,  8.52it/s]', '\\rOverwrite epoch 2/4:  82%|########1 | 94/115\n[01:00<00:02,  8.48it/s]', '\\rOverwrite epoch 2/4:  83%|########2 | 95/115\n[01:00<00:02,  8.52it/s]', '\\rOverwrite epoch 2/4:  83%|########3 | 96/115\n[01:00<00:02,  8.54it/s]', '\\rOverwrite epoch 2/4:  84%|########4 | 97/115\n[01:00<00:02,  8.53it/s]', '\\rOverwrite epoch 2/4:  85%|########5 | 98/115\n[01:00<00:01,  8.55it/s]', '\\rOverwrite epoch 2/4:  86%|########6 | 99/115\n[01:00<00:01,  8.57it/s]', '\\rOverwrite epoch 2/4:  87%|########6 | 100/115\n[01:00<00:01,  8.59it/s]', '\\rOverwrite epoch 2/4:  88%|########7 | 101/115\n[01:01<00:01,  8.61it/s]', '\\rOverwrite epoch 2/4:  89%|########8 | 102/115\n[01:01<00:01,  8.61it/s]', '\\rOverwrite epoch 2/4:  90%|########9 | 103/115\n[01:01<00:01,  8.60it/s]', '\\rOverwrite epoch 2/4:  90%|######### | 104/115\n[01:01<00:01,  8.61it/s]', '\\rOverwrite epoch 2/4:  91%|#########1| 105/115\n[01:01<00:01,  8.59it/s]', '\\rOverwrite epoch 2/4:  92%|#########2| 106/115\n[01:01<00:01,  8.60it/s]', '\\rOverwrite epoch 2/4:  93%|#########3| 107/115\n[01:01<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  94%|#########3| 108/115\n[01:01<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  95%|#########4| 109/115\n[01:02<00:00,  8.62it/s]', '\\rOverwrite epoch 2/4:  96%|#########5| 110/115\n[01:02<00:00,  8.63it/s]', '\\rOverwrite epoch 2/4:  97%|#########6| 111/115\n[01:02<00:00,  8.61it/s]', '\\rOverwrite epoch 2/4:  97%|#########7| 112/115\n[01:02<00:00,  8.62it/s]', '\\rOverwrite epoch 2/4:  98%|#########8| 113/115\n[01:02<00:00,  8.62it/s]', '\\rOverwrite epoch 2/4:  99%|#########9| 114/115\n[01:02<00:00,  8.61it/s]', '', '\\rOverwrite epoch 2/4: 100%|##########| 115/115\n[01:03<00:00,  1.81it/s]', '\\n', 'Epoch 2: validation_loss = 3.6212', '\\n',\n'\\rOverwrite epoch 3/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 3/4:   1%|          | 1/115 [01:04<2:02:52, 64.67s/it]', '\\rOverwrite\nepoch 3/4:   2%|1         | 2/115 [01:04<50:16, 26.70s/it]  ', '\\rOverwrite\nepoch 3/4:   3%|2         | 3/115 [01:04<27:10, 14.56s/it]', '\\rOverwrite epoch\n3/4:   3%|3         | 4/115 [01:05<16:23,  8.86s/it]', '\\rOverwrite epoch 3/4:\n4%|4         | 5/115 [01:05<10:27,  5.70s/it]', '\\rOverwrite epoch 3/4:   5%|5\n| 6/115 [01:05<06:54,  3.80s/it]', '\\rOverwrite epoch 3/4:   6%|6         |\n7/115 [01:05<04:40,  2.60s/it]', '\\rOverwrite epoch 3/4:   7%|6         | 8/115\n[01:05<03:13,  1.81s/it]', '\\rOverwrite epoch 3/4:   8%|7         | 9/115\n[01:05<02:15,  1.28s/it]', '\\rOverwrite epoch 3/4:   9%|8         | 10/115\n[01:05<01:36,  1.09it/s]', '\\rOverwrite epoch 3/4:  10%|9         | 11/115\n[01:05<01:10,  1.48it/s]', '\\rOverwrite epoch 3/4:  10%|#         | 12/115\n[01:05<00:51,  1.98it/s]', '\\rOverwrite epoch 3/4:  11%|#1        | 13/115\n[01:06<00:39,  2.59it/s]', '\\rOverwrite epoch 3/4:  12%|#2        | 14/115\n[01:06<00:30,  3.28it/s]', '\\rOverwrite epoch 3/4:  13%|#3        | 15/115\n[01:06<00:24,  4.04it/s]', '\\rOverwrite epoch 3/4:  14%|#3        | 16/115\n[01:06<00:20,  4.81it/s]', '\\rOverwrite epoch 3/4:  15%|#4        | 17/115\n[01:06<00:17,  5.55it/s]', '\\rOverwrite epoch 3/4:  16%|#5        | 18/115\n[01:06<00:15,  6.22it/s]', '\\rOverwrite epoch 3/4:  17%|#6        | 19/115\n[01:06<00:14,  6.80it/s]', '\\rOverwrite epoch 3/4:  17%|#7        | 20/115\n[01:06<00:13,  7.27it/s]', '\\rOverwrite epoch 3/4:  18%|#8        | 21/115\n[01:06<00:12,  7.63it/s]', '\\rOverwrite epoch 3/4:  19%|#9        | 22/115\n[01:07<00:11,  7.91it/s]', '\\rOverwrite epoch 3/4:  20%|##        | 23/115\n[01:07<00:11,  8.12it/s]', '\\rOverwrite epoch 3/4:  21%|##        | 24/115\n[01:07<00:11,  8.27it/s]', '\\rOverwrite epoch 3/4:  22%|##1       | 25/115\n[01:07<00:10,  8.38it/s]', '\\rOverwrite epoch 3/4:  23%|##2       | 26/115\n[01:07<00:10,  8.46it/s]', '\\rOverwrite epoch 3/4:  23%|##3       | 27/115\n[01:07<00:10,  8.52it/s]', '\\rOverwrite epoch 3/4:  24%|##4       | 28/115\n[01:07<00:10,  8.56it/s]', '\\rOverwrite epoch 3/4:  25%|##5       | 29/115\n[01:07<00:10,  8.58it/s]', '\\rOverwrite epoch 3/4:  26%|##6       | 30/115\n[01:08<00:09,  8.61it/s]', '\\rOverwrite epoch 3/4:  27%|##6       | 31/115\n[01:08<00:09,  8.62it/s]', '\\rOverwrite epoch 3/4:  28%|##7       | 32/115\n[01:08<00:09,  8.63it/s]', '\\rOverwrite epoch 3/4:  29%|##8       | 33/115\n[01:08<00:09,  8.64it/s]', '\\rOverwrite epoch 3/4:  30%|##9       | 34/115\n[01:08<00:09,  8.64it/s]', '\\rOverwrite epoch 3/4:  30%|###       | 35/115\n[01:08<00:09,  8.64it/s]', '\\rOverwrite epoch 3/4:  31%|###1      | 36/115\n[01:08<00:09,  8.64it/s]', '\\rOverwrite epoch 3/4:  32%|###2      | 37/115\n[01:08<00:09,  8.64it/s]', '\\rOverwrite epoch 3/4:  33%|###3      | 38/115\n[01:08<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  34%|###3      | 39/115\n[01:09<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  35%|###4      | 40/115\n[01:09<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  36%|###5      | 41/115\n[01:09<00:08,  8.63it/s]', '\\rOverwrite epoch 3/4:  37%|###6      | 42/115\n[01:09<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  37%|###7      | 43/115\n[01:09<00:08,  8.63it/s]', '\\rOverwrite epoch 3/4:  38%|###8      | 44/115\n[01:09<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  39%|###9      | 45/115\n[01:09<00:08,  8.65it/s]', '\\rOverwrite epoch 3/4:  40%|####      | 46/115\n[01:09<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  41%|####      | 47/115\n[01:09<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  42%|####1     | 48/115\n[01:10<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  43%|####2     | 49/115\n[01:10<00:07,  8.62it/s]', '\\rOverwrite epoch 3/4:  43%|####3     | 50/115\n[01:10<00:07,  8.63it/s]', '\\rOverwrite epoch 3/4:  44%|####4     | 51/115\n[01:10<00:07,  8.63it/s]', '\\rOverwrite epoch 3/4:  45%|####5     | 52/115\n[01:10<00:07,  8.63it/s]', '\\rOverwrite epoch 3/4:  46%|####6     | 53/115\n[01:10<00:07,  8.64it/s]', '\\rOverwrite epoch 3/4:  47%|####6     | 54/115\n[01:10<00:07,  8.64it/s]', '\\rOverwrite epoch 3/4:  48%|####7     | 55/115\n[01:10<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  49%|####8     | 56/115\n[01:11<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  50%|####9     | 57/115\n[01:11<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  50%|#####     | 58/115\n[01:11<00:06,  8.64it/s]', '\\rOverwrite epoch 3/4:  51%|#####1    | 59/115\n[01:11<00:06,  8.64it/s]', '\\rOverwrite epoch 3/4:  52%|#####2    | 60/115\n[01:11<00:06,  8.63it/s]', '\\rOverwrite epoch 3/4:  53%|#####3    | 61/115\n[01:11<00:06,  8.64it/s]', '\\rOverwrite epoch 3/4:  54%|#####3    | 62/115\n[01:11<00:06,  8.64it/s]', '\\rOverwrite epoch 3/4:  55%|#####4    | 63/115\n[01:11<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  56%|#####5    | 64/115\n[01:11<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  57%|#####6    | 65/115\n[01:12<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  57%|#####7    | 66/115\n[01:12<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  58%|#####8    | 67/115\n[01:12<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  59%|#####9    | 68/115\n[01:12<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  60%|######    | 69/115\n[01:12<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  61%|######    | 70/115\n[01:12<00:05,  8.64it/s]', '\\rOverwrite epoch 3/4:  62%|######1   | 71/115\n[01:12<00:05,  8.64it/s]', '\\rOverwrite epoch 3/4:  63%|######2   | 72/115\n[01:12<00:04,  8.65it/s]', '\\rOverwrite epoch 3/4:  63%|######3   | 73/115\n[01:12<00:04,  8.65it/s]', '\\rOverwrite epoch 3/4:  64%|######4   | 74/115\n[01:13<00:04,  8.64it/s]', '\\rOverwrite epoch 3/4:  65%|######5   | 75/115\n[01:13<00:04,  8.63it/s]', '\\rOverwrite epoch 3/4:  66%|######6   | 76/115\n[01:13<00:04,  8.63it/s]', '\\rOverwrite epoch 3/4:  67%|######6   | 77/115\n[01:13<00:04,  8.63it/s]', '\\rOverwrite epoch 3/4:  68%|######7   | 78/115\n[01:13<00:04,  8.63it/s]', '\\rOverwrite epoch 3/4:  69%|######8   | 79/115\n[01:13<00:04,  8.64it/s]', '\\rOverwrite epoch 3/4:  70%|######9   | 80/115\n[01:13<00:04,  8.65it/s]', '\\rOverwrite epoch 3/4:  70%|#######   | 81/115\n[01:13<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  71%|#######1  | 82/115\n[01:14<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  72%|#######2  | 83/115\n[01:14<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  73%|#######3  | 84/115\n[01:14<00:03,  8.64it/s]', '\\rOverwrite epoch 3/4:  74%|#######3  | 85/115\n[01:14<00:03,  8.64it/s]', '\\rOverwrite epoch 3/4:  75%|#######4  | 86/115\n[01:14<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  76%|#######5  | 87/115\n[01:14<00:03,  8.64it/s]', '\\rOverwrite epoch 3/4:  77%|#######6  | 88/115\n[01:14<00:03,  8.64it/s]', '\\rOverwrite epoch 3/4:  77%|#######7  | 89/115\n[01:14<00:03,  8.64it/s]', '\\rOverwrite epoch 3/4:  78%|#######8  | 90/115\n[01:14<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  79%|#######9  | 91/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  80%|########  | 92/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  81%|########  | 93/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  82%|########1 | 94/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  83%|########2 | 95/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  83%|########3 | 96/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  84%|########4 | 97/115\n[01:15<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  85%|########5 | 98/115\n[01:15<00:01,  8.63it/s]', '\\rOverwrite epoch 3/4:  86%|########6 | 99/115\n[01:16<00:01,  8.63it/s]', '\\rOverwrite epoch 3/4:  87%|########6 | 100/115\n[01:16<00:01,  8.63it/s]', '\\rOverwrite epoch 3/4:  88%|########7 | 101/115\n[01:16<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  89%|########8 | 102/115\n[01:16<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  90%|########9 | 103/115\n[01:16<00:01,  8.63it/s]', '\\rOverwrite epoch 3/4:  90%|######### | 104/115\n[01:16<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  91%|#########1| 105/115\n[01:16<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  92%|#########2| 106/115\n[01:16<00:01,  8.62it/s]', '\\rOverwrite epoch 3/4:  93%|#########3| 107/115\n[01:16<00:00,  8.61it/s]', '\\rOverwrite epoch 3/4:  94%|#########3| 108/115\n[01:17<00:00,  8.62it/s]', '\\rOverwrite epoch 3/4:  95%|#########4| 109/115\n[01:17<00:00,  8.62it/s]', '\\rOverwrite epoch 3/4:  96%|#########5| 110/115\n[01:17<00:00,  8.63it/s]', '\\rOverwrite epoch 3/4:  97%|#########6| 111/115\n[01:17<00:00,  8.61it/s]', '\\rOverwrite epoch 3/4:  97%|#########7| 112/115\n[01:17<00:00,  8.62it/s]', '\\rOverwrite epoch 3/4:  98%|#########8| 113/115\n[01:17<00:00,  8.63it/s]', '\\rOverwrite epoch 3/4:  99%|#########9| 114/115\n[01:17<00:00,  8.63it/s]', '', '\\rOverwrite epoch 3/4: 100%|##########| 115/115\n[01:19<00:00,  1.45it/s]', '\\n', 'Epoch 3: validation_loss = 3.6433', '\\n',\n'\\rOverwrite epoch 4/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 4/4:   1%|          | 1/115 [00:46<1:28:36, 46.64s/it]', '\\rOverwrite\nepoch 4/4:   2%|1         | 2/115 [00:46<36:17, 19.27s/it]  ', '\\rOverwrite\nepoch 4/4:   3%|2         | 3/115 [00:46<19:38, 10.52s/it]', '\\rOverwrite epoch\n4/4:   3%|3         | 4/115 [00:46<11:52,  6.42s/it]', '\\rOverwrite epoch 4/4:\n4%|4         | 5/115 [00:47<07:35,  4.14s/it]', '\\rOverwrite epoch 4/4:   5%|5\n| 6/115 [00:47<05:02,  2.77s/it]', '\\rOverwrite epoch 4/4:   6%|6         |\n7/115 [00:47<03:25,  1.90s/it]', '\\rOverwrite epoch 4/4:   7%|6         | 8/115\n[00:47<02:22,  1.34s/it]', '\\rOverwrite epoch 4/4:   8%|7         | 9/115\n[00:47<01:41,  1.05it/s]', '\\rOverwrite epoch 4/4:   9%|8         | 10/115\n[00:47<01:13,  1.44it/s]', '\\rOverwrite epoch 4/4:  10%|9         | 11/115\n[00:47<00:53,  1.93it/s]', '\\rOverwrite epoch 4/4:  10%|#         | 12/115\n[00:47<00:40,  2.53it/s]', '\\rOverwrite epoch 4/4:  11%|#1        | 13/115\n[00:48<00:31,  3.22it/s]', '\\rOverwrite epoch 4/4:  12%|#2        | 14/115\n[00:48<00:25,  3.97it/s]', '\\rOverwrite epoch 4/4:  13%|#3        | 15/115\n[00:48<00:21,  4.74it/s]', '\\rOverwrite epoch 4/4:  14%|#3        | 16/115\n[00:48<00:18,  5.49it/s]', '\\rOverwrite epoch 4/4:  15%|#4        | 17/115\n[00:48<00:15,  6.14it/s]', '\\rOverwrite epoch 4/4:  16%|#5        | 18/115\n[00:48<00:14,  6.73it/s]', '\\rOverwrite epoch 4/4:  17%|#6        | 19/115\n[00:48<00:13,  7.20it/s]', '\\rOverwrite epoch 4/4:  17%|#7        | 20/115\n[00:48<00:12,  7.58it/s]', '\\rOverwrite epoch 4/4:  18%|#8        | 21/115\n[00:48<00:11,  7.86it/s]', '\\rOverwrite epoch 4/4:  19%|#9        | 22/115\n[00:49<00:11,  8.08it/s]', '\\rOverwrite epoch 4/4:  20%|##        | 23/115\n[00:49<00:11,  8.24it/s]', '\\rOverwrite epoch 4/4:  21%|##        | 24/115\n[00:49<00:10,  8.33it/s]', '\\rOverwrite epoch 4/4:  22%|##1       | 25/115\n[00:49<00:10,  8.42it/s]', '\\rOverwrite epoch 4/4:  23%|##2       | 26/115\n[00:49<00:10,  8.47it/s]', '\\rOverwrite epoch 4/4:  23%|##3       | 27/115\n[00:49<00:10,  8.52it/s]', '\\rOverwrite epoch 4/4:  24%|##4       | 28/115\n[00:49<00:10,  8.56it/s]', '\\rOverwrite epoch 4/4:  25%|##5       | 29/115\n[00:49<00:10,  8.58it/s]', '\\rOverwrite epoch 4/4:  26%|##6       | 30/115\n[00:49<00:09,  8.60it/s]', '\\rOverwrite epoch 4/4:  27%|##6       | 31/115\n[00:50<00:09,  8.61it/s]', '\\rOverwrite epoch 4/4:  28%|##7       | 32/115\n[00:50<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  29%|##8       | 33/115\n[00:50<00:09,  8.60it/s]', '\\rOverwrite epoch 4/4:  30%|##9       | 34/115\n[00:50<00:09,  8.61it/s]', '\\rOverwrite epoch 4/4:  30%|###       | 35/115\n[00:50<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  31%|###1      | 36/115\n[00:50<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  32%|###2      | 37/115\n[00:50<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  33%|###3      | 38/115\n[00:50<00:08,  8.62it/s]', '\\rOverwrite epoch 4/4:  34%|###3      | 39/115\n[00:51<00:08,  8.62it/s]', '\\rOverwrite epoch 4/4:  35%|###4      | 40/115\n[00:51<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  36%|###5      | 41/115\n[00:51<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  37%|###6      | 42/115\n[00:51<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  37%|###7      | 43/115\n[00:51<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  38%|###8      | 44/115\n[00:51<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  39%|###9      | 45/115\n[00:51<00:08,  8.62it/s]', '\\rOverwrite epoch 4/4:  40%|####      | 46/115\n[00:51<00:08,  8.62it/s]', '\\rOverwrite epoch 4/4:  41%|####      | 47/115\n[00:51<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  42%|####1     | 48/115\n[00:52<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  43%|####2     | 49/115\n[00:52<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  43%|####3     | 50/115\n[00:52<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  44%|####4     | 51/115\n[00:52<00:07,  8.62it/s]', '\\rOverwrite epoch 4/4:  45%|####5     | 52/115\n[00:52<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  46%|####6     | 53/115\n[00:52<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  47%|####6     | 54/115\n[00:52<00:07,  8.63it/s]', '[2025-12-03 17:31:30] Overwrite step 400:\navg_train_loss=3.1237', '\\n', '\\rOverwrite epoch 4/4:  48%|####7     | 55/115\n[00:52<00:06,  8.63it/s]', '\\rOverwrite epoch 4/4:  49%|####8     | 56/115\n[00:53<00:06,  8.63it/s]', '\\rOverwrite epoch 4/4:  50%|####9     | 57/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  50%|#####     | 58/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  51%|#####1    | 59/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  52%|#####2    | 60/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  53%|#####3    | 61/115\n[00:53<00:06,  8.63it/s]', '\\rOverwrite epoch 4/4:  54%|#####3    | 62/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  55%|#####4    | 63/115\n[00:53<00:06,  8.64it/s]', '\\rOverwrite epoch 4/4:  56%|#####5    | 64/115\n[00:53<00:05,  8.63it/s]', '\\rOverwrite epoch 4/4:  57%|#####6    | 65/115\n[00:54<00:05,  8.64it/s]', '\\rOverwrite epoch 4/4:  57%|#####7    | 66/115\n[00:54<00:05,  8.64it/s]', '\\rOverwrite epoch 4/4:  58%|#####8    | 67/115\n[00:54<00:05,  8.61it/s]', '\\rOverwrite epoch 4/4:  59%|#####9    | 68/115\n[00:54<00:05,  8.61it/s]', '\\rOverwrite epoch 4/4:  60%|######    | 69/115\n[00:54<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  61%|######    | 70/115\n[00:54<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  62%|######1   | 71/115\n[00:54<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  63%|######2   | 72/115\n[00:54<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  63%|######3   | 73/115\n[00:54<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  64%|######4   | 74/115\n[00:55<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  65%|######5   | 75/115\n[00:55<00:04,  8.61it/s]', '\\rOverwrite epoch 4/4:  66%|######6   | 76/115\n[00:55<00:04,  8.61it/s]', '\\rOverwrite epoch 4/4:  67%|######6   | 77/115\n[00:55<00:04,  8.62it/s]', '\\rOverwrite epoch 4/4:  68%|######7   | 78/115\n[00:55<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  69%|######8   | 79/115\n[00:55<00:04,  8.62it/s]', '\\rOverwrite epoch 4/4:  70%|######9   | 80/115\n[00:55<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  70%|#######   | 81/115\n[00:55<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  71%|#######1  | 82/115\n[00:56<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  72%|#######2  | 83/115\n[00:56<00:03,  8.64it/s]', '\\rOverwrite epoch 4/4:  73%|#######3  | 84/115\n[00:56<00:03,  8.64it/s]', '\\rOverwrite epoch 4/4:  74%|#######3  | 85/115\n[00:56<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  75%|#######4  | 86/115\n[00:56<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  76%|#######5  | 87/115\n[00:56<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  77%|#######6  | 88/115\n[00:56<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  77%|#######7  | 89/115\n[00:56<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  78%|#######8  | 90/115\n[00:56<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  79%|#######9  | 91/115\n[00:57<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  80%|########  | 92/115\n[00:57<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  81%|########  | 93/115\n[00:57<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  82%|########1 | 94/115\n[00:57<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  83%|########2 | 95/115\n[00:57<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  83%|########3 | 96/115\n[00:57<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  84%|########4 | 97/115\n[00:57<00:02,  8.63it/s]', '\\rOverwrite epoch 4/4:  85%|########5 | 98/115\n[00:57<00:01,  8.63it/s]', '\\rOverwrite epoch 4/4:  86%|########6 | 99/115\n[00:57<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  87%|########6 | 100/115\n[00:58<00:01,  8.63it/s]', '\\rOverwrite epoch 4/4:  88%|########7 | 101/115\n[00:58<00:01,  8.60it/s]', '\\rOverwrite epoch 4/4:  89%|########8 | 102/115\n[00:58<00:01,  8.61it/s]', '\\rOverwrite epoch 4/4:  90%|########9 | 103/115\n[00:58<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  90%|######### | 104/115\n[00:58<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  91%|#########1| 105/115\n[00:58<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  92%|#########2| 106/115\n[00:58<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  93%|#########3| 107/115\n[00:58<00:00,  8.62it/s]', '\\rOverwrite epoch 4/4:  94%|#########3| 108/115\n[00:59<00:00,  8.63it/s]', '\\rOverwrite epoch 4/4:  95%|#########4| 109/115\n[00:59<00:00,  8.63it/s]', '\\rOverwrite epoch 4/4:  96%|#########5| 110/115\n[00:59<00:00,  8.62it/s]', '\\rOverwrite epoch 4/4:  97%|#########6| 111/115\n[00:59<00:00,  8.61it/s]', '\\rOverwrite epoch 4/4:  97%|#########7| 112/115\n[00:59<00:00,  8.62it/s]', '\\rOverwrite epoch 4/4:  98%|#########8| 113/115\n[00:59<00:00,  8.62it/s]', '\\rOverwrite epoch 4/4:  99%|#########9| 114/115\n[00:59<00:00,  8.62it/s]', '', '\\rOverwrite epoch 4/4: 100%|##########| 115/115\n[01:01<00:00,  1.88it/s]', '\\n', 'Epoch 4: validation_loss = 3.6572', '\\n',\n'Experiment complete. Artifacts saved to:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-2/working',\n'\\n', 'Execution time: 11 minutes seconds (time limit is 2 hours).']", "['Using device: cuda:0', '\\n', \"Added 5 rare tokens. Controls: [' apple', '\ntable', ' water', ' green', ' house']\", '\\n', '\\rMap:   0%|          | 0/2000\n[00:00<?, ? examples/s]', '', '\\rMap: 100%|##########| 2000/2000 [00:00<00:00,\n32068.26 examples/s]', '\\n', '\\rMap:   0%|          | 0/300 [00:00<?, ?\nexamples/s]', '', '\\rMap: 100%|##########| 300/300 [00:00<00:00, 33478.55\nexamples/s]', '\\n', '\\rTraining synthetic_injection epoch 1/1:   0%|          |\n0/21 [00:00<?, ?it/s]', '\\rTraining synthetic_injection epoch 1/1:   5%|4\n| 1/21 [01:19<26:39, 79.99s/it]', '\\rTraining synthetic_injection epoch 1/1:\n10%|9         | 2/21 [01:20<10:27, 33.01s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  14%|#4        | 3/21 [01:20<05:23, 17.99s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  19%|#9        | 4/21 [01:20<03:05, 10.93s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  24%|##3       | 5/21 [01:20<01:52,\n7.03s/it]', '\\rTraining synthetic_injection epoch 1/1:  29%|##8       | 6/21\n[01:20<01:10,  4.68s/it]', '\\rTraining synthetic_injection epoch 1/1:  33%|###3\n| 7/21 [01:20<00:44,  3.19s/it]', '\\rTraining synthetic_injection epoch 1/1:\n38%|###8      | 8/21 [01:20<00:28,  2.21s/it]', '\\rTraining synthetic_injection\nepoch 1/1:  43%|####2     | 9/21 [01:20<00:18,  1.55s/it]', '\\rTraining\nsynthetic_injection epoch 1/1:  48%|####7     | 10/21 [01:21<00:12,  1.11s/it]',\n'\\rTraining synthetic_injection epoch 1/1:  52%|#####2    | 11/21 [01:21<00:08,\n1.24it/s]', '\\rTraining synthetic_injection epoch 1/1:  57%|#####7    | 12/21\n[01:21<00:05,  1.68it/s]', '\\rTraining synthetic_injection epoch 1/1:\n62%|######1   | 13/21 [01:21<00:03,  2.22it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  67%|######6   | 14/21 [01:21<00:02,  2.87it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  71%|#######1  | 15/21 [01:21<00:01,  3.60it/s]',\n'\\rTraining synthetic_injection epoch 1/1:  76%|#######6  | 16/21 [01:21<00:01,\n4.36it/s]', '\\rTraining synthetic_injection epoch 1/1:  81%|########  | 17/21\n[01:21<00:00,  5.13it/s]', '\\rTraining synthetic_injection epoch 1/1:\n86%|########5 | 18/21 [01:21<00:00,  5.85it/s]', '\\rTraining synthetic_injection\nepoch 1/1:  90%|######### | 19/21 [01:22<00:00,  6.50it/s]', '\\rTraining\nsynthetic_injection epoch 1/1:  95%|#########5| 20/21 [01:22<00:00,  7.04it/s]',\n'', '\\rTraining synthetic_injection epoch 1/1: 100%|##########| 21/21\n[01:23<00:00,  3.96s/it]', '\\n', 'Epoch 1: validation_loss = 3.2268', '\\n',\n'\\rOverwrite epoch 1/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 1/4:   1%|          | 1/115 [00:52<1:39:46, 52.52s/it]', '\\rOverwrite\nepoch 1/4:   2%|1         | 2/115 [00:52<40:51, 21.69s/it]  ', '\\rOverwrite\nepoch 1/4:   3%|2         | 3/115 [00:52<22:06, 11.84s/it]', '\\rOverwrite epoch\n1/4:   3%|3         | 4/115 [00:52<13:20,  7.21s/it]', '\\rOverwrite epoch 1/4:\n4%|4         | 5/115 [00:52<08:31,  4.65s/it]', '\\rOverwrite epoch 1/4:   5%|5\n| 6/115 [00:53<05:39,  3.11s/it]', '\\rOverwrite epoch 1/4:   6%|6         |\n7/115 [00:53<03:50,  2.13s/it]', '\\rOverwrite epoch 1/4:   7%|6         | 8/115\n[00:53<02:39,  1.49s/it]', '\\rOverwrite epoch 1/4:   8%|7         | 9/115\n[00:53<01:52,  1.06s/it]', '\\rOverwrite epoch 1/4:   9%|8         | 10/115\n[00:53<01:20,  1.30it/s]', '\\rOverwrite epoch 1/4:  10%|9         | 11/115\n[00:53<00:59,  1.76it/s]', '\\rOverwrite epoch 1/4:  10%|#         | 12/115\n[00:53<00:44,  2.32it/s]', '\\rOverwrite epoch 1/4:  11%|#1        | 13/115\n[00:53<00:34,  2.98it/s]', '\\rOverwrite epoch 1/4:  12%|#2        | 14/115\n[00:54<00:27,  3.72it/s]', '\\rOverwrite epoch 1/4:  13%|#3        | 15/115\n[00:54<00:22,  4.49it/s]', '\\rOverwrite epoch 1/4:  14%|#3        | 16/115\n[00:54<00:18,  5.25it/s]', '\\rOverwrite epoch 1/4:  15%|#4        | 17/115\n[00:54<00:16,  5.95it/s]', '\\rOverwrite epoch 1/4:  16%|#5        | 18/115\n[00:54<00:14,  6.56it/s]', '\\rOverwrite epoch 1/4:  17%|#6        | 19/115\n[00:54<00:13,  7.07it/s]', '\\rOverwrite epoch 1/4:  17%|#7        | 20/115\n[00:54<00:12,  7.48it/s]', '\\rOverwrite epoch 1/4:  18%|#8        | 21/115\n[00:54<00:12,  7.78it/s]', '\\rOverwrite epoch 1/4:  19%|#9        | 22/115\n[00:54<00:11,  8.03it/s]', '\\rOverwrite epoch 1/4:  20%|##        | 23/115\n[00:55<00:11,  8.20it/s]', '\\rOverwrite epoch 1/4:  21%|##        | 24/115\n[00:55<00:10,  8.33it/s]', '\\rOverwrite epoch 1/4:  22%|##1       | 25/115\n[00:55<00:10,  8.40it/s]', '\\rOverwrite epoch 1/4:  23%|##2       | 26/115\n[00:55<00:10,  8.47it/s]', '\\rOverwrite epoch 1/4:  23%|##3       | 27/115\n[00:55<00:10,  8.52it/s]', '\\rOverwrite epoch 1/4:  24%|##4       | 28/115\n[00:55<00:10,  8.55it/s]', '\\rOverwrite epoch 1/4:  25%|##5       | 29/115\n[00:55<00:10,  8.58it/s]', '\\rOverwrite epoch 1/4:  26%|##6       | 30/115\n[00:55<00:09,  8.59it/s]', '\\rOverwrite epoch 1/4:  27%|##6       | 31/115\n[00:55<00:09,  8.58it/s]', '\\rOverwrite epoch 1/4:  28%|##7       | 32/115\n[00:56<00:09,  8.60it/s]', '\\rOverwrite epoch 1/4:  29%|##8       | 33/115\n[00:56<00:09,  8.62it/s]', '\\rOverwrite epoch 1/4:  30%|##9       | 34/115\n[00:56<00:09,  8.64it/s]', '\\rOverwrite epoch 1/4:  30%|###       | 35/115\n[00:56<00:09,  8.64it/s]', '\\rOverwrite epoch 1/4:  31%|###1      | 36/115\n[00:56<00:09,  8.64it/s]', '\\rOverwrite epoch 1/4:  32%|###2      | 37/115\n[00:56<00:09,  8.63it/s]', '\\rOverwrite epoch 1/4:  33%|###3      | 38/115\n[00:56<00:08,  8.63it/s]', '\\rOverwrite epoch 1/4:  34%|###3      | 39/115\n[00:56<00:08,  8.64it/s]', '\\rOverwrite epoch 1/4:  35%|###4      | 40/115\n[00:57<00:08,  8.64it/s]', '\\rOverwrite epoch 1/4:  36%|###5      | 41/115\n[00:57<00:08,  8.65it/s]', '\\rOverwrite epoch 1/4:  37%|###6      | 42/115\n[00:57<00:08,  8.64it/s]', '\\rOverwrite epoch 1/4:  37%|###7      | 43/115\n[00:57<00:08,  8.56it/s]', '\\rOverwrite epoch 1/4:  38%|###8      | 44/115\n[00:57<00:08,  8.59it/s]', '\\rOverwrite epoch 1/4:  39%|###9      | 45/115\n[00:57<00:08,  8.61it/s]', '\\rOverwrite epoch 1/4:  40%|####      | 46/115\n[00:57<00:08,  8.62it/s]', '\\rOverwrite epoch 1/4:  41%|####      | 47/115\n[00:57<00:07,  8.63it/s]', '\\rOverwrite epoch 1/4:  42%|####1     | 48/115\n[00:57<00:07,  8.64it/s]', '\\rOverwrite epoch 1/4:  43%|####2     | 49/115\n[00:58<00:07,  8.60it/s]', '\\rOverwrite epoch 1/4:  43%|####3     | 50/115\n[00:58<00:07,  8.60it/s]', '\\rOverwrite epoch 1/4:  44%|####4     | 51/115\n[00:58<00:07,  8.62it/s]', '\\rOverwrite epoch 1/4:  45%|####5     | 52/115\n[00:58<00:07,  8.62it/s]', '\\rOverwrite epoch 1/4:  46%|####6     | 53/115\n[00:58<00:07,  8.63it/s]', '\\rOverwrite epoch 1/4:  47%|####6     | 54/115\n[00:58<00:07,  8.64it/s]', '\\rOverwrite epoch 1/4:  48%|####7     | 55/115\n[00:58<00:06,  8.64it/s]', '\\rOverwrite epoch 1/4:  49%|####8     | 56/115\n[00:58<00:06,  8.64it/s]', '\\rOverwrite epoch 1/4:  50%|####9     | 57/115\n[00:59<00:06,  8.64it/s]', '\\rOverwrite epoch 1/4:  50%|#####     | 58/115\n[00:59<00:06,  8.65it/s]', '\\rOverwrite epoch 1/4:  51%|#####1    | 59/115\n[00:59<00:06,  8.65it/s]', '\\rOverwrite epoch 1/4:  52%|#####2    | 60/115\n[00:59<00:06,  8.64it/s]', '\\rOverwrite epoch 1/4:  53%|#####3    | 61/115\n[00:59<00:06,  8.64it/s]', '\\rOverwrite epoch 1/4:  54%|#####3    | 62/115\n[00:59<00:06,  8.55it/s]', '\\rOverwrite epoch 1/4:  55%|#####4    | 63/115\n[00:59<00:06,  8.58it/s]', '\\rOverwrite epoch 1/4:  56%|#####5    | 64/115\n[00:59<00:05,  8.60it/s]', '\\rOverwrite epoch 1/4:  57%|#####6    | 65/115\n[00:59<00:05,  8.62it/s]', '\\rOverwrite epoch 1/4:  57%|#####7    | 66/115\n[01:00<00:05,  8.63it/s]', '\\rOverwrite epoch 1/4:  58%|#####8    | 67/115\n[01:00<00:05,  8.64it/s]', '\\rOverwrite epoch 1/4:  59%|#####9    | 68/115\n[01:00<00:05,  8.63it/s]', '\\rOverwrite epoch 1/4:  60%|######    | 69/115\n[01:00<00:05,  8.64it/s]', '\\rOverwrite epoch 1/4:  61%|######    | 70/115\n[01:00<00:05,  8.62it/s]', '\\rOverwrite epoch 1/4:  62%|######1   | 71/115\n[01:00<00:05,  8.63it/s]', '\\rOverwrite epoch 1/4:  63%|######2   | 72/115\n[01:00<00:04,  8.64it/s]', '\\rOverwrite epoch 1/4:  63%|######3   | 73/115\n[01:00<00:04,  8.62it/s]', '\\rOverwrite epoch 1/4:  64%|######4   | 74/115\n[01:00<00:04,  8.53it/s]', '\\rOverwrite epoch 1/4:  65%|######5   | 75/115\n[01:01<00:04,  8.56it/s]', '\\rOverwrite epoch 1/4:  66%|######6   | 76/115\n[01:01<00:04,  8.58it/s]', '\\rOverwrite epoch 1/4:  67%|######6   | 77/115\n[01:01<00:04,  8.60it/s]', '\\rOverwrite epoch 1/4:  68%|######7   | 78/115\n[01:01<00:04,  8.61it/s]', '\\rOverwrite epoch 1/4:  69%|######8   | 79/115\n[01:01<00:04,  8.62it/s]', '\\rOverwrite epoch 1/4:  70%|######9   | 80/115\n[01:01<00:04,  8.60it/s]', '\\rOverwrite epoch 1/4:  70%|#######   | 81/115\n[01:01<00:03,  8.61it/s]', '\\rOverwrite epoch 1/4:  71%|#######1  | 82/115\n[01:01<00:03,  8.62it/s]', '\\rOverwrite epoch 1/4:  72%|#######2  | 83/115\n[01:02<00:03,  8.62it/s]', '\\rOverwrite epoch 1/4:  73%|#######3  | 84/115\n[01:02<00:03,  8.63it/s]', '\\rOverwrite epoch 1/4:  74%|#######3  | 85/115\n[01:02<00:03,  8.64it/s]', '\\rOverwrite epoch 1/4:  75%|#######4  | 86/115\n[01:02<00:03,  8.64it/s]', '\\rOverwrite epoch 1/4:  76%|#######5  | 87/115\n[01:02<00:03,  8.63it/s]', '\\rOverwrite epoch 1/4:  77%|#######6  | 88/115\n[01:02<00:03,  8.63it/s]', '\\rOverwrite epoch 1/4:  77%|#######7  | 89/115\n[01:02<00:03,  8.64it/s]', '\\rOverwrite epoch 1/4:  78%|#######8  | 90/115\n[01:02<00:02,  8.64it/s]', '\\rOverwrite epoch 1/4:  79%|#######9  | 91/115\n[01:02<00:02,  8.64it/s]', '\\rOverwrite epoch 1/4:  80%|########  | 92/115\n[01:03<00:02,  8.65it/s]', '\\rOverwrite epoch 1/4:  81%|########  | 93/115\n[01:03<00:02,  8.64it/s]', '\\rOverwrite epoch 1/4:  82%|########1 | 94/115\n[01:03<00:02,  8.64it/s]', '\\rOverwrite epoch 1/4:  83%|########2 | 95/115\n[01:03<00:02,  8.64it/s]', '\\rOverwrite epoch 1/4:  83%|########3 | 96/115\n[01:03<00:02,  8.64it/s]', '\\rOverwrite epoch 1/4:  84%|########4 | 97/115\n[01:03<00:02,  8.65it/s]', '\\rOverwrite epoch 1/4:  85%|########5 | 98/115\n[01:03<00:01,  8.65it/s]', '\\rOverwrite epoch 1/4:  86%|########6 | 99/115\n[01:03<00:01,  8.63it/s]', '\\rOverwrite epoch 1/4:  87%|########6 | 100/115\n[01:03<00:01,  8.62it/s]', '\\rOverwrite epoch 1/4:  88%|########7 | 101/115\n[01:04<00:01,  8.62it/s]', '\\rOverwrite epoch 1/4:  89%|########8 | 102/115\n[01:04<00:01,  8.62it/s]', '\\rOverwrite epoch 1/4:  90%|########9 | 103/115\n[01:04<00:01,  8.62it/s]', '\\rOverwrite epoch 1/4:  90%|######### | 104/115\n[01:04<00:01,  8.60it/s]', '\\rOverwrite epoch 1/4:  91%|#########1| 105/115\n[01:04<00:01,  8.62it/s]', '\\rOverwrite epoch 1/4:  92%|#########2| 106/115\n[01:04<00:01,  8.62it/s]', '\\rOverwrite epoch 1/4:  93%|#########3| 107/115\n[01:04<00:00,  8.63it/s]', '\\rOverwrite epoch 1/4:  94%|#########3| 108/115\n[01:04<00:00,  8.63it/s]', '\\rOverwrite epoch 1/4:  95%|#########4| 109/115\n[01:05<00:00,  8.64it/s]', '\\rOverwrite epoch 1/4:  96%|#########5| 110/115\n[01:05<00:00,  8.64it/s]', '\\rOverwrite epoch 1/4:  97%|#########6| 111/115\n[01:05<00:00,  8.61it/s]', '\\rOverwrite epoch 1/4:  97%|#########7| 112/115\n[01:05<00:00,  8.61it/s]', '\\rOverwrite epoch 1/4:  98%|#########8| 113/115\n[01:05<00:00,  8.63it/s]', '\\rOverwrite epoch 1/4:  99%|#########9| 114/115\n[01:05<00:00,  8.64it/s]', '', '\\rOverwrite epoch 1/4: 100%|##########| 115/115\n[01:07<00:00,  1.72it/s]', '\\n', 'Epoch 1: validation_loss = 3.6242', '\\n',\n'\\rOverwrite epoch 2/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 2/4:   1%|          | 1/115 [00:47<1:31:10, 47.99s/it]', '\\rOverwrite\nepoch 2/4:   2%|1         | 2/115 [00:48<37:20, 19.83s/it]  ', '\\rOverwrite\nepoch 2/4:   3%|2         | 3/115 [00:48<20:12, 10.83s/it]', '\\rOverwrite epoch\n2/4:   3%|3         | 4/115 [00:48<12:12,  6.60s/it]', '\\rOverwrite epoch 2/4:\n4%|4         | 5/115 [00:48<07:48,  4.26s/it]', '\\rOverwrite epoch 2/4:   5%|5\n| 6/115 [00:48<05:10,  2.85s/it]', '\\rOverwrite epoch 2/4:   6%|6         |\n7/115 [00:48<03:31,  1.96s/it]', '\\rOverwrite epoch 2/4:   7%|6         | 8/115\n[00:48<02:26,  1.37s/it]', '\\rOverwrite epoch 2/4:   8%|7         | 9/115\n[00:48<01:43,  1.02it/s]', '\\rOverwrite epoch 2/4:   9%|8         | 10/115\n[00:49<01:14,  1.40it/s]', '\\rOverwrite epoch 2/4:  10%|9         | 11/115\n[00:49<00:55,  1.89it/s]', '\\rOverwrite epoch 2/4:  10%|#         | 12/115\n[00:49<00:41,  2.48it/s]', '\\rOverwrite epoch 2/4:  11%|#1        | 13/115\n[00:49<00:32,  3.16it/s]', '\\rOverwrite epoch 2/4:  12%|#2        | 14/115\n[00:49<00:25,  3.91it/s]', '\\rOverwrite epoch 2/4:  13%|#3        | 15/115\n[00:49<00:21,  4.68it/s]', '\\rOverwrite epoch 2/4:  14%|#3        | 16/115\n[00:49<00:18,  5.43it/s]', '\\rOverwrite epoch 2/4:  15%|#4        | 17/115\n[00:49<00:16,  6.12it/s]', '\\rOverwrite epoch 2/4:  16%|#5        | 18/115\n[00:49<00:14,  6.71it/s]', '\\rOverwrite epoch 2/4:  17%|#6        | 19/115\n[00:50<00:13,  7.19it/s]', '\\rOverwrite epoch 2/4:  17%|#7        | 20/115\n[00:50<00:12,  7.57it/s]', '\\rOverwrite epoch 2/4:  18%|#8        | 21/115\n[00:50<00:11,  7.86it/s]', '\\rOverwrite epoch 2/4:  19%|#9        | 22/115\n[00:50<00:11,  8.08it/s]', '\\rOverwrite epoch 2/4:  20%|##        | 23/115\n[00:50<00:11,  8.24it/s]', '\\rOverwrite epoch 2/4:  21%|##        | 24/115\n[00:50<00:10,  8.35it/s]', '\\rOverwrite epoch 2/4:  22%|##1       | 25/115\n[00:50<00:10,  8.44it/s]', '\\rOverwrite epoch 2/4:  23%|##2       | 26/115\n[00:50<00:10,  8.50it/s]', '\\rOverwrite epoch 2/4:  23%|##3       | 27/115\n[00:50<00:10,  8.54it/s]', '\\rOverwrite epoch 2/4:  24%|##4       | 28/115\n[00:51<00:10,  8.57it/s]', '\\rOverwrite epoch 2/4:  25%|##5       | 29/115\n[00:51<00:10,  8.59it/s]', '\\rOverwrite epoch 2/4:  26%|##6       | 30/115\n[00:51<00:09,  8.60it/s]', '\\rOverwrite epoch 2/4:  27%|##6       | 31/115\n[00:51<00:09,  8.62it/s]', '\\rOverwrite epoch 2/4:  28%|##7       | 32/115\n[00:51<00:09,  8.63it/s]', '\\rOverwrite epoch 2/4:  29%|##8       | 33/115\n[00:51<00:09,  8.63it/s]', '\\rOverwrite epoch 2/4:  30%|##9       | 34/115\n[00:51<00:09,  8.63it/s]', '\\rOverwrite epoch 2/4:  30%|###       | 35/115\n[00:51<00:09,  8.64it/s]', '\\rOverwrite epoch 2/4:  31%|###1      | 36/115\n[00:52<00:09,  8.64it/s]', '\\rOverwrite epoch 2/4:  32%|###2      | 37/115\n[00:52<00:09,  8.64it/s]', '\\rOverwrite epoch 2/4:  33%|###3      | 38/115\n[00:52<00:08,  8.64it/s]', '\\rOverwrite epoch 2/4:  34%|###3      | 39/115\n[00:52<00:08,  8.64it/s]', '\\rOverwrite epoch 2/4:  35%|###4      | 40/115\n[00:52<00:08,  8.65it/s]', '\\rOverwrite epoch 2/4:  36%|###5      | 41/115\n[00:52<00:08,  8.66it/s]', '\\rOverwrite epoch 2/4:  37%|###6      | 42/115\n[00:52<00:08,  8.66it/s]', '\\rOverwrite epoch 2/4:  37%|###7      | 43/115\n[00:52<00:08,  8.65it/s]', '\\rOverwrite epoch 2/4:  38%|###8      | 44/115\n[00:52<00:08,  8.64it/s]', '\\rOverwrite epoch 2/4:  39%|###9      | 45/115\n[00:53<00:08,  8.64it/s]', '\\rOverwrite epoch 2/4:  40%|####      | 46/115\n[00:53<00:07,  8.64it/s]', '\\rOverwrite epoch 2/4:  41%|####      | 47/115\n[00:53<00:07,  8.64it/s]', '\\rOverwrite epoch 2/4:  42%|####1     | 48/115\n[00:53<00:07,  8.64it/s]', '\\rOverwrite epoch 2/4:  43%|####2     | 49/115\n[00:53<00:07,  8.64it/s]', '\\rOverwrite epoch 2/4:  43%|####3     | 50/115\n[00:53<00:07,  8.65it/s]', '\\rOverwrite epoch 2/4:  44%|####4     | 51/115\n[00:53<00:07,  8.65it/s]', '\\rOverwrite epoch 2/4:  45%|####5     | 52/115\n[00:53<00:07,  8.65it/s]', '\\rOverwrite epoch 2/4:  46%|####6     | 53/115\n[00:54<00:07,  8.65it/s]', '\\rOverwrite epoch 2/4:  47%|####6     | 54/115\n[00:54<00:07,  8.65it/s]', '\\rOverwrite epoch 2/4:  48%|####7     | 55/115\n[00:54<00:06,  8.65it/s]', '\\rOverwrite epoch 2/4:  49%|####8     | 56/115\n[00:54<00:06,  8.65it/s]', '\\rOverwrite epoch 2/4:  50%|####9     | 57/115\n[00:54<00:06,  8.65it/s]', '\\rOverwrite epoch 2/4:  50%|#####     | 58/115\n[00:54<00:06,  8.65it/s]', '\\rOverwrite epoch 2/4:  51%|#####1    | 59/115\n[00:54<00:06,  8.65it/s]', '\\rOverwrite epoch 2/4:  52%|#####2    | 60/115\n[00:54<00:06,  8.64it/s]', '\\rOverwrite epoch 2/4:  53%|#####3    | 61/115\n[00:54<00:06,  8.63it/s]', '\\rOverwrite epoch 2/4:  54%|#####3    | 62/115\n[00:55<00:06,  8.63it/s]', '\\rOverwrite epoch 2/4:  55%|#####4    | 63/115\n[00:55<00:06,  8.64it/s]', '\\rOverwrite epoch 2/4:  56%|#####5    | 64/115\n[00:55<00:05,  8.64it/s]', '\\rOverwrite epoch 2/4:  57%|#####6    | 65/115\n[00:55<00:05,  8.64it/s]', '\\rOverwrite epoch 2/4:  57%|#####7    | 66/115\n[00:55<00:05,  8.64it/s]', '\\rOverwrite epoch 2/4:  58%|#####8    | 67/115\n[00:55<00:05,  8.65it/s]', '\\rOverwrite epoch 2/4:  59%|#####9    | 68/115\n[00:55<00:05,  8.65it/s]', '\\rOverwrite epoch 2/4:  60%|######    | 69/115\n[00:55<00:05,  8.65it/s]', '\\rOverwrite epoch 2/4:  61%|######    | 70/115\n[00:55<00:05,  8.64it/s]', '\\rOverwrite epoch 2/4:  62%|######1   | 71/115\n[00:56<00:05,  8.64it/s]', '\\rOverwrite epoch 2/4:  63%|######2   | 72/115\n[00:56<00:04,  8.64it/s]', '\\rOverwrite epoch 2/4:  63%|######3   | 73/115\n[00:56<00:04,  8.64it/s]', '\\rOverwrite epoch 2/4:  64%|######4   | 74/115\n[00:56<00:04,  8.65it/s]', '\\rOverwrite epoch 2/4:  65%|######5   | 75/115\n[00:56<00:04,  8.64it/s]', '\\rOverwrite epoch 2/4:  66%|######6   | 76/115\n[00:56<00:04,  8.64it/s]', '\\rOverwrite epoch 2/4:  67%|######6   | 77/115\n[00:56<00:04,  8.64it/s]', '\\rOverwrite epoch 2/4:  68%|######7   | 78/115\n[00:56<00:04,  8.65it/s]', '\\rOverwrite epoch 2/4:  69%|######8   | 79/115\n[00:57<00:04,  8.64it/s]', '\\rOverwrite epoch 2/4:  70%|######9   | 80/115\n[00:57<00:04,  8.64it/s]', '\\rOverwrite epoch 2/4:  70%|#######   | 81/115\n[00:57<00:03,  8.65it/s]', '\\rOverwrite epoch 2/4:  71%|#######1  | 82/115\n[00:57<00:03,  8.65it/s]', '\\rOverwrite epoch 2/4:  72%|#######2  | 83/115\n[00:57<00:03,  8.64it/s]', '\\rOverwrite epoch 2/4:  73%|#######3  | 84/115\n[00:57<00:03,  8.65it/s]', '[2025-12-03 17:44:21] Overwrite step 200:\navg_train_loss=3.4555', '\\n', '\\rOverwrite epoch 2/4:  74%|#######3  | 85/115\n[00:57<00:03,  8.64it/s]', '\\rOverwrite epoch 2/4:  75%|#######4  | 86/115\n[00:57<00:03,  8.64it/s]', '\\rOverwrite epoch 2/4:  76%|#######5  | 87/115\n[00:57<00:03,  8.63it/s]', '\\rOverwrite epoch 2/4:  77%|#######6  | 88/115\n[00:58<00:03,  8.63it/s]', '\\rOverwrite epoch 2/4:  77%|#######7  | 89/115\n[00:58<00:03,  8.62it/s]', '\\rOverwrite epoch 2/4:  78%|#######8  | 90/115\n[00:58<00:02,  8.63it/s]', '\\rOverwrite epoch 2/4:  79%|#######9  | 91/115\n[00:58<00:02,  8.64it/s]', '\\rOverwrite epoch 2/4:  80%|########  | 92/115\n[00:58<00:02,  8.65it/s]', '\\rOverwrite epoch 2/4:  81%|########  | 93/115\n[00:58<00:02,  8.65it/s]', '\\rOverwrite epoch 2/4:  82%|########1 | 94/115\n[00:58<00:02,  8.65it/s]', '\\rOverwrite epoch 2/4:  83%|########2 | 95/115\n[00:58<00:02,  8.65it/s]', '\\rOverwrite epoch 2/4:  83%|########3 | 96/115\n[00:58<00:02,  8.65it/s]', '\\rOverwrite epoch 2/4:  84%|########4 | 97/115\n[00:59<00:02,  8.64it/s]', '\\rOverwrite epoch 2/4:  85%|########5 | 98/115\n[00:59<00:01,  8.63it/s]', '\\rOverwrite epoch 2/4:  86%|########6 | 99/115\n[00:59<00:01,  8.64it/s]', '\\rOverwrite epoch 2/4:  87%|########6 | 100/115\n[00:59<00:01,  8.65it/s]', '\\rOverwrite epoch 2/4:  88%|########7 | 101/115\n[00:59<00:01,  8.66it/s]', '\\rOverwrite epoch 2/4:  89%|########8 | 102/115\n[00:59<00:01,  8.65it/s]', '\\rOverwrite epoch 2/4:  90%|########9 | 103/115\n[00:59<00:01,  8.65it/s]', '\\rOverwrite epoch 2/4:  90%|######### | 104/115\n[00:59<00:01,  8.65it/s]', '\\rOverwrite epoch 2/4:  91%|#########1| 105/115\n[01:00<00:01,  8.65it/s]', '\\rOverwrite epoch 2/4:  92%|#########2| 106/115\n[01:00<00:01,  8.64it/s]', '\\rOverwrite epoch 2/4:  93%|#########3| 107/115\n[01:00<00:00,  8.64it/s]', '\\rOverwrite epoch 2/4:  94%|#########3| 108/115\n[01:00<00:00,  8.64it/s]', '\\rOverwrite epoch 2/4:  95%|#########4| 109/115\n[01:00<00:00,  8.64it/s]', '\\rOverwrite epoch 2/4:  96%|#########5| 110/115\n[01:00<00:00,  8.65it/s]', '\\rOverwrite epoch 2/4:  97%|#########6| 111/115\n[01:00<00:00,  8.64it/s]', '\\rOverwrite epoch 2/4:  97%|#########7| 112/115\n[01:00<00:00,  8.64it/s]', '\\rOverwrite epoch 2/4:  98%|#########8| 113/115\n[01:00<00:00,  8.64it/s]', '\\rOverwrite epoch 2/4:  99%|#########9| 114/115\n[01:01<00:00,  8.65it/s]', '', '\\rOverwrite epoch 2/4: 100%|##########| 115/115\n[01:02<00:00,  1.85it/s]', '\\n', 'Epoch 2: validation_loss = 3.6212', '\\n',\n'\\rOverwrite epoch 3/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 3/4:   1%|          | 1/115 [00:48<1:32:42, 48.80s/it]', '\\rOverwrite\nepoch 3/4:   2%|1         | 2/115 [00:48<37:58, 20.16s/it]  ', '\\rOverwrite\nepoch 3/4:   3%|2         | 3/115 [00:49<20:32, 11.01s/it]', '\\rOverwrite epoch\n3/4:   3%|3         | 4/115 [00:49<12:24,  6.71s/it]', '\\rOverwrite epoch 3/4:\n4%|4         | 5/115 [00:49<07:56,  4.33s/it]', '\\rOverwrite epoch 3/4:   5%|5\n| 6/115 [00:49<05:15,  2.90s/it]', '\\rOverwrite epoch 3/4:   6%|6         |\n7/115 [00:49<03:34,  1.99s/it]', '\\rOverwrite epoch 3/4:   7%|6         | 8/115\n[00:49<02:28,  1.39s/it]', '\\rOverwrite epoch 3/4:   8%|7         | 9/115\n[00:49<01:45,  1.01it/s]', '\\rOverwrite epoch 3/4:   9%|8         | 10/115\n[00:49<01:15,  1.38it/s]', '\\rOverwrite epoch 3/4:  10%|9         | 11/115\n[00:49<00:55,  1.86it/s]', '\\rOverwrite epoch 3/4:  10%|#         | 12/115\n[00:50<00:42,  2.45it/s]', '\\rOverwrite epoch 3/4:  11%|#1        | 13/115\n[00:50<00:32,  3.13it/s]', '\\rOverwrite epoch 3/4:  12%|#2        | 14/115\n[00:50<00:26,  3.88it/s]', '\\rOverwrite epoch 3/4:  13%|#3        | 15/115\n[00:50<00:21,  4.65it/s]', '\\rOverwrite epoch 3/4:  14%|#3        | 16/115\n[00:50<00:18,  5.40it/s]', '\\rOverwrite epoch 3/4:  15%|#4        | 17/115\n[00:50<00:16,  6.09it/s]', '\\rOverwrite epoch 3/4:  16%|#5        | 18/115\n[00:50<00:14,  6.68it/s]', '\\rOverwrite epoch 3/4:  17%|#6        | 19/115\n[00:50<00:13,  7.15it/s]', '\\rOverwrite epoch 3/4:  17%|#7        | 20/115\n[00:50<00:12,  7.54it/s]', '\\rOverwrite epoch 3/4:  18%|#8        | 21/115\n[00:51<00:11,  7.85it/s]', '\\rOverwrite epoch 3/4:  19%|#9        | 22/115\n[00:51<00:11,  8.07it/s]', '\\rOverwrite epoch 3/4:  20%|##        | 23/115\n[00:51<00:11,  8.23it/s]', '\\rOverwrite epoch 3/4:  21%|##        | 24/115\n[00:51<00:10,  8.35it/s]', '\\rOverwrite epoch 3/4:  22%|##1       | 25/115\n[00:51<00:10,  8.44it/s]', '\\rOverwrite epoch 3/4:  23%|##2       | 26/115\n[00:51<00:10,  8.50it/s]', '\\rOverwrite epoch 3/4:  23%|##3       | 27/115\n[00:51<00:10,  8.53it/s]', '\\rOverwrite epoch 3/4:  24%|##4       | 28/115\n[00:51<00:10,  8.57it/s]', '\\rOverwrite epoch 3/4:  25%|##5       | 29/115\n[00:52<00:10,  8.55it/s]', '\\rOverwrite epoch 3/4:  26%|##6       | 30/115\n[00:52<00:09,  8.59it/s]', '\\rOverwrite epoch 3/4:  27%|##6       | 31/115\n[00:52<00:09,  8.60it/s]', '\\rOverwrite epoch 3/4:  28%|##7       | 32/115\n[00:52<00:09,  8.62it/s]', '\\rOverwrite epoch 3/4:  29%|##8       | 33/115\n[00:52<00:09,  8.63it/s]', '\\rOverwrite epoch 3/4:  30%|##9       | 34/115\n[00:52<00:09,  8.58it/s]', '\\rOverwrite epoch 3/4:  30%|###       | 35/115\n[00:52<00:09,  8.57it/s]', '\\rOverwrite epoch 3/4:  31%|###1      | 36/115\n[00:52<00:09,  8.59it/s]', '\\rOverwrite epoch 3/4:  32%|###2      | 37/115\n[00:52<00:09,  8.61it/s]', '\\rOverwrite epoch 3/4:  33%|###3      | 38/115\n[00:53<00:08,  8.63it/s]', '\\rOverwrite epoch 3/4:  34%|###3      | 39/115\n[00:53<00:08,  8.63it/s]', '\\rOverwrite epoch 3/4:  35%|###4      | 40/115\n[00:53<00:08,  8.63it/s]', '\\rOverwrite epoch 3/4:  36%|###5      | 41/115\n[00:53<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  37%|###6      | 42/115\n[00:53<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  37%|###7      | 43/115\n[00:53<00:08,  8.64it/s]', '\\rOverwrite epoch 3/4:  38%|###8      | 44/115\n[00:53<00:08,  8.65it/s]', '\\rOverwrite epoch 3/4:  39%|###9      | 45/115\n[00:53<00:08,  8.65it/s]', '\\rOverwrite epoch 3/4:  40%|####      | 46/115\n[00:54<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  41%|####      | 47/115\n[00:54<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  42%|####1     | 48/115\n[00:54<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  43%|####2     | 49/115\n[00:54<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  43%|####3     | 50/115\n[00:54<00:07,  8.66it/s]', '\\rOverwrite epoch 3/4:  44%|####4     | 51/115\n[00:54<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  45%|####5     | 52/115\n[00:54<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  46%|####6     | 53/115\n[00:54<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  47%|####6     | 54/115\n[00:54<00:07,  8.65it/s]', '\\rOverwrite epoch 3/4:  48%|####7     | 55/115\n[00:55<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  49%|####8     | 56/115\n[00:55<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  50%|####9     | 57/115\n[00:55<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  50%|#####     | 58/115\n[00:55<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  51%|#####1    | 59/115\n[00:55<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  52%|#####2    | 60/115\n[00:55<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  53%|#####3    | 61/115\n[00:55<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  54%|#####3    | 62/115\n[00:55<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  55%|#####4    | 63/115\n[00:55<00:06,  8.65it/s]', '\\rOverwrite epoch 3/4:  56%|#####5    | 64/115\n[00:56<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  57%|#####6    | 65/115\n[00:56<00:05,  8.66it/s]', '\\rOverwrite epoch 3/4:  57%|#####7    | 66/115\n[00:56<00:05,  8.66it/s]', '\\rOverwrite epoch 3/4:  58%|#####8    | 67/115\n[00:56<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  59%|#####9    | 68/115\n[00:56<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  60%|######    | 69/115\n[00:56<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  61%|######    | 70/115\n[00:56<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  62%|######1   | 71/115\n[00:56<00:05,  8.65it/s]', '\\rOverwrite epoch 3/4:  63%|######2   | 72/115\n[00:57<00:04,  8.65it/s]', '\\rOverwrite epoch 3/4:  63%|######3   | 73/115\n[00:57<00:04,  8.65it/s]', '\\rOverwrite epoch 3/4:  64%|######4   | 74/115\n[00:57<00:04,  8.64it/s]', '\\rOverwrite epoch 3/4:  65%|######5   | 75/115\n[00:57<00:04,  8.64it/s]', '\\rOverwrite epoch 3/4:  66%|######6   | 76/115\n[00:57<00:04,  8.64it/s]', '\\rOverwrite epoch 3/4:  67%|######6   | 77/115\n[00:57<00:04,  8.64it/s]', '\\rOverwrite epoch 3/4:  68%|######7   | 78/115\n[00:57<00:04,  8.64it/s]', '\\rOverwrite epoch 3/4:  69%|######8   | 79/115\n[00:57<00:04,  8.64it/s]', '\\rOverwrite epoch 3/4:  70%|######9   | 80/115\n[00:57<00:04,  8.65it/s]', '\\rOverwrite epoch 3/4:  70%|#######   | 81/115\n[00:58<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  71%|#######1  | 82/115\n[00:58<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  72%|#######2  | 83/115\n[00:58<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  73%|#######3  | 84/115\n[00:58<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  74%|#######3  | 85/115\n[00:58<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  75%|#######4  | 86/115\n[00:58<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  76%|#######5  | 87/115\n[00:58<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  77%|#######6  | 88/115\n[00:58<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  77%|#######7  | 89/115\n[00:58<00:03,  8.65it/s]', '\\rOverwrite epoch 3/4:  78%|#######8  | 90/115\n[00:59<00:02,  8.65it/s]', '\\rOverwrite epoch 3/4:  79%|#######9  | 91/115\n[00:59<00:02,  8.65it/s]', '\\rOverwrite epoch 3/4:  80%|########  | 92/115\n[00:59<00:02,  8.65it/s]', '\\rOverwrite epoch 3/4:  81%|########  | 93/115\n[00:59<00:02,  8.65it/s]', '\\rOverwrite epoch 3/4:  82%|########1 | 94/115\n[00:59<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  83%|########2 | 95/115\n[00:59<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  83%|########3 | 96/115\n[00:59<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  84%|########4 | 97/115\n[00:59<00:02,  8.64it/s]', '\\rOverwrite epoch 3/4:  85%|########5 | 98/115\n[01:00<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  86%|########6 | 99/115\n[01:00<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  87%|########6 | 100/115\n[01:00<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  88%|########7 | 101/115\n[01:00<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  89%|########8 | 102/115\n[01:00<00:01,  8.64it/s]', '\\rOverwrite epoch 3/4:  90%|########9 | 103/115\n[01:00<00:01,  8.65it/s]', '\\rOverwrite epoch 3/4:  90%|######### | 104/115\n[01:00<00:01,  8.65it/s]', '\\rOverwrite epoch 3/4:  91%|#########1| 105/115\n[01:00<00:01,  8.65it/s]', '\\rOverwrite epoch 3/4:  92%|#########2| 106/115\n[01:00<00:01,  8.62it/s]', '\\rOverwrite epoch 3/4:  93%|#########3| 107/115\n[01:01<00:00,  8.63it/s]', '\\rOverwrite epoch 3/4:  94%|#########3| 108/115\n[01:01<00:00,  8.63it/s]', '\\rOverwrite epoch 3/4:  95%|#########4| 109/115\n[01:01<00:00,  8.64it/s]', '\\rOverwrite epoch 3/4:  96%|#########5| 110/115\n[01:01<00:00,  8.64it/s]', '\\rOverwrite epoch 3/4:  97%|#########6| 111/115\n[01:01<00:00,  8.62it/s]', '\\rOverwrite epoch 3/4:  97%|#########7| 112/115\n[01:01<00:00,  8.63it/s]', '\\rOverwrite epoch 3/4:  98%|#########8| 113/115\n[01:01<00:00,  8.64it/s]', '\\rOverwrite epoch 3/4:  99%|#########9| 114/115\n[01:01<00:00,  8.64it/s]', '', '\\rOverwrite epoch 3/4: 100%|##########| 115/115\n[01:03<00:00,  1.82it/s]', '\\n', 'Epoch 3: validation_loss = 3.6433', '\\n',\n'\\rOverwrite epoch 4/4:   0%|          | 0/115 [00:00<?, ?it/s]', '\\rOverwrite\nepoch 4/4:   1%|          | 1/115 [01:24<2:41:22, 84.93s/it]', '\\rOverwrite\nepoch 4/4:   2%|1         | 2/115 [01:25<1:05:59, 35.04s/it]', '\\rOverwrite\nepoch 4/4:   3%|2         | 3/115 [01:25<35:38, 19.09s/it]  ', '\\rOverwrite\nepoch 4/4:   3%|3         | 4/115 [01:25<21:27, 11.60s/it]', '\\rOverwrite epoch\n4/4:   4%|4         | 5/115 [01:25<13:40,  7.46s/it]', '\\rOverwrite epoch 4/4:\n5%|5         | 6/115 [01:25<09:00,  4.96s/it]', '\\rOverwrite epoch 4/4:   6%|6\n| 7/115 [01:25<06:04,  3.38s/it]', '\\rOverwrite epoch 4/4:   7%|6         |\n8/115 [01:25<04:10,  2.34s/it]', '\\rOverwrite epoch 4/4:   8%|7         | 9/115\n[01:25<02:54,  1.64s/it]', '\\rOverwrite epoch 4/4:   9%|8         | 10/115\n[01:25<02:03,  1.17s/it]', '\\rOverwrite epoch 4/4:  10%|9         | 11/115\n[01:26<01:28,  1.18it/s]', '\\rOverwrite epoch 4/4:  10%|#         | 12/115\n[01:26<01:04,  1.60it/s]', '\\rOverwrite epoch 4/4:  11%|#1        | 13/115\n[01:26<00:48,  2.12it/s]', '\\rOverwrite epoch 4/4:  12%|#2        | 14/115\n[01:26<00:36,  2.75it/s]', '\\rOverwrite epoch 4/4:  13%|#3        | 15/115\n[01:26<00:28,  3.46it/s]', '\\rOverwrite epoch 4/4:  14%|#3        | 16/115\n[01:26<00:23,  4.23it/s]', '\\rOverwrite epoch 4/4:  15%|#4        | 17/115\n[01:26<00:19,  4.99it/s]', '\\rOverwrite epoch 4/4:  16%|#5        | 18/115\n[01:26<00:16,  5.72it/s]', '\\rOverwrite epoch 4/4:  17%|#6        | 19/115\n[01:27<00:15,  6.37it/s]', '\\rOverwrite epoch 4/4:  17%|#7        | 20/115\n[01:27<00:13,  6.91it/s]', '\\rOverwrite epoch 4/4:  18%|#8        | 21/115\n[01:27<00:12,  7.35it/s]', '\\rOverwrite epoch 4/4:  19%|#9        | 22/115\n[01:27<00:12,  7.70it/s]', '\\rOverwrite epoch 4/4:  20%|##        | 23/115\n[01:27<00:11,  7.96it/s]', '\\rOverwrite epoch 4/4:  21%|##        | 24/115\n[01:27<00:11,  8.15it/s]', '\\rOverwrite epoch 4/4:  22%|##1       | 25/115\n[01:27<00:10,  8.29it/s]', '\\rOverwrite epoch 4/4:  23%|##2       | 26/115\n[01:27<00:10,  8.40it/s]', '\\rOverwrite epoch 4/4:  23%|##3       | 27/115\n[01:27<00:10,  8.47it/s]', '\\rOverwrite epoch 4/4:  24%|##4       | 28/115\n[01:28<00:10,  8.53it/s]', '\\rOverwrite epoch 4/4:  25%|##5       | 29/115\n[01:28<00:10,  8.56it/s]', '\\rOverwrite epoch 4/4:  26%|##6       | 30/115\n[01:28<00:09,  8.55it/s]', '\\rOverwrite epoch 4/4:  27%|##6       | 31/115\n[01:28<00:09,  8.57it/s]', '\\rOverwrite epoch 4/4:  28%|##7       | 32/115\n[01:28<00:09,  8.59it/s]', '\\rOverwrite epoch 4/4:  29%|##8       | 33/115\n[01:28<00:09,  8.61it/s]', '\\rOverwrite epoch 4/4:  30%|##9       | 34/115\n[01:28<00:09,  8.62it/s]', '\\rOverwrite epoch 4/4:  30%|###       | 35/115\n[01:28<00:09,  8.63it/s]', '\\rOverwrite epoch 4/4:  31%|###1      | 36/115\n[01:28<00:09,  8.63it/s]', '\\rOverwrite epoch 4/4:  32%|###2      | 37/115\n[01:29<00:09,  8.59it/s]', '\\rOverwrite epoch 4/4:  33%|###3      | 38/115\n[01:29<00:08,  8.61it/s]', '\\rOverwrite epoch 4/4:  34%|###3      | 39/115\n[01:29<00:08,  8.61it/s]', '\\rOverwrite epoch 4/4:  35%|###4      | 40/115\n[01:29<00:08,  8.62it/s]', '\\rOverwrite epoch 4/4:  36%|###5      | 41/115\n[01:29<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  37%|###6      | 42/115\n[01:29<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  37%|###7      | 43/115\n[01:29<00:08,  8.64it/s]', '\\rOverwrite epoch 4/4:  38%|###8      | 44/115\n[01:29<00:08,  8.63it/s]', '\\rOverwrite epoch 4/4:  39%|###9      | 45/115\n[01:30<00:08,  8.64it/s]', '\\rOverwrite epoch 4/4:  40%|####      | 46/115\n[01:30<00:07,  8.63it/s]', '\\rOverwrite epoch 4/4:  41%|####      | 47/115\n[01:30<00:07,  8.62it/s]', '\\rOverwrite epoch 4/4:  42%|####1     | 48/115\n[01:30<00:07,  8.62it/s]', '\\rOverwrite epoch 4/4:  43%|####2     | 49/115\n[01:30<00:07,  8.62it/s]', '\\rOverwrite epoch 4/4:  43%|####3     | 50/115\n[01:30<00:07,  8.61it/s]', '\\rOverwrite epoch 4/4:  44%|####4     | 51/115\n[01:30<00:07,  8.57it/s]', '\\rOverwrite epoch 4/4:  45%|####5     | 52/115\n[01:30<00:07,  8.58it/s]', '\\rOverwrite epoch 4/4:  46%|####6     | 53/115\n[01:30<00:07,  8.59it/s]', '\\rOverwrite epoch 4/4:  47%|####6     | 54/115\n[01:31<00:07,  8.61it/s]', '[2025-12-03 17:48:53] Overwrite step 400:\navg_train_loss=3.1237', '\\n', '\\rOverwrite epoch 4/4:  48%|####7     | 55/115\n[01:31<00:06,  8.61it/s]', '\\rOverwrite epoch 4/4:  49%|####8     | 56/115\n[01:31<00:06,  8.60it/s]', '\\rOverwrite epoch 4/4:  50%|####9     | 57/115\n[01:31<00:06,  8.61it/s]', '\\rOverwrite epoch 4/4:  50%|#####     | 58/115\n[01:31<00:06,  8.62it/s]', '\\rOverwrite epoch 4/4:  51%|#####1    | 59/115\n[01:31<00:06,  8.59it/s]', '\\rOverwrite epoch 4/4:  52%|#####2    | 60/115\n[01:31<00:06,  8.61it/s]', '\\rOverwrite epoch 4/4:  53%|#####3    | 61/115\n[01:31<00:06,  8.61it/s]', '\\rOverwrite epoch 4/4:  54%|#####3    | 62/115\n[01:32<00:06,  8.62it/s]', '\\rOverwrite epoch 4/4:  55%|#####4    | 63/115\n[01:32<00:06,  8.62it/s]', '\\rOverwrite epoch 4/4:  56%|#####5    | 64/115\n[01:32<00:05,  8.62it/s]', '\\rOverwrite epoch 4/4:  57%|#####6    | 65/115\n[01:32<00:05,  8.63it/s]', '\\rOverwrite epoch 4/4:  57%|#####7    | 66/115\n[01:32<00:05,  8.60it/s]', '\\rOverwrite epoch 4/4:  58%|#####8    | 67/115\n[01:32<00:05,  8.59it/s]', '\\rOverwrite epoch 4/4:  59%|#####9    | 68/115\n[01:32<00:05,  8.60it/s]', '\\rOverwrite epoch 4/4:  60%|######    | 69/115\n[01:32<00:05,  8.60it/s]', '\\rOverwrite epoch 4/4:  61%|######    | 70/115\n[01:32<00:05,  8.61it/s]', '\\rOverwrite epoch 4/4:  62%|######1   | 71/115\n[01:33<00:05,  8.59it/s]', '\\rOverwrite epoch 4/4:  63%|######2   | 72/115\n[01:33<00:05,  8.59it/s]', '\\rOverwrite epoch 4/4:  63%|######3   | 73/115\n[01:33<00:04,  8.61it/s]', '\\rOverwrite epoch 4/4:  64%|######4   | 74/115\n[01:33<00:04,  8.61it/s]', '\\rOverwrite epoch 4/4:  65%|######5   | 75/115\n[01:33<00:04,  8.61it/s]', '\\rOverwrite epoch 4/4:  66%|######6   | 76/115\n[01:33<00:04,  8.62it/s]', '\\rOverwrite epoch 4/4:  67%|######6   | 77/115\n[01:33<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  68%|######7   | 78/115\n[01:33<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  69%|######8   | 79/115\n[01:33<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  70%|######9   | 80/115\n[01:34<00:04,  8.63it/s]', '\\rOverwrite epoch 4/4:  70%|#######   | 81/115\n[01:34<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  71%|#######1  | 82/115\n[01:34<00:03,  8.64it/s]', '\\rOverwrite epoch 4/4:  72%|#######2  | 83/115\n[01:34<00:03,  8.64it/s]', '\\rOverwrite epoch 4/4:  73%|#######3  | 84/115\n[01:34<00:03,  8.64it/s]', '\\rOverwrite epoch 4/4:  74%|#######3  | 85/115\n[01:34<00:03,  8.64it/s]', '\\rOverwrite epoch 4/4:  75%|#######4  | 86/115\n[01:34<00:03,  8.63it/s]', '\\rOverwrite epoch 4/4:  76%|#######5  | 87/115\n[01:34<00:03,  8.62it/s]', '\\rOverwrite epoch 4/4:  77%|#######6  | 88/115\n[01:35<00:03,  8.60it/s]', '\\rOverwrite epoch 4/4:  77%|#######7  | 89/115\n[01:35<00:03,  8.60it/s]', '\\rOverwrite epoch 4/4:  78%|#######8  | 90/115\n[01:35<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  79%|#######9  | 91/115\n[01:35<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  80%|########  | 92/115\n[01:35<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  81%|########  | 93/115\n[01:35<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  82%|########1 | 94/115\n[01:35<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  83%|########2 | 95/115\n[01:35<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  83%|########3 | 96/115\n[01:35<00:02,  8.62it/s]', '\\rOverwrite epoch 4/4:  84%|########4 | 97/115\n[01:36<00:02,  8.61it/s]', '\\rOverwrite epoch 4/4:  85%|########5 | 98/115\n[01:36<00:01,  8.61it/s]', '\\rOverwrite epoch 4/4:  86%|########6 | 99/115\n[01:36<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  87%|########6 | 100/115\n[01:36<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  88%|########7 | 101/115\n[01:36<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  89%|########8 | 102/115\n[01:36<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  90%|########9 | 103/115\n[01:36<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  90%|######### | 104/115\n[01:36<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  91%|#########1| 105/115\n[01:36<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  92%|#########2| 106/115\n[01:37<00:01,  8.62it/s]', '\\rOverwrite epoch 4/4:  93%|#########3| 107/115\n[01:37<00:00,  8.61it/s]', '\\rOverwrite epoch 4/4:  94%|#########3| 108/115\n[01:37<00:00,  8.61it/s]', '\\rOverwrite epoch 4/4:  95%|#########4| 109/115\n[01:37<00:00,  8.61it/s]', '\\rOverwrite epoch 4/4:  96%|#########5| 110/115\n[01:37<00:00,  8.61it/s]', '\\rOverwrite epoch 4/4:  97%|#########6| 111/115\n[01:37<00:00,  8.60it/s]', '\\rOverwrite epoch 4/4:  97%|#########7| 112/115\n[01:37<00:00,  8.61it/s]', '\\rOverwrite epoch 4/4:  98%|#########8| 113/115\n[01:37<00:00,  8.62it/s]', '\\rOverwrite epoch 4/4:  99%|#########9| 114/115\n[01:38<00:00,  8.62it/s]', '', '\\rOverwrite epoch 4/4: 100%|##########| 115/115\n[01:39<00:00,  1.16it/s]', '\\n', 'Epoch 4: validation_loss = 3.6572', '\\n',\n'Experiment complete. Artifacts saved to:', ' ', '/workspace/AE-\nScientist/research_pipeline/workspaces/0-run/process_SpawnProcess-2/working',\n'\\n', 'Execution time: 11 minutes seconds (time limit is 2 hours).']"], "analysis": ["", ""], "exc_type": [null, null], "exc_info": [null, null], "exc_stack": [null, null], "exp_name": "0-run", "metrics": [{"metric_names": [{"metric_name": "train avg loss", "lower_is_better": true, "description": "Average training loss; lower indicates better fit.", "data": [{"dataset_name": "synthetic_injection", "final_value": 0.0, "best_value": 3.085596}, {"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 3.121017}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss on the validation set; lower is better.", "data": [{"dataset_name": "synthetic_injection", "final_value": 0.0, "best_value": 3.226789}, {"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 3.621235}]}, {"metric_name": "validation rare_recall@50", "lower_is_better": false, "description": "Recall@50 for rare items/categories on the validation set; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation common_recall@50", "lower_is_better": false, "description": "Recall@50 for common items/categories on the validation set; higher is better.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.2}]}, {"metric_name": "validation RCRG@50", "lower_is_better": true, "description": "Rare/Common Recall Gap at top-50 on validation; smaller gap is better (lower is better).", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}]}, {"metric_names": [{"metric_name": "train avg loss", "lower_is_better": true, "description": "Average training loss per batch/step over the epoch.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.085596, "best_value": 3.085596}, {"dataset_name": "overwrite_wikitext", "final_value": 3.121017, "best_value": 3.121017}]}, {"metric_name": "validation loss", "lower_is_better": true, "description": "Loss computed on the validation set.", "data": [{"dataset_name": "synthetic_injection", "final_value": 3.226789, "best_value": 3.226789}, {"dataset_name": "overwrite_wikitext", "final_value": 3.621235, "best_value": 3.621235}]}, {"metric_name": "validation common_recall@50", "lower_is_better": false, "description": "Recall at 50 for common items on the validation set.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.2, "best_value": 0.2}]}, {"metric_name": "validation RCRG@50", "lower_is_better": false, "description": "RCRG at 50 on the validation set (higher is better).", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}, {"metric_name": "validation rare_recall@50", "lower_is_better": false, "description": "Recall at 50 for rare items on the validation set.", "data": [{"dataset_name": "overwrite_wikitext", "final_value": 0.0, "best_value": 0.0}]}]}], "is_best_node": [true, false], "plots": [[], []], "plot_paths": [[], []], "plot_analyses": [[], []], "vlm_feedback_summary": ["[]", "[]"], "exec_time": [676.835667848587, 690.9208271503448], "exec_time_feedback": ["", ""], "datasets_successfully_tested": [["synthetic_injection", "overwrite_wikitext"], ["synthetic_injection", "overwrite_wikitext"]], "plot_code": [null, null], "plot_plan": [null, null], "ablation_name": [null, null], "hyperparam_name": [null, null], "is_seed_node": [false, true], "is_seed_agg_node": [false, false], "parse_metrics_plan": ["Load experiment_data.npy from the working directory. Parse the metrics structure\nfor each dataset and split (train/val). For each metric key, compute the best\nvalue (min for losses, max for accuracy/recall/F1/RCRG) or fall back to the\nfinal value if direction is unknown. Print the dataset name, then print each\nmetric with clear labels like 'train loss' or 'validation RCRG@50' followed by\nthe selected value. Enforce GPU device index 0 when CUDA is available. Avoid any\nplotting and ensure the script runs on import without needing a main guard.", "Load experiment_data.npy from the working directory. Parse the metrics structure\nfor each dataset and split (train/val). For each metric key, compute the best\nvalue (min for losses, max for accuracy/recall/F1/RCRG) or fall back to the\nfinal value if direction is unknown. Print the dataset name, then print each\nmetric with clear labels like 'train loss' or 'validation RCRG@50' followed by\nthe selected value. Enforce GPU device index 0 when CUDA is available. Avoid any\nplotting and ensure the script runs on import without needing a main guard."], "parse_metrics_code": ["import os\nimport numpy as np\nimport torch\nfrom typing import Dict, List, Any, Tuple\n\n# Enforce device selection\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\n\n\ndef load_experiment_data() -> Dict[str, Dict[str, Any]]:\n    working_dir = os.path.join(os.getcwd(), 'working')\n    path = os.path.join(working_dir, 'experiment_data.npy')\n    exp = np.load(path, allow_pickle=True).item()\n    return exp\n\n\ndef metric_direction(metric_key: str) -> str:\n    # Returns 'min' for loss-like, 'max' for accuracy/recall/F1/RCRG-like, '' if unknown\n    k = metric_key.lower()\n    if 'loss' in k or 'perplex' in k:\n        return 'min'\n    if 'acc' in k or 'accuracy' in k or 'f1' in k or 'precision' in k or 'recall' in k or 'rcrg' in k:\n        return 'max'\n    return ''\n\n\ndef pretty_metric_name(split: str, key: str) -> str:\n    # Normalize labels like 'val_loss' -> 'validation loss', 'avg_loss' -> 'train avg loss', etc.\n    split_label = 'train' if split == 'train' else 'validation'\n    if key.startswith('val_'):\n        base = key[4:]\n    else:\n        base = key\n    # Keep original case for tokens like RCRG@50; otherwise replace underscores with spaces\n    if any(ch.isupper() for ch in base) or '@' in base:\n        metric_label = base\n    else:\n        metric_label = base.replace('_', ' ')\n    # Special case: if metric is exactly 'loss' and split is validation, show 'loss'\n    return f\"{split_label} {metric_label}\"\n\n\ndef best_or_final(values: List[float], direction: str) -> Tuple[float, str]:\n    if not values:\n        return float('nan'), 'final'\n    if direction == 'min':\n        return min(values), 'best'\n    if direction == 'max':\n        return max(values), 'best'\n    return values[-1], 'final'\n\n\ndef extract_and_print_metrics(dataset_name: str, data: Dict[str, Any]) -> None:\n    print(f\"Dataset: {dataset_name}\")\n\n    metrics: Dict[str, Dict[str, List[float]]] = {'train': {}, 'val': {}}\n\n    # Collect metrics from data['metrics'] if present\n    if 'metrics' in data:\n        for split in ('train', 'val'):\n            entries = data['metrics'].get(split, [])\n            # Aggregate all keys except bookkeeping\n            keys = set()\n            for e in entries:\n                for k in e.keys():\n                    if k not in ('epoch', 'ts') and isinstance(e[k], (int, float, np.floating)):\n                        keys.add(k)\n            for k in keys:\n                seq = []\n                for e in entries:\n                    if k in e and isinstance(e[k], (int, float, np.floating)):\n                        seq.append(float(e[k]))\n                if seq:\n                    metrics[split][k] = seq\n\n    # If no metrics found, fall back to data['losses']\n    if not any(metrics[sp] for sp in metrics):\n        if 'losses' in data:\n            for split in ('train', 'val'):\n                entries = data['losses'].get(split, [])\n                seq = [float(e['loss']) for e in entries if 'loss' in e]\n                if seq:\n                    metrics[split]['loss'] = seq\n\n    # Print best/final value for each metric per split\n    for split in ('train', 'val'):\n        for key, seq in metrics[split].items():\n            direction = metric_direction(key)\n            value, which = best_or_final(seq, direction)\n            label = pretty_metric_name(split, key)\n            # Print with 6 decimal places for consistency\n            print(f\"{label} ({which}): {value:.6f}\")\n\n\ndef main():\n    experiment_data = load_experiment_data()\n    for dataset_name, data in experiment_data.items():\n        extract_and_print_metrics(dataset_name, data)\n\n\n# Execute immediately when run\nmain()", "import os\nimport numpy as np\nimport torch\nfrom typing import Dict, List, Any, Tuple\n\n# Enforce device selection\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\n\n\ndef load_experiment_data() -> Dict[str, Dict[str, Any]]:\n    working_dir = os.path.join(os.getcwd(), 'working')\n    path = os.path.join(working_dir, 'experiment_data.npy')\n    exp = np.load(path, allow_pickle=True).item()\n    return exp\n\n\ndef metric_direction(metric_key: str) -> str:\n    # Returns 'min' for loss-like, 'max' for accuracy/recall/F1/RCRG-like, '' if unknown\n    k = metric_key.lower()\n    if 'loss' in k or 'perplex' in k:\n        return 'min'\n    if 'acc' in k or 'accuracy' in k or 'f1' in k or 'precision' in k or 'recall' in k or 'rcrg' in k:\n        return 'max'\n    return ''\n\n\ndef pretty_metric_name(split: str, key: str) -> str:\n    # Normalize labels like 'val_loss' -> 'validation loss', 'avg_loss' -> 'train avg loss', etc.\n    split_label = 'train' if split == 'train' else 'validation'\n    if key.startswith('val_'):\n        base = key[4:]\n    else:\n        base = key\n    # Keep original case for tokens like RCRG@50; otherwise replace underscores with spaces\n    if any(ch.isupper() for ch in base) or '@' in base:\n        metric_label = base\n    else:\n        metric_label = base.replace('_', ' ')\n    # Special case: if metric is exactly 'loss' and split is validation, show 'loss'\n    return f\"{split_label} {metric_label}\"\n\n\ndef best_or_final(values: List[float], direction: str) -> Tuple[float, str]:\n    if not values:\n        return float('nan'), 'final'\n    if direction == 'min':\n        return min(values), 'best'\n    if direction == 'max':\n        return max(values), 'best'\n    return values[-1], 'final'\n\n\ndef extract_and_print_metrics(dataset_name: str, data: Dict[str, Any]) -> None:\n    print(f\"Dataset: {dataset_name}\")\n\n    metrics: Dict[str, Dict[str, List[float]]] = {'train': {}, 'val': {}}\n\n    # Collect metrics from data['metrics'] if present\n    if 'metrics' in data:\n        for split in ('train', 'val'):\n            entries = data['metrics'].get(split, [])\n            # Aggregate all keys except bookkeeping\n            keys = set()\n            for e in entries:\n                for k in e.keys():\n                    if k not in ('epoch', 'ts') and isinstance(e[k], (int, float, np.floating)):\n                        keys.add(k)\n            for k in keys:\n                seq = []\n                for e in entries:\n                    if k in e and isinstance(e[k], (int, float, np.floating)):\n                        seq.append(float(e[k]))\n                if seq:\n                    metrics[split][k] = seq\n\n    # If no metrics found, fall back to data['losses']\n    if not any(metrics[sp] for sp in metrics):\n        if 'losses' in data:\n            for split in ('train', 'val'):\n                entries = data['losses'].get(split, [])\n                seq = [float(e['loss']) for e in entries if 'loss' in e]\n                if seq:\n                    metrics[split]['loss'] = seq\n\n    # Print best/final value for each metric per split\n    for split in ('train', 'val'):\n        for key, seq in metrics[split].items():\n            direction = metric_direction(key)\n            value, which = best_or_final(seq, direction)\n            label = pretty_metric_name(split, key)\n            # Print with 6 decimal places for consistency\n            print(f\"{label} ({which}): {value:.6f}\")\n\n\ndef main():\n    experiment_data = load_experiment_data()\n    for dataset_name, data in experiment_data.items():\n        extract_and_print_metrics(dataset_name, data)\n\n\n# Execute immediately when run\nmain()"], "parse_term_out": ["['Dataset: synthetic_injection', '\\n', 'train avg loss (best): 3.085596', '\\n',\n'validation loss (best): 3.226789', '\\n', 'Dataset: overwrite_wikitext', '\\n',\n'train avg loss (best): 3.121017', '\\n', 'validation loss (best): 3.621235',\n'\\n', 'validation rare_recall@50 (best): 0.000000', '\\n', 'validation\ncommon_recall@50 (best): 0.200000', '\\n', 'validation RCRG@50 (best): 0.000000',\n'\\n', 'Execution time: a moment seconds (time limit is 2 hours).']", "['Dataset: synthetic_injection', '\\n', 'train avg loss (best): 3.085596', '\\n',\n'validation loss (best): 3.226789', '\\n', 'Dataset: overwrite_wikitext', '\\n',\n'train avg loss (best): 3.121017', '\\n', 'validation common_recall@50 (best):\n0.200000', '\\n', 'validation loss (best): 3.621235', '\\n', 'validation RCRG@50\n(best): 0.000000', '\\n', 'validation rare_recall@50 (best): 0.000000', '\\n',\n'Execution time: a moment seconds (time limit is 2 hours).']"], "parse_exc_type": [null, null], "parse_exc_info": [null, null], "parse_exc_stack": [null, null], "completed_stages": ["Stage_1"]}